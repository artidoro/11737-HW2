2020-10-12 19:48:33 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_azerus_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='aze-eng,rus-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_azerus_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 19:48:33 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 19:48:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['aze', 'eng', 'rus']
2020-10-12 19:48:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | [aze] dictionary: 22075 types
2020-10-12 19:48:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 22075 types
2020-10-12 19:48:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | [rus] dictionary: 22075 types
2020-10-12 19:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 19:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13404.10546875Mb; avail=475393.71484375Mb
2020-10-12 19:48:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 19:48:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:aze-eng': 1, 'main:rus-eng': 1}
2020-10-12 19:48:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:33 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azerus_sepspm8000/M2O/valid.aze-eng.aze
2020-10-12 19:48:33 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azerus_sepspm8000/M2O/valid.aze-eng.eng
2020-10-12 19:48:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azerus_sepspm8000/M2O/ valid aze-eng 671 examples
2020-10-12 19:48:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:rus-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:33 | INFO | fairseq.data.data_utils | loaded 4814 examples from: fairseq/data-bin/ted_azerus_sepspm8000/M2O/valid.rus-eng.rus
2020-10-12 19:48:33 | INFO | fairseq.data.data_utils | loaded 4814 examples from: fairseq/data-bin/ted_azerus_sepspm8000/M2O/valid.rus-eng.eng
2020-10-12 19:48:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azerus_sepspm8000/M2O/ valid rus-eng 4814 examples
2020-10-12 19:48:34 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22075, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22075, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=22075, bias=False)
  )
)
2020-10-12 19:48:34 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 19:48:34 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 19:48:34 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 19:48:34 | INFO | fairseq_cli.train | num. model params: 42845696 (num. trained: 42845696)
2020-10-12 19:48:37 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 19:48:37 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 19:48:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 19:48:37 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 19:48:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 19:48:37 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 19:48:37 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 19:48:37 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_last.pt
2020-10-12 19:48:37 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 19:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 19:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15549.80078125Mb; avail=473237.87109375Mb
2020-10-12 19:48:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 19:48:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:aze-eng': 1, 'main:rus-eng': 1}
2020-10-12 19:48:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:37 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azerus_sepspm8000/M2O/train.aze-eng.aze
2020-10-12 19:48:37 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azerus_sepspm8000/M2O/train.aze-eng.eng
2020-10-12 19:48:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azerus_sepspm8000/M2O/ train aze-eng 5946 examples
2020-10-12 19:48:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:rus-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:37 | INFO | fairseq.data.data_utils | loaded 19993 examples from: fairseq/data-bin/ted_azerus_sepspm8000/M2O/train.rus-eng.rus
2020-10-12 19:48:37 | INFO | fairseq.data.data_utils | loaded 19993 examples from: fairseq/data-bin/ted_azerus_sepspm8000/M2O/train.rus-eng.eng
2020-10-12 19:48:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azerus_sepspm8000/M2O/ train rus-eng 19993 examples
2020-10-12 19:48:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:aze-eng', 5946), ('main:rus-eng', 19993)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 19:48:37 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 19:48:37 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 25939
2020-10-12 19:48:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 25939; virtual dataset size 25939
2020-10-12 19:48:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 19:48:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:aze-eng': 5946, 'main:rus-eng': 19993}; raw total size: 25939
2020-10-12 19:48:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:aze-eng': 5946, 'main:rus-eng': 19993}; resampled total size: 25939
2020-10-12 19:48:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 19:48:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.004293
2020-10-12 19:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15550.25390625Mb; avail=473237.41796875Mb
2020-10-12 19:48:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000473
2020-10-12 19:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004750
2020-10-12 19:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15550.25390625Mb; avail=473237.41796875Mb
2020-10-12 19:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000188
2020-10-12 19:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15550.25390625Mb; avail=473237.41796875Mb
2020-10-12 19:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091415
2020-10-12 19:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097112
2020-10-12 19:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15550.8203125Mb; avail=473236.8515625Mb
2020-10-12 19:48:38 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 19:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15551.28515625Mb; avail=473236.27734375Mb
2020-10-12 19:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003602
2020-10-12 19:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15551.28515625Mb; avail=473236.27734375Mb
2020-10-12 19:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 19:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15551.28515625Mb; avail=473236.27734375Mb
2020-10-12 19:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.087961
2020-10-12 19:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.092492
2020-10-12 19:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15551.46484375Mb; avail=473235.671875Mb
2020-10-12 19:48:38 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 19:49:07 | INFO | train_inner | epoch 001:    100 / 113 loss=14.116, nll_loss=14.001, ppl=16397.1, wps=22001.7, ups=3.54, wpb=6209.9, bsz=230.8, num_updates=100, lr=5.0975e-06, gnorm=4.615, clip=0, train_wall=28, wall=29
2020-10-12 19:49:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17836.1171875Mb; avail=470869.5859375Mb
2020-10-12 19:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001760
2020-10-12 19:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.1171875Mb; avail=470869.5859375Mb
2020-10-12 19:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.084315
2020-10-12 19:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.94921875Mb; avail=470867.828125Mb
2020-10-12 19:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.058020
2020-10-12 19:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.144944
2020-10-12 19:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.96875Mb; avail=470867.828125Mb
2020-10-12 19:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17837.99609375Mb; avail=470867.5859375Mb
2020-10-12 19:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001273
2020-10-12 19:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.03515625Mb; avail=470867.46484375Mb
2020-10-12 19:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082619
2020-10-12 19:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.0078125Mb; avail=470867.5859375Mb
2020-10-12 19:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057532
2020-10-12 19:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142257
2020-10-12 19:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.13671875Mb; avail=470867.34375Mb
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2020-10-12 19:49:13 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.38 | nll_loss 12.059 | ppl 4267.46 | wps 58433.1 | wpb 2270.2 | bsz 84.4 | num_updates 113
2020-10-12 19:49:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:49:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 1 @ 113 updates, score 12.38) (writing took 1.5678709350004283 seconds)
2020-10-12 19:49:15 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 19:49:15 | INFO | train | epoch 001 | loss 13.974 | nll_loss 13.843 | ppl 14696 | wps 19248.5 | ups 3.1 | wpb 6195.7 | bsz 229.5 | num_updates 113 | lr 5.74718e-06 | gnorm 4.401 | clip 0 | train_wall 31 | wall 37
2020-10-12 19:49:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 19:49:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 19:49:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17855.64453125Mb; avail=470834.1875Mb
2020-10-12 19:49:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000688
2020-10-12 19:49:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005229
2020-10-12 19:49:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17855.703125Mb; avail=470834.06640625Mb
2020-10-12 19:49:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 19:49:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17855.703125Mb; avail=470834.06640625Mb
2020-10-12 19:49:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088432
2020-10-12 19:49:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094594
2020-10-12 19:49:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17856.2265625Mb; avail=470833.58203125Mb
2020-10-12 19:49:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17856.94140625Mb; avail=470832.81640625Mb
2020-10-12 19:49:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003699
2020-10-12 19:49:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17856.94140625Mb; avail=470832.81640625Mb
2020-10-12 19:49:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 19:49:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17856.94140625Mb; avail=470832.81640625Mb
2020-10-12 19:49:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.087816
2020-10-12 19:49:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.092433
2020-10-12 19:49:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17856.828125Mb; avail=470832.9375Mb
2020-10-12 19:49:15 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 19:49:40 | INFO | train_inner | epoch 002:     87 / 113 loss=12.37, nll_loss=12.054, ppl=4253.27, wps=18675.2, ups=3.01, wpb=6196.2, bsz=231.7, num_updates=200, lr=1.0095e-05, gnorm=2.173, clip=0, train_wall=27, wall=62
2020-10-12 19:49:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:49:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18350.51171875Mb; avail=470323.67578125Mb
2020-10-12 19:49:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001705
2020-10-12 19:49:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18350.51171875Mb; avail=470323.67578125Mb
2020-10-12 19:49:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081442
2020-10-12 19:49:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18350.6015625Mb; avail=470323.67578125Mb
2020-10-12 19:49:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057269
2020-10-12 19:49:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141258
2020-10-12 19:49:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18350.609375Mb; avail=470323.5546875Mb
2020-10-12 19:49:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18350.6640625Mb; avail=470323.296875Mb
2020-10-12 19:49:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001280
2020-10-12 19:49:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18350.6640625Mb; avail=470323.296875Mb
2020-10-12 19:49:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082831
2020-10-12 19:49:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18350.546875Mb; avail=470323.4296875Mb
2020-10-12 19:49:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056963
2020-10-12 19:49:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141862
2020-10-12 19:49:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18350.5546875Mb; avail=470323.53515625Mb
2020-10-12 19:49:50 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.518 | nll_loss 11.096 | ppl 2189.02 | wps 58740.6 | wpb 2270.2 | bsz 84.4 | num_updates 226 | best_loss 11.518
2020-10-12 19:49:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:50:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 2 @ 226 updates, score 11.518) (writing took 11.083974375999787 seconds)
2020-10-12 19:50:01 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 19:50:01 | INFO | train | epoch 002 | loss 12.209 | nll_loss 11.874 | ppl 3753.4 | wps 15108.2 | ups 2.44 | wpb 6195.7 | bsz 229.5 | num_updates 226 | lr 1.13944e-05 | gnorm 2.045 | clip 0 | train_wall 31 | wall 84
2020-10-12 19:50:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 19:50:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 19:50:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18499.87109375Mb; avail=470174.07421875Mb
2020-10-12 19:50:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000920
2020-10-12 19:50:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006223
2020-10-12 19:50:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18500.4765625Mb; avail=470173.46875Mb
2020-10-12 19:50:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000219
2020-10-12 19:50:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18500.4765625Mb; avail=470173.46875Mb
2020-10-12 19:50:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101950
2020-10-12 19:50:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109415
2020-10-12 19:50:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18500.43359375Mb; avail=470173.28515625Mb
2020-10-12 19:50:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18500.4296875Mb; avail=470173.49609375Mb
2020-10-12 19:50:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004415
2020-10-12 19:50:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18500.98828125Mb; avail=470172.9375Mb
2020-10-12 19:50:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-12 19:50:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18500.98828125Mb; avail=470172.9375Mb
2020-10-12 19:50:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101734
2020-10-12 19:50:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107870
2020-10-12 19:50:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18500.625Mb; avail=470173.515625Mb
2020-10-12 19:50:01 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 19:50:23 | INFO | train_inner | epoch 003:     74 / 113 loss=11.572, nll_loss=11.162, ppl=2291.42, wps=14540.3, ups=2.32, wpb=6254.4, bsz=227, num_updates=300, lr=1.50925e-05, gnorm=1.703, clip=0, train_wall=28, wall=105
2020-10-12 19:50:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18527.453125Mb; avail=470150.5703125Mb
2020-10-12 19:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002018
2020-10-12 19:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18530.09375Mb; avail=470148.1875Mb
2020-10-12 19:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083146
2020-10-12 19:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18633.34375Mb; avail=470043.6796875Mb
2020-10-12 19:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056324
2020-10-12 19:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142669
2020-10-12 19:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18703.90625Mb; avail=469972.3359375Mb
2020-10-12 19:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18724.94921875Mb; avail=469950.4140625Mb
2020-10-12 19:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001475
2020-10-12 19:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18727.11328125Mb; avail=469948.2578125Mb
2020-10-12 19:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.084382
2020-10-12 19:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18836.26171875Mb; avail=469837.9609375Mb
2020-10-12 19:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056501
2020-10-12 19:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.143384
2020-10-12 19:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18900.8984375Mb; avail=469773.12890625Mb
2020-10-12 19:50:37 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.482 | nll_loss 9.909 | ppl 961.35 | wps 58035.4 | wpb 2270.2 | bsz 84.4 | num_updates 339 | best_loss 10.482
2020-10-12 19:50:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:50:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 3 @ 339 updates, score 10.482) (writing took 11.308982759000173 seconds)
2020-10-12 19:50:48 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 19:50:48 | INFO | train | epoch 003 | loss 11.268 | nll_loss 10.82 | ppl 1808.18 | wps 14854.8 | ups 2.4 | wpb 6195.7 | bsz 229.5 | num_updates 339 | lr 1.70415e-05 | gnorm 1.59 | clip 0 | train_wall 31 | wall 131
2020-10-12 19:50:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 19:50:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 19:50:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18390.45703125Mb; avail=470283.765625Mb
2020-10-12 19:50:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000948
2020-10-12 19:50:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006151
2020-10-12 19:50:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18391.0625Mb; avail=470283.16015625Mb
2020-10-12 19:50:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000227
2020-10-12 19:50:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18391.0625Mb; avail=470283.16015625Mb
2020-10-12 19:50:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099285
2020-10-12 19:50:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106613
2020-10-12 19:50:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18391.04296875Mb; avail=470283.203125Mb
2020-10-12 19:50:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18391.6484375Mb; avail=470282.59765625Mb
2020-10-12 19:50:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004152
2020-10-12 19:50:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18391.6484375Mb; avail=470282.59765625Mb
2020-10-12 19:50:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000225
2020-10-12 19:50:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18391.6484375Mb; avail=470282.59765625Mb
2020-10-12 19:50:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099868
2020-10-12 19:50:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105105
2020-10-12 19:50:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18391.51171875Mb; avail=470282.671875Mb
2020-10-12 19:50:48 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 19:51:06 | INFO | train_inner | epoch 004:     61 / 113 loss=10.568, nll_loss=10.021, ppl=1039.12, wps=14363.9, ups=2.31, wpb=6213.5, bsz=239.5, num_updates=400, lr=2.009e-05, gnorm=1.784, clip=0, train_wall=28, wall=149
2020-10-12 19:51:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:51:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17787.91015625Mb; avail=470892.51953125Mb
2020-10-12 19:51:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001877
2020-10-12 19:51:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17787.5234375Mb; avail=470892.1796875Mb
2020-10-12 19:51:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082925
2020-10-12 19:51:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17787.5625Mb; avail=470890.9609375Mb
2020-10-12 19:51:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056026
2020-10-12 19:51:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141876
2020-10-12 19:51:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17787.57421875Mb; avail=470890.47265625Mb
2020-10-12 19:51:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17787.28515625Mb; avail=470890.015625Mb
2020-10-12 19:51:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001303
2020-10-12 19:51:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17787.05078125Mb; avail=470889.66796875Mb
2020-10-12 19:51:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082289
2020-10-12 19:51:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17787.32421875Mb; avail=470889.15234375Mb
2020-10-12 19:51:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055003
2020-10-12 19:51:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139584
2020-10-12 19:51:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17787.078125Mb; avail=470888.2734375Mb
2020-10-12 19:51:24 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.464 | nll_loss 8.676 | ppl 409.09 | wps 57385.2 | wpb 2270.2 | bsz 84.4 | num_updates 452 | best_loss 9.464
2020-10-12 19:51:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:51:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 4 @ 452 updates, score 9.464) (writing took 7.878465110999969 seconds)
2020-10-12 19:51:31 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 19:51:31 | INFO | train | epoch 004 | loss 10.102 | nll_loss 9.477 | ppl 712.66 | wps 16203.6 | ups 2.62 | wpb 6195.7 | bsz 229.5 | num_updates 452 | lr 2.26887e-05 | gnorm 1.68 | clip 0 | train_wall 31 | wall 174
2020-10-12 19:51:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 19:51:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 19:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17766.3984375Mb; avail=470907.37890625Mb
2020-10-12 19:51:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000909
2020-10-12 19:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005770
2020-10-12 19:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17766.3984375Mb; avail=470907.37890625Mb
2020-10-12 19:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000199
2020-10-12 19:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17766.3984375Mb; avail=470907.37890625Mb
2020-10-12 19:51:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089646
2020-10-12 19:51:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096387
2020-10-12 19:51:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17766.39453125Mb; avail=470907.390625Mb
2020-10-12 19:51:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17766.2734375Mb; avail=470907.51171875Mb
2020-10-12 19:51:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003721
2020-10-12 19:51:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17766.2734375Mb; avail=470907.51171875Mb
2020-10-12 19:51:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 19:51:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17766.2734375Mb; avail=470907.51171875Mb
2020-10-12 19:51:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088484
2020-10-12 19:51:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093129
2020-10-12 19:51:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17766.37890625Mb; avail=470907.390625Mb
2020-10-12 19:51:32 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 19:51:45 | INFO | train_inner | epoch 005:     48 / 113 loss=9.62, nll_loss=8.904, ppl=479.11, wps=15530, ups=2.56, wpb=6078.2, bsz=212.7, num_updates=500, lr=2.50875e-05, gnorm=1.227, clip=0, train_wall=27, wall=188
2020-10-12 19:52:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:52:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17762.9921875Mb; avail=470910.4296875Mb
2020-10-12 19:52:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001752
2020-10-12 19:52:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17762.9921875Mb; avail=470910.4296875Mb
2020-10-12 19:52:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.085036
2020-10-12 19:52:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17763.546875Mb; avail=470910.79296875Mb
2020-10-12 19:52:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056467
2020-10-12 19:52:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.144060
2020-10-12 19:52:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17763.4609375Mb; avail=470910.79296875Mb
2020-10-12 19:52:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17763.4296875Mb; avail=470910.9140625Mb
2020-10-12 19:52:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001211
2020-10-12 19:52:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17763.4296875Mb; avail=470910.9140625Mb
2020-10-12 19:52:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083068
2020-10-12 19:52:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17762.46484375Mb; avail=470911.0546875Mb
2020-10-12 19:52:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056100
2020-10-12 19:52:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141134
2020-10-12 19:52:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17762.46484375Mb; avail=470910.5703125Mb
2020-10-12 19:52:07 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.108 | nll_loss 8.229 | ppl 299.95 | wps 57972.9 | wpb 2270.2 | bsz 84.4 | num_updates 565 | best_loss 9.108
2020-10-12 19:52:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:52:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 5 @ 565 updates, score 9.108) (writing took 9.203511560999686 seconds)
2020-10-12 19:52:16 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 19:52:16 | INFO | train | epoch 005 | loss 9.414 | nll_loss 8.646 | ppl 400.65 | wps 15750.4 | ups 2.54 | wpb 6195.7 | bsz 229.5 | num_updates 565 | lr 2.83359e-05 | gnorm 1.189 | clip 0 | train_wall 31 | wall 218
2020-10-12 19:52:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 19:52:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17744.7578125Mb; avail=470929.1953125Mb
2020-10-12 19:52:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000759
2020-10-12 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006039
2020-10-12 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17744.9140625Mb; avail=470929.4375Mb
2020-10-12 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000202
2020-10-12 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17744.9140625Mb; avail=470929.4375Mb
2020-10-12 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100055
2020-10-12 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107183
2020-10-12 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17736.65625Mb; avail=470937.08203125Mb
2020-10-12 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17763.09765625Mb; avail=470910.828125Mb
2020-10-12 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003707
2020-10-12 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17767.94140625Mb; avail=470905.984375Mb
2020-10-12 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17768.546875Mb; avail=470905.37890625Mb
2020-10-12 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088290
2020-10-12 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.092980
2020-10-12 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.44921875Mb; avail=470796.39453125Mb
2020-10-12 19:52:16 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 19:52:26 | INFO | train_inner | epoch 006:     35 / 113 loss=9.329, nll_loss=8.533, ppl=370.49, wps=15119.7, ups=2.44, wpb=6195.8, bsz=236.8, num_updates=600, lr=3.0085e-05, gnorm=1.354, clip=0, train_wall=27, wall=229
2020-10-12 19:52:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17978.33203125Mb; avail=470694.6875Mb
2020-10-12 19:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001751
2020-10-12 19:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17978.33203125Mb; avail=470694.6875Mb
2020-10-12 19:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082624
2020-10-12 19:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17977.6875Mb; avail=470695.453125Mb
2020-10-12 19:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055926
2020-10-12 19:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141108
2020-10-12 19:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17977.8125Mb; avail=470695.33203125Mb
2020-10-12 19:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17977.8125Mb; avail=470695.33203125Mb
2020-10-12 19:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001285
2020-10-12 19:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17977.8125Mb; avail=470695.33203125Mb
2020-10-12 19:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082295
2020-10-12 19:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17977.72265625Mb; avail=470695.33203125Mb
2020-10-12 19:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054743
2020-10-12 19:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139121
2020-10-12 19:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17977.69140625Mb; avail=470695.57421875Mb
2020-10-12 19:52:51 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.913 | nll_loss 7.984 | ppl 253.13 | wps 58374.2 | wpb 2270.2 | bsz 84.4 | num_updates 678 | best_loss 8.913
2020-10-12 19:52:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:53:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 6 @ 678 updates, score 8.913) (writing took 12.896722183000747 seconds)
2020-10-12 19:53:04 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 19:53:04 | INFO | train | epoch 006 | loss 9.166 | nll_loss 8.325 | ppl 320.77 | wps 14450.9 | ups 2.33 | wpb 6195.7 | bsz 229.5 | num_updates 678 | lr 3.39831e-05 | gnorm 1.366 | clip 0 | train_wall 31 | wall 267
2020-10-12 19:53:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 19:53:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17938.64453125Mb; avail=470734.68359375Mb
2020-10-12 19:53:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001003
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006541
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17939.25Mb; avail=470734.078125Mb
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000236
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17939.25Mb; avail=470734.078125Mb
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093598
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101256
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17939.4140625Mb; avail=470733.98046875Mb
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17939.46484375Mb; avail=470733.73828125Mb
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003749
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17939.46484375Mb; avail=470733.73828125Mb
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17939.46484375Mb; avail=470733.73828125Mb
2020-10-12 19:53:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088953
2020-10-12 19:53:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093620
2020-10-12 19:53:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17939.39453125Mb; avail=470733.6171875Mb
2020-10-12 19:53:05 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 19:53:11 | INFO | train_inner | epoch 007:     22 / 113 loss=9.115, nll_loss=8.262, ppl=306.99, wps=14002, ups=2.23, wpb=6280.7, bsz=233.9, num_updates=700, lr=3.50825e-05, gnorm=1.328, clip=0, train_wall=28, wall=274
2020-10-12 19:53:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17739.0Mb; avail=470934.484375Mb
2020-10-12 19:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001705
2020-10-12 19:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17739.0Mb; avail=470934.484375Mb
2020-10-12 19:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.085622
2020-10-12 19:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17739.10546875Mb; avail=470934.6953125Mb
2020-10-12 19:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057073
2020-10-12 19:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.145233
2020-10-12 19:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17739.16796875Mb; avail=470934.4375Mb
2020-10-12 19:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17739.09765625Mb; avail=470934.6796875Mb
2020-10-12 19:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001290
2020-10-12 19:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17739.09375Mb; avail=470934.6796875Mb
2020-10-12 19:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083034
2020-10-12 19:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17739.14453125Mb; avail=470934.6484375Mb
2020-10-12 19:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057049
2020-10-12 19:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142213
2020-10-12 19:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17739.14453125Mb; avail=470934.6484375Mb
2020-10-12 19:53:40 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.774 | nll_loss 7.819 | ppl 225.89 | wps 57967.5 | wpb 2270.2 | bsz 84.4 | num_updates 791 | best_loss 8.774
2020-10-12 19:53:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:53:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 7 @ 791 updates, score 8.774) (writing took 4.435944017999645 seconds)
2020-10-12 19:53:44 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 19:53:44 | INFO | train | epoch 007 | loss 8.994 | nll_loss 8.116 | ppl 277.36 | wps 17492.5 | ups 2.82 | wpb 6195.7 | bsz 229.5 | num_updates 791 | lr 3.96302e-05 | gnorm 1.365 | clip 0 | train_wall 31 | wall 307
2020-10-12 19:53:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 19:53:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17741.42578125Mb; avail=470931.859375Mb
2020-10-12 19:53:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000812
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005856
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17741.42578125Mb; avail=470931.859375Mb
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17741.42578125Mb; avail=470931.859375Mb
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094371
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101305
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17753.12109375Mb; avail=470920.171875Mb
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17753.8828125Mb; avail=470919.56640625Mb
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003649
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17753.8828125Mb; avail=470919.56640625Mb
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17753.8828125Mb; avail=470919.56640625Mb
2020-10-12 19:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089515
2020-10-12 19:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094080
2020-10-12 19:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17758.015625Mb; avail=470914.98046875Mb
2020-10-12 19:53:45 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 19:53:47 | INFO | train_inner | epoch 008:      9 / 113 loss=8.978, nll_loss=8.097, ppl=273.73, wps=16937.8, ups=2.77, wpb=6124.5, bsz=221.8, num_updates=800, lr=4.008e-05, gnorm=1.341, clip=0, train_wall=27, wall=310
2020-10-12 19:54:16 | INFO | train_inner | epoch 008:    109 / 113 loss=8.834, nll_loss=7.932, ppl=244.13, wps=21604, ups=3.45, wpb=6253.8, bsz=233.6, num_updates=900, lr=4.50775e-05, gnorm=1.363, clip=0, train_wall=28, wall=339
2020-10-12 19:54:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17745.64453125Mb; avail=470927.94921875Mb
2020-10-12 19:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001716
2020-10-12 19:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17745.64453125Mb; avail=470927.94921875Mb
2020-10-12 19:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083496
2020-10-12 19:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17746.11328125Mb; avail=470927.22265625Mb
2020-10-12 19:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056806
2020-10-12 19:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142883
2020-10-12 19:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17745.984375Mb; avail=470927.46484375Mb
2020-10-12 19:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17745.984375Mb; avail=470927.46484375Mb
2020-10-12 19:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001285
2020-10-12 19:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17745.984375Mb; avail=470927.46484375Mb
2020-10-12 19:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082007
2020-10-12 19:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17745.984375Mb; avail=470927.46484375Mb
2020-10-12 19:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055537
2020-10-12 19:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139608
2020-10-12 19:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17746.83203125Mb; avail=470926.6171875Mb
2020-10-12 19:54:20 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.661 | nll_loss 7.673 | ppl 204.02 | wps 58071.4 | wpb 2270.2 | bsz 84.4 | num_updates 904 | best_loss 8.661
2020-10-12 19:54:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:54:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 8 @ 904 updates, score 8.661) (writing took 4.440946803999395 seconds)
2020-10-12 19:54:25 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 19:54:25 | INFO | train | epoch 008 | loss 8.831 | nll_loss 7.928 | ppl 243.52 | wps 17398.3 | ups 2.81 | wpb 6195.7 | bsz 229.5 | num_updates 904 | lr 4.52774e-05 | gnorm 1.355 | clip 0 | train_wall 31 | wall 347
2020-10-12 19:54:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 19:54:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17754.71484375Mb; avail=470919.27734375Mb
2020-10-12 19:54:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001560
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.009669
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17761.8125Mb; avail=470912.13671875Mb
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000343
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17762.41796875Mb; avail=470911.53125Mb
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.116006
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.127338
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17768.48828125Mb; avail=470905.15234375Mb
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17768.67578125Mb; avail=470905.16015625Mb
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003696
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17768.67578125Mb; avail=470905.16015625Mb
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17768.67578125Mb; avail=470905.16015625Mb
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089233
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093852
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17768.3125Mb; avail=470905.28125Mb
2020-10-12 19:54:25 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 19:54:52 | INFO | train_inner | epoch 009:     96 / 113 loss=8.643, nll_loss=7.713, ppl=209.83, wps=16991.9, ups=2.75, wpb=6172.3, bsz=231, num_updates=1000, lr=5.0075e-05, gnorm=1.351, clip=0, train_wall=28, wall=375
2020-10-12 19:54:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17795.28125Mb; avail=470878.39453125Mb
2020-10-12 19:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001761
2020-10-12 19:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17795.28125Mb; avail=470878.39453125Mb
2020-10-12 19:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.084145
2020-10-12 19:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17795.53515625Mb; avail=470878.15234375Mb
2020-10-12 19:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055061
2020-10-12 19:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141792
2020-10-12 19:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17795.53515625Mb; avail=470878.15234375Mb
2020-10-12 19:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17795.53515625Mb; avail=470878.15234375Mb
2020-10-12 19:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001288
2020-10-12 19:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17795.53515625Mb; avail=470878.15234375Mb
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082077
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17795.4375Mb; avail=470878.03125Mb
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055711
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139957
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17795.43359375Mb; avail=470878.03125Mb
2020-10-12 19:55:00 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.435 | nll_loss 7.437 | ppl 173.28 | wps 57639.1 | wpb 2270.2 | bsz 84.4 | num_updates 1017 | best_loss 8.435
2020-10-12 19:55:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:55:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 9 @ 1017 updates, score 8.435) (writing took 4.494859846000509 seconds)
2020-10-12 19:55:05 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 19:55:05 | INFO | train | epoch 009 | loss 8.667 | nll_loss 7.74 | ppl 213.85 | wps 17443.5 | ups 2.82 | wpb 6195.7 | bsz 229.5 | num_updates 1017 | lr 5.09246e-05 | gnorm 1.358 | clip 0 | train_wall 31 | wall 387
2020-10-12 19:55:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 19:55:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17805.87890625Mb; avail=470867.390625Mb
2020-10-12 19:55:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001173
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006533
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17779.79296875Mb; avail=470893.4765625Mb
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000301
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17779.79296875Mb; avail=470893.4765625Mb
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095332
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103063
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17774.37890625Mb; avail=470899.140625Mb
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17773.9453125Mb; avail=470899.26953125Mb
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003769
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17773.9453125Mb; avail=470899.26953125Mb
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17773.9453125Mb; avail=470899.26953125Mb
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088187
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.092879
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17773.76171875Mb; avail=470900.0Mb
2020-10-12 19:55:05 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 19:55:29 | INFO | train_inner | epoch 010:     83 / 113 loss=8.561, nll_loss=7.617, ppl=196.34, wps=16957.2, ups=2.74, wpb=6184.6, bsz=230, num_updates=1100, lr=5.50725e-05, gnorm=1.433, clip=0, train_wall=28, wall=411
2020-10-12 19:55:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:55:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17821.85546875Mb; avail=470851.203125Mb
2020-10-12 19:55:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001805
2020-10-12 19:55:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.203125Mb; avail=470851.46875Mb
2020-10-12 19:55:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082240
2020-10-12 19:55:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17821.90234375Mb; avail=470851.61328125Mb
2020-10-12 19:55:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056334
2020-10-12 19:55:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141389
2020-10-12 19:55:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.08984375Mb; avail=470851.21875Mb
2020-10-12 19:55:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17822.265625Mb; avail=470851.21875Mb
2020-10-12 19:55:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001361
2020-10-12 19:55:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.265625Mb; avail=470851.21875Mb
2020-10-12 19:55:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080918
2020-10-12 19:55:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.37890625Mb; avail=470851.09765625Mb
2020-10-12 19:55:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.089989
2020-10-12 19:55:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.173245
2020-10-12 19:55:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.81640625Mb; avail=470850.85546875Mb
2020-10-12 19:55:41 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.283 | nll_loss 7.263 | ppl 153.58 | wps 57641.6 | wpb 2270.2 | bsz 84.4 | num_updates 1130 | best_loss 8.283
2020-10-12 19:55:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:55:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 10 @ 1130 updates, score 8.283) (writing took 4.454089414999544 seconds)
2020-10-12 19:55:45 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 19:55:45 | INFO | train | epoch 010 | loss 8.523 | nll_loss 7.574 | ppl 190.53 | wps 17391.4 | ups 2.81 | wpb 6195.7 | bsz 229.5 | num_updates 1130 | lr 5.65718e-05 | gnorm 1.474 | clip 0 | train_wall 31 | wall 427
2020-10-12 19:55:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 19:55:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 19:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17864.8671875Mb; avail=470808.21484375Mb
2020-10-12 19:55:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000890
2020-10-12 19:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006028
2020-10-12 19:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.4609375Mb; avail=470807.73046875Mb
2020-10-12 19:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000214
2020-10-12 19:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.4609375Mb; avail=470807.73046875Mb
2020-10-12 19:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093967
2020-10-12 19:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101106
2020-10-12 19:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.421875Mb; avail=470807.48828125Mb
2020-10-12 19:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17865.49609375Mb; avail=470807.24609375Mb
2020-10-12 19:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003704
2020-10-12 19:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.49609375Mb; avail=470807.24609375Mb
2020-10-12 19:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-12 19:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.49609375Mb; avail=470807.24609375Mb
2020-10-12 19:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088688
2020-10-12 19:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093325
2020-10-12 19:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.51171875Mb; avail=470807.24609375Mb
2020-10-12 19:55:45 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 19:56:05 | INFO | train_inner | epoch 011:     70 / 113 loss=8.44, nll_loss=7.479, ppl=178.36, wps=17014.9, ups=2.75, wpb=6184.2, bsz=227, num_updates=1200, lr=6.007e-05, gnorm=1.403, clip=0, train_wall=28, wall=448
2020-10-12 19:56:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18450.40625Mb; avail=470222.69921875Mb
2020-10-12 19:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001759
2020-10-12 19:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18450.40625Mb; avail=470222.69921875Mb
2020-10-12 19:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081836
2020-10-12 19:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18450.44140625Mb; avail=470222.578125Mb
2020-10-12 19:56:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055319
2020-10-12 19:56:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139739
2020-10-12 19:56:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18177.26953125Mb; avail=470500.78515625Mb
2020-10-12 19:56:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17874.6953125Mb; avail=470798.9296875Mb
2020-10-12 19:56:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001293
2020-10-12 19:56:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.6953125Mb; avail=470798.68359375Mb
2020-10-12 19:56:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081023
2020-10-12 19:56:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.06640625Mb; avail=470806.2109375Mb
2020-10-12 19:56:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054941
2020-10-12 19:56:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138278
2020-10-12 19:56:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.7421875Mb; avail=470805.16015625Mb
2020-10-12 19:56:20 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.2 | nll_loss 7.142 | ppl 141.24 | wps 57641.2 | wpb 2270.2 | bsz 84.4 | num_updates 1243 | best_loss 8.2
2020-10-12 19:56:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:56:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 11 @ 1243 updates, score 8.2) (writing took 7.412549586999376 seconds)
2020-10-12 19:56:28 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 19:56:28 | INFO | train | epoch 011 | loss 8.378 | nll_loss 7.409 | ppl 169.92 | wps 16348.3 | ups 2.64 | wpb 6195.7 | bsz 229.5 | num_updates 1243 | lr 6.22189e-05 | gnorm 1.409 | clip 0 | train_wall 31 | wall 470
2020-10-12 19:56:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 19:56:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17854.78125Mb; avail=470818.45703125Mb
2020-10-12 19:56:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000967
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006480
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17855.38671875Mb; avail=470817.8515625Mb
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000202
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17855.38671875Mb; avail=470817.8515625Mb
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100916
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108456
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17855.22265625Mb; avail=470818.21484375Mb
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17854.86328125Mb; avail=470818.59375Mb
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004570
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.8828125Mb; avail=470818.47265625Mb
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.8828125Mb; avail=470818.47265625Mb
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100701
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106407
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.86328125Mb; avail=470818.59375Mb
2020-10-12 19:56:28 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 19:56:45 | INFO | train_inner | epoch 012:     57 / 113 loss=8.307, nll_loss=7.328, ppl=160.64, wps=15774.5, ups=2.53, wpb=6222.8, bsz=229.8, num_updates=1300, lr=6.50675e-05, gnorm=1.387, clip=0, train_wall=28, wall=487
2020-10-12 19:57:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:57:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17870.328125Mb; avail=470803.8671875Mb
2020-10-12 19:57:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001839
2020-10-12 19:57:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17872.75Mb; avail=470801.4453125Mb
2020-10-12 19:57:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083545
2020-10-12 19:57:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17978.609375Mb; avail=470694.875Mb
2020-10-12 19:57:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055666
2020-10-12 19:57:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141866
2020-10-12 19:57:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18048.5625Mb; avail=470624.640625Mb
2020-10-12 19:57:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18071.6484375Mb; avail=470601.75390625Mb
2020-10-12 19:57:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001166
2020-10-12 19:57:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18072.859375Mb; avail=470599.9375Mb
2020-10-12 19:57:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083238
2020-10-12 19:57:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18182.421875Mb; avail=470490.34765625Mb
2020-10-12 19:57:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057520
2020-10-12 19:57:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142708
2020-10-12 19:57:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18261.09375Mb; avail=470412.484375Mb
2020-10-12 19:57:04 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.085 | nll_loss 7.025 | ppl 130.26 | wps 57707.3 | wpb 2270.2 | bsz 84.4 | num_updates 1356 | best_loss 8.085
2020-10-12 19:57:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:57:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 12 @ 1356 updates, score 8.085) (writing took 7.070328623000023 seconds)
2020-10-12 19:57:11 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 19:57:11 | INFO | train | epoch 012 | loss 8.246 | nll_loss 7.258 | ppl 153.06 | wps 16276.4 | ups 2.63 | wpb 6195.7 | bsz 229.5 | num_updates 1356 | lr 6.78661e-05 | gnorm 1.272 | clip 0 | train_wall 31 | wall 513
2020-10-12 19:57:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 19:57:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 19:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17853.48828125Mb; avail=470819.62890625Mb
2020-10-12 19:57:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000821
2020-10-12 19:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005506
2020-10-12 19:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.4453125Mb; avail=470819.87109375Mb
2020-10-12 19:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000200
2020-10-12 19:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.4453125Mb; avail=470819.87109375Mb
2020-10-12 19:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091290
2020-10-12 19:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097795
2020-10-12 19:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.828125Mb; avail=470819.0234375Mb
2020-10-12 19:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17853.78515625Mb; avail=470819.7578125Mb
2020-10-12 19:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003759
2020-10-12 19:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.01171875Mb; avail=470819.0390625Mb
2020-10-12 19:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000188
2020-10-12 19:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.01171875Mb; avail=470819.0390625Mb
2020-10-12 19:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088233
2020-10-12 19:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.092953
2020-10-12 19:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.34765625Mb; avail=470819.41015625Mb
2020-10-12 19:57:11 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 19:57:24 | INFO | train_inner | epoch 013:     44 / 113 loss=8.167, nll_loss=7.168, ppl=143.85, wps=15777.2, ups=2.57, wpb=6150.8, bsz=232.3, num_updates=1400, lr=7.0065e-05, gnorm=1.267, clip=0, train_wall=28, wall=526
2020-10-12 19:57:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17841.65234375Mb; avail=470831.48828125Mb
2020-10-12 19:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001820
2020-10-12 19:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.65234375Mb; avail=470831.48828125Mb
2020-10-12 19:57:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081781
2020-10-12 19:57:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.62890625Mb; avail=470831.76953125Mb
2020-10-12 19:57:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056971
2020-10-12 19:57:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141371
2020-10-12 19:57:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.390625Mb; avail=470832.01171875Mb
2020-10-12 19:57:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17841.390625Mb; avail=470832.01171875Mb
2020-10-12 19:57:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001212
2020-10-12 19:57:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.390625Mb; avail=470832.01171875Mb
2020-10-12 19:57:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083922
2020-10-12 19:57:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.35546875Mb; avail=470832.1328125Mb
2020-10-12 19:57:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057921
2020-10-12 19:57:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.144254
2020-10-12 19:57:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.29296875Mb; avail=470832.25390625Mb
2020-10-12 19:57:46 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.942 | nll_loss 6.869 | ppl 116.91 | wps 57831.6 | wpb 2270.2 | bsz 84.4 | num_updates 1469 | best_loss 7.942
2020-10-12 19:57:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:57:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 13 @ 1469 updates, score 7.942) (writing took 4.443221344000449 seconds)
2020-10-12 19:57:51 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 19:57:51 | INFO | train | epoch 013 | loss 8.128 | nll_loss 7.123 | ppl 139.37 | wps 17485.4 | ups 2.82 | wpb 6195.7 | bsz 229.5 | num_updates 1469 | lr 7.35133e-05 | gnorm 1.334 | clip 0 | train_wall 31 | wall 553
2020-10-12 19:57:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 19:57:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 19:57:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17842.421875Mb; avail=470830.50390625Mb
2020-10-12 19:57:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000702
2020-10-12 19:57:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005330
2020-10-12 19:57:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.421875Mb; avail=470830.50390625Mb
2020-10-12 19:57:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000203
2020-10-12 19:57:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.421875Mb; avail=470830.50390625Mb
2020-10-12 19:57:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089456
2020-10-12 19:57:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095752
2020-10-12 19:57:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.8359375Mb; avail=470830.07421875Mb
2020-10-12 19:57:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17842.9453125Mb; avail=470830.08984375Mb
2020-10-12 19:57:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003698
2020-10-12 19:57:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.83984375Mb; avail=470830.08984375Mb
2020-10-12 19:57:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 19:57:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.83984375Mb; avail=470830.08984375Mb
2020-10-12 19:57:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090081
2020-10-12 19:57:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094737
2020-10-12 19:57:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17843.0Mb; avail=470830.33203125Mb
2020-10-12 19:57:51 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 19:58:00 | INFO | train_inner | epoch 014:     31 / 113 loss=8.101, nll_loss=7.092, ppl=136.45, wps=17208.5, ups=2.75, wpb=6250.9, bsz=238.8, num_updates=1500, lr=7.50625e-05, gnorm=1.43, clip=0, train_wall=28, wall=563
2020-10-12 19:58:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17839.0625Mb; avail=470833.7109375Mb
2020-10-12 19:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001898
2020-10-12 19:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.0625Mb; avail=470833.7109375Mb
2020-10-12 19:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081363
2020-10-12 19:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.37890625Mb; avail=470833.34765625Mb
2020-10-12 19:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.058156
2020-10-12 19:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142241
2020-10-12 19:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.07421875Mb; avail=470834.36328125Mb
2020-10-12 19:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17838.828125Mb; avail=470834.36328125Mb
2020-10-12 19:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001180
2020-10-12 19:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.828125Mb; avail=470834.36328125Mb
2020-10-12 19:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082686
2020-10-12 19:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.80859375Mb; avail=470834.36328125Mb
2020-10-12 19:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061203
2020-10-12 19:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.145846
2020-10-12 19:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.74609375Mb; avail=470834.2421875Mb
2020-10-12 19:58:26 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.859 | nll_loss 6.775 | ppl 109.55 | wps 58179.1 | wpb 2270.2 | bsz 84.4 | num_updates 1582 | best_loss 7.859
2020-10-12 19:58:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:58:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 14 @ 1582 updates, score 7.859) (writing took 4.487924650000423 seconds)
2020-10-12 19:58:31 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 19:58:31 | INFO | train | epoch 014 | loss 8.026 | nll_loss 7.005 | ppl 128.49 | wps 17562 | ups 2.83 | wpb 6195.7 | bsz 229.5 | num_updates 1582 | lr 7.91605e-05 | gnorm 1.451 | clip 0 | train_wall 31 | wall 593
2020-10-12 19:58:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 19:58:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 19:58:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17858.09765625Mb; avail=470814.7890625Mb
2020-10-12 19:58:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000784
2020-10-12 19:58:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005428
2020-10-12 19:58:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.09765625Mb; avail=470814.7890625Mb
2020-10-12 19:58:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000204
2020-10-12 19:58:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.09765625Mb; avail=470814.7890625Mb
2020-10-12 19:58:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089153
2020-10-12 19:58:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095595
2020-10-12 19:58:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.51171875Mb; avail=470814.546875Mb
2020-10-12 19:58:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17858.3359375Mb; avail=470814.91015625Mb
2020-10-12 19:58:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003742
2020-10-12 19:58:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.3359375Mb; avail=470814.91015625Mb
2020-10-12 19:58:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 19:58:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.3359375Mb; avail=470814.91015625Mb
2020-10-12 19:58:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088861
2020-10-12 19:58:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093528
2020-10-12 19:58:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.609375Mb; avail=470814.65625Mb
2020-10-12 19:58:31 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 19:58:36 | INFO | train_inner | epoch 015:     18 / 113 loss=8.029, nll_loss=7.008, ppl=128.69, wps=17021.9, ups=2.77, wpb=6139.5, bsz=212.1, num_updates=1600, lr=8.006e-05, gnorm=1.364, clip=0, train_wall=27, wall=599
2020-10-12 19:59:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:59:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17816.78125Mb; avail=470855.69140625Mb
2020-10-12 19:59:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001738
2020-10-12 19:59:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.78125Mb; avail=470855.69140625Mb
2020-10-12 19:59:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082465
2020-10-12 19:59:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.02734375Mb; avail=470855.93359375Mb
2020-10-12 19:59:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056609
2020-10-12 19:59:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141632
2020-10-12 19:59:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.02734375Mb; avail=470855.93359375Mb
2020-10-12 19:59:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17817.02734375Mb; avail=470855.93359375Mb
2020-10-12 19:59:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001140
2020-10-12 19:59:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.02734375Mb; avail=470855.93359375Mb
2020-10-12 19:59:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082679
2020-10-12 19:59:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.17578125Mb; avail=470855.44921875Mb
2020-10-12 19:59:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055559
2020-10-12 19:59:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140137
2020-10-12 19:59:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.3515625Mb; avail=470855.20703125Mb
2020-10-12 19:59:06 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.764 | nll_loss 6.658 | ppl 100.99 | wps 57843.4 | wpb 2270.2 | bsz 84.4 | num_updates 1695 | best_loss 7.764
2020-10-12 19:59:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:59:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 15 @ 1695 updates, score 7.764) (writing took 4.437975345999803 seconds)
2020-10-12 19:59:10 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 19:59:10 | INFO | train | epoch 015 | loss 7.908 | nll_loss 6.871 | ppl 117.05 | wps 17688.2 | ups 2.85 | wpb 6195.7 | bsz 229.5 | num_updates 1695 | lr 8.48076e-05 | gnorm 1.221 | clip 0 | train_wall 31 | wall 633
2020-10-12 19:59:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 19:59:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 19:59:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17818.05078125Mb; avail=470854.51953125Mb
2020-10-12 19:59:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000783
2020-10-12 19:59:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006046
2020-10-12 19:59:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.98828125Mb; avail=470854.76171875Mb
2020-10-12 19:59:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000252
2020-10-12 19:59:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.98828125Mb; avail=470854.76171875Mb
2020-10-12 19:59:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094075
2020-10-12 19:59:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101192
2020-10-12 19:59:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.0546875Mb; avail=470854.76953125Mb
2020-10-12 19:59:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17818.05078125Mb; avail=470854.76953125Mb
2020-10-12 19:59:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003727
2020-10-12 19:59:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.05078125Mb; avail=470854.76953125Mb
2020-10-12 19:59:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 19:59:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.05078125Mb; avail=470854.76953125Mb
2020-10-12 19:59:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088817
2020-10-12 19:59:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093466
2020-10-12 19:59:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.05078125Mb; avail=470854.76953125Mb
2020-10-12 19:59:11 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 19:59:12 | INFO | train_inner | epoch 016:      5 / 113 loss=7.883, nll_loss=6.843, ppl=114.81, wps=17314.2, ups=2.79, wpb=6213.3, bsz=233.5, num_updates=1700, lr=8.50575e-05, gnorm=1.24, clip=0, train_wall=27, wall=635
2020-10-12 19:59:41 | INFO | train_inner | epoch 016:    105 / 113 loss=7.826, nll_loss=6.776, ppl=109.58, wps=21660.7, ups=3.48, wpb=6225.2, bsz=231.3, num_updates=1800, lr=9.0055e-05, gnorm=1.41, clip=0, train_wall=28, wall=663
2020-10-12 19:59:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:59:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18158.78515625Mb; avail=470514.640625Mb
2020-10-12 19:59:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001798
2020-10-12 19:59:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18158.78515625Mb; avail=470514.640625Mb
2020-10-12 19:59:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083411
2020-10-12 19:59:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18158.65625Mb; avail=470514.76953125Mb
2020-10-12 19:59:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055963
2020-10-12 19:59:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141992
2020-10-12 19:59:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18158.68359375Mb; avail=470514.6484375Mb
2020-10-12 19:59:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18158.62890625Mb; avail=470514.40625Mb
2020-10-12 19:59:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001206
2020-10-12 19:59:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18158.62890625Mb; avail=470514.40625Mb
2020-10-12 19:59:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081943
2020-10-12 19:59:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18158.3359375Mb; avail=470515.1328125Mb
2020-10-12 19:59:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055272
2020-10-12 19:59:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139188
2020-10-12 19:59:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18158.3359375Mb; avail=470515.1328125Mb
2020-10-12 19:59:46 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.688 | nll_loss 6.551 | ppl 93.74 | wps 57872.8 | wpb 2270.2 | bsz 84.4 | num_updates 1808 | best_loss 7.688
2020-10-12 19:59:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:59:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 16 @ 1808 updates, score 7.688) (writing took 4.798483638000107 seconds)
2020-10-12 19:59:51 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 19:59:51 | INFO | train | epoch 016 | loss 7.822 | nll_loss 6.771 | ppl 109.24 | wps 17322 | ups 2.8 | wpb 6195.7 | bsz 229.5 | num_updates 1808 | lr 9.04548e-05 | gnorm 1.393 | clip 0 | train_wall 31 | wall 673
2020-10-12 19:59:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 19:59:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 19:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17790.5078125Mb; avail=470882.57421875Mb
2020-10-12 19:59:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000777
2020-10-12 19:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006072
2020-10-12 19:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17790.5078125Mb; avail=470882.57421875Mb
2020-10-12 19:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000201
2020-10-12 19:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17790.5078125Mb; avail=470882.57421875Mb
2020-10-12 19:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094101
2020-10-12 19:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101259
2020-10-12 19:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17790.47265625Mb; avail=470882.453125Mb
2020-10-12 19:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17790.05078125Mb; avail=470883.0703125Mb
2020-10-12 19:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003722
2020-10-12 19:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17790.05078125Mb; avail=470883.0703125Mb
2020-10-12 19:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 19:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17790.05078125Mb; avail=470883.0703125Mb
2020-10-12 19:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088770
2020-10-12 19:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093438
2020-10-12 19:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17790.546875Mb; avail=470882.68359375Mb
2020-10-12 19:59:51 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 20:00:17 | INFO | train_inner | epoch 017:     92 / 113 loss=7.753, nll_loss=6.693, ppl=103.45, wps=16799.3, ups=2.73, wpb=6153.3, bsz=225.2, num_updates=1900, lr=9.50525e-05, gnorm=1.288, clip=0, train_wall=28, wall=700
2020-10-12 20:00:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:00:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18372.37890625Mb; avail=470300.79296875Mb
2020-10-12 20:00:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001709
2020-10-12 20:00:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18372.37890625Mb; avail=470300.79296875Mb
2020-10-12 20:00:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083660
2020-10-12 20:00:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18372.46875Mb; avail=470300.79296875Mb
2020-10-12 20:00:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055728
2020-10-12 20:00:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142100
2020-10-12 20:00:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18372.54296875Mb; avail=470300.79296875Mb
2020-10-12 20:00:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18372.54296875Mb; avail=470300.79296875Mb
2020-10-12 20:00:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001274
2020-10-12 20:00:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18372.54296875Mb; avail=470300.79296875Mb
2020-10-12 20:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081240
2020-10-12 20:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18372.484375Mb; avail=470300.8984375Mb
2020-10-12 20:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054787
2020-10-12 20:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138097
2020-10-12 20:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18372.296875Mb; avail=470300.8984375Mb
2020-10-12 20:00:26 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.655 | nll_loss 6.533 | ppl 92.58 | wps 57690.2 | wpb 2270.2 | bsz 84.4 | num_updates 1921 | best_loss 7.655
2020-10-12 20:00:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:00:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 17 @ 1921 updates, score 7.655) (writing took 4.82676282400007 seconds)
2020-10-12 20:00:31 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 20:00:31 | INFO | train | epoch 017 | loss 7.727 | nll_loss 6.663 | ppl 101.32 | wps 17357.1 | ups 2.8 | wpb 6195.7 | bsz 229.5 | num_updates 1921 | lr 9.6102e-05 | gnorm 1.312 | clip 0 | train_wall 31 | wall 714
2020-10-12 20:00:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 20:00:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 20:00:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17813.1015625Mb; avail=470860.0625Mb
2020-10-12 20:00:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000832
2020-10-12 20:00:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006208
2020-10-12 20:00:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17812.609375Mb; avail=470860.5546875Mb
2020-10-12 20:00:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000259
2020-10-12 20:00:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17812.609375Mb; avail=470860.5546875Mb
2020-10-12 20:00:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094487
2020-10-12 20:00:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101810
2020-10-12 20:00:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.296875Mb; avail=470859.828125Mb
2020-10-12 20:00:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17813.15234375Mb; avail=470859.94921875Mb
2020-10-12 20:00:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003679
2020-10-12 20:00:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.15234375Mb; avail=470859.94921875Mb
2020-10-12 20:00:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 20:00:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.15234375Mb; avail=470859.94921875Mb
2020-10-12 20:00:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088600
2020-10-12 20:00:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093240
2020-10-12 20:00:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.0Mb; avail=470860.3125Mb
2020-10-12 20:00:31 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 20:00:54 | INFO | train_inner | epoch 018:     79 / 113 loss=7.635, nll_loss=6.558, ppl=94.19, wps=17298.9, ups=2.74, wpb=6302.1, bsz=228.4, num_updates=2000, lr=0.00010005, gnorm=1.309, clip=0, train_wall=27, wall=736
2020-10-12 20:01:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18428.40625Mb; avail=470244.5703125Mb
2020-10-12 20:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001881
2020-10-12 20:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18428.01953125Mb; avail=470244.1171875Mb
2020-10-12 20:01:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.084207
2020-10-12 20:01:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18422.0078125Mb; avail=470250.62109375Mb
2020-10-12 20:01:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057570
2020-10-12 20:01:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.144711
2020-10-12 20:01:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18422.1015625Mb; avail=470250.7421875Mb
2020-10-12 20:01:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18422.0625Mb; avail=470250.984375Mb
2020-10-12 20:01:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001255
2020-10-12 20:01:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18422.0625Mb; avail=470250.984375Mb
2020-10-12 20:01:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082122
2020-10-12 20:01:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18421.89453125Mb; avail=470250.7421875Mb
2020-10-12 20:01:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056380
2020-10-12 20:01:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140734
2020-10-12 20:01:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18421.91015625Mb; avail=470250.74609375Mb
2020-10-12 20:01:06 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.562 | nll_loss 6.424 | ppl 85.84 | wps 57958.1 | wpb 2270.2 | bsz 84.4 | num_updates 2034 | best_loss 7.562
2020-10-12 20:01:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:01:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 18 @ 2034 updates, score 7.562) (writing took 5.039816172999963 seconds)
2020-10-12 20:01:11 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 20:01:11 | INFO | train | epoch 018 | loss 7.636 | nll_loss 6.559 | ppl 94.28 | wps 17339.6 | ups 2.8 | wpb 6195.7 | bsz 229.5 | num_updates 2034 | lr 0.000101749 | gnorm 1.281 | clip 0 | train_wall 31 | wall 754
2020-10-12 20:01:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 20:01:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 20:01:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17841.3125Mb; avail=470831.609375Mb
2020-10-12 20:01:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001099
2020-10-12 20:01:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006200
2020-10-12 20:01:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.91796875Mb; avail=470831.00390625Mb
2020-10-12 20:01:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000197
2020-10-12 20:01:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.91796875Mb; avail=470831.00390625Mb
2020-10-12 20:01:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093871
2020-10-12 20:01:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101144
2020-10-12 20:01:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.08203125Mb; avail=470830.8828125Mb
2020-10-12 20:01:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17842.01953125Mb; avail=470830.76171875Mb
2020-10-12 20:01:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003659
2020-10-12 20:01:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.01953125Mb; avail=470830.76171875Mb
2020-10-12 20:01:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 20:01:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.01953125Mb; avail=470830.76171875Mb
2020-10-12 20:01:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088554
2020-10-12 20:01:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093144
2020-10-12 20:01:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.3046875Mb; avail=470830.640625Mb
2020-10-12 20:01:12 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 20:01:31 | INFO | train_inner | epoch 019:     66 / 113 loss=7.587, nll_loss=6.503, ppl=90.72, wps=16563.6, ups=2.72, wpb=6093.3, bsz=235.3, num_updates=2100, lr=0.000105048, gnorm=1.312, clip=0, train_wall=27, wall=773
2020-10-12 20:01:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:01:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18397.80859375Mb; avail=470275.6015625Mb
2020-10-12 20:01:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002078
2020-10-12 20:01:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18397.80859375Mb; avail=470275.6015625Mb
2020-10-12 20:01:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082280
2020-10-12 20:01:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18397.80859375Mb; avail=470275.6015625Mb
2020-10-12 20:01:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056165
2020-10-12 20:01:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141492
2020-10-12 20:01:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18397.2890625Mb; avail=470275.97265625Mb
2020-10-12 20:01:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18397.86328125Mb; avail=470275.48828125Mb
2020-10-12 20:01:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001335
2020-10-12 20:01:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18397.86328125Mb; avail=470275.48828125Mb
2020-10-12 20:01:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081777
2020-10-12 20:01:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18397.79296875Mb; avail=470275.73046875Mb
2020-10-12 20:01:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056323
2020-10-12 20:01:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140266
2020-10-12 20:01:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18397.70703125Mb; avail=470275.73046875Mb
2020-10-12 20:01:47 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.504 | nll_loss 6.344 | ppl 81.22 | wps 57980.5 | wpb 2270.2 | bsz 84.4 | num_updates 2147 | best_loss 7.504
2020-10-12 20:01:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:01:52 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 19 @ 2147 updates, score 7.504) (writing took 4.668447287000163 seconds)
2020-10-12 20:01:52 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 20:01:52 | INFO | train | epoch 019 | loss 7.555 | nll_loss 6.465 | ppl 88.35 | wps 17374.9 | ups 2.8 | wpb 6195.7 | bsz 229.5 | num_updates 2147 | lr 0.000107396 | gnorm 1.322 | clip 0 | train_wall 31 | wall 794
2020-10-12 20:01:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 20:01:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 20:01:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17815.51171875Mb; avail=470858.1171875Mb
2020-10-12 20:01:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001044
2020-10-12 20:01:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006615
2020-10-12 20:01:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17815.51171875Mb; avail=470858.1171875Mb
2020-10-12 20:01:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000238
2020-10-12 20:01:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17815.51171875Mb; avail=470858.1171875Mb
2020-10-12 20:01:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093317
2020-10-12 20:01:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101195
2020-10-12 20:01:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17815.12109375Mb; avail=470858.375Mb
2020-10-12 20:01:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17814.77734375Mb; avail=470858.50390625Mb
2020-10-12 20:01:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003690
2020-10-12 20:01:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17814.80859375Mb; avail=470858.3828125Mb
2020-10-12 20:01:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 20:01:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17814.80859375Mb; avail=470858.3828125Mb
2020-10-12 20:01:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088433
2020-10-12 20:01:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093039
2020-10-12 20:01:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17814.765625Mb; avail=470858.3671875Mb
2020-10-12 20:01:52 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 20:02:07 | INFO | train_inner | epoch 020:     53 / 113 loss=7.505, nll_loss=6.408, ppl=84.93, wps=16766, ups=2.72, wpb=6173.2, bsz=225.2, num_updates=2200, lr=0.000110045, gnorm=1.307, clip=0, train_wall=28, wall=810
2020-10-12 20:02:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:02:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18394.20703125Mb; avail=470278.7890625Mb
2020-10-12 20:02:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001793
2020-10-12 20:02:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18394.20703125Mb; avail=470278.7890625Mb
2020-10-12 20:02:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083323
2020-10-12 20:02:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18394.2734375Mb; avail=470278.515625Mb
2020-10-12 20:02:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056067
2020-10-12 20:02:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142002
2020-10-12 20:02:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18394.37109375Mb; avail=470278.63671875Mb
2020-10-12 20:02:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18394.25Mb; avail=470278.7421875Mb
2020-10-12 20:02:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001229
2020-10-12 20:02:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18394.265625Mb; avail=470278.7421875Mb
2020-10-12 20:02:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083143
2020-10-12 20:02:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18394.12109375Mb; avail=470278.8828125Mb
2020-10-12 20:02:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056835
2020-10-12 20:02:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142030
2020-10-12 20:02:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18394.12109375Mb; avail=470278.8828125Mb
2020-10-12 20:02:27 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.455 | nll_loss 6.282 | ppl 77.82 | wps 57875.7 | wpb 2270.2 | bsz 84.4 | num_updates 2260 | best_loss 7.455
2020-10-12 20:02:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:02:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 20 @ 2260 updates, score 7.455) (writing took 4.467958354000075 seconds)
2020-10-12 20:02:32 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 20:02:32 | INFO | train | epoch 020 | loss 7.47 | nll_loss 6.368 | ppl 82.61 | wps 17443.3 | ups 2.82 | wpb 6195.7 | bsz 229.5 | num_updates 2260 | lr 0.000113044 | gnorm 1.271 | clip 0 | train_wall 31 | wall 834
2020-10-12 20:02:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 20:02:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 20:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17793.29296875Mb; avail=470878.58203125Mb
2020-10-12 20:02:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000871
2020-10-12 20:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005560
2020-10-12 20:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17792.80078125Mb; avail=470879.07421875Mb
2020-10-12 20:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000210
2020-10-12 20:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17792.80078125Mb; avail=470879.07421875Mb
2020-10-12 20:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088526
2020-10-12 20:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095049
2020-10-12 20:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17792.96875Mb; avail=470880.03125Mb
2020-10-12 20:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17792.96875Mb; avail=470880.03125Mb
2020-10-12 20:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003713
2020-10-12 20:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17792.96875Mb; avail=470880.03125Mb
2020-10-12 20:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-12 20:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17792.96875Mb; avail=470880.03125Mb
2020-10-12 20:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088942
2020-10-12 20:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093570
2020-10-12 20:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17793.0546875Mb; avail=470879.546875Mb
2020-10-12 20:02:32 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 20:02:44 | INFO | train_inner | epoch 021:     40 / 113 loss=7.456, nll_loss=6.353, ppl=81.74, wps=17166.9, ups=2.76, wpb=6214, bsz=231.6, num_updates=2300, lr=0.000115043, gnorm=1.246, clip=0, train_wall=27, wall=846
2020-10-12 20:03:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18396.52734375Mb; avail=470276.72265625Mb
2020-10-12 20:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001806
2020-10-12 20:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18396.52734375Mb; avail=470276.72265625Mb
2020-10-12 20:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080960
2020-10-12 20:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18396.6015625Mb; avail=470276.359375Mb
2020-10-12 20:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056093
2020-10-12 20:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139862
2020-10-12 20:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18396.55078125Mb; avail=470276.72265625Mb
2020-10-12 20:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18396.55078125Mb; avail=470276.72265625Mb
2020-10-12 20:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001255
2020-10-12 20:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18396.55078125Mb; avail=470276.72265625Mb
2020-10-12 20:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083609
2020-10-12 20:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18396.5625Mb; avail=470276.48046875Mb
2020-10-12 20:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056280
2020-10-12 20:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142102
2020-10-12 20:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18396.3984375Mb; avail=470276.84375Mb
2020-10-12 20:03:07 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.423 | nll_loss 6.248 | ppl 75.99 | wps 56027.9 | wpb 2270.2 | bsz 84.4 | num_updates 2373 | best_loss 7.423
2020-10-12 20:03:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:03:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 21 @ 2373 updates, score 7.423) (writing took 4.7588370290004605 seconds)
2020-10-12 20:03:12 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 20:03:12 | INFO | train | epoch 021 | loss 7.414 | nll_loss 6.302 | ppl 78.91 | wps 17390.4 | ups 2.81 | wpb 6195.7 | bsz 229.5 | num_updates 2373 | lr 0.000118691 | gnorm 1.398 | clip 0 | train_wall 31 | wall 875
2020-10-12 20:03:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 20:03:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 20:03:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17855.0390625Mb; avail=470817.48828125Mb
2020-10-12 20:03:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000922
2020-10-12 20:03:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006073
2020-10-12 20:03:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17855.0390625Mb; avail=470817.48828125Mb
2020-10-12 20:03:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000206
2020-10-12 20:03:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17855.0390625Mb; avail=470817.48828125Mb
2020-10-12 20:03:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094601
2020-10-12 20:03:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101928
2020-10-12 20:03:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.96875Mb; avail=470817.73046875Mb
2020-10-12 20:03:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17855.10546875Mb; avail=470817.609375Mb
2020-10-12 20:03:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003718
2020-10-12 20:03:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17855.10546875Mb; avail=470817.609375Mb
2020-10-12 20:03:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 20:03:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17855.10546875Mb; avail=470817.609375Mb
2020-10-12 20:03:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089979
2020-10-12 20:03:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094615
2020-10-12 20:03:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17855.05859375Mb; avail=470817.515625Mb
2020-10-12 20:03:12 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 20:03:20 | INFO | train_inner | epoch 022:     27 / 113 loss=7.383, nll_loss=6.265, ppl=76.91, wps=17048.1, ups=2.74, wpb=6229.9, bsz=226.4, num_updates=2400, lr=0.00012004, gnorm=1.416, clip=0, train_wall=27, wall=883
2020-10-12 20:03:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18398.58203125Mb; avail=470274.33984375Mb
2020-10-12 20:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001773
2020-10-12 20:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18398.58203125Mb; avail=470274.21875Mb
2020-10-12 20:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081827
2020-10-12 20:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18398.66015625Mb; avail=470274.33984375Mb
2020-10-12 20:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054997
2020-10-12 20:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139405
2020-10-12 20:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18398.65625Mb; avail=470274.36328125Mb
2020-10-12 20:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18398.5859375Mb; avail=470274.36328125Mb
2020-10-12 20:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001199
2020-10-12 20:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18398.5859375Mb; avail=470274.36328125Mb
2020-10-12 20:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083170
2020-10-12 20:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18398.62890625Mb; avail=470274.484375Mb
2020-10-12 20:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055427
2020-10-12 20:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140582
2020-10-12 20:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18398.75Mb; avail=470274.484375Mb
2020-10-12 20:03:48 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.353 | nll_loss 6.171 | ppl 72.07 | wps 55830.9 | wpb 2270.2 | bsz 84.4 | num_updates 2486 | best_loss 7.353
2020-10-12 20:03:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:03:52 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 22 @ 2486 updates, score 7.353) (writing took 4.594600429999446 seconds)
2020-10-12 20:03:52 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 20:03:52 | INFO | train | epoch 022 | loss 7.318 | nll_loss 6.194 | ppl 73.19 | wps 17344.3 | ups 2.8 | wpb 6195.7 | bsz 229.5 | num_updates 2486 | lr 0.000124338 | gnorm 1.248 | clip 0 | train_wall 31 | wall 915
2020-10-12 20:03:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 20:03:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 20:03:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17839.75390625Mb; avail=470833.296875Mb
2020-10-12 20:03:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000861
2020-10-12 20:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005968
2020-10-12 20:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.75390625Mb; avail=470833.296875Mb
2020-10-12 20:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000350
2020-10-12 20:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.75390625Mb; avail=470833.296875Mb
2020-10-12 20:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.140717
2020-10-12 20:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.148318
2020-10-12 20:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.74609375Mb; avail=470833.296875Mb
2020-10-12 20:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17839.68359375Mb; avail=470832.93359375Mb
2020-10-12 20:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003749
2020-10-12 20:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.68359375Mb; avail=470832.93359375Mb
2020-10-12 20:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-12 20:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.68359375Mb; avail=470832.93359375Mb
2020-10-12 20:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.133492
2020-10-12 20:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.138208
2020-10-12 20:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.64453125Mb; avail=470833.296875Mb
2020-10-12 20:03:53 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 20:03:57 | INFO | train_inner | epoch 023:     14 / 113 loss=7.292, nll_loss=6.164, ppl=71.71, wps=16776.8, ups=2.72, wpb=6165.1, bsz=226.6, num_updates=2500, lr=0.000125037, gnorm=1.228, clip=0, train_wall=28, wall=919
2020-10-12 20:04:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:04:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19050.51953125Mb; avail=469622.98828125Mb
2020-10-12 20:04:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001874
2020-10-12 20:04:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19050.51953125Mb; avail=469622.98828125Mb
2020-10-12 20:04:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.087452
2020-10-12 20:04:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19050.51171875Mb; avail=469622.87109375Mb
2020-10-12 20:04:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056860
2020-10-12 20:04:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.147073
2020-10-12 20:04:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19050.5859375Mb; avail=469622.87109375Mb
2020-10-12 20:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19050.70703125Mb; avail=469622.75Mb
2020-10-12 20:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001235
2020-10-12 20:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19050.70703125Mb; avail=469622.75Mb
2020-10-12 20:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083087
2020-10-12 20:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19050.73828125Mb; avail=469622.7578125Mb
2020-10-12 20:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056646
2020-10-12 20:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141836
2020-10-12 20:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19051.0703125Mb; avail=469622.515625Mb
2020-10-12 20:04:28 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.34 | nll_loss 6.156 | ppl 71.29 | wps 56253.3 | wpb 2270.2 | bsz 84.4 | num_updates 2599 | best_loss 7.34
2020-10-12 20:04:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:04:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 23 @ 2599 updates, score 7.34) (writing took 13.634345047000352 seconds)
2020-10-12 20:04:42 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 20:04:42 | INFO | train | epoch 023 | loss 7.245 | nll_loss 6.109 | ppl 69.04 | wps 14129.5 | ups 2.28 | wpb 6195.7 | bsz 229.5 | num_updates 2599 | lr 0.000129985 | gnorm 1.336 | clip 0 | train_wall 31 | wall 965
2020-10-12 20:04:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 20:04:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18085.41796875Mb; avail=470587.5234375Mb
2020-10-12 20:04:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000663
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005066
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18085.41796875Mb; avail=470587.5234375Mb
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18085.41796875Mb; avail=470587.5234375Mb
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092419
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098482
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18085.78125Mb; avail=470587.16015625Mb
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18085.65625Mb; avail=470587.26953125Mb
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004606
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18085.65625Mb; avail=470587.26953125Mb
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000223
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18085.65625Mb; avail=470587.26953125Mb
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088769
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094371
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18085.87890625Mb; avail=470587.02734375Mb
2020-10-12 20:04:42 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 20:04:43 | INFO | train_inner | epoch 024:      1 / 113 loss=7.255, nll_loss=6.12, ppl=69.57, wps=13578.4, ups=2.19, wpb=6207.3, bsz=235.3, num_updates=2600, lr=0.000130035, gnorm=1.368, clip=0, train_wall=28, wall=965
2020-10-12 20:05:11 | INFO | train_inner | epoch 024:    101 / 113 loss=7.132, nll_loss=5.98, ppl=63.11, wps=22066.3, ups=3.51, wpb=6285.6, bsz=240.3, num_updates=2700, lr=0.000135032, gnorm=1.264, clip=0, train_wall=27, wall=994
2020-10-12 20:05:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:05:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18674.0Mb; avail=469999.04296875Mb
2020-10-12 20:05:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001830
2020-10-12 20:05:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18673.9609375Mb; avail=469999.28515625Mb
2020-10-12 20:05:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.085184
2020-10-12 20:05:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18674.03125Mb; avail=469998.921875Mb
2020-10-12 20:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055728
2020-10-12 20:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.143574
2020-10-12 20:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18674.109375Mb; avail=469999.04296875Mb
2020-10-12 20:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18674.109375Mb; avail=469999.04296875Mb
2020-10-12 20:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001211
2020-10-12 20:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18674.109375Mb; avail=469999.04296875Mb
2020-10-12 20:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.084397
2020-10-12 20:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18674.0703125Mb; avail=469999.0546875Mb
2020-10-12 20:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056136
2020-10-12 20:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142589
2020-10-12 20:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18674.09375Mb; avail=469999.0546875Mb
2020-10-12 20:05:17 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.268 | nll_loss 6.071 | ppl 67.22 | wps 55521.3 | wpb 2270.2 | bsz 84.4 | num_updates 2712 | best_loss 7.268
2020-10-12 20:05:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:05:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 24 @ 2712 updates, score 7.268) (writing took 4.504173651000201 seconds)
2020-10-12 20:05:22 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 20:05:22 | INFO | train | epoch 024 | loss 7.155 | nll_loss 6.005 | ppl 64.23 | wps 17534.7 | ups 2.83 | wpb 6195.7 | bsz 229.5 | num_updates 2712 | lr 0.000135632 | gnorm 1.272 | clip 0 | train_wall 31 | wall 1004
2020-10-12 20:05:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 20:05:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17834.0703125Mb; avail=470839.4453125Mb
2020-10-12 20:05:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001050
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006213
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.0703125Mb; avail=470839.4453125Mb
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.0703125Mb; avail=470839.4453125Mb
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092686
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099914
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.8515625Mb; avail=470834.1171875Mb
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17838.953125Mb; avail=470833.99609375Mb
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003899
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.96875Mb; avail=470833.875Mb
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000223
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.96875Mb; avail=470833.875Mb
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088747
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093651
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.26171875Mb; avail=470833.75390625Mb
2020-10-12 20:05:22 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 20:05:48 | INFO | train_inner | epoch 025:     88 / 113 loss=7.131, nll_loss=5.977, ppl=63, wps=16853.4, ups=2.74, wpb=6142.3, bsz=216.8, num_updates=2800, lr=0.00014003, gnorm=1.29, clip=0, train_wall=27, wall=1030
2020-10-12 20:05:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18398.4921875Mb; avail=470274.8046875Mb
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001799
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18398.4921875Mb; avail=470274.8046875Mb
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083469
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18398.5078125Mb; avail=470274.68359375Mb
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056084
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142199
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18384.234375Mb; avail=470288.95703125Mb
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18378.25390625Mb; avail=470294.9375Mb
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001350
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18378.25390625Mb; avail=470294.9375Mb
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082701
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18377.57421875Mb; avail=470295.9453125Mb
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056087
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140931
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18377.59765625Mb; avail=470295.82421875Mb
2020-10-12 20:05:58 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.277 | nll_loss 6.073 | ppl 67.34 | wps 57588.5 | wpb 2270.2 | bsz 84.4 | num_updates 2825 | best_loss 7.268
2020-10-12 20:05:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:06:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_last.pt (epoch 25 @ 2825 updates, score 7.277) (writing took 6.143480916999579 seconds)
2020-10-12 20:06:04 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 20:06:04 | INFO | train | epoch 025 | loss 7.09 | nll_loss 5.93 | ppl 60.97 | wps 16743.2 | ups 2.7 | wpb 6195.7 | bsz 229.5 | num_updates 2825 | lr 0.000141279 | gnorm 1.263 | clip 0 | train_wall 31 | wall 1046
2020-10-12 20:06:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 20:06:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 20:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18377.484375Mb; avail=470296.64453125Mb
2020-10-12 20:06:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000755
2020-10-12 20:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005433
2020-10-12 20:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18377.484375Mb; avail=470296.64453125Mb
2020-10-12 20:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000213
2020-10-12 20:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18377.484375Mb; avail=470296.64453125Mb
2020-10-12 20:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088499
2020-10-12 20:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094930
2020-10-12 20:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18376.6171875Mb; avail=470297.4921875Mb
2020-10-12 20:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18376.640625Mb; avail=470297.0078125Mb
2020-10-12 20:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003800
2020-10-12 20:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18376.640625Mb; avail=470297.0078125Mb
2020-10-12 20:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 20:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18376.640625Mb; avail=470297.0078125Mb
2020-10-12 20:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088024
2020-10-12 20:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.092767
2020-10-12 20:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18376.54296875Mb; avail=470297.1015625Mb
2020-10-12 20:06:04 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 20:06:25 | INFO | train_inner | epoch 026:     75 / 113 loss=7.033, nll_loss=5.864, ppl=58.24, wps=16429.7, ups=2.64, wpb=6222.9, bsz=227.9, num_updates=2900, lr=0.000145028, gnorm=1.252, clip=0, train_wall=27, wall=1068
2020-10-12 20:06:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17842.01953125Mb; avail=470830.93359375Mb
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002342
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.01953125Mb; avail=470830.93359375Mb
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081755
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.234375Mb; avail=470830.69140625Mb
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057008
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141930
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17848.4921875Mb; avail=470824.14453125Mb
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17854.19140625Mb; avail=470818.4453125Mb
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001252
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.26953125Mb; avail=470818.32421875Mb
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081628
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.40625Mb; avail=470814.69140625Mb
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054257
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137902
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17859.23828125Mb; avail=470813.84375Mb
2020-10-12 20:06:39 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.275 | nll_loss 6.072 | ppl 67.3 | wps 57660.6 | wpb 2270.2 | bsz 84.4 | num_updates 2938 | best_loss 7.268
2020-10-12 20:06:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:06:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_last.pt (epoch 26 @ 2938 updates, score 7.275) (writing took 8.732142873000157 seconds)
2020-10-12 20:06:48 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 20:06:48 | INFO | train | epoch 026 | loss 7.002 | nll_loss 5.829 | ppl 56.83 | wps 15923.3 | ups 2.57 | wpb 6195.7 | bsz 229.5 | num_updates 2938 | lr 0.000146927 | gnorm 1.288 | clip 0 | train_wall 31 | wall 1090
2020-10-12 20:06:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 20:06:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 20:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18385.84765625Mb; avail=470287.84765625Mb
2020-10-12 20:06:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000958
2020-10-12 20:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007180
2020-10-12 20:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18385.84765625Mb; avail=470287.84765625Mb
2020-10-12 20:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000377
2020-10-12 20:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18385.84765625Mb; avail=470287.84765625Mb
2020-10-12 20:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100520
2020-10-12 20:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109038
2020-10-12 20:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18384.86328125Mb; avail=470288.83203125Mb
2020-10-12 20:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18384.58984375Mb; avail=470289.01953125Mb
2020-10-12 20:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004217
2020-10-12 20:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18384.58984375Mb; avail=470289.01953125Mb
2020-10-12 20:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18384.58984375Mb; avail=470289.01953125Mb
2020-10-12 20:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100206
2020-10-12 20:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105495
2020-10-12 20:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18384.60546875Mb; avail=470289.01953125Mb
2020-10-12 20:06:48 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 20:07:06 | INFO | train_inner | epoch 027:     62 / 113 loss=6.975, nll_loss=5.797, ppl=55.58, wps=15270.5, ups=2.45, wpb=6227, bsz=232.6, num_updates=3000, lr=0.000150025, gnorm=1.375, clip=0, train_wall=28, wall=1109
2020-10-12 20:07:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18410.39453125Mb; avail=470269.6875Mb
2020-10-12 20:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001705
2020-10-12 20:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18410.15625Mb; avail=470269.68359375Mb
2020-10-12 20:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081873
2020-10-12 20:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18410.58984375Mb; avail=470267.96484375Mb
2020-10-12 20:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055816
2020-10-12 20:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140215
2020-10-12 20:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18411.1640625Mb; avail=470266.953125Mb
2020-10-12 20:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18410.22265625Mb; avail=470266.40625Mb
2020-10-12 20:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001417
2020-10-12 20:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18410.44140625Mb; avail=470266.55859375Mb
2020-10-12 20:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081991
2020-10-12 20:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18411.2734375Mb; avail=470264.609375Mb
2020-10-12 20:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056462
2020-10-12 20:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140630
2020-10-12 20:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18411.0859375Mb; avail=470264.1796875Mb
2020-10-12 20:07:24 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.166 | nll_loss 5.939 | ppl 61.33 | wps 56190.9 | wpb 2270.2 | bsz 84.4 | num_updates 3051 | best_loss 7.166
2020-10-12 20:07:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:07:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 27 @ 3051 updates, score 7.166) (writing took 7.523866900999565 seconds)
2020-10-12 20:07:31 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 20:07:31 | INFO | train | epoch 027 | loss 6.949 | nll_loss 5.767 | ppl 54.44 | wps 16094.5 | ups 2.6 | wpb 6195.7 | bsz 229.5 | num_updates 3051 | lr 0.000152574 | gnorm 1.352 | clip 0 | train_wall 31 | wall 1134
2020-10-12 20:07:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 20:07:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 20:07:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18240.12890625Mb; avail=470432.8125Mb
2020-10-12 20:07:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000984
2020-10-12 20:07:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006061
2020-10-12 20:07:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18240.12890625Mb; avail=470432.8125Mb
2020-10-12 20:07:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000226
2020-10-12 20:07:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18240.12890625Mb; avail=470432.8125Mb
2020-10-12 20:07:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101457
2020-10-12 20:07:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108613
2020-10-12 20:07:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18239.28125Mb; avail=470433.62890625Mb
2020-10-12 20:07:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18239.578125Mb; avail=470433.35546875Mb
2020-10-12 20:07:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004193
2020-10-12 20:07:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18239.58984375Mb; avail=470433.234375Mb
2020-10-12 20:07:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000301
2020-10-12 20:07:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18239.58984375Mb; avail=470433.234375Mb
2020-10-12 20:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101787
2020-10-12 20:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107071
2020-10-12 20:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18239.71875Mb; avail=470433.234375Mb
2020-10-12 20:07:32 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 20:07:46 | INFO | train_inner | epoch 028:     49 / 113 loss=6.842, nll_loss=5.644, ppl=50.01, wps=15645.5, ups=2.54, wpb=6147.6, bsz=235.3, num_updates=3100, lr=0.000155023, gnorm=1.232, clip=0, train_wall=27, wall=1148
2020-10-12 20:08:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18500.9765625Mb; avail=470171.63671875Mb
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001795
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18501.58203125Mb; avail=470171.03125Mb
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083088
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18514.49609375Mb; avail=470158.01953125Mb
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056986
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142901
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18514.93359375Mb; avail=470157.78125Mb
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18514.95703125Mb; avail=470157.66015625Mb
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001320
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18514.95703125Mb; avail=470157.66015625Mb
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081958
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18515.4921875Mb; avail=470157.14453125Mb
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056598
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140686
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18515.91015625Mb; avail=470157.51953125Mb
2020-10-12 20:08:07 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.157 | nll_loss 5.911 | ppl 60.18 | wps 57765.8 | wpb 2270.2 | bsz 84.4 | num_updates 3164 | best_loss 7.157
2020-10-12 20:08:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:08:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 28 @ 3164 updates, score 7.157) (writing took 12.153331402000731 seconds)
2020-10-12 20:08:19 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 20:08:19 | INFO | train | epoch 028 | loss 6.844 | nll_loss 5.646 | ppl 50.08 | wps 14738.9 | ups 2.38 | wpb 6195.7 | bsz 229.5 | num_updates 3164 | lr 0.000158221 | gnorm 1.243 | clip 0 | train_wall 31 | wall 1181
2020-10-12 20:08:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 20:08:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 20:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17820.46484375Mb; avail=470851.71484375Mb
2020-10-12 20:08:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000948
2020-10-12 20:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006106
2020-10-12 20:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.4921875Mb; avail=470851.59375Mb
2020-10-12 20:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000271
2020-10-12 20:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.4921875Mb; avail=470851.59375Mb
2020-10-12 20:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093264
2020-10-12 20:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100466
2020-10-12 20:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17819.625Mb; avail=470853.15625Mb
2020-10-12 20:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17819.95703125Mb; avail=470853.03515625Mb
2020-10-12 20:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003690
2020-10-12 20:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17819.95703125Mb; avail=470853.03515625Mb
2020-10-12 20:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000253
2020-10-12 20:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17819.95703125Mb; avail=470853.03515625Mb
2020-10-12 20:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088738
2020-10-12 20:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093424
2020-10-12 20:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17819.796875Mb; avail=470853.3984375Mb
2020-10-12 20:08:19 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 20:08:29 | INFO | train_inner | epoch 029:     36 / 113 loss=6.842, nll_loss=5.643, ppl=49.99, wps=14049.3, ups=2.28, wpb=6162.6, bsz=227.9, num_updates=3200, lr=0.00016002, gnorm=1.314, clip=0, train_wall=27, wall=1192
2020-10-12 20:08:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18369.62890625Mb; avail=470303.015625Mb
2020-10-12 20:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001871
2020-10-12 20:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18369.62890625Mb; avail=470303.015625Mb
2020-10-12 20:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082372
2020-10-12 20:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18363.984375Mb; avail=470308.96875Mb
2020-10-12 20:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056258
2020-10-12 20:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141542
2020-10-12 20:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18363.8671875Mb; avail=470308.875Mb
2020-10-12 20:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18363.765625Mb; avail=470308.875Mb
2020-10-12 20:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001301
2020-10-12 20:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18363.765625Mb; avail=470308.875Mb
2020-10-12 20:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082856
2020-10-12 20:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18363.76171875Mb; avail=470309.05859375Mb
2020-10-12 20:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056860
2020-10-12 20:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142000
2020-10-12 20:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18364.12109375Mb; avail=470308.6953125Mb
2020-10-12 20:08:54 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.109 | nll_loss 5.875 | ppl 58.7 | wps 57127.1 | wpb 2270.2 | bsz 84.4 | num_updates 3277 | best_loss 7.109
2020-10-12 20:08:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:08:59 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 29 @ 3277 updates, score 7.109) (writing took 4.817493307999939 seconds)
2020-10-12 20:08:59 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 20:08:59 | INFO | train | epoch 029 | loss 6.764 | nll_loss 5.554 | ppl 46.98 | wps 17382.9 | ups 2.81 | wpb 6195.7 | bsz 229.5 | num_updates 3277 | lr 0.000163868 | gnorm 1.293 | clip 0 | train_wall 31 | wall 1222
2020-10-12 20:08:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 20:08:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 20:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18387.8828125Mb; avail=470285.06640625Mb
2020-10-12 20:08:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000981
2020-10-12 20:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006390
2020-10-12 20:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18394.54296875Mb; avail=470278.40625Mb
2020-10-12 20:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000318
2020-10-12 20:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18395.1484375Mb; avail=470277.1953125Mb
2020-10-12 20:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094627
2020-10-12 20:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102168
2020-10-12 20:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18430.00390625Mb; avail=470248.70703125Mb
2020-10-12 20:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18430.03125Mb; avail=470250.30859375Mb
2020-10-12 20:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003728
2020-10-12 20:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18430.61328125Mb; avail=470249.7265625Mb
2020-10-12 20:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000206
2020-10-12 20:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18430.61328125Mb; avail=470249.7265625Mb
2020-10-12 20:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089141
2020-10-12 20:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093859
2020-10-12 20:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18431.640625Mb; avail=470248.38671875Mb
2020-10-12 20:08:59 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 20:09:06 | INFO | train_inner | epoch 030:     23 / 113 loss=6.739, nll_loss=5.525, ppl=46.03, wps=16752.5, ups=2.73, wpb=6127.3, bsz=225.6, num_updates=3300, lr=0.000165018, gnorm=1.298, clip=0, train_wall=27, wall=1228
2020-10-12 20:09:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17824.9375Mb; avail=470847.890625Mb
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001874
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.9375Mb; avail=470847.890625Mb
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081516
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17825.34375Mb; avail=470847.04296875Mb
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054601
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138786
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17825.28515625Mb; avail=470847.28515625Mb
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17825.28515625Mb; avail=470847.28515625Mb
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001213
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17825.28515625Mb; avail=470847.28515625Mb
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082553
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.1640625Mb; avail=470846.07421875Mb
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054825
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139384
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.01953125Mb; avail=470846.31640625Mb
2020-10-12 20:09:35 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.095 | nll_loss 5.842 | ppl 57.34 | wps 58038.2 | wpb 2270.2 | bsz 84.4 | num_updates 3390 | best_loss 7.095
2020-10-12 20:09:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:09:39 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 30 @ 3390 updates, score 7.095) (writing took 4.447902003999843 seconds)
2020-10-12 20:09:39 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 20:09:39 | INFO | train | epoch 030 | loss 6.693 | nll_loss 5.471 | ppl 44.36 | wps 17351 | ups 2.8 | wpb 6195.7 | bsz 229.5 | num_updates 3390 | lr 0.000169515 | gnorm 1.35 | clip 0 | train_wall 31 | wall 1262
2020-10-12 20:09:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 20:09:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 20:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17830.375Mb; avail=470842.453125Mb
2020-10-12 20:09:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000919
2020-10-12 20:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006577
2020-10-12 20:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17830.3359375Mb; avail=470842.6953125Mb
2020-10-12 20:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000301
2020-10-12 20:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17830.3359375Mb; avail=470842.6953125Mb
2020-10-12 20:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093718
2020-10-12 20:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101476
2020-10-12 20:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17830.19921875Mb; avail=470842.6953125Mb
2020-10-12 20:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17829.68359375Mb; avail=470843.4296875Mb
2020-10-12 20:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003733
2020-10-12 20:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17829.68359375Mb; avail=470843.4296875Mb
2020-10-12 20:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000218
2020-10-12 20:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17829.68359375Mb; avail=470843.4296875Mb
2020-10-12 20:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088621
2020-10-12 20:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093341
2020-10-12 20:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17829.63671875Mb; avail=470843.671875Mb
2020-10-12 20:09:40 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 20:09:43 | INFO | train_inner | epoch 031:     10 / 113 loss=6.69, nll_loss=5.467, ppl=44.22, wps=17162.3, ups=2.74, wpb=6270.8, bsz=235, num_updates=3400, lr=0.000170015, gnorm=1.329, clip=0, train_wall=28, wall=1265
2020-10-12 20:10:11 | INFO | train_inner | epoch 031:    110 / 113 loss=6.589, nll_loss=5.35, ppl=40.79, wps=21865.9, ups=3.52, wpb=6206, bsz=227, num_updates=3500, lr=0.000175013, gnorm=1.264, clip=0, train_wall=27, wall=1293
2020-10-12 20:10:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18097.69921875Mb; avail=470574.859375Mb
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001844
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18097.69921875Mb; avail=470574.859375Mb
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083146
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18104.41015625Mb; avail=470568.55859375Mb
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055841
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141764
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18104.3515625Mb; avail=470568.4375Mb
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18104.46484375Mb; avail=470568.54296875Mb
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001224
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18104.46484375Mb; avail=470568.54296875Mb
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.085426
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18104.70703125Mb; avail=470568.30078125Mb
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056839
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.144680
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18106.03125Mb; avail=470566.453125Mb
2020-10-12 20:10:15 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.048 | nll_loss 5.796 | ppl 55.57 | wps 56953.2 | wpb 2270.2 | bsz 84.4 | num_updates 3503 | best_loss 7.048
2020-10-12 20:10:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:10:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 31 @ 3503 updates, score 7.048) (writing took 4.439566146000288 seconds)
2020-10-12 20:10:19 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 20:10:19 | INFO | train | epoch 031 | loss 6.598 | nll_loss 5.361 | ppl 41.1 | wps 17629.8 | ups 2.85 | wpb 6195.7 | bsz 229.5 | num_updates 3503 | lr 0.000175162 | gnorm 1.271 | clip 0 | train_wall 31 | wall 1302
2020-10-12 20:10:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 20:10:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 20:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18083.8125Mb; avail=470588.79296875Mb
2020-10-12 20:10:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000703
2020-10-12 20:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005356
2020-10-12 20:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18083.8125Mb; avail=470588.79296875Mb
2020-10-12 20:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000203
2020-10-12 20:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18083.8125Mb; avail=470588.79296875Mb
2020-10-12 20:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089549
2020-10-12 20:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095866
2020-10-12 20:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18083.7578125Mb; avail=470588.6796875Mb
2020-10-12 20:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18083.8125Mb; avail=470588.80078125Mb
2020-10-12 20:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003746
2020-10-12 20:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18083.8125Mb; avail=470588.80078125Mb
2020-10-12 20:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000221
2020-10-12 20:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18083.8125Mb; avail=470588.80078125Mb
2020-10-12 20:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088085
2020-10-12 20:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.092802
2020-10-12 20:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18083.80078125Mb; avail=470588.921875Mb
2020-10-12 20:10:19 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 20:10:47 | INFO | train_inner | epoch 032:     97 / 113 loss=6.523, nll_loss=5.274, ppl=38.7, wps=16938.4, ups=2.75, wpb=6161.9, bsz=224.2, num_updates=3600, lr=0.00018001, gnorm=1.293, clip=0, train_wall=28, wall=1330
2020-10-12 20:10:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:10:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17806.91796875Mb; avail=470865.4609375Mb
2020-10-12 20:10:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001886
2020-10-12 20:10:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17806.91796875Mb; avail=470865.4609375Mb
2020-10-12 20:10:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.102065
2020-10-12 20:10:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17806.24609375Mb; avail=470866.30859375Mb
2020-10-12 20:10:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055599
2020-10-12 20:10:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.160382
2020-10-12 20:10:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17806.2734375Mb; avail=470866.06640625Mb
2020-10-12 20:10:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17806.2734375Mb; avail=470866.06640625Mb
2020-10-12 20:10:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001157
2020-10-12 20:10:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17806.2734375Mb; avail=470866.06640625Mb
2020-10-12 20:10:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081764
2020-10-12 20:10:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17806.2734375Mb; avail=470866.06640625Mb
2020-10-12 20:10:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054338
2020-10-12 20:10:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138017
2020-10-12 20:10:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17806.28125Mb; avail=470866.06640625Mb
2020-10-12 20:10:55 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.007 | nll_loss 5.739 | ppl 53.4 | wps 57338 | wpb 2270.2 | bsz 84.4 | num_updates 3616 | best_loss 7.007
2020-10-12 20:10:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:10:59 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 32 @ 3616 updates, score 7.007) (writing took 4.447919426000226 seconds)
2020-10-12 20:10:59 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 20:10:59 | INFO | train | epoch 032 | loss 6.501 | nll_loss 5.249 | ppl 38.04 | wps 17388.7 | ups 2.81 | wpb 6195.7 | bsz 229.5 | num_updates 3616 | lr 0.00018081 | gnorm 1.275 | clip 0 | train_wall 31 | wall 1342
2020-10-12 20:10:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 20:10:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 20:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18095.109375Mb; avail=470577.26171875Mb
2020-10-12 20:10:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000713
2020-10-12 20:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005975
2020-10-12 20:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18102.98046875Mb; avail=470569.390625Mb
2020-10-12 20:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000201
2020-10-12 20:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18103.5859375Mb; avail=470568.78515625Mb
2020-10-12 20:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094557
2020-10-12 20:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101591
2020-10-12 20:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18221.68359375Mb; avail=470449.87109375Mb
2020-10-12 20:11:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18263.76953125Mb; avail=470407.98046875Mb
2020-10-12 20:11:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004578
2020-10-12 20:11:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18269.12890625Mb; avail=470403.13671875Mb
2020-10-12 20:11:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000232
2020-10-12 20:11:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18269.734375Mb; avail=470402.53125Mb
2020-10-12 20:11:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089600
2020-10-12 20:11:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095194
2020-10-12 20:11:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18374.234375Mb; avail=470297.90625Mb
2020-10-12 20:11:00 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 20:11:24 | INFO | train_inner | epoch 033:     84 / 113 loss=6.399, nll_loss=5.131, ppl=35.04, wps=17178.9, ups=2.74, wpb=6276.2, bsz=248.3, num_updates=3700, lr=0.000185008, gnorm=1.292, clip=0, train_wall=28, wall=1366
2020-10-12 20:11:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17852.7421875Mb; avail=470819.6640625Mb
2020-10-12 20:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001784
2020-10-12 20:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.7421875Mb; avail=470819.6640625Mb
2020-10-12 20:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082095
2020-10-12 20:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.4609375Mb; avail=470821.12109375Mb
2020-10-12 20:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056583
2020-10-12 20:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141315
2020-10-12 20:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.40625Mb; avail=470821.48828125Mb
2020-10-12 20:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17851.40625Mb; avail=470821.48828125Mb
2020-10-12 20:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001237
2020-10-12 20:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.40625Mb; avail=470821.48828125Mb
2020-10-12 20:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083423
2020-10-12 20:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.41796875Mb; avail=470821.3671875Mb
2020-10-12 20:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056229
2020-10-12 20:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141683
2020-10-12 20:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.48046875Mb; avail=470821.125Mb
2020-10-12 20:11:35 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.969 | nll_loss 5.681 | ppl 51.31 | wps 57967.2 | wpb 2270.2 | bsz 84.4 | num_updates 3729 | best_loss 6.969
2020-10-12 20:11:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:11:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 33 @ 3729 updates, score 6.969) (writing took 7.53062539199982 seconds)
2020-10-12 20:11:42 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 20:11:42 | INFO | train | epoch 033 | loss 6.418 | nll_loss 5.153 | ppl 35.57 | wps 16280.8 | ups 2.63 | wpb 6195.7 | bsz 229.5 | num_updates 3729 | lr 0.000186457 | gnorm 1.328 | clip 0 | train_wall 31 | wall 1385
2020-10-12 20:11:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 20:11:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 20:11:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18566.01953125Mb; avail=470106.92578125Mb
2020-10-12 20:11:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000717
2020-10-12 20:11:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006061
2020-10-12 20:11:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18566.01953125Mb; avail=470106.92578125Mb
2020-10-12 20:11:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000204
2020-10-12 20:11:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18566.01953125Mb; avail=470106.92578125Mb
2020-10-12 20:11:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099595
2020-10-12 20:11:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106747
2020-10-12 20:11:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18565.84375Mb; avail=470107.01171875Mb
2020-10-12 20:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18565.97265625Mb; avail=470106.76953125Mb
2020-10-12 20:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004172
2020-10-12 20:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18565.97265625Mb; avail=470106.76953125Mb
2020-10-12 20:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000210
2020-10-12 20:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18565.97265625Mb; avail=470106.76953125Mb
2020-10-12 20:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099989
2020-10-12 20:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105390
2020-10-12 20:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18566.234375Mb; avail=470106.2890625Mb
2020-10-12 20:11:43 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 20:12:03 | INFO | train_inner | epoch 034:     71 / 113 loss=6.363, nll_loss=5.088, ppl=34.02, wps=15749.6, ups=2.55, wpb=6178.3, bsz=224.4, num_updates=3800, lr=0.000190005, gnorm=1.318, clip=0, train_wall=27, wall=1406
2020-10-12 20:12:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:12:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17826.9296875Mb; avail=470846.3671875Mb
2020-10-12 20:12:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001705
2020-10-12 20:12:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.9296875Mb; avail=470846.3671875Mb
2020-10-12 20:12:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083803
2020-10-12 20:12:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.48046875Mb; avail=470846.48828125Mb
2020-10-12 20:12:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055735
2020-10-12 20:12:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142048
2020-10-12 20:12:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.86328125Mb; avail=470844.9140625Mb
2020-10-12 20:12:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17827.8515625Mb; avail=470845.03515625Mb
2020-10-12 20:12:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001226
2020-10-12 20:12:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.8515625Mb; avail=470845.03515625Mb
2020-10-12 20:12:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082651
2020-10-12 20:12:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.8125Mb; avail=470844.9140625Mb
2020-10-12 20:12:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056620
2020-10-12 20:12:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141331
2020-10-12 20:12:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.85546875Mb; avail=470844.671875Mb
2020-10-12 20:12:18 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.934 | nll_loss 5.635 | ppl 49.71 | wps 57818.6 | wpb 2270.2 | bsz 84.4 | num_updates 3842 | best_loss 6.934
2020-10-12 20:12:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:12:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 34 @ 3842 updates, score 6.934) (writing took 6.8993797690000065 seconds)
2020-10-12 20:12:25 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 20:12:25 | INFO | train | epoch 034 | loss 6.327 | nll_loss 5.046 | ppl 33.03 | wps 16574.9 | ups 2.68 | wpb 6195.7 | bsz 229.5 | num_updates 3842 | lr 0.000192104 | gnorm 1.315 | clip 0 | train_wall 31 | wall 1427
2020-10-12 20:12:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 20:12:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 20:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17854.1953125Mb; avail=470821.65625Mb
2020-10-12 20:12:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000760
2020-10-12 20:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005923
2020-10-12 20:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.921875Mb; avail=470821.95703125Mb
2020-10-12 20:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000249
2020-10-12 20:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.79296875Mb; avail=470821.84375Mb
2020-10-12 20:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094780
2020-10-12 20:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101806
2020-10-12 20:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.08203125Mb; avail=470820.70703125Mb
2020-10-12 20:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17853.81640625Mb; avail=470820.14453125Mb
2020-10-12 20:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003839
2020-10-12 20:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.51171875Mb; avail=470819.95703125Mb
2020-10-12 20:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 20:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.3828125Mb; avail=470819.84375Mb
2020-10-12 20:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088903
2020-10-12 20:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093689
2020-10-12 20:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.80859375Mb; avail=470819.1171875Mb
2020-10-12 20:12:25 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 20:12:41 | INFO | train_inner | epoch 035:     58 / 113 loss=6.258, nll_loss=4.966, ppl=31.26, wps=15754.4, ups=2.61, wpb=6037.5, bsz=217.1, num_updates=3900, lr=0.000195003, gnorm=1.327, clip=0, train_wall=27, wall=1444
2020-10-12 20:12:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17857.01171875Mb; avail=470815.88671875Mb
2020-10-12 20:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001825
2020-10-12 20:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.01171875Mb; avail=470815.88671875Mb
2020-10-12 20:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083024
2020-10-12 20:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.02734375Mb; avail=470815.87109375Mb
2020-10-12 20:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056014
2020-10-12 20:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141687
2020-10-12 20:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.04296875Mb; avail=470815.85546875Mb
2020-10-12 20:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17856.98828125Mb; avail=470815.91015625Mb
2020-10-12 20:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001201
2020-10-12 20:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17856.98828125Mb; avail=470815.91015625Mb
2020-10-12 20:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082785
2020-10-12 20:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.03125Mb; avail=470815.66796875Mb
2020-10-12 20:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056348
2020-10-12 20:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141130
2020-10-12 20:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.09375Mb; avail=470816.13671875Mb
2020-10-12 20:13:00 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.895 | nll_loss 5.593 | ppl 48.28 | wps 57657.4 | wpb 2270.2 | bsz 84.4 | num_updates 3955 | best_loss 6.895
2020-10-12 20:13:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:13:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 35 @ 3955 updates, score 6.895) (writing took 4.439144513000429 seconds)
2020-10-12 20:13:05 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 20:13:05 | INFO | train | epoch 035 | loss 6.234 | nll_loss 4.939 | ppl 30.67 | wps 17420.1 | ups 2.81 | wpb 6195.7 | bsz 229.5 | num_updates 3955 | lr 0.000197751 | gnorm 1.341 | clip 0 | train_wall 31 | wall 1467
2020-10-12 20:13:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 20:13:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 20:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17835.89453125Mb; avail=470836.7734375Mb
2020-10-12 20:13:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000968
2020-10-12 20:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006226
2020-10-12 20:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.79296875Mb; avail=470836.7734375Mb
2020-10-12 20:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000252
2020-10-12 20:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.79296875Mb; avail=470836.7734375Mb
2020-10-12 20:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091892
2020-10-12 20:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099316
2020-10-12 20:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.73828125Mb; avail=470838.05859375Mb
2020-10-12 20:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17834.73828125Mb; avail=470838.05859375Mb
2020-10-12 20:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003727
2020-10-12 20:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.73828125Mb; avail=470838.05859375Mb
2020-10-12 20:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 20:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.73828125Mb; avail=470838.05859375Mb
2020-10-12 20:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088971
2020-10-12 20:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093613
2020-10-12 20:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.71484375Mb; avail=470838.05859375Mb
2020-10-12 20:13:05 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 20:13:18 | INFO | train_inner | epoch 036:     45 / 113 loss=6.215, nll_loss=4.915, ppl=30.17, wps=17222.1, ups=2.76, wpb=6246.6, bsz=221.1, num_updates=4000, lr=0.0002, gnorm=1.333, clip=0, train_wall=28, wall=1480
2020-10-12 20:13:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:13:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17826.171875Mb; avail=470846.421875Mb
2020-10-12 20:13:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001928
2020-10-12 20:13:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.171875Mb; avail=470846.421875Mb
2020-10-12 20:13:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082856
2020-10-12 20:13:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.07421875Mb; avail=470846.54296875Mb
2020-10-12 20:13:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057311
2020-10-12 20:13:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142892
2020-10-12 20:13:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.19140625Mb; avail=470846.421875Mb
2020-10-12 20:13:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17826.19140625Mb; avail=470846.421875Mb
2020-10-12 20:13:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001270
2020-10-12 20:13:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.19140625Mb; avail=470846.421875Mb
2020-10-12 20:13:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081953
2020-10-12 20:13:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.0546875Mb; avail=470845.57421875Mb
2020-10-12 20:13:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056604
2020-10-12 20:13:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140587
2020-10-12 20:13:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.1171875Mb; avail=470845.6953125Mb
2020-10-12 20:13:40 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.872 | nll_loss 5.552 | ppl 46.91 | wps 57920.3 | wpb 2270.2 | bsz 84.4 | num_updates 4068 | best_loss 6.872
2020-10-12 20:13:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:13:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 36 @ 4068 updates, score 6.872) (writing took 4.438843971000097 seconds)
2020-10-12 20:13:45 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 20:13:45 | INFO | train | epoch 036 | loss 6.132 | nll_loss 4.82 | ppl 28.24 | wps 17626.7 | ups 2.85 | wpb 6195.7 | bsz 229.5 | num_updates 4068 | lr 0.000198321 | gnorm 1.302 | clip 0 | train_wall 31 | wall 1507
2020-10-12 20:13:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 20:13:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 20:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17830.1796875Mb; avail=470842.7734375Mb
2020-10-12 20:13:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001100
2020-10-12 20:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006600
2020-10-12 20:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17830.1796875Mb; avail=470842.7734375Mb
2020-10-12 20:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000249
2020-10-12 20:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17830.1796875Mb; avail=470842.7734375Mb
2020-10-12 20:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094020
2020-10-12 20:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101754
2020-10-12 20:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17829.625Mb; avail=470843.14453125Mb
2020-10-12 20:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17829.6484375Mb; avail=470843.5078125Mb
2020-10-12 20:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003766
2020-10-12 20:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17829.6484375Mb; avail=470843.5078125Mb
2020-10-12 20:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000188
2020-10-12 20:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17829.6484375Mb; avail=470843.5078125Mb
2020-10-12 20:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088420
2020-10-12 20:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093109
2020-10-12 20:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17829.7421875Mb; avail=470843.265625Mb
2020-10-12 20:13:45 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 20:13:54 | INFO | train_inner | epoch 037:     32 / 113 loss=6.114, nll_loss=4.798, ppl=27.82, wps=17238.6, ups=2.75, wpb=6268.5, bsz=238.1, num_updates=4100, lr=0.000197546, gnorm=1.3, clip=0, train_wall=28, wall=1517
2020-10-12 20:14:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17901.55078125Mb; avail=470771.33984375Mb
2020-10-12 20:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001800
2020-10-12 20:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17901.55078125Mb; avail=470771.33984375Mb
2020-10-12 20:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083562
2020-10-12 20:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17901.62890625Mb; avail=470771.2265625Mb
2020-10-12 20:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057553
2020-10-12 20:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.143714
2020-10-12 20:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17901.54296875Mb; avail=470771.10546875Mb
2020-10-12 20:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17901.60546875Mb; avail=470771.2265625Mb
2020-10-12 20:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001181
2020-10-12 20:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17901.60546875Mb; avail=470771.2265625Mb
2020-10-12 20:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081943
2020-10-12 20:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17901.484375Mb; avail=470771.34765625Mb
2020-10-12 20:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055010
2020-10-12 20:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138903
2020-10-12 20:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17901.59375Mb; avail=470771.10546875Mb
2020-10-12 20:14:20 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.857 | nll_loss 5.522 | ppl 45.95 | wps 58051.3 | wpb 2270.2 | bsz 84.4 | num_updates 4181 | best_loss 6.857
2020-10-12 20:14:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:14:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 37 @ 4181 updates, score 6.857) (writing took 4.446407751000152 seconds)
2020-10-12 20:14:25 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 20:14:25 | INFO | train | epoch 037 | loss 6.035 | nll_loss 4.707 | ppl 26.12 | wps 17500.3 | ups 2.82 | wpb 6195.7 | bsz 229.5 | num_updates 4181 | lr 0.000195623 | gnorm 1.293 | clip 0 | train_wall 31 | wall 1547
2020-10-12 20:14:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 20:14:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 20:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17900.09375Mb; avail=470772.39453125Mb
2020-10-12 20:14:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000904
2020-10-12 20:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006217
2020-10-12 20:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17900.09375Mb; avail=470772.39453125Mb
2020-10-12 20:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000202
2020-10-12 20:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17900.09375Mb; avail=470772.39453125Mb
2020-10-12 20:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094823
2020-10-12 20:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102294
2020-10-12 20:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17900.05078125Mb; avail=470772.63671875Mb
2020-10-12 20:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17900.125Mb; avail=470772.2734375Mb
2020-10-12 20:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003712
2020-10-12 20:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17900.140625Mb; avail=470772.15234375Mb
2020-10-12 20:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000200
2020-10-12 20:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17900.140625Mb; avail=470772.15234375Mb
2020-10-12 20:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088056
2020-10-12 20:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.092741
2020-10-12 20:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17900.0859375Mb; avail=470772.046875Mb
2020-10-12 20:14:25 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 20:14:30 | INFO | train_inner | epoch 038:     19 / 113 loss=6.008, nll_loss=4.674, ppl=25.53, wps=17104.3, ups=2.77, wpb=6179.1, bsz=228.6, num_updates=4200, lr=0.00019518, gnorm=1.277, clip=0, train_wall=27, wall=1553
2020-10-12 20:14:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:14:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17906.859375Mb; avail=470765.46875Mb
2020-10-12 20:14:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002596
2020-10-12 20:14:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17906.8125Mb; avail=470765.7109375Mb
2020-10-12 20:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.088688
2020-10-12 20:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.6953125Mb; avail=470864.015625Mb
2020-10-12 20:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056181
2020-10-12 20:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.148529
2020-10-12 20:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.7109375Mb; avail=470864.015625Mb
2020-10-12 20:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17808.7734375Mb; avail=470863.7734375Mb
2020-10-12 20:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001190
2020-10-12 20:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.7734375Mb; avail=470863.7734375Mb
2020-10-12 20:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082361
2020-10-12 20:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.7109375Mb; avail=470863.7734375Mb
2020-10-12 20:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055711
2020-10-12 20:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140045
2020-10-12 20:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.80078125Mb; avail=470863.7734375Mb
2020-10-12 20:15:01 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.821 | nll_loss 5.476 | ppl 44.52 | wps 55451.4 | wpb 2270.2 | bsz 84.4 | num_updates 4294 | best_loss 6.821
2020-10-12 20:15:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:15:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 38 @ 4294 updates, score 6.821) (writing took 4.509619075000046 seconds)
2020-10-12 20:15:05 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 20:15:05 | INFO | train | epoch 038 | loss 5.93 | nll_loss 4.584 | ppl 23.99 | wps 17265.7 | ups 2.79 | wpb 6195.7 | bsz 229.5 | num_updates 4294 | lr 0.000193032 | gnorm 1.278 | clip 0 | train_wall 31 | wall 1588
2020-10-12 20:15:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 20:15:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 20:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17868.3125Mb; avail=470806.171875Mb
2020-10-12 20:15:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000742
2020-10-12 20:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006056
2020-10-12 20:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.15625Mb; avail=470801.328125Mb
2020-10-12 20:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000254
2020-10-12 20:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.76171875Mb; avail=470800.72265625Mb
2020-10-12 20:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094269
2020-10-12 20:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101410
2020-10-12 20:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17943.6953125Mb; avail=470730.7890625Mb
2020-10-12 20:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17952.4140625Mb; avail=470722.0703125Mb
2020-10-12 20:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003681
2020-10-12 20:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17953.01953125Mb; avail=470721.46484375Mb
2020-10-12 20:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-12 20:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17953.26171875Mb; avail=470721.22265625Mb
2020-10-12 20:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088927
2020-10-12 20:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093566
2020-10-12 20:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18009.50390625Mb; avail=470664.765625Mb
2020-10-12 20:15:05 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 20:15:07 | INFO | train_inner | epoch 039:      6 / 113 loss=5.918, nll_loss=4.571, ppl=23.76, wps=16668.4, ups=2.7, wpb=6162.4, bsz=235.3, num_updates=4300, lr=0.000192897, gnorm=1.305, clip=0, train_wall=28, wall=1590
2020-10-12 20:15:36 | INFO | train_inner | epoch 039:    106 / 113 loss=5.841, nll_loss=4.48, ppl=22.31, wps=21800.2, ups=3.48, wpb=6258.6, bsz=231.9, num_updates=4400, lr=0.000190693, gnorm=1.335, clip=0, train_wall=28, wall=1618
2020-10-12 20:15:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16675.1328125Mb; avail=471999.453125Mb
2020-10-12 20:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001733
2020-10-12 20:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16675.1328125Mb; avail=471999.453125Mb
2020-10-12 20:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.084494
2020-10-12 20:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16710.0703125Mb; avail=471964.515625Mb
2020-10-12 20:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055864
2020-10-12 20:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.143159
2020-10-12 20:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16742.6328125Mb; avail=471932.0625Mb
2020-10-12 20:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16748.234375Mb; avail=471925.765625Mb
2020-10-12 20:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001288
2020-10-12 20:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16750.65625Mb; avail=471923.94921875Mb
2020-10-12 20:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083760
2020-10-12 20:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16786.57421875Mb; avail=471887.83203125Mb
2020-10-12 20:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054854
2020-10-12 20:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140828
2020-10-12 20:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16810.87890625Mb; avail=471863.734375Mb
2020-10-12 20:15:41 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.785 | nll_loss 5.436 | ppl 43.28 | wps 57699.7 | wpb 2270.2 | bsz 84.4 | num_updates 4407 | best_loss 6.785
2020-10-12 20:15:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:15:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 39 @ 4407 updates, score 6.785) (writing took 7.450382037000054 seconds)
2020-10-12 20:15:48 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 20:15:48 | INFO | train | epoch 039 | loss 5.848 | nll_loss 4.488 | ppl 22.44 | wps 16256.8 | ups 2.62 | wpb 6195.7 | bsz 229.5 | num_updates 4407 | lr 0.000190541 | gnorm 1.339 | clip 0 | train_wall 31 | wall 1631
2020-10-12 20:15:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 20:15:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 20:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17971.7890625Mb; avail=470701.01171875Mb
2020-10-12 20:15:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000715
2020-10-12 20:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006088
2020-10-12 20:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17971.7890625Mb; avail=470701.01171875Mb
2020-10-12 20:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000229
2020-10-12 20:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17971.7890625Mb; avail=470701.01171875Mb
2020-10-12 20:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094069
2020-10-12 20:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101279
2020-10-12 20:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17971.64453125Mb; avail=470700.8984375Mb
2020-10-12 20:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17971.64453125Mb; avail=470700.8984375Mb
2020-10-12 20:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003745
2020-10-12 20:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17971.64453125Mb; avail=470700.8984375Mb
2020-10-12 20:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 20:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17971.64453125Mb; avail=470700.8984375Mb
2020-10-12 20:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089178
2020-10-12 20:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093843
2020-10-12 20:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17971.61328125Mb; avail=470701.140625Mb
2020-10-12 20:15:48 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 20:16:15 | INFO | train_inner | epoch 040:     93 / 113 loss=5.747, nll_loss=4.371, ppl=20.69, wps=15903.3, ups=2.55, wpb=6224.5, bsz=229.4, num_updates=4500, lr=0.000188562, gnorm=1.287, clip=0, train_wall=27, wall=1657
2020-10-12 20:16:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:16:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18043.48046875Mb; avail=470613.70703125Mb
2020-10-12 20:16:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001843
2020-10-12 20:16:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18043.48046875Mb; avail=470613.70703125Mb
2020-10-12 20:16:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082515
2020-10-12 20:16:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18043.54296875Mb; avail=470613.34375Mb
2020-10-12 20:16:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055493
2020-10-12 20:16:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140679
2020-10-12 20:16:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18043.5390625Mb; avail=470613.34375Mb
2020-10-12 20:16:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18043.5390625Mb; avail=470613.34375Mb
2020-10-12 20:16:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001213
2020-10-12 20:16:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18043.5390625Mb; avail=470613.34375Mb
2020-10-12 20:16:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081182
2020-10-12 20:16:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18043.5390625Mb; avail=470613.34375Mb
2020-10-12 20:16:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055826
2020-10-12 20:16:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138989
2020-10-12 20:16:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18043.5625Mb; avail=470613.46875Mb
2020-10-12 20:16:24 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.779 | nll_loss 5.421 | ppl 42.83 | wps 56785.6 | wpb 2270.2 | bsz 84.4 | num_updates 4520 | best_loss 6.779
2020-10-12 20:16:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:16:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/M2O/checkpoint_best.pt (epoch 40 @ 4520 updates, score 6.779) (writing took 4.630357503000596 seconds)
2020-10-12 20:16:28 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 20:16:28 | INFO | train | epoch 040 | loss 5.759 | nll_loss 4.384 | ppl 20.88 | wps 17466.3 | ups 2.82 | wpb 6195.7 | bsz 229.5 | num_updates 4520 | lr 0.000188144 | gnorm 1.325 | clip 0 | train_wall 31 | wall 1671
2020-10-12 20:16:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 20:16:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 20:16:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18240.87109375Mb; avail=470418.4140625Mb
2020-10-12 20:16:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000712
2020-10-12 20:16:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006262
2020-10-12 20:16:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18240.87109375Mb; avail=470418.4140625Mb
2020-10-12 20:16:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000306
2020-10-12 20:16:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18240.87109375Mb; avail=470418.4140625Mb
2020-10-12 20:16:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101620
2020-10-12 20:16:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109130
2020-10-12 20:16:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18308.71875Mb; avail=470349.94921875Mb
2020-10-12 20:16:28 | INFO | fairseq_cli.train | done training in 1670.7 seconds
