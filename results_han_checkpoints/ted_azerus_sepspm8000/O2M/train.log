2020-10-12 20:17:47 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_azerus_sepspm8000/O2M/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok='tgt', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='eng-aze,eng-rus', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_azerus_sepspm8000/O2M/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 20:17:47 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 20:17:47 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['aze', 'eng', 'rus']
2020-10-12 20:17:47 | INFO | fairseq.data.multilingual.multilingual_data_manager | [aze] dictionary: 22075 types
2020-10-12 20:17:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 22075 types
2020-10-12 20:17:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | [rus] dictionary: 22075 types
2020-10-12 20:17:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 20:17:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15684.94921875Mb; avail=472968.75Mb
2020-10-12 20:17:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 20:17:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:eng-aze': 1, 'main:eng-rus': 1}
2020-10-12 20:17:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-aze src_langtok: 22072; tgt_langtok: None
2020-10-12 20:17:48 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azerus_sepspm8000/O2M/valid.eng-aze.eng
2020-10-12 20:17:48 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azerus_sepspm8000/O2M/valid.eng-aze.aze
2020-10-12 20:17:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azerus_sepspm8000/O2M/ valid eng-aze 671 examples
2020-10-12 20:17:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-rus src_langtok: 22074; tgt_langtok: None
2020-10-12 20:17:48 | INFO | fairseq.data.data_utils | loaded 4814 examples from: fairseq/data-bin/ted_azerus_sepspm8000/O2M/valid.eng-rus.eng
2020-10-12 20:17:48 | INFO | fairseq.data.data_utils | loaded 4814 examples from: fairseq/data-bin/ted_azerus_sepspm8000/O2M/valid.eng-rus.rus
2020-10-12 20:17:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azerus_sepspm8000/O2M/ valid eng-rus 4814 examples
2020-10-12 20:17:48 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22075, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22075, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=22075, bias=False)
  )
)
2020-10-12 20:17:48 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 20:17:48 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 20:17:48 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 20:17:48 | INFO | fairseq_cli.train | num. model params: 42845696 (num. trained: 42845696)
2020-10-12 20:17:52 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 20:17:52 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 20:17:52 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 20:17:52 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 20:17:52 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 20:17:52 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 20:17:52 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 20:17:52 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_last.pt
2020-10-12 20:17:52 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18332.5Mb; avail=470310.31640625Mb
2020-10-12 20:17:52 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 20:17:52 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:eng-aze': 1, 'main:eng-rus': 1}
2020-10-12 20:17:52 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-aze src_langtok: 22072; tgt_langtok: None
2020-10-12 20:17:52 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azerus_sepspm8000/O2M/train.eng-aze.eng
2020-10-12 20:17:52 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azerus_sepspm8000/O2M/train.eng-aze.aze
2020-10-12 20:17:52 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azerus_sepspm8000/O2M/ train eng-aze 5946 examples
2020-10-12 20:17:52 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-rus src_langtok: 22074; tgt_langtok: None
2020-10-12 20:17:52 | INFO | fairseq.data.data_utils | loaded 19993 examples from: fairseq/data-bin/ted_azerus_sepspm8000/O2M/train.eng-rus.eng
2020-10-12 20:17:52 | INFO | fairseq.data.data_utils | loaded 19993 examples from: fairseq/data-bin/ted_azerus_sepspm8000/O2M/train.eng-rus.rus
2020-10-12 20:17:52 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azerus_sepspm8000/O2M/ train eng-rus 19993 examples
2020-10-12 20:17:52 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:eng-aze', 5946), ('main:eng-rus', 19993)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 20:17:52 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 20:17:52 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 25939
2020-10-12 20:17:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 25939; virtual dataset size 25939
2020-10-12 20:17:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 20:17:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:eng-aze': 5946, 'main:eng-rus': 19993}; raw total size: 25939
2020-10-12 20:17:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:eng-aze': 5946, 'main:eng-rus': 19993}; resampled total size: 25939
2020-10-12 20:17:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 20:17:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.004687
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18348.08984375Mb; avail=470294.7265625Mb
2020-10-12 20:17:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000484
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004987
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18353.5390625Mb; avail=470289.27734375Mb
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000208
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18354.14453125Mb; avail=470288.671875Mb
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091506
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097446
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18423.921875Mb; avail=470222.8671875Mb
2020-10-12 20:17:52 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18435.15234375Mb; avail=470216.06640625Mb
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003741
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18435.15234375Mb; avail=470216.06640625Mb
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18435.15234375Mb; avail=470216.06640625Mb
2020-10-12 20:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088794
2020-10-12 20:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093448
2020-10-12 20:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18441.53515625Mb; avail=470208.9921875Mb
2020-10-12 20:17:53 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 20:18:22 | INFO | train_inner | epoch 001:    100 / 110 loss=14.378, nll_loss=14.29, ppl=20029.7, wps=20883.5, ups=3.45, wpb=6031.5, bsz=242.8, num_updates=100, lr=5.0975e-06, gnorm=3.687, clip=0, train_wall=29, wall=30
2020-10-12 20:18:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17926.84765625Mb; avail=470681.41015625Mb
2020-10-12 20:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001971
2020-10-12 20:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17926.84765625Mb; avail=470681.41015625Mb
2020-10-12 20:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080484
2020-10-12 20:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17927.49609375Mb; avail=470680.5625Mb
2020-10-12 20:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054563
2020-10-12 20:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137843
2020-10-12 20:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17927.48046875Mb; avail=470680.68359375Mb
2020-10-12 20:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17927.5078125Mb; avail=470680.5625Mb
2020-10-12 20:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001271
2020-10-12 20:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17927.5078125Mb; avail=470680.5625Mb
2020-10-12 20:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080727
2020-10-12 20:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17927.56640625Mb; avail=470680.3203125Mb
2020-10-12 20:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054119
2020-10-12 20:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136860
2020-10-12 20:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17927.5Mb; avail=470680.19921875Mb
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2020-10-12 20:18:28 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.317 | nll_loss 13.083 | ppl 8676.71 | wps 57405.8 | wpb 2300 | bsz 88.5 | num_updates 110
2020-10-12 20:18:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:18:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 1 @ 110 updates, score 13.317) (writing took 1.568624861999524 seconds)
2020-10-12 20:18:29 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 20:18:29 | INFO | train | epoch 001 | loss 14.309 | nll_loss 14.213 | ppl 18985.6 | wps 18219.3 | ups 3.03 | wpb 5987.1 | bsz 235.8 | num_updates 110 | lr 5.59725e-06 | gnorm 3.523 | clip 0 | train_wall 32 | wall 37
2020-10-12 20:18:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 20:18:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 20:18:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17852.7578125Mb; avail=470738.98828125Mb
2020-10-12 20:18:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000908
2020-10-12 20:18:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005532
2020-10-12 20:18:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.7578125Mb; avail=470738.98828125Mb
2020-10-12 20:18:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000188
2020-10-12 20:18:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.7578125Mb; avail=470738.98828125Mb
2020-10-12 20:18:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.087997
2020-10-12 20:18:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094442
2020-10-12 20:18:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.328125Mb; avail=470744.04296875Mb
2020-10-12 20:18:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17847.52734375Mb; avail=470744.04296875Mb
2020-10-12 20:18:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003647
2020-10-12 20:18:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.5Mb; avail=470744.04296875Mb
2020-10-12 20:18:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 20:18:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.4296875Mb; avail=470744.04296875Mb
2020-10-12 20:18:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089474
2020-10-12 20:18:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094011
2020-10-12 20:18:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.16796875Mb; avail=470744.40625Mb
2020-10-12 20:18:30 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 20:18:56 | INFO | train_inner | epoch 002:     90 / 110 loss=13.229, nll_loss=13.006, ppl=8223.59, wps=17491.8, ups=2.98, wpb=5871.7, bsz=230.2, num_updates=200, lr=1.0095e-05, gnorm=1.503, clip=0, train_wall=28, wall=63
2020-10-12 20:19:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18475.5234375Mb; avail=470117.1171875Mb
2020-10-12 20:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001811
2020-10-12 20:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18475.5234375Mb; avail=470117.1171875Mb
2020-10-12 20:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079325
2020-10-12 20:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18475.53515625Mb; avail=470116.99609375Mb
2020-10-12 20:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055267
2020-10-12 20:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137596
2020-10-12 20:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18475.55078125Mb; avail=470117.1015625Mb
2020-10-12 20:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18475.4296875Mb; avail=470117.22265625Mb
2020-10-12 20:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001479
2020-10-12 20:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18475.4296875Mb; avail=470117.22265625Mb
2020-10-12 20:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079796
2020-10-12 20:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18475.3046875Mb; avail=470117.34375Mb
2020-10-12 20:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055141
2020-10-12 20:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137309
2020-10-12 20:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18475.47265625Mb; avail=470117.17578125Mb
2020-10-12 20:19:04 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 12.605 | nll_loss 12.302 | ppl 5051.06 | wps 56113.9 | wpb 2300 | bsz 88.5 | num_updates 220 | best_loss 12.605
2020-10-12 20:19:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:19:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 2 @ 220 updates, score 12.605) (writing took 13.642679261000012 seconds)
2020-10-12 20:19:18 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 20:19:18 | INFO | train | epoch 002 | loss 13.141 | nll_loss 12.908 | ppl 7688.19 | wps 13509 | ups 2.26 | wpb 5987.1 | bsz 235.8 | num_updates 220 | lr 1.10945e-05 | gnorm 1.424 | clip 0 | train_wall 31 | wall 86
2020-10-12 20:19:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 20:19:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 20:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18400.27734375Mb; avail=470192.24609375Mb
2020-10-12 20:19:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001128
2020-10-12 20:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006859
2020-10-12 20:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18400.99609375Mb; avail=470191.52734375Mb
2020-10-12 20:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000203
2020-10-12 20:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18400.99609375Mb; avail=470191.52734375Mb
2020-10-12 20:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102210
2020-10-12 20:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110328
2020-10-12 20:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18396.046875Mb; avail=470196.36328125Mb
2020-10-12 20:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18395.8671875Mb; avail=470196.796875Mb
2020-10-12 20:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003985
2020-10-12 20:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18395.8671875Mb; avail=470196.796875Mb
2020-10-12 20:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000209
2020-10-12 20:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18395.8671875Mb; avail=470196.796875Mb
2020-10-12 20:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091875
2020-10-12 20:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096999
2020-10-12 20:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18395.78125Mb; avail=470197.1171875Mb
2020-10-12 20:19:18 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 20:19:41 | INFO | train_inner | epoch 003:     80 / 110 loss=12.675, nll_loss=12.393, ppl=5376.92, wps=13078.1, ups=2.19, wpb=5965.4, bsz=228, num_updates=300, lr=1.50925e-05, gnorm=1.174, clip=0, train_wall=28, wall=109
2020-10-12 20:19:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:19:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18990.55859375Mb; avail=469605.73046875Mb
2020-10-12 20:19:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002017
2020-10-12 20:19:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18946.9921875Mb; avail=469651.265625Mb
2020-10-12 20:19:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.089941
2020-10-12 20:19:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18412.98046875Mb; avail=470187.0078125Mb
2020-10-12 20:19:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063336
2020-10-12 20:19:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.156224
2020-10-12 20:19:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18412.12109375Mb; avail=470187.66015625Mb
2020-10-12 20:19:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18411.578125Mb; avail=470187.28125Mb
2020-10-12 20:19:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001609
2020-10-12 20:19:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18412.53125Mb; avail=470186.94140625Mb
2020-10-12 20:19:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.090215
2020-10-12 20:19:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18413.15625Mb; avail=470185.23046875Mb
2020-10-12 20:19:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061238
2020-10-12 20:19:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.153975
2020-10-12 20:19:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18413.64453125Mb; avail=470184.19921875Mb
2020-10-12 20:19:53 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 11.878 | nll_loss 11.482 | ppl 2860.63 | wps 56989.3 | wpb 2300 | bsz 88.5 | num_updates 330 | best_loss 11.878
2020-10-12 20:19:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:20:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 3 @ 330 updates, score 11.878) (writing took 13.863956359999975 seconds)
2020-10-12 20:20:07 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 20:20:07 | INFO | train | epoch 003 | loss 12.495 | nll_loss 12.191 | ppl 4676.91 | wps 13511.2 | ups 2.26 | wpb 5987.1 | bsz 235.8 | num_updates 330 | lr 1.65917e-05 | gnorm 1.227 | clip 0 | train_wall 31 | wall 135
2020-10-12 20:20:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 20:20:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18430.06640625Mb; avail=470162.453125Mb
2020-10-12 20:20:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001217
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006305
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18430.671875Mb; avail=470161.84765625Mb
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000217
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18430.671875Mb; avail=470161.84765625Mb
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106081
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113845
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18425.5078125Mb; avail=470166.91796875Mb
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18425.64453125Mb; avail=470166.91015625Mb
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003812
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18425.64453125Mb; avail=470166.91015625Mb
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18425.64453125Mb; avail=470166.91015625Mb
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088358
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093108
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18425.88671875Mb; avail=470166.65234375Mb
2020-10-12 20:20:07 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 20:20:27 | INFO | train_inner | epoch 004:     70 / 110 loss=11.983, nll_loss=11.613, ppl=3132.43, wps=13205.9, ups=2.17, wpb=6074.5, bsz=249, num_updates=400, lr=2.009e-05, gnorm=1.326, clip=0, train_wall=28, wall=155
2020-10-12 20:20:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17817.4609375Mb; avail=470782.21875Mb
2020-10-12 20:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001951
2020-10-12 20:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.80859375Mb; avail=470781.87890625Mb
2020-10-12 20:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080559
2020-10-12 20:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.25390625Mb; avail=470781.33984375Mb
2020-10-12 20:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056381
2020-10-12 20:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140006
2020-10-12 20:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.3828125Mb; avail=470780.55859375Mb
2020-10-12 20:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17817.54296875Mb; avail=470780.03125Mb
2020-10-12 20:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001433
2020-10-12 20:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.28515625Mb; avail=470780.296875Mb
2020-10-12 20:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080539
2020-10-12 20:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.94921875Mb; avail=470778.78515625Mb
2020-10-12 20:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056423
2020-10-12 20:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139442
2020-10-12 20:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.046875Mb; avail=470778.87109375Mb
2020-10-12 20:20:42 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 11.232 | nll_loss 10.733 | ppl 1701.9 | wps 56917.6 | wpb 2300 | bsz 88.5 | num_updates 440 | best_loss 11.232
2020-10-12 20:20:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:20:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 4 @ 440 updates, score 11.232) (writing took 4.457414207000511 seconds)
2020-10-12 20:20:46 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 20:20:46 | INFO | train | epoch 004 | loss 11.787 | nll_loss 11.387 | ppl 2678.75 | wps 16782 | ups 2.8 | wpb 5987.1 | bsz 235.8 | num_updates 440 | lr 2.2089e-05 | gnorm 1.229 | clip 0 | train_wall 31 | wall 174
2020-10-12 20:20:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 20:20:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 20:20:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17804.8984375Mb; avail=470787.1953125Mb
2020-10-12 20:20:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000757
2020-10-12 20:20:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005297
2020-10-12 20:20:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17804.8984375Mb; avail=470787.1953125Mb
2020-10-12 20:20:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:20:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17804.8984375Mb; avail=470787.1953125Mb
2020-10-12 20:20:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091226
2020-10-12 20:20:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097814
2020-10-12 20:20:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.72265625Mb; avail=470793.34765625Mb
2020-10-12 20:20:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17798.3359375Mb; avail=470793.71875Mb
2020-10-12 20:20:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004028
2020-10-12 20:20:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.3359375Mb; avail=470793.71875Mb
2020-10-12 20:20:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000238
2020-10-12 20:20:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.3359375Mb; avail=470793.71875Mb
2020-10-12 20:20:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088284
2020-10-12 20:20:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093310
2020-10-12 20:20:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.3359375Mb; avail=470793.71875Mb
2020-10-12 20:20:46 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 20:21:04 | INFO | train_inner | epoch 005:     60 / 110 loss=11.477, nll_loss=11.025, ppl=2084.27, wps=16649.9, ups=2.75, wpb=6060.6, bsz=236, num_updates=500, lr=2.50875e-05, gnorm=1.242, clip=0, train_wall=28, wall=192
2020-10-12 20:21:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18475.8828125Mb; avail=470124.3046875Mb
2020-10-12 20:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002107
2020-10-12 20:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18476.73046875Mb; avail=470123.46484375Mb
2020-10-12 20:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079344
2020-10-12 20:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18476.75390625Mb; avail=470122.3046875Mb
2020-10-12 20:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057449
2020-10-12 20:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140080
2020-10-12 20:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18476.23828125Mb; avail=470121.88671875Mb
2020-10-12 20:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18475.8984375Mb; avail=470121.85546875Mb
2020-10-12 20:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001447
2020-10-12 20:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18476.375Mb; avail=470121.62890625Mb
2020-10-12 20:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083286
2020-10-12 20:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18475.54296875Mb; avail=470120.484375Mb
2020-10-12 20:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055264
2020-10-12 20:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141022
2020-10-12 20:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18476.0859375Mb; avail=470120.47265625Mb
2020-10-12 20:21:21 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.794 | nll_loss 10.211 | ppl 1184.86 | wps 57071.6 | wpb 2300 | bsz 88.5 | num_updates 550 | best_loss 10.794
2020-10-12 20:21:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:21:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 5 @ 550 updates, score 10.794) (writing took 11.814543110000159 seconds)
2020-10-12 20:21:33 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 20:21:33 | INFO | train | epoch 005 | loss 11.277 | nll_loss 10.789 | ppl 1768.82 | wps 14141 | ups 2.36 | wpb 5987.1 | bsz 235.8 | num_updates 550 | lr 2.75863e-05 | gnorm 1.269 | clip 0 | train_wall 31 | wall 220
2020-10-12 20:21:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 20:21:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 20:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17806.7109375Mb; avail=470785.05078125Mb
2020-10-12 20:21:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000660
2020-10-12 20:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005298
2020-10-12 20:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17806.7109375Mb; avail=470785.05078125Mb
2020-10-12 20:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000201
2020-10-12 20:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17806.7109375Mb; avail=470785.05078125Mb
2020-10-12 20:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089087
2020-10-12 20:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095353
2020-10-12 20:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.87890625Mb; avail=470790.9609375Mb
2020-10-12 20:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17800.98046875Mb; avail=470791.1875Mb
2020-10-12 20:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003686
2020-10-12 20:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.98046875Mb; avail=470791.1875Mb
2020-10-12 20:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-12 20:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.98046875Mb; avail=470791.1875Mb
2020-10-12 20:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089318
2020-10-12 20:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093979
2020-10-12 20:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17801.4296875Mb; avail=470790.6953125Mb
2020-10-12 20:21:33 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 20:21:47 | INFO | train_inner | epoch 006:     50 / 110 loss=11.075, nll_loss=10.544, ppl=1493.26, wps=13431.4, ups=2.3, wpb=5848.8, bsz=222.3, num_updates=600, lr=3.0085e-05, gnorm=1.158, clip=0, train_wall=28, wall=235
2020-10-12 20:22:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17823.421875Mb; avail=470768.08203125Mb
2020-10-12 20:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001972
2020-10-12 20:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.37890625Mb; avail=470768.32421875Mb
2020-10-12 20:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080320
2020-10-12 20:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.40234375Mb; avail=470768.203125Mb
2020-10-12 20:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053981
2020-10-12 20:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137086
2020-10-12 20:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.265625Mb; avail=470767.7265625Mb
2020-10-12 20:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17824.078125Mb; avail=470767.7265625Mb
2020-10-12 20:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001365
2020-10-12 20:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.078125Mb; avail=470767.7265625Mb
2020-10-12 20:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079312
2020-10-12 20:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.1875Mb; avail=470767.60546875Mb
2020-10-12 20:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054247
2020-10-12 20:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.135725
2020-10-12 20:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.21484375Mb; avail=470767.4921875Mb
2020-10-12 20:22:08 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 10.572 | nll_loss 9.927 | ppl 973.33 | wps 57084.4 | wpb 2300 | bsz 88.5 | num_updates 660 | best_loss 10.572
2020-10-12 20:22:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:22:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 6 @ 660 updates, score 10.572) (writing took 4.447369833999801 seconds)
2020-10-12 20:22:12 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 20:22:12 | INFO | train | epoch 006 | loss 10.954 | nll_loss 10.394 | ppl 1345.28 | wps 16724.8 | ups 2.79 | wpb 5987.1 | bsz 235.8 | num_updates 660 | lr 3.30835e-05 | gnorm 1.115 | clip 0 | train_wall 31 | wall 260
2020-10-12 20:22:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 20:22:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 20:22:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17811.9296875Mb; avail=470780.01953125Mb
2020-10-12 20:22:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000839
2020-10-12 20:22:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005892
2020-10-12 20:22:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.9296875Mb; avail=470780.01953125Mb
2020-10-12 20:22:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000235
2020-10-12 20:22:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.9296875Mb; avail=470780.01953125Mb
2020-10-12 20:22:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095235
2020-10-12 20:22:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102487
2020-10-12 20:22:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.08984375Mb; avail=470786.86328125Mb
2020-10-12 20:22:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17805.16796875Mb; avail=470786.78125Mb
2020-10-12 20:22:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003939
2020-10-12 20:22:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.16796875Mb; avail=470786.78125Mb
2020-10-12 20:22:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000207
2020-10-12 20:22:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.16796875Mb; avail=470786.78125Mb
2020-10-12 20:22:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090040
2020-10-12 20:22:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094914
2020-10-12 20:22:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.42578125Mb; avail=470786.5390625Mb
2020-10-12 20:22:12 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 20:22:24 | INFO | train_inner | epoch 007:     40 / 110 loss=10.861, nll_loss=10.28, ppl=1243.23, wps=16497.5, ups=2.73, wpb=6042.8, bsz=250.3, num_updates=700, lr=3.50825e-05, gnorm=1.193, clip=0, train_wall=28, wall=272
2020-10-12 20:22:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:22:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18198.01953125Mb; avail=470393.47265625Mb
2020-10-12 20:22:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002299
2020-10-12 20:22:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18198.625Mb; avail=470393.47265625Mb
2020-10-12 20:22:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080550
2020-10-12 20:22:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18198.0078125Mb; avail=470394.0859375Mb
2020-10-12 20:22:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054969
2020-10-12 20:22:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138708
2020-10-12 20:22:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18198.42578125Mb; avail=470393.48046875Mb
2020-10-12 20:22:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18198.0703125Mb; avail=470393.8359375Mb
2020-10-12 20:22:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001693
2020-10-12 20:22:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18198.0703125Mb; avail=470393.8359375Mb
2020-10-12 20:22:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.090582
2020-10-12 20:22:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18198.39453125Mb; avail=470394.0390625Mb
2020-10-12 20:22:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062761
2020-10-12 20:22:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.156059
2020-10-12 20:22:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18198.44140625Mb; avail=470393.9921875Mb
2020-10-12 20:22:47 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 10.415 | nll_loss 9.745 | ppl 858.03 | wps 56856.7 | wpb 2300 | bsz 88.5 | num_updates 770 | best_loss 10.415
2020-10-12 20:22:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:22:52 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 7 @ 770 updates, score 10.415) (writing took 5.051137920999281 seconds)
2020-10-12 20:22:52 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 20:22:52 | INFO | train | epoch 007 | loss 10.744 | nll_loss 10.138 | ppl 1126.47 | wps 16371 | ups 2.73 | wpb 5987.1 | bsz 235.8 | num_updates 770 | lr 3.85808e-05 | gnorm 1.225 | clip 0 | train_wall 31 | wall 300
2020-10-12 20:22:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 20:22:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 20:22:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17795.64453125Mb; avail=470796.015625Mb
2020-10-12 20:22:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000896
2020-10-12 20:22:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006354
2020-10-12 20:22:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17796.25Mb; avail=470795.41015625Mb
2020-10-12 20:22:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000252
2020-10-12 20:22:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17796.25Mb; avail=470795.41015625Mb
2020-10-12 20:22:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091903
2020-10-12 20:22:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099356
2020-10-12 20:22:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17790.4453125Mb; avail=470801.59375Mb
2020-10-12 20:22:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17790.4453125Mb; avail=470801.59375Mb
2020-10-12 20:22:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003650
2020-10-12 20:22:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17790.4453125Mb; avail=470801.59375Mb
2020-10-12 20:22:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:22:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17790.4453125Mb; avail=470801.59375Mb
2020-10-12 20:22:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089866
2020-10-12 20:22:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094427
2020-10-12 20:22:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17790.5625Mb; avail=470801.5625Mb
2020-10-12 20:22:53 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 20:23:01 | INFO | train_inner | epoch 008:     30 / 110 loss=10.696, nll_loss=10.079, ppl=1081.75, wps=16107.9, ups=2.68, wpb=6008.7, bsz=232.4, num_updates=800, lr=4.008e-05, gnorm=1.215, clip=0, train_wall=28, wall=309
2020-10-12 20:23:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:23:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17811.49609375Mb; avail=470780.1796875Mb
2020-10-12 20:23:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002038
2020-10-12 20:23:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17812.1015625Mb; avail=470779.57421875Mb
2020-10-12 20:23:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080993
2020-10-12 20:23:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17812.1171875Mb; avail=470779.57421875Mb
2020-10-12 20:23:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056790
2020-10-12 20:23:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140668
2020-10-12 20:23:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.64453125Mb; avail=470780.30078125Mb
2020-10-12 20:23:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17811.671875Mb; avail=470780.30078125Mb
2020-10-12 20:23:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001358
2020-10-12 20:23:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.6875Mb; avail=470780.1796875Mb
2020-10-12 20:23:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081091
2020-10-12 20:23:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.609375Mb; avail=470780.05859375Mb
2020-10-12 20:23:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055481
2020-10-12 20:23:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138690
2020-10-12 20:23:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.66796875Mb; avail=470779.9375Mb
2020-10-12 20:23:27 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.391 | nll_loss 9.705 | ppl 834.43 | wps 57103.3 | wpb 2300 | bsz 88.5 | num_updates 880 | best_loss 10.391
2020-10-12 20:23:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:23:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 8 @ 880 updates, score 10.391) (writing took 4.500837784999931 seconds)
2020-10-12 20:23:32 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 20:23:32 | INFO | train | epoch 008 | loss 10.577 | nll_loss 9.938 | ppl 981.02 | wps 16806.1 | ups 2.81 | wpb 5987.1 | bsz 235.8 | num_updates 880 | lr 4.4078e-05 | gnorm 1.185 | clip 0 | train_wall 31 | wall 339
2020-10-12 20:23:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 20:23:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 20:23:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17802.2578125Mb; avail=470790.015625Mb
2020-10-12 20:23:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000735
2020-10-12 20:23:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005234
2020-10-12 20:23:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.2578125Mb; avail=470790.015625Mb
2020-10-12 20:23:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 20:23:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.2578125Mb; avail=470790.015625Mb
2020-10-12 20:23:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091005
2020-10-12 20:23:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097213
2020-10-12 20:23:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17796.10546875Mb; avail=470795.3359375Mb
2020-10-12 20:23:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17796.60546875Mb; avail=470795.22265625Mb
2020-10-12 20:23:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003724
2020-10-12 20:23:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17796.60546875Mb; avail=470795.22265625Mb
2020-10-12 20:23:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 20:23:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17796.60546875Mb; avail=470795.22265625Mb
2020-10-12 20:23:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089440
2020-10-12 20:23:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094116
2020-10-12 20:23:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17796.703125Mb; avail=470795.1015625Mb
2020-10-12 20:23:32 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 20:23:37 | INFO | train_inner | epoch 009:     20 / 110 loss=10.532, nll_loss=9.886, ppl=946.26, wps=16387.8, ups=2.76, wpb=5939.3, bsz=232.2, num_updates=900, lr=4.50775e-05, gnorm=1.258, clip=0, train_wall=28, wall=345
2020-10-12 20:24:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:24:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18501.640625Mb; avail=470091.30859375Mb
2020-10-12 20:24:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002003
2020-10-12 20:24:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18501.640625Mb; avail=470091.30859375Mb
2020-10-12 20:24:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082375
2020-10-12 20:24:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18501.52734375Mb; avail=470091.4296875Mb
2020-10-12 20:24:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057112
2020-10-12 20:24:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142308
2020-10-12 20:24:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18502.1328125Mb; avail=470090.82421875Mb
2020-10-12 20:24:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18502.1328125Mb; avail=470090.82421875Mb
2020-10-12 20:24:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001304
2020-10-12 20:24:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18502.1328125Mb; avail=470090.82421875Mb
2020-10-12 20:24:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079340
2020-10-12 20:24:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18502.37890625Mb; avail=470090.55078125Mb
2020-10-12 20:24:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055515
2020-10-12 20:24:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136947
2020-10-12 20:24:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18502.37890625Mb; avail=470090.55078125Mb
2020-10-12 20:24:06 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 10.167 | nll_loss 9.436 | ppl 692.79 | wps 55775.4 | wpb 2300 | bsz 88.5 | num_updates 990 | best_loss 10.167
2020-10-12 20:24:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:24:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 9 @ 990 updates, score 10.167) (writing took 5.397034343999621 seconds)
2020-10-12 20:24:12 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 20:24:12 | INFO | train | epoch 009 | loss 10.466 | nll_loss 9.805 | ppl 894.32 | wps 16390.9 | ups 2.74 | wpb 5987.1 | bsz 235.8 | num_updates 990 | lr 4.95753e-05 | gnorm 1.3 | clip 0 | train_wall 31 | wall 379
2020-10-12 20:24:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 20:24:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 20:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17825.609375Mb; avail=470766.09375Mb
2020-10-12 20:24:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000954
2020-10-12 20:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006318
2020-10-12 20:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.21484375Mb; avail=470765.48828125Mb
2020-10-12 20:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000218
2020-10-12 20:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.21484375Mb; avail=470765.48828125Mb
2020-10-12 20:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092161
2020-10-12 20:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099578
2020-10-12 20:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.80078125Mb; avail=470770.90234375Mb
2020-10-12 20:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17820.8125Mb; avail=470770.78125Mb
2020-10-12 20:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003717
2020-10-12 20:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.8203125Mb; avail=470770.66015625Mb
2020-10-12 20:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 20:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.8203125Mb; avail=470770.66015625Mb
2020-10-12 20:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088848
2020-10-12 20:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093485
2020-10-12 20:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17819.78515625Mb; avail=470771.8984375Mb
2020-10-12 20:24:12 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 20:24:15 | INFO | train_inner | epoch 010:     10 / 110 loss=10.46, nll_loss=9.797, ppl=889.6, wps=15994.8, ups=2.68, wpb=5970.3, bsz=232.7, num_updates=1000, lr=5.0075e-05, gnorm=1.188, clip=0, train_wall=28, wall=383
2020-10-12 20:24:44 | INFO | train_inner | epoch 010:    110 / 110 loss=10.358, nll_loss=9.676, ppl=817.84, wps=20894.4, ups=3.46, wpb=6044.9, bsz=237.9, num_updates=1100, lr=5.50725e-05, gnorm=1.091, clip=0, train_wall=28, wall=411
2020-10-12 20:24:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18400.83203125Mb; avail=470190.54296875Mb
2020-10-12 20:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001822
2020-10-12 20:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18402.6484375Mb; avail=470189.33203125Mb
2020-10-12 20:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080250
2020-10-12 20:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18466.37109375Mb; avail=470127.8515625Mb
2020-10-12 20:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055882
2020-10-12 20:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138825
2020-10-12 20:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18466.79296875Mb; avail=470132.58984375Mb
2020-10-12 20:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18467.890625Mb; avail=470131.53125Mb
2020-10-12 20:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001472
2020-10-12 20:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18468.23828125Mb; avail=470131.68359375Mb
2020-10-12 20:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080172
2020-10-12 20:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18467.55859375Mb; avail=470129.90625Mb
2020-10-12 20:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055494
2020-10-12 20:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138213
2020-10-12 20:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18468.00390625Mb; avail=470128.62890625Mb
2020-10-12 20:24:47 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 10.099 | nll_loss 9.352 | ppl 653.49 | wps 56578.4 | wpb 2300 | bsz 88.5 | num_updates 1100 | best_loss 10.099
2020-10-12 20:24:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:24:52 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 10 @ 1100 updates, score 10.099) (writing took 5.645678599000348 seconds)
2020-10-12 20:24:52 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 20:24:52 | INFO | train | epoch 010 | loss 10.355 | nll_loss 9.673 | ppl 816.07 | wps 16208.7 | ups 2.71 | wpb 5987.1 | bsz 235.8 | num_updates 1100 | lr 5.50725e-05 | gnorm 1.08 | clip 0 | train_wall 31 | wall 420
2020-10-12 20:24:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 20:24:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 20:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17899.01171875Mb; avail=470692.8984375Mb
2020-10-12 20:24:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000851
2020-10-12 20:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005514
2020-10-12 20:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17899.0234375Mb; avail=470692.77734375Mb
2020-10-12 20:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000196
2020-10-12 20:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17899.0234375Mb; avail=470692.77734375Mb
2020-10-12 20:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100321
2020-10-12 20:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107007
2020-10-12 20:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.625Mb; avail=470697.30078125Mb
2020-10-12 20:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17893.59765625Mb; avail=470698.15625Mb
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003775
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.59765625Mb; avail=470698.15625Mb
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000196
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.59765625Mb; avail=470698.15625Mb
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092191
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096913
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.734375Mb; avail=470698.03515625Mb
2020-10-12 20:24:53 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 20:25:21 | INFO | train_inner | epoch 011:    100 / 110 loss=10.276, nll_loss=9.576, ppl=763.45, wps=15826.5, ups=2.66, wpb=5958.4, bsz=238.3, num_updates=1200, lr=6.007e-05, gnorm=1.282, clip=0, train_wall=28, wall=449
2020-10-12 20:25:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17862.015625Mb; avail=470730.25390625Mb
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001879
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.015625Mb; avail=470730.25390625Mb
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079469
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.0546875Mb; avail=470730.1328125Mb
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057371
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139605
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.83984375Mb; avail=470728.30859375Mb
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17871.921875Mb; avail=470720.1796875Mb
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001426
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17872.44921875Mb; avail=470719.57421875Mb
2020-10-12 20:25:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080006
2020-10-12 20:25:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17878.6875Mb; avail=470713.26171875Mb
2020-10-12 20:25:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056484
2020-10-12 20:25:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138730
2020-10-12 20:25:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17878.7890625Mb; avail=470713.26171875Mb
2020-10-12 20:25:27 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 10.055 | nll_loss 9.296 | ppl 628.77 | wps 56838.9 | wpb 2300 | bsz 88.5 | num_updates 1210 | best_loss 10.055
2020-10-12 20:25:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:25:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 11 @ 1210 updates, score 10.055) (writing took 10.333772924000186 seconds)
2020-10-12 20:25:38 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 20:25:38 | INFO | train | epoch 011 | loss 10.284 | nll_loss 9.586 | ppl 768.59 | wps 14577.5 | ups 2.43 | wpb 5987.1 | bsz 235.8 | num_updates 1210 | lr 6.05698e-05 | gnorm 1.271 | clip 0 | train_wall 31 | wall 465
2020-10-12 20:25:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 20:25:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 20:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18404.3828125Mb; avail=470187.32421875Mb
2020-10-12 20:25:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000822
2020-10-12 20:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005540
2020-10-12 20:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18404.49609375Mb; avail=470187.2109375Mb
2020-10-12 20:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-12 20:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18404.49609375Mb; avail=470187.2109375Mb
2020-10-12 20:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088249
2020-10-12 20:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094738
2020-10-12 20:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18398.08203125Mb; avail=470193.7890625Mb
2020-10-12 20:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18398.25390625Mb; avail=470193.42578125Mb
2020-10-12 20:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003662
2020-10-12 20:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18398.25390625Mb; avail=470193.42578125Mb
2020-10-12 20:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 20:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18398.25390625Mb; avail=470193.42578125Mb
2020-10-12 20:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088360
2020-10-12 20:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.092996
2020-10-12 20:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18398.22265625Mb; avail=470193.45703125Mb
2020-10-12 20:25:38 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 20:26:04 | INFO | train_inner | epoch 012:     90 / 110 loss=10.212, nll_loss=9.501, ppl=724.73, wps=14275.9, ups=2.36, wpb=6050.1, bsz=232.2, num_updates=1300, lr=6.50675e-05, gnorm=0.959, clip=0, train_wall=28, wall=492
2020-10-12 20:26:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17876.40625Mb; avail=470715.21875Mb
2020-10-12 20:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002303
2020-10-12 20:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.01171875Mb; avail=470714.61328125Mb
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079956
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.8984375Mb; avail=470714.85546875Mb
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055412
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138473
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.59765625Mb; avail=470713.765625Mb
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17877.12109375Mb; avail=470714.2578125Mb
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001222
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.12109375Mb; avail=470714.2578125Mb
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079858
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.70703125Mb; avail=470714.01953125Mb
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056258
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138108
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.9921875Mb; avail=470714.01171875Mb
2020-10-12 20:26:12 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 9.937 | nll_loss 9.165 | ppl 573.85 | wps 57261.6 | wpb 2300 | bsz 88.5 | num_updates 1320 | best_loss 9.937
2020-10-12 20:26:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:26:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 12 @ 1320 updates, score 9.937) (writing took 16.925616653999896 seconds)
2020-10-12 20:26:29 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 20:26:29 | INFO | train | epoch 012 | loss 10.181 | nll_loss 9.465 | ppl 706.95 | wps 12731.4 | ups 2.13 | wpb 5987.1 | bsz 235.8 | num_updates 1320 | lr 6.6067e-05 | gnorm 0.968 | clip 0 | train_wall 31 | wall 517
2020-10-12 20:26:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 20:26:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18540.39453125Mb; avail=470052.70703125Mb
2020-10-12 20:26:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001210
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006765
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18541.0Mb; avail=470052.1015625Mb
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000252
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18541.0Mb; avail=470052.1015625Mb
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105600
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113648
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18534.92578125Mb; avail=470057.8671875Mb
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18535.1640625Mb; avail=470057.13671875Mb
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004268
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18535.1640625Mb; avail=470057.13671875Mb
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000429
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18535.1640625Mb; avail=470057.13671875Mb
2020-10-12 20:26:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100671
2020-10-12 20:26:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106209
2020-10-12 20:26:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18535.17578125Mb; avail=470057.31640625Mb
2020-10-12 20:26:30 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 20:26:52 | INFO | train_inner | epoch 013:     80 / 110 loss=10.106, nll_loss=9.378, ppl=665.33, wps=12097.2, ups=2.05, wpb=5890.8, bsz=237.7, num_updates=1400, lr=7.0065e-05, gnorm=1.224, clip=0, train_wall=28, wall=540
2020-10-12 20:27:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18125.45703125Mb; avail=470465.8984375Mb
2020-10-12 20:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002229
2020-10-12 20:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18128.484375Mb; avail=470462.87109375Mb
2020-10-12 20:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081641
2020-10-12 20:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18235.3203125Mb; avail=470356.06640625Mb
2020-10-12 20:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056455
2020-10-12 20:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141298
2020-10-12 20:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18301.7890625Mb; avail=470289.70703125Mb
2020-10-12 20:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18333.47265625Mb; avail=470257.1328125Mb
2020-10-12 20:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001304
2020-10-12 20:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18335.2890625Mb; avail=470255.921875Mb
2020-10-12 20:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082717
2020-10-12 20:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18428.1953125Mb; avail=470163.40625Mb
2020-10-12 20:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055860
2020-10-12 20:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140901
2020-10-12 20:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18443.8828125Mb; avail=470152.0859375Mb
2020-10-12 20:27:04 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 9.859 | nll_loss 9.069 | ppl 537.06 | wps 54876.4 | wpb 2300 | bsz 88.5 | num_updates 1430 | best_loss 9.859
2020-10-12 20:27:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:27:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 13 @ 1430 updates, score 9.859) (writing took 10.844433442000081 seconds)
2020-10-12 20:27:15 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 20:27:15 | INFO | train | epoch 013 | loss 10.109 | nll_loss 9.38 | ppl 666.4 | wps 14381.9 | ups 2.4 | wpb 5987.1 | bsz 235.8 | num_updates 1430 | lr 7.15643e-05 | gnorm 1.192 | clip 0 | train_wall 31 | wall 563
2020-10-12 20:27:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 20:27:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 20:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18402.36328125Mb; avail=470189.8671875Mb
2020-10-12 20:27:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000695
2020-10-12 20:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005503
2020-10-12 20:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18402.36328125Mb; avail=470189.8671875Mb
2020-10-12 20:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000197
2020-10-12 20:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18402.36328125Mb; avail=470189.8671875Mb
2020-10-12 20:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099072
2020-10-12 20:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105564
2020-10-12 20:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18396.75390625Mb; avail=470195.36328125Mb
2020-10-12 20:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18397.28125Mb; avail=470194.96875Mb
2020-10-12 20:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004284
2020-10-12 20:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18397.2578125Mb; avail=470195.08984375Mb
2020-10-12 20:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 20:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18397.2578125Mb; avail=470195.08984375Mb
2020-10-12 20:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102999
2020-10-12 20:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108720
2020-10-12 20:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18397.140625Mb; avail=470195.20703125Mb
2020-10-12 20:27:15 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 20:27:36 | INFO | train_inner | epoch 014:     70 / 110 loss=10.058, nll_loss=9.32, ppl=638.98, wps=13857.3, ups=2.32, wpb=5972.6, bsz=229.6, num_updates=1500, lr=7.50625e-05, gnorm=1.039, clip=0, train_wall=28, wall=583
2020-10-12 20:27:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17820.55078125Mb; avail=470771.67578125Mb
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002145
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17821.15625Mb; avail=470771.0703125Mb
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081762
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.19140625Mb; avail=470769.55078125Mb
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056393
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141159
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.3125Mb; avail=470769.4296875Mb
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17822.3125Mb; avail=470769.4296875Mb
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001297
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.3125Mb; avail=470769.4296875Mb
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080613
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.54296875Mb; avail=470769.1875Mb
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055339
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138002
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.9921875Mb; avail=470767.7265625Mb
2020-10-12 20:27:50 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 9.77 | nll_loss 8.958 | ppl 497.35 | wps 56981.5 | wpb 2300 | bsz 88.5 | num_updates 1540 | best_loss 9.77
2020-10-12 20:27:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:28:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 14 @ 1540 updates, score 9.77) (writing took 10.614411015999394 seconds)
2020-10-12 20:28:01 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 20:28:01 | INFO | train | epoch 014 | loss 10.016 | nll_loss 9.272 | ppl 618.27 | wps 14427.2 | ups 2.41 | wpb 5987.1 | bsz 235.8 | num_updates 1540 | lr 7.70615e-05 | gnorm 1.122 | clip 0 | train_wall 31 | wall 608
2020-10-12 20:28:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 20:28:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 20:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18352.48046875Mb; avail=470240.70703125Mb
2020-10-12 20:28:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000890
2020-10-12 20:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005537
2020-10-12 20:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18351.87890625Mb; avail=470241.3203125Mb
2020-10-12 20:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-12 20:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18351.87890625Mb; avail=470241.3203125Mb
2020-10-12 20:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091106
2020-10-12 20:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097712
2020-10-12 20:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18346.12109375Mb; avail=470247.09375Mb
2020-10-12 20:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18345.48828125Mb; avail=470247.7265625Mb
2020-10-12 20:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003776
2020-10-12 20:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18345.48828125Mb; avail=470247.7265625Mb
2020-10-12 20:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000203
2020-10-12 20:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18345.48828125Mb; avail=470247.7265625Mb
2020-10-12 20:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089710
2020-10-12 20:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094471
2020-10-12 20:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18346.1015625Mb; avail=470246.58984375Mb
2020-10-12 20:28:01 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 20:28:18 | INFO | train_inner | epoch 015:     60 / 110 loss=9.975, nll_loss=9.224, ppl=597.8, wps=13999, ups=2.35, wpb=5959, bsz=224, num_updates=1600, lr=8.006e-05, gnorm=1.192, clip=0, train_wall=28, wall=626
2020-10-12 20:28:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:28:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17893.59765625Mb; avail=470697.765625Mb
2020-10-12 20:28:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002011
2020-10-12 20:28:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.59765625Mb; avail=470697.765625Mb
2020-10-12 20:28:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080883
2020-10-12 20:28:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.76953125Mb; avail=470697.5234375Mb
2020-10-12 20:28:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056577
2020-10-12 20:28:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140279
2020-10-12 20:28:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17894.3515625Mb; avail=470697.0390625Mb
2020-10-12 20:28:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17894.29296875Mb; avail=470696.796875Mb
2020-10-12 20:28:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001282
2020-10-12 20:28:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17894.29296875Mb; avail=470696.796875Mb
2020-10-12 20:28:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079768
2020-10-12 20:28:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17894.21484375Mb; avail=470696.67578125Mb
2020-10-12 20:28:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055249
2020-10-12 20:28:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137055
2020-10-12 20:28:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17894.203125Mb; avail=470696.796875Mb
2020-10-12 20:28:36 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 9.65 | nll_loss 8.841 | ppl 458.55 | wps 56965.9 | wpb 2300 | bsz 88.5 | num_updates 1650 | best_loss 9.65
2020-10-12 20:28:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:28:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 15 @ 1650 updates, score 9.65) (writing took 19.539144308000687 seconds)
2020-10-12 20:28:55 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 20:28:55 | INFO | train | epoch 015 | loss 9.916 | nll_loss 9.156 | ppl 570.43 | wps 12097.8 | ups 2.02 | wpb 5987.1 | bsz 235.8 | num_updates 1650 | lr 8.25588e-05 | gnorm 1.17 | clip 0 | train_wall 31 | wall 663
2020-10-12 20:28:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 20:28:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 20:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19098.5546875Mb; avail=469494.0859375Mb
2020-10-12 20:28:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001475
2020-10-12 20:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.009185
2020-10-12 20:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19092.6484375Mb; avail=469499.9921875Mb
2020-10-12 20:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000363
2020-10-12 20:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19092.6484375Mb; avail=469499.9921875Mb
2020-10-12 20:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102304
2020-10-12 20:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112975
2020-10-12 20:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19092.62890625Mb; avail=469500.2265625Mb
2020-10-12 20:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19092.64453125Mb; avail=469500.10546875Mb
2020-10-12 20:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004276
2020-10-12 20:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19092.64453125Mb; avail=469500.10546875Mb
2020-10-12 20:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 20:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19092.64453125Mb; avail=469500.10546875Mb
2020-10-12 20:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102771
2020-10-12 20:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108148
2020-10-12 20:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19092.703125Mb; avail=469500.11328125Mb
2020-10-12 20:28:55 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 20:29:10 | INFO | train_inner | epoch 016:     50 / 110 loss=9.831, nll_loss=9.06, ppl=533.64, wps=11822.1, ups=1.94, wpb=6095.2, bsz=262.8, num_updates=1700, lr=8.50575e-05, gnorm=1.248, clip=0, train_wall=28, wall=677
2020-10-12 20:29:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18426.3984375Mb; avail=470170.8359375Mb
2020-10-12 20:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002370
2020-10-12 20:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18426.3984375Mb; avail=470171.08203125Mb
2020-10-12 20:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081062
2020-10-12 20:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18426.3125Mb; avail=470172.16015625Mb
2020-10-12 20:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056291
2020-10-12 20:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140884
2020-10-12 20:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18426.546875Mb; avail=470171.83984375Mb
2020-10-12 20:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18427.296875Mb; avail=470170.4609375Mb
2020-10-12 20:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001696
2020-10-12 20:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18427.515625Mb; avail=470170.12109375Mb
2020-10-12 20:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080401
2020-10-12 20:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18426.52734375Mb; avail=470168.984375Mb
2020-10-12 20:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.058670
2020-10-12 20:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141780
2020-10-12 20:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18426.82421875Mb; avail=470167.2890625Mb
2020-10-12 20:29:30 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 9.545 | nll_loss 8.707 | ppl 418 | wps 56950.2 | wpb 2300 | bsz 88.5 | num_updates 1760 | best_loss 9.545
2020-10-12 20:29:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:29:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 16 @ 1760 updates, score 9.545) (writing took 7.949401949000276 seconds)
2020-10-12 20:29:38 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 20:29:38 | INFO | train | epoch 016 | loss 9.808 | nll_loss 9.032 | ppl 523.45 | wps 15420.2 | ups 2.58 | wpb 5987.1 | bsz 235.8 | num_updates 1760 | lr 8.8056e-05 | gnorm 1.286 | clip 0 | train_wall 31 | wall 706
2020-10-12 20:29:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 20:29:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18435.67578125Mb; avail=470162.85546875Mb
2020-10-12 20:29:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000918
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005923
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18436.16796875Mb; avail=470163.1015625Mb
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000299
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18436.16796875Mb; avail=470163.59375Mb
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101766
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108821
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18430.46875Mb; avail=470168.171875Mb
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18430.8984375Mb; avail=470166.99609375Mb
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004340
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18431.078125Mb; avail=470166.9609375Mb
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000200
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18430.94921875Mb; avail=470166.734375Mb
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102859
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108243
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18430.61328125Mb; avail=470164.8515625Mb
2020-10-12 20:29:38 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 20:29:50 | INFO | train_inner | epoch 017:     40 / 110 loss=9.781, nll_loss=9, ppl=512.02, wps=14995.7, ups=2.49, wpb=6016, bsz=225.2, num_updates=1800, lr=9.0055e-05, gnorm=1.201, clip=0, train_wall=28, wall=718
2020-10-12 20:30:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17898.4140625Mb; avail=470693.3359375Mb
2020-10-12 20:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002086
2020-10-12 20:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17899.01953125Mb; avail=470692.73046875Mb
2020-10-12 20:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081456
2020-10-12 20:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17898.796875Mb; avail=470692.97265625Mb
2020-10-12 20:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057185
2020-10-12 20:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141675
2020-10-12 20:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17898.40234375Mb; avail=470693.3359375Mb
2020-10-12 20:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17898.375Mb; avail=470693.453125Mb
2020-10-12 20:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001404
2020-10-12 20:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17898.375Mb; avail=470693.453125Mb
2020-10-12 20:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080579
2020-10-12 20:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17898.35546875Mb; avail=470693.453125Mb
2020-10-12 20:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057349
2020-10-12 20:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140191
2020-10-12 20:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17898.33984375Mb; avail=470693.453125Mb
2020-10-12 20:30:13 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 9.405 | nll_loss 8.542 | ppl 372.61 | wps 56572.8 | wpb 2300 | bsz 88.5 | num_updates 1870 | best_loss 9.405
2020-10-12 20:30:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:30:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 17 @ 1870 updates, score 9.405) (writing took 8.047541711999656 seconds)
2020-10-12 20:30:21 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 20:30:21 | INFO | train | epoch 017 | loss 9.684 | nll_loss 8.89 | ppl 474.32 | wps 15267.9 | ups 2.55 | wpb 5987.1 | bsz 235.8 | num_updates 1870 | lr 9.35533e-05 | gnorm 1.162 | clip 0 | train_wall 31 | wall 749
2020-10-12 20:30:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 20:30:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 20:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18415.21875Mb; avail=470177.18359375Mb
2020-10-12 20:30:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000861
2020-10-12 20:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005798
2020-10-12 20:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18415.421875Mb; avail=470177.18359375Mb
2020-10-12 20:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000238
2020-10-12 20:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18415.421875Mb; avail=470177.18359375Mb
2020-10-12 20:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100597
2020-10-12 20:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107459
2020-10-12 20:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18409.31640625Mb; avail=470183.2890625Mb
2020-10-12 20:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18409.09375Mb; avail=470183.171875Mb
2020-10-12 20:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004176
2020-10-12 20:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18409.0546875Mb; avail=470183.4140625Mb
2020-10-12 20:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 20:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18409.0546875Mb; avail=470183.4140625Mb
2020-10-12 20:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100695
2020-10-12 20:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105993
2020-10-12 20:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18408.96484375Mb; avail=470183.4140625Mb
2020-10-12 20:30:21 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 20:30:30 | INFO | train_inner | epoch 018:     30 / 110 loss=9.653, nll_loss=8.854, ppl=462.65, wps=14622.9, ups=2.49, wpb=5876.3, bsz=224.3, num_updates=1900, lr=9.50525e-05, gnorm=1.209, clip=0, train_wall=28, wall=758
2020-10-12 20:30:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18239.05078125Mb; avail=470356.52734375Mb
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002153
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18240.03125Mb; avail=470355.953125Mb
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080977
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18239.375Mb; avail=470355.27734375Mb
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055147
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139099
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18239.98046875Mb; avail=470353.88671875Mb
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18240.21484375Mb; avail=470353.3984375Mb
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001414
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18240.69140625Mb; avail=470353.171875Mb
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080153
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18240.21484375Mb; avail=470352.0Mb
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054770
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137085
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18240.609375Mb; avail=470350.48828125Mb
2020-10-12 20:30:56 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 9.33 | nll_loss 8.446 | ppl 348.85 | wps 56833.2 | wpb 2300 | bsz 88.5 | num_updates 1980 | best_loss 9.33
2020-10-12 20:30:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:31:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 18 @ 1980 updates, score 9.33) (writing took 10.535093021000648 seconds)
2020-10-12 20:31:07 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 20:31:07 | INFO | train | epoch 018 | loss 9.565 | nll_loss 8.753 | ppl 431.35 | wps 14456.7 | ups 2.41 | wpb 5987.1 | bsz 235.8 | num_updates 1980 | lr 9.90505e-05 | gnorm 1.203 | clip 0 | train_wall 31 | wall 794
2020-10-12 20:31:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 20:31:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18245.94921875Mb; avail=470346.140625Mb
2020-10-12 20:31:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000773
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005359
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18245.94921875Mb; avail=470346.140625Mb
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000214
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18245.94921875Mb; avail=470346.140625Mb
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089822
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096164
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18240.28515625Mb; avail=470351.80859375Mb
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18240.3203125Mb; avail=470351.56640625Mb
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003851
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18240.3203125Mb; avail=470351.56640625Mb
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18240.3203125Mb; avail=470351.56640625Mb
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089185
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094020
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18240.15625Mb; avail=470352.140625Mb
2020-10-12 20:31:07 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 20:31:13 | INFO | train_inner | epoch 019:     20 / 110 loss=9.525, nll_loss=8.707, ppl=418.01, wps=14264.4, ups=2.35, wpb=6078.3, bsz=246.5, num_updates=2000, lr=0.00010005, gnorm=1.189, clip=0, train_wall=28, wall=800
2020-10-12 20:31:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17836.7578125Mb; avail=470754.7578125Mb
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002307
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.7578125Mb; avail=470754.7578125Mb
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.089578
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.10546875Mb; avail=470741.07421875Mb
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062255
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.155176
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.828125Mb; avail=470736.23046875Mb
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17854.84375Mb; avail=470736.109375Mb
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001561
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.84375Mb; avail=470736.109375Mb
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.088660
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17855.453125Mb; avail=470736.2890625Mb
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063330
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.154407
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17855.49609375Mb; avail=470736.15234375Mb
2020-10-12 20:31:42 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 9.222 | nll_loss 8.329 | ppl 321.55 | wps 56294.5 | wpb 2300 | bsz 88.5 | num_updates 2090 | best_loss 9.222
2020-10-12 20:31:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:31:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 19 @ 2090 updates, score 9.222) (writing took 8.202624461999221 seconds)
2020-10-12 20:31:50 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 20:31:50 | INFO | train | epoch 019 | loss 9.449 | nll_loss 8.619 | ppl 393.29 | wps 15209.8 | ups 2.54 | wpb 5987.1 | bsz 235.8 | num_updates 2090 | lr 0.000104548 | gnorm 1.172 | clip 0 | train_wall 31 | wall 838
2020-10-12 20:31:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 20:31:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17866.890625Mb; avail=470724.3984375Mb
2020-10-12 20:31:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000775
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005379
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17866.8984375Mb; avail=470724.27734375Mb
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17866.8984375Mb; avail=470724.27734375Mb
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088296
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094672
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.66015625Mb; avail=470731.2890625Mb
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17861.01171875Mb; avail=470730.94140625Mb
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004778
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.01171875Mb; avail=470730.94140625Mb
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000232
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.01171875Mb; avail=470730.94140625Mb
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089686
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095465
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.01171875Mb; avail=470730.3359375Mb
2020-10-12 20:31:50 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 20:31:53 | INFO | train_inner | epoch 020:     10 / 110 loss=9.444, nll_loss=8.613, ppl=391.66, wps=14819.9, ups=2.48, wpb=5983.5, bsz=233.1, num_updates=2100, lr=0.000105048, gnorm=1.172, clip=0, train_wall=28, wall=841
2020-10-12 20:32:22 | INFO | train_inner | epoch 020:    110 / 110 loss=9.329, nll_loss=8.48, ppl=357.15, wps=20831.3, ups=3.48, wpb=5978.5, bsz=240.3, num_updates=2200, lr=0.000110045, gnorm=1.233, clip=0, train_wall=28, wall=869
2020-10-12 20:32:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18510.17578125Mb; avail=470081.4296875Mb
2020-10-12 20:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001859
2020-10-12 20:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18510.17578125Mb; avail=470081.4296875Mb
2020-10-12 20:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081561
2020-10-12 20:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18509.9140625Mb; avail=470081.09765625Mb
2020-10-12 20:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056902
2020-10-12 20:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141305
2020-10-12 20:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18509.88671875Mb; avail=470081.21875Mb
2020-10-12 20:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18509.8046875Mb; avail=470080.9765625Mb
2020-10-12 20:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001581
2020-10-12 20:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18509.8046875Mb; avail=470080.85546875Mb
2020-10-12 20:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080033
2020-10-12 20:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18510.41015625Mb; avail=470081.09765625Mb
2020-10-12 20:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055610
2020-10-12 20:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138062
2020-10-12 20:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18510.296875Mb; avail=470081.21875Mb
2020-10-12 20:32:25 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 9.157 | nll_loss 8.247 | ppl 303.72 | wps 56833.6 | wpb 2300 | bsz 88.5 | num_updates 2200 | best_loss 9.157
2020-10-12 20:32:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:32:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 20 @ 2200 updates, score 9.157) (writing took 7.236265095998533 seconds)
2020-10-12 20:32:32 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 20:32:32 | INFO | train | epoch 020 | loss 9.338 | nll_loss 8.49 | ppl 359.66 | wps 15665.7 | ups 2.62 | wpb 5987.1 | bsz 235.8 | num_updates 2200 | lr 0.000110045 | gnorm 1.224 | clip 0 | train_wall 31 | wall 880
2020-10-12 20:32:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 20:32:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 20:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18402.50390625Mb; avail=470192.61328125Mb
2020-10-12 20:32:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000780
2020-10-12 20:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005379
2020-10-12 20:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18402.4453125Mb; avail=470192.609375Mb
2020-10-12 20:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18402.31640625Mb; avail=470191.890625Mb
2020-10-12 20:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088585
2020-10-12 20:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095003
2020-10-12 20:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18400.51953125Mb; avail=470192.4765625Mb
2020-10-12 20:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18400.16796875Mb; avail=470191.9296875Mb
2020-10-12 20:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005508
2020-10-12 20:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18401.453125Mb; avail=470191.53125Mb
2020-10-12 20:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000264
2020-10-12 20:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18401.32421875Mb; avail=470191.41796875Mb
2020-10-12 20:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088704
2020-10-12 20:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095360
2020-10-12 20:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18395.48828125Mb; avail=470196.19140625Mb
2020-10-12 20:32:32 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 20:33:01 | INFO | train_inner | epoch 021:    100 / 110 loss=9.246, nll_loss=8.383, ppl=333.95, wps=15126.5, ups=2.55, wpb=5932.6, bsz=233.8, num_updates=2300, lr=0.000115043, gnorm=1.255, clip=0, train_wall=28, wall=909
2020-10-12 20:33:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:33:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18509.03125Mb; avail=470082.421875Mb
2020-10-12 20:33:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001940
2020-10-12 20:33:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18509.03125Mb; avail=470082.421875Mb
2020-10-12 20:33:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080043
2020-10-12 20:33:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18509.03125Mb; avail=470082.421875Mb
2020-10-12 20:33:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055162
2020-10-12 20:33:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137988
2020-10-12 20:33:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18509.73828125Mb; avail=470081.9375Mb
2020-10-12 20:33:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18509.73046875Mb; avail=470081.9375Mb
2020-10-12 20:33:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001220
2020-10-12 20:33:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18509.73046875Mb; avail=470081.9375Mb
2020-10-12 20:33:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080148
2020-10-12 20:33:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18509.74609375Mb; avail=470081.9375Mb
2020-10-12 20:33:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055817
2020-10-12 20:33:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137963
2020-10-12 20:33:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18509.546875Mb; avail=470081.9375Mb
2020-10-12 20:33:07 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 9.085 | nll_loss 8.169 | ppl 287.81 | wps 56637.2 | wpb 2300 | bsz 88.5 | num_updates 2310 | best_loss 9.085
2020-10-12 20:33:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:33:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 21 @ 2310 updates, score 9.085) (writing took 9.08943526099938 seconds)
2020-10-12 20:33:16 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 20:33:16 | INFO | train | epoch 021 | loss 9.246 | nll_loss 8.383 | ppl 333.77 | wps 14966 | ups 2.5 | wpb 5987.1 | bsz 235.8 | num_updates 2310 | lr 0.000115542 | gnorm 1.241 | clip 0 | train_wall 31 | wall 924
2020-10-12 20:33:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 20:33:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 20:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17879.14453125Mb; avail=470713.18359375Mb
2020-10-12 20:33:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001025
2020-10-12 20:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005936
2020-10-12 20:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.73046875Mb; avail=470718.10546875Mb
2020-10-12 20:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000226
2020-10-12 20:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.73046875Mb; avail=470718.10546875Mb
2020-10-12 20:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091831
2020-10-12 20:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098745
2020-10-12 20:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17872.07421875Mb; avail=470719.96484375Mb
2020-10-12 20:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17871.67578125Mb; avail=470720.45703125Mb
2020-10-12 20:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003631
2020-10-12 20:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.7109375Mb; avail=470720.21484375Mb
2020-10-12 20:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 20:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.7109375Mb; avail=470720.21484375Mb
2020-10-12 20:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090560
2020-10-12 20:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095087
2020-10-12 20:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17872.42578125Mb; avail=470719.609375Mb
2020-10-12 20:33:16 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 20:33:42 | INFO | train_inner | epoch 022:     90 / 110 loss=9.138, nll_loss=8.258, ppl=306.16, wps=14587.4, ups=2.42, wpb=6023.8, bsz=240.3, num_updates=2400, lr=0.00012004, gnorm=1.222, clip=0, train_wall=28, wall=950
2020-10-12 20:33:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18236.42578125Mb; avail=470358.3828125Mb
2020-10-12 20:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002159
2020-10-12 20:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18237.984375Mb; avail=470357.9296875Mb
2020-10-12 20:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081182
2020-10-12 20:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18238.4921875Mb; avail=470355.81640625Mb
2020-10-12 20:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056871
2020-10-12 20:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141326
2020-10-12 20:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18238.03515625Mb; avail=470355.7109375Mb
2020-10-12 20:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18237.828125Mb; avail=470355.7578125Mb
2020-10-12 20:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001471
2020-10-12 20:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18237.5703125Mb; avail=470355.41796875Mb
2020-10-12 20:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079534
2020-10-12 20:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18238.28515625Mb; avail=470354.51953125Mb
2020-10-12 20:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056720
2020-10-12 20:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138757
2020-10-12 20:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18237.8046875Mb; avail=470354.05078125Mb
2020-10-12 20:33:51 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 8.991 | nll_loss 8.036 | ppl 262.4 | wps 56660.9 | wpb 2300 | bsz 88.5 | num_updates 2420 | best_loss 8.991
2020-10-12 20:33:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:33:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 22 @ 2420 updates, score 8.991) (writing took 4.460132830001385 seconds)
2020-10-12 20:33:55 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 20:33:55 | INFO | train | epoch 022 | loss 9.138 | nll_loss 8.257 | ppl 306.01 | wps 16653.5 | ups 2.78 | wpb 5987.1 | bsz 235.8 | num_updates 2420 | lr 0.00012104 | gnorm 1.244 | clip 0 | train_wall 31 | wall 963
2020-10-12 20:33:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 20:33:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 20:33:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15438.76953125Mb; avail=473164.65234375Mb
2020-10-12 20:33:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000990
2020-10-12 20:33:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006112
2020-10-12 20:33:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15439.375Mb; avail=473164.046875Mb
2020-10-12 20:33:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000276
2020-10-12 20:33:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15439.375Mb; avail=473164.046875Mb
2020-10-12 20:33:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.108441
2020-10-12 20:33:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115718
2020-10-12 20:33:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15447.45703125Mb; avail=473156.1796875Mb
2020-10-12 20:33:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15459.1875Mb; avail=473144.44921875Mb
2020-10-12 20:33:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003864
2020-10-12 20:33:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15459.79296875Mb; avail=473143.84375Mb
2020-10-12 20:33:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-12 20:33:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15459.79296875Mb; avail=473143.84375Mb
2020-10-12 20:33:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093541
2020-10-12 20:33:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098701
2020-10-12 20:33:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15471.22265625Mb; avail=473132.14453125Mb
2020-10-12 20:33:56 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 20:34:19 | INFO | train_inner | epoch 023:     80 / 110 loss=9.06, nll_loss=8.167, ppl=287.41, wps=16368.8, ups=2.75, wpb=5960.1, bsz=234.1, num_updates=2500, lr=0.000125037, gnorm=1.273, clip=0, train_wall=28, wall=986
2020-10-12 20:34:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17934.44140625Mb; avail=470657.26171875Mb
2020-10-12 20:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002220
2020-10-12 20:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17935.046875Mb; avail=470656.65625Mb
2020-10-12 20:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083999
2020-10-12 20:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17935.0Mb; avail=470656.4140625Mb
2020-10-12 20:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056895
2020-10-12 20:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.143940
2020-10-12 20:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17934.8046875Mb; avail=470656.8984375Mb
2020-10-12 20:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17935.2578125Mb; avail=470656.65625Mb
2020-10-12 20:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001284
2020-10-12 20:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17935.2578125Mb; avail=470656.65625Mb
2020-10-12 20:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080884
2020-10-12 20:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17935.265625Mb; avail=470656.4140625Mb
2020-10-12 20:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055685
2020-10-12 20:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138598
2020-10-12 20:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17935.29296875Mb; avail=470656.29296875Mb
2020-10-12 20:34:30 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 8.914 | nll_loss 7.956 | ppl 248.32 | wps 56463.6 | wpb 2300 | bsz 88.5 | num_updates 2530 | best_loss 8.914
2020-10-12 20:34:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:34:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 23 @ 2530 updates, score 8.914) (writing took 4.4636011620004865 seconds)
2020-10-12 20:34:35 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 20:34:35 | INFO | train | epoch 023 | loss 9.032 | nll_loss 8.135 | ppl 281.1 | wps 16751.7 | ups 2.8 | wpb 5987.1 | bsz 235.8 | num_updates 2530 | lr 0.000126537 | gnorm 1.224 | clip 0 | train_wall 31 | wall 1002
2020-10-12 20:34:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 20:34:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 20:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17922.60546875Mb; avail=470668.80078125Mb
2020-10-12 20:34:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001027
2020-10-12 20:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006299
2020-10-12 20:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17922.65234375Mb; avail=470668.6796875Mb
2020-10-12 20:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000241
2020-10-12 20:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17922.5546875Mb; avail=470668.6796875Mb
2020-10-12 20:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090106
2020-10-12 20:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097477
2020-10-12 20:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17916.39453125Mb; avail=470674.83203125Mb
2020-10-12 20:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17917.03125Mb; avail=470674.8359375Mb
2020-10-12 20:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003712
2020-10-12 20:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17917.03125Mb; avail=470674.8359375Mb
2020-10-12 20:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 20:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17917.03125Mb; avail=470674.8359375Mb
2020-10-12 20:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091558
2020-10-12 20:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096268
2020-10-12 20:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17916.97265625Mb; avail=470674.8359375Mb
2020-10-12 20:34:35 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 20:34:55 | INFO | train_inner | epoch 024:     70 / 110 loss=8.953, nll_loss=8.044, ppl=263.9, wps=16342.5, ups=2.74, wpb=5960.1, bsz=238.3, num_updates=2600, lr=0.000130035, gnorm=1.219, clip=0, train_wall=28, wall=1023
2020-10-12 20:35:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17826.46484375Mb; avail=470747.359375Mb
2020-10-12 20:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002229
2020-10-12 20:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.078125Mb; avail=470746.75390625Mb
2020-10-12 20:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079515
2020-10-12 20:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.95703125Mb; avail=470746.875Mb
2020-10-12 20:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055733
2020-10-12 20:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138277
2020-10-12 20:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.921875Mb; avail=470747.1171875Mb
2020-10-12 20:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17826.8515625Mb; avail=470747.359375Mb
2020-10-12 20:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001212
2020-10-12 20:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.8515625Mb; avail=470747.359375Mb
2020-10-12 20:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080692
2020-10-12 20:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.5859375Mb; avail=470747.21484375Mb
2020-10-12 20:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055663
2020-10-12 20:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138314
2020-10-12 20:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.4921875Mb; avail=470747.3359375Mb
2020-10-12 20:35:10 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.852 | nll_loss 7.881 | ppl 235.75 | wps 57039.5 | wpb 2300 | bsz 88.5 | num_updates 2640 | best_loss 8.852
2020-10-12 20:35:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:35:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 24 @ 2640 updates, score 8.852) (writing took 4.565261372999885 seconds)
2020-10-12 20:35:14 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 20:35:14 | INFO | train | epoch 024 | loss 8.929 | nll_loss 8.015 | ppl 258.64 | wps 16723.9 | ups 2.79 | wpb 5987.1 | bsz 235.8 | num_updates 2640 | lr 0.000132034 | gnorm 1.251 | clip 0 | train_wall 31 | wall 1042
2020-10-12 20:35:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 20:35:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 20:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17835.11328125Mb; avail=470738.46484375Mb
2020-10-12 20:35:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000813
2020-10-12 20:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005472
2020-10-12 20:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.11328125Mb; avail=470738.46484375Mb
2020-10-12 20:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-12 20:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.11328125Mb; avail=470738.46484375Mb
2020-10-12 20:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089156
2020-10-12 20:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095565
2020-10-12 20:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17828.859375Mb; avail=470744.5078125Mb
2020-10-12 20:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17828.859375Mb; avail=470744.5078125Mb
2020-10-12 20:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003715
2020-10-12 20:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17828.859375Mb; avail=470744.5078125Mb
2020-10-12 20:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000210
2020-10-12 20:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17828.859375Mb; avail=470744.5078125Mb
2020-10-12 20:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088180
2020-10-12 20:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.092843
2020-10-12 20:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17828.79296875Mb; avail=470744.265625Mb
2020-10-12 20:35:14 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 20:35:32 | INFO | train_inner | epoch 025:     60 / 110 loss=8.903, nll_loss=7.985, ppl=253.28, wps=16674, ups=2.71, wpb=6148.8, bsz=226.6, num_updates=2700, lr=0.000135032, gnorm=1.216, clip=0, train_wall=28, wall=1060
2020-10-12 20:35:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18880.625Mb; avail=469692.8828125Mb
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002270
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18858.171875Mb; avail=469716.8125Mb
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081430
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18449.83203125Mb; avail=470128.24609375Mb
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056695
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141442
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18514.57421875Mb; avail=470065.703125Mb
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18546.71875Mb; avail=470034.29296875Mb
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001344
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18547.671875Mb; avail=470032.25Mb
2020-10-12 20:35:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079461
2020-10-12 20:35:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18605.62890625Mb; avail=469976.26171875Mb
2020-10-12 20:35:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056270
2020-10-12 20:35:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138095
2020-10-12 20:35:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18605.8359375Mb; avail=469980.81640625Mb
2020-10-12 20:35:49 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.813 | nll_loss 7.823 | ppl 226.44 | wps 56832.6 | wpb 2300 | bsz 88.5 | num_updates 2750 | best_loss 8.813
2020-10-12 20:35:49 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:35:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 25 @ 2750 updates, score 8.813) (writing took 7.8990496110000095 seconds)
2020-10-12 20:35:57 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 20:35:57 | INFO | train | epoch 025 | loss 8.834 | nll_loss 7.904 | ppl 239.5 | wps 15324.1 | ups 2.56 | wpb 5987.1 | bsz 235.8 | num_updates 2750 | lr 0.000137531 | gnorm 1.299 | clip 0 | train_wall 31 | wall 1085
2020-10-12 20:35:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 20:35:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 20:35:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17271.46875Mb; avail=471304.96484375Mb
2020-10-12 20:35:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000824
2020-10-12 20:35:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006148
2020-10-12 20:35:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17278.734375Mb; avail=471297.69921875Mb
2020-10-12 20:35:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000244
2020-10-12 20:35:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17279.33984375Mb; avail=471297.09375Mb
2020-10-12 20:35:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088980
2020-10-12 20:35:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096230
2020-10-12 20:35:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17339.24609375Mb; avail=471237.1328125Mb
2020-10-12 20:35:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17364.09765625Mb; avail=471211.6875Mb
2020-10-12 20:35:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004886
2020-10-12 20:35:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17368.30859375Mb; avail=471208.17578125Mb
2020-10-12 20:35:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000237
2020-10-12 20:35:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17368.9140625Mb; avail=471207.5703125Mb
2020-10-12 20:35:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088289
2020-10-12 20:35:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094295
2020-10-12 20:35:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17414.56640625Mb; avail=471161.91796875Mb
2020-10-12 20:35:57 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 20:36:12 | INFO | train_inner | epoch 026:     50 / 110 loss=8.763, nll_loss=7.821, ppl=226.21, wps=14895.5, ups=2.51, wpb=5923.5, bsz=239.6, num_updates=2800, lr=0.00014003, gnorm=1.324, clip=0, train_wall=28, wall=1099
2020-10-12 20:36:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15902.87890625Mb; avail=472682.953125Mb
2020-10-12 20:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002109
2020-10-12 20:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15905.90625Mb; avail=472680.53125Mb
2020-10-12 20:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081160
2020-10-12 20:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16007.875Mb; avail=472578.5703125Mb
2020-10-12 20:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056985
2020-10-12 20:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141125
2020-10-12 20:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16074.17578125Mb; avail=472512.4609375Mb
2020-10-12 20:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16106.765625Mb; avail=472479.87109375Mb
2020-10-12 20:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001384
2020-10-12 20:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16107.86328125Mb; avail=472478.7734375Mb
2020-10-12 20:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079632
2020-10-12 20:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16111.48828125Mb; avail=472482.28515625Mb
2020-10-12 20:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056711
2020-10-12 20:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138719
2020-10-12 20:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16105.0390625Mb; avail=472487.52734375Mb
2020-10-12 20:36:32 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.775 | nll_loss 7.782 | ppl 220.13 | wps 56446.8 | wpb 2300 | bsz 88.5 | num_updates 2860 | best_loss 8.775
2020-10-12 20:36:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:36:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 26 @ 2860 updates, score 8.775) (writing took 10.102432566000061 seconds)
2020-10-12 20:36:42 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 20:36:42 | INFO | train | epoch 026 | loss 8.731 | nll_loss 7.786 | ppl 220.66 | wps 14588.4 | ups 2.44 | wpb 5987.1 | bsz 235.8 | num_updates 2860 | lr 0.000143029 | gnorm 1.288 | clip 0 | train_wall 31 | wall 1130
2020-10-12 20:36:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 20:36:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17867.4765625Mb; avail=470706.41796875Mb
2020-10-12 20:36:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000931
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005690
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17867.4921875Mb; avail=470706.296875Mb
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000205
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17867.4921875Mb; avail=470706.296875Mb
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088917
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095573
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.5703125Mb; avail=470712.328125Mb
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17861.8203125Mb; avail=470711.97265625Mb
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003726
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.8203125Mb; avail=470711.97265625Mb
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000188
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.8203125Mb; avail=470711.97265625Mb
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089230
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093912
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.62109375Mb; avail=470711.23828125Mb
2020-10-12 20:36:42 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 20:36:54 | INFO | train_inner | epoch 027:     40 / 110 loss=8.729, nll_loss=7.781, ppl=220.02, wps=14167.4, ups=2.36, wpb=6005.4, bsz=231.7, num_updates=2900, lr=0.000145028, gnorm=1.374, clip=0, train_wall=28, wall=1142
2020-10-12 20:37:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:37:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18479.26171875Mb; avail=470101.40625Mb
2020-10-12 20:37:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001942
2020-10-12 20:37:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18480.6015625Mb; avail=470099.2265625Mb
2020-10-12 20:37:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.086233
2020-10-12 20:37:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18552.8125Mb; avail=470026.10546875Mb
2020-10-12 20:37:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064095
2020-10-12 20:37:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.153597
2020-10-12 20:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18578.7265625Mb; avail=469998.94921875Mb
2020-10-12 20:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18578.6171875Mb; avail=469998.95703125Mb
2020-10-12 20:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001740
2020-10-12 20:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18578.23046875Mb; avail=469998.6171875Mb
2020-10-12 20:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.091067
2020-10-12 20:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18642.4453125Mb; avail=469933.95703125Mb
2020-10-12 20:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062831
2020-10-12 20:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.156654
2020-10-12 20:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18642.83203125Mb; avail=469933.45703125Mb
2020-10-12 20:37:17 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.672 | nll_loss 7.664 | ppl 202.84 | wps 57184.1 | wpb 2300 | bsz 88.5 | num_updates 2970 | best_loss 8.672
2020-10-12 20:37:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:37:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 27 @ 2970 updates, score 8.672) (writing took 9.263877184999728 seconds)
2020-10-12 20:37:27 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 20:37:27 | INFO | train | epoch 027 | loss 8.638 | nll_loss 7.677 | ppl 204.6 | wps 14861.7 | ups 2.48 | wpb 5987.1 | bsz 235.8 | num_updates 2970 | lr 0.000148526 | gnorm 1.319 | clip 0 | train_wall 31 | wall 1174
2020-10-12 20:37:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 20:37:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 20:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17882.3671875Mb; avail=470691.3671875Mb
2020-10-12 20:37:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000833
2020-10-12 20:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006735
2020-10-12 20:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17882.3671875Mb; avail=470691.3671875Mb
2020-10-12 20:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000248
2020-10-12 20:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17882.390625Mb; avail=470691.24609375Mb
2020-10-12 20:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091172
2020-10-12 20:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099000
2020-10-12 20:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.29296875Mb; avail=470697.0703125Mb
2020-10-12 20:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17876.41796875Mb; avail=470697.078125Mb
2020-10-12 20:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004961
2020-10-12 20:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.41796875Mb; avail=470697.078125Mb
2020-10-12 20:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000230
2020-10-12 20:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.41796875Mb; avail=470697.078125Mb
2020-10-12 20:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088996
2020-10-12 20:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094952
2020-10-12 20:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.63671875Mb; avail=470697.34375Mb
2020-10-12 20:37:27 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 20:37:35 | INFO | train_inner | epoch 028:     30 / 110 loss=8.578, nll_loss=7.608, ppl=195.15, wps=14454, ups=2.42, wpb=5967.9, bsz=234.7, num_updates=3000, lr=0.000150025, gnorm=1.26, clip=0, train_wall=28, wall=1183
2020-10-12 20:37:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17933.40625Mb; avail=470642.234375Mb
2020-10-12 20:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002031
2020-10-12 20:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17933.609375Mb; avail=470642.11328125Mb
2020-10-12 20:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082372
2020-10-12 20:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17932.875Mb; avail=470647.51171875Mb
2020-10-12 20:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057009
2020-10-12 20:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142394
2020-10-12 20:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17934.45703125Mb; avail=470646.734375Mb
2020-10-12 20:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17934.203125Mb; avail=470646.50390625Mb
2020-10-12 20:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001349
2020-10-12 20:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17934.07421875Mb; avail=470646.390625Mb
2020-10-12 20:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081876
2020-10-12 20:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17934.30859375Mb; avail=470645.05078125Mb
2020-10-12 20:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056720
2020-10-12 20:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140942
2020-10-12 20:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17933.625Mb; avail=470644.2578125Mb
2020-10-12 20:38:01 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.589 | nll_loss 7.559 | ppl 188.57 | wps 56571.8 | wpb 2300 | bsz 88.5 | num_updates 3080 | best_loss 8.589
2020-10-12 20:38:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:38:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 28 @ 3080 updates, score 8.589) (writing took 11.57769716400071 seconds)
2020-10-12 20:38:13 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 20:38:13 | INFO | train | epoch 028 | loss 8.506 | nll_loss 7.525 | ppl 184.21 | wps 14170 | ups 2.37 | wpb 5987.1 | bsz 235.8 | num_updates 3080 | lr 0.000154023 | gnorm 1.27 | clip 0 | train_wall 31 | wall 1221
2020-10-12 20:38:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 20:38:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 20:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19084.47265625Mb; avail=469490.43359375Mb
2020-10-12 20:38:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000778
2020-10-12 20:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006007
2020-10-12 20:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19084.47265625Mb; avail=469490.43359375Mb
2020-10-12 20:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000265
2020-10-12 20:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19084.47265625Mb; avail=469490.43359375Mb
2020-10-12 20:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101519
2020-10-12 20:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108617
2020-10-12 20:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19079.17578125Mb; avail=469495.75Mb
2020-10-12 20:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19079.3125Mb; avail=469495.61328125Mb
2020-10-12 20:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004238
2020-10-12 20:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19079.3125Mb; avail=469495.61328125Mb
2020-10-12 20:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000272
2020-10-12 20:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19079.3125Mb; avail=469495.61328125Mb
2020-10-12 20:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101583
2020-10-12 20:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106918
2020-10-12 20:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19078.87890625Mb; avail=469496.01953125Mb
2020-10-12 20:38:13 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 20:38:19 | INFO | train_inner | epoch 029:     20 / 110 loss=8.467, nll_loss=7.481, ppl=178.62, wps=13732.3, ups=2.29, wpb=5996.9, bsz=243, num_updates=3100, lr=0.000155023, gnorm=1.26, clip=0, train_wall=28, wall=1227
2020-10-12 20:38:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:38:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17856.49609375Mb; avail=470697.9765625Mb
2020-10-12 20:38:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002150
2020-10-12 20:38:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.1015625Mb; avail=470697.37109375Mb
2020-10-12 20:38:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078970
2020-10-12 20:38:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.140625Mb; avail=470697.12890625Mb
2020-10-12 20:38:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056301
2020-10-12 20:38:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138246
2020-10-12 20:38:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17856.9765625Mb; avail=470697.4921875Mb
2020-10-12 20:38:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17856.9765625Mb; avail=470697.4921875Mb
2020-10-12 20:38:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001206
2020-10-12 20:38:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17856.9765625Mb; avail=470697.4921875Mb
2020-10-12 20:38:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080330
2020-10-12 20:38:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.09375Mb; avail=470697.25390625Mb
2020-10-12 20:38:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057781
2020-10-12 20:38:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140075
2020-10-12 20:38:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.0625Mb; avail=470697.375Mb
2020-10-12 20:38:48 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.513 | nll_loss 7.458 | ppl 175.86 | wps 57203.5 | wpb 2300 | bsz 88.5 | num_updates 3190 | best_loss 8.513
2020-10-12 20:38:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:39:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 29 @ 3190 updates, score 8.513) (writing took 16.616364981000515 seconds)
2020-10-12 20:39:04 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 20:39:04 | INFO | train | epoch 029 | loss 8.382 | nll_loss 7.381 | ppl 166.68 | wps 12879.7 | ups 2.15 | wpb 5987.1 | bsz 235.8 | num_updates 3190 | lr 0.00015952 | gnorm 1.284 | clip 0 | train_wall 30 | wall 1272
2020-10-12 20:39:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 20:39:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 20:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18401.8046875Mb; avail=470152.98046875Mb
2020-10-12 20:39:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001180
2020-10-12 20:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006617
2020-10-12 20:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18396.44921875Mb; avail=470158.515625Mb
2020-10-12 20:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000307
2020-10-12 20:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18396.44921875Mb; avail=470158.515625Mb
2020-10-12 20:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100681
2020-10-12 20:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108392
2020-10-12 20:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18395.109375Mb; avail=470159.9609375Mb
2020-10-12 20:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18395.1328125Mb; avail=470159.82421875Mb
2020-10-12 20:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004180
2020-10-12 20:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18395.1328125Mb; avail=470159.82421875Mb
2020-10-12 20:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000264
2020-10-12 20:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18395.1328125Mb; avail=470159.82421875Mb
2020-10-12 20:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100007
2020-10-12 20:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105290
2020-10-12 20:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18395.26953125Mb; avail=470159.8984375Mb
2020-10-12 20:39:04 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 20:39:07 | INFO | train_inner | epoch 030:     10 / 110 loss=8.372, nll_loss=7.369, ppl=165.36, wps=12260.4, ups=2.07, wpb=5927, bsz=232.4, num_updates=3200, lr=0.00016002, gnorm=1.296, clip=0, train_wall=28, wall=1275
2020-10-12 20:39:36 | INFO | train_inner | epoch 030:    110 / 110 loss=8.259, nll_loss=7.238, ppl=150.97, wps=20856.5, ups=3.47, wpb=6012.5, bsz=239.4, num_updates=3300, lr=0.000165018, gnorm=1.316, clip=0, train_wall=28, wall=1304
2020-10-12 20:39:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17796.45703125Mb; avail=470758.26953125Mb
2020-10-12 20:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002162
2020-10-12 20:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17797.0625Mb; avail=470757.6640625Mb
2020-10-12 20:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080364
2020-10-12 20:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17797.0625Mb; avail=470757.6640625Mb
2020-10-12 20:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055923
2020-10-12 20:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139321
2020-10-12 20:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17796.9296875Mb; avail=470757.78515625Mb
2020-10-12 20:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17797.171875Mb; avail=470757.54296875Mb
2020-10-12 20:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001363
2020-10-12 20:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17797.171875Mb; avail=470757.54296875Mb
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081195
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17797.296875Mb; avail=470757.421875Mb
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056146
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139557
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17797.296875Mb; avail=470757.421875Mb
2020-10-12 20:39:39 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.486 | nll_loss 7.428 | ppl 172.2 | wps 57221.9 | wpb 2300 | bsz 88.5 | num_updates 3300 | best_loss 8.486
2020-10-12 20:39:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:39:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 30 @ 3300 updates, score 8.486) (writing took 6.733984700000292 seconds)
2020-10-12 20:39:46 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 20:39:46 | INFO | train | epoch 030 | loss 8.265 | nll_loss 7.245 | ppl 151.72 | wps 15773.4 | ups 2.63 | wpb 5987.1 | bsz 235.8 | num_updates 3300 | lr 0.000165018 | gnorm 1.32 | clip 0 | train_wall 31 | wall 1314
2020-10-12 20:39:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 20:39:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 20:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18937.953125Mb; avail=469623.015625Mb
2020-10-12 20:39:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001051
2020-10-12 20:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006421
2020-10-12 20:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18937.67578125Mb; avail=469622.328125Mb
2020-10-12 20:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000222
2020-10-12 20:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18938.0234375Mb; avail=469623.0859375Mb
2020-10-12 20:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106226
2020-10-12 20:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114246
2020-10-12 20:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18937.55078125Mb; avail=469618.984375Mb
2020-10-12 20:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18937.9609375Mb; avail=469618.5703125Mb
2020-10-12 20:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005204
2020-10-12 20:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18937.4296875Mb; avail=469618.53125Mb
2020-10-12 20:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000296
2020-10-12 20:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18937.171875Mb; avail=469618.41796875Mb
2020-10-12 20:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103085
2020-10-12 20:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109547
2020-10-12 20:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18930.94140625Mb; avail=469623.80078125Mb
2020-10-12 20:39:46 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 20:40:15 | INFO | train_inner | epoch 031:    100 / 110 loss=8.148, nll_loss=7.11, ppl=138.12, wps=15380.1, ups=2.6, wpb=5920.4, bsz=234.3, num_updates=3400, lr=0.000170015, gnorm=1.393, clip=0, train_wall=28, wall=1343
2020-10-12 20:40:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17837.98046875Mb; avail=470716.5625Mb
2020-10-12 20:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002092
2020-10-12 20:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.5859375Mb; avail=470715.95703125Mb
2020-10-12 20:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081926
2020-10-12 20:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.28515625Mb; avail=470716.3203125Mb
2020-10-12 20:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056495
2020-10-12 20:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141331
2020-10-12 20:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.37890625Mb; avail=470716.44140625Mb
2020-10-12 20:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17838.38671875Mb; avail=470716.3203125Mb
2020-10-12 20:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001320
2020-10-12 20:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.38671875Mb; avail=470716.3203125Mb
2020-10-12 20:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080900
2020-10-12 20:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.46875Mb; avail=470715.83984375Mb
2020-10-12 20:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056698
2020-10-12 20:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139684
2020-10-12 20:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.5078125Mb; avail=470715.59765625Mb
2020-10-12 20:40:21 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.389 | nll_loss 7.306 | ppl 158.24 | wps 57388.8 | wpb 2300 | bsz 88.5 | num_updates 3410 | best_loss 8.389
2020-10-12 20:40:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:40:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 31 @ 3410 updates, score 8.389) (writing took 4.468104407000283 seconds)
2020-10-12 20:40:25 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 20:40:25 | INFO | train | epoch 031 | loss 8.154 | nll_loss 7.116 | ppl 138.75 | wps 16837.1 | ups 2.81 | wpb 5987.1 | bsz 235.8 | num_updates 3410 | lr 0.000170515 | gnorm 1.372 | clip 0 | train_wall 31 | wall 1353
2020-10-12 20:40:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 20:40:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 20:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18432.11328125Mb; avail=470121.98046875Mb
2020-10-12 20:40:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001143
2020-10-12 20:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006399
2020-10-12 20:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18439.984375Mb; avail=470114.109375Mb
2020-10-12 20:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000206
2020-10-12 20:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18440.58984375Mb; avail=470113.50390625Mb
2020-10-12 20:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089766
2020-10-12 20:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097231
2020-10-12 20:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18435.19921875Mb; avail=470126.04296875Mb
2020-10-12 20:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18436.5703125Mb; avail=470124.19140625Mb
2020-10-12 20:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004066
2020-10-12 20:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18436.87890625Mb; avail=470124.26953125Mb
2020-10-12 20:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 20:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18436.75Mb; avail=470124.15625Mb
2020-10-12 20:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090063
2020-10-12 20:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095065
2020-10-12 20:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18437.1640625Mb; avail=470122.2890625Mb
2020-10-12 20:40:25 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 20:40:51 | INFO | train_inner | epoch 032:     90 / 110 loss=8.037, nll_loss=6.98, ppl=126.26, wps=16910, ups=2.75, wpb=6138.5, bsz=241, num_updates=3500, lr=0.000175013, gnorm=1.31, clip=0, train_wall=28, wall=1379
2020-10-12 20:40:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:40:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17844.2890625Mb; avail=470710.24609375Mb
2020-10-12 20:40:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002142
2020-10-12 20:40:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17844.25Mb; avail=470710.24609375Mb
2020-10-12 20:40:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.085099
2020-10-12 20:40:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17844.5234375Mb; avail=470710.3671875Mb
2020-10-12 20:40:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057253
2020-10-12 20:40:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.145348
2020-10-12 20:40:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17844.421875Mb; avail=470710.48828125Mb
2020-10-12 20:40:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17844.30078125Mb; avail=470710.609375Mb
2020-10-12 20:40:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001351
2020-10-12 20:40:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17844.30078125Mb; avail=470710.609375Mb
2020-10-12 20:40:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080374
2020-10-12 20:40:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17844.30078125Mb; avail=470710.609375Mb
2020-10-12 20:40:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057852
2020-10-12 20:40:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140365
2020-10-12 20:40:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17844.35546875Mb; avail=470710.45703125Mb
2020-10-12 20:41:00 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.28 | nll_loss 7.18 | ppl 145.04 | wps 56433.8 | wpb 2300 | bsz 88.5 | num_updates 3520 | best_loss 8.28
2020-10-12 20:41:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:41:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 32 @ 3520 updates, score 8.28) (writing took 5.623270635000154 seconds)
2020-10-12 20:41:05 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 20:41:05 | INFO | train | epoch 032 | loss 8.015 | nll_loss 6.955 | ppl 124.09 | wps 16339.3 | ups 2.73 | wpb 5987.1 | bsz 235.8 | num_updates 3520 | lr 0.000176012 | gnorm 1.331 | clip 0 | train_wall 31 | wall 1393
2020-10-12 20:41:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 20:41:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 20:41:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18410.53515625Mb; avail=470144.0234375Mb
2020-10-12 20:41:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001052
2020-10-12 20:41:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006179
2020-10-12 20:41:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18410.53515625Mb; avail=470144.0234375Mb
2020-10-12 20:41:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000249
2020-10-12 20:41:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18410.53515625Mb; avail=470144.0234375Mb
2020-10-12 20:41:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101809
2020-10-12 20:41:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109095
2020-10-12 20:41:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18403.67578125Mb; avail=470150.83984375Mb
2020-10-12 20:41:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18403.69140625Mb; avail=470151.171875Mb
2020-10-12 20:41:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004129
2020-10-12 20:41:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18403.69140625Mb; avail=470151.171875Mb
2020-10-12 20:41:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 20:41:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18403.69140625Mb; avail=470151.171875Mb
2020-10-12 20:41:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101036
2020-10-12 20:41:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106529
2020-10-12 20:41:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18403.76171875Mb; avail=470151.01953125Mb
2020-10-12 20:41:06 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 20:41:29 | INFO | train_inner | epoch 033:     80 / 110 loss=7.913, nll_loss=6.837, ppl=114.33, wps=15651.6, ups=2.64, wpb=5921.1, bsz=242.8, num_updates=3600, lr=0.00018001, gnorm=1.406, clip=0, train_wall=28, wall=1417
2020-10-12 20:41:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17958.34765625Mb; avail=470596.04296875Mb
2020-10-12 20:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002049
2020-10-12 20:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17958.953125Mb; avail=470595.4375Mb
2020-10-12 20:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080007
2020-10-12 20:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17959.2890625Mb; avail=470595.07421875Mb
2020-10-12 20:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056030
2020-10-12 20:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138899
2020-10-12 20:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17959.01953125Mb; avail=470595.4375Mb
2020-10-12 20:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17958.94921875Mb; avail=470595.6796875Mb
2020-10-12 20:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001280
2020-10-12 20:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17958.94921875Mb; avail=470595.6796875Mb
2020-10-12 20:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081231
2020-10-12 20:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17959.140625Mb; avail=470595.31640625Mb
2020-10-12 20:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055860
2020-10-12 20:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139143
2020-10-12 20:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17959.14453125Mb; avail=470595.32421875Mb
2020-10-12 20:41:40 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.228 | nll_loss 7.115 | ppl 138.6 | wps 57125.1 | wpb 2300 | bsz 88.5 | num_updates 3630 | best_loss 8.228
2020-10-12 20:41:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:41:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 33 @ 3630 updates, score 8.228) (writing took 14.575121052999748 seconds)
2020-10-12 20:41:55 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 20:41:55 | INFO | train | epoch 033 | loss 7.899 | nll_loss 6.82 | ppl 113 | wps 13261.6 | ups 2.22 | wpb 5987.1 | bsz 235.8 | num_updates 3630 | lr 0.000181509 | gnorm 1.405 | clip 0 | train_wall 31 | wall 1443
2020-10-12 20:41:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 20:41:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 20:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19182.03125Mb; avail=469381.79296875Mb
2020-10-12 20:41:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000768
2020-10-12 20:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006282
2020-10-12 20:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19181.73828125Mb; avail=469382.14453125Mb
2020-10-12 20:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000266
2020-10-12 20:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19182.21484375Mb; avail=469382.03125Mb
2020-10-12 20:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101093
2020-10-12 20:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108544
2020-10-12 20:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19176.0Mb; avail=469386.08203125Mb
2020-10-12 20:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19176.66015625Mb; avail=469385.28515625Mb
2020-10-12 20:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004220
2020-10-12 20:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19176.21875Mb; avail=469384.77734375Mb
2020-10-12 20:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000241
2020-10-12 20:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19176.08984375Mb; avail=469385.15625Mb
2020-10-12 20:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100141
2020-10-12 20:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105417
2020-10-12 20:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19177.14453125Mb; avail=469383.22265625Mb
2020-10-12 20:41:55 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 20:42:16 | INFO | train_inner | epoch 034:     70 / 110 loss=7.8, nll_loss=6.705, ppl=104.3, wps=12844.6, ups=2.15, wpb=5987.2, bsz=234.1, num_updates=3700, lr=0.000185008, gnorm=1.468, clip=0, train_wall=28, wall=1463
2020-10-12 20:42:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17990.20703125Mb; avail=470564.109375Mb
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002029
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17990.20703125Mb; avail=470564.109375Mb
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080721
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17993.1171875Mb; avail=470561.07421875Mb
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055160
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138720
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18004.07421875Mb; avail=470550.06640625Mb
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18007.24609375Mb; avail=470546.8046875Mb
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001353
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18007.24609375Mb; avail=470546.8046875Mb
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081039
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18008.18359375Mb; avail=470545.47265625Mb
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055823
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138987
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18008.359375Mb; avail=470545.109375Mb
2020-10-12 20:42:30 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.137 | nll_loss 7.002 | ppl 128.19 | wps 56911.1 | wpb 2300 | bsz 88.5 | num_updates 3740 | best_loss 8.137
2020-10-12 20:42:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:42:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 34 @ 3740 updates, score 8.137) (writing took 10.587745535000067 seconds)
2020-10-12 20:42:40 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 20:42:40 | INFO | train | epoch 034 | loss 7.781 | nll_loss 6.682 | ppl 102.66 | wps 14483.6 | ups 2.42 | wpb 5987.1 | bsz 235.8 | num_updates 3740 | lr 0.000187007 | gnorm 1.477 | clip 0 | train_wall 31 | wall 1488
2020-10-12 20:42:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 20:42:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 20:42:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15641.18359375Mb; avail=472925.57421875Mb
2020-10-12 20:42:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000756
2020-10-12 20:42:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006109
2020-10-12 20:42:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15650.265625Mb; avail=472916.4921875Mb
2020-10-12 20:42:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000234
2020-10-12 20:42:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15650.87109375Mb; avail=472915.28125Mb
2020-10-12 20:42:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102092
2020-10-12 20:42:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109301
2020-10-12 20:42:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15794.32421875Mb; avail=472771.87890625Mb
2020-10-12 20:42:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15865.1640625Mb; avail=472701.0390625Mb
2020-10-12 20:42:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004169
2020-10-12 20:42:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15871.82421875Mb; avail=472694.984375Mb
2020-10-12 20:42:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000262
2020-10-12 20:42:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15872.4296875Mb; avail=472693.7734375Mb
2020-10-12 20:42:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101284
2020-10-12 20:42:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106527
2020-10-12 20:42:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16027.25390625Mb; avail=472538.75Mb
2020-10-12 20:42:41 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 20:42:58 | INFO | train_inner | epoch 035:     60 / 110 loss=7.704, nll_loss=6.592, ppl=96.47, wps=14049.7, ups=2.36, wpb=5952.3, bsz=225.2, num_updates=3800, lr=0.000190005, gnorm=1.362, clip=0, train_wall=28, wall=1506
2020-10-12 20:43:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18496.765625Mb; avail=470058.59375Mb
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002021
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18496.765625Mb; avail=470058.59375Mb
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080455
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18496.87109375Mb; avail=470058.59375Mb
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057074
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140717
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18501.61328125Mb; avail=470053.7421875Mb
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18508.69140625Mb; avail=470046.74609375Mb
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001567
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18508.69140625Mb; avail=470046.74609375Mb
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080790
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18512.828125Mb; avail=470042.38671875Mb
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054537
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137673
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18512.83984375Mb; avail=470042.265625Mb
2020-10-12 20:43:15 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.101 | nll_loss 6.958 | ppl 124.33 | wps 56963 | wpb 2300 | bsz 88.5 | num_updates 3850 | best_loss 8.101
2020-10-12 20:43:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:43:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 35 @ 3850 updates, score 8.101) (writing took 10.511685250001392 seconds)
2020-10-12 20:43:26 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 20:43:26 | INFO | train | epoch 035 | loss 7.621 | nll_loss 6.498 | ppl 90.37 | wps 14630.6 | ups 2.44 | wpb 5987.1 | bsz 235.8 | num_updates 3850 | lr 0.000192504 | gnorm 1.322 | clip 0 | train_wall 30 | wall 1533
2020-10-12 20:43:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 20:43:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 20:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17912.71484375Mb; avail=470642.1171875Mb
2020-10-12 20:43:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000797
2020-10-12 20:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005989
2020-10-12 20:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17912.71484375Mb; avail=470642.1171875Mb
2020-10-12 20:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000196
2020-10-12 20:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17912.71484375Mb; avail=470642.1171875Mb
2020-10-12 20:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089402
2020-10-12 20:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096554
2020-10-12 20:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17907.3671875Mb; avail=470647.41015625Mb
2020-10-12 20:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17907.4765625Mb; avail=470647.296875Mb
2020-10-12 20:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004706
2020-10-12 20:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17907.4765625Mb; avail=470647.296875Mb
2020-10-12 20:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000237
2020-10-12 20:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17907.4765625Mb; avail=470647.296875Mb
2020-10-12 20:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088975
2020-10-12 20:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094676
2020-10-12 20:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17907.078125Mb; avail=470647.66796875Mb
2020-10-12 20:43:26 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 20:43:40 | INFO | train_inner | epoch 036:     50 / 110 loss=7.566, nll_loss=6.434, ppl=86.44, wps=14326.6, ups=2.37, wpb=6032.4, bsz=232.2, num_updates=3900, lr=0.000195003, gnorm=1.309, clip=0, train_wall=28, wall=1548
2020-10-12 20:43:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16013.078125Mb; avail=472555.16015625Mb
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001792
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16013.68359375Mb; avail=472554.6875Mb
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081070
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16016.15625Mb; avail=472557.5625Mb
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055695
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139518
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16018.0234375Mb; avail=472554.8359375Mb
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16018.39453125Mb; avail=472554.1953125Mb
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001597
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16018.61328125Mb; avail=472554.34765625Mb
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079175
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16018.20703125Mb; avail=472553.12109375Mb
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056038
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137843
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16019.359375Mb; avail=472551.53125Mb
2020-10-12 20:44:00 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.018 | nll_loss 6.84 | ppl 114.55 | wps 56915 | wpb 2300 | bsz 88.5 | num_updates 3960 | best_loss 8.018
2020-10-12 20:44:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:44:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 36 @ 3960 updates, score 8.018) (writing took 9.355874404000133 seconds)
2020-10-12 20:44:09 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 20:44:09 | INFO | train | epoch 036 | loss 7.493 | nll_loss 6.347 | ppl 81.39 | wps 15066.3 | ups 2.52 | wpb 5987.1 | bsz 235.8 | num_updates 3960 | lr 0.000198001 | gnorm 1.365 | clip 0 | train_wall 30 | wall 1577
2020-10-12 20:44:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 20:44:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 20:44:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15539.3671875Mb; avail=473027.4296875Mb
2020-10-12 20:44:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000809
2020-10-12 20:44:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005654
2020-10-12 20:44:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15539.3671875Mb; avail=473027.4296875Mb
2020-10-12 20:44:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000226
2020-10-12 20:44:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15539.3671875Mb; avail=473027.4296875Mb
2020-10-12 20:44:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090605
2020-10-12 20:44:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097288
2020-10-12 20:44:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15534.41015625Mb; avail=473032.5703125Mb
2020-10-12 20:44:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15534.45703125Mb; avail=473032.5234375Mb
2020-10-12 20:44:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003773
2020-10-12 20:44:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15534.45703125Mb; avail=473032.5234375Mb
2020-10-12 20:44:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000221
2020-10-12 20:44:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15534.45703125Mb; avail=473032.5234375Mb
2020-10-12 20:44:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089568
2020-10-12 20:44:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094325
2020-10-12 20:44:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15534.43359375Mb; avail=473032.33984375Mb
2020-10-12 20:44:09 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 20:44:21 | INFO | train_inner | epoch 037:     40 / 110 loss=7.437, nll_loss=6.282, ppl=77.8, wps=14487.1, ups=2.45, wpb=5924.5, bsz=238, num_updates=4000, lr=0.0002, gnorm=1.417, clip=0, train_wall=28, wall=1589
2020-10-12 20:44:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:44:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15676.546875Mb; avail=472892.93359375Mb
2020-10-12 20:44:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001746
2020-10-12 20:44:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15676.546875Mb; avail=472892.93359375Mb
2020-10-12 20:44:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081711
2020-10-12 20:44:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15641.19140625Mb; avail=472928.4921875Mb
2020-10-12 20:44:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055839
2020-10-12 20:44:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140095
2020-10-12 20:44:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15629.609375Mb; avail=472939.69921875Mb
2020-10-12 20:44:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15379.46484375Mb; avail=473189.73046875Mb
2020-10-12 20:44:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001472
2020-10-12 20:44:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15379.46484375Mb; avail=473189.73046875Mb
2020-10-12 20:44:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080004
2020-10-12 20:44:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15392.5078125Mb; avail=473176.48046875Mb
2020-10-12 20:44:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056431
2020-10-12 20:44:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138813
2020-10-12 20:44:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15397.5625Mb; avail=473171.5390625Mb
2020-10-12 20:44:44 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.966 | nll_loss 6.776 | ppl 109.56 | wps 57116.8 | wpb 2300 | bsz 88.5 | num_updates 4070 | best_loss 7.966
2020-10-12 20:44:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:44:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 37 @ 4070 updates, score 7.966) (writing took 4.4623310700008005 seconds)
2020-10-12 20:44:48 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 20:44:48 | INFO | train | epoch 037 | loss 7.368 | nll_loss 6.2 | ppl 73.52 | wps 16911.7 | ups 2.82 | wpb 5987.1 | bsz 235.8 | num_updates 4070 | lr 0.000198273 | gnorm 1.411 | clip 0 | train_wall 31 | wall 1616
2020-10-12 20:44:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 20:44:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 20:44:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15571.80859375Mb; avail=472995.30859375Mb
2020-10-12 20:44:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000797
2020-10-12 20:44:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005384
2020-10-12 20:44:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15571.80859375Mb; avail=472995.30859375Mb
2020-10-12 20:44:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:44:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15571.80859375Mb; avail=472995.30859375Mb
2020-10-12 20:44:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089945
2020-10-12 20:44:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096258
2020-10-12 20:44:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15565.9609375Mb; avail=473000.97265625Mb
2020-10-12 20:44:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15565.91796875Mb; avail=473001.21484375Mb
2020-10-12 20:44:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004012
2020-10-12 20:44:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15565.91796875Mb; avail=473001.21484375Mb
2020-10-12 20:44:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 20:44:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15565.91796875Mb; avail=473001.21484375Mb
2020-10-12 20:44:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091135
2020-10-12 20:44:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096075
2020-10-12 20:44:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15565.87109375Mb; avail=473001.45703125Mb
2020-10-12 20:44:48 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 20:44:57 | INFO | train_inner | epoch 038:     30 / 110 loss=7.337, nll_loss=6.164, ppl=71.71, wps=16678.1, ups=2.77, wpb=6017.3, bsz=233.8, num_updates=4100, lr=0.000197546, gnorm=1.396, clip=0, train_wall=28, wall=1625
2020-10-12 20:45:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15609.66015625Mb; avail=472957.3125Mb
2020-10-12 20:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002017
2020-10-12 20:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15609.66015625Mb; avail=472957.3125Mb
2020-10-12 20:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080694
2020-10-12 20:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15609.296875Mb; avail=472957.796875Mb
2020-10-12 20:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055766
2020-10-12 20:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139276
2020-10-12 20:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15609.61328125Mb; avail=472957.1953125Mb
2020-10-12 20:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15609.70703125Mb; avail=472957.1953125Mb
2020-10-12 20:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001196
2020-10-12 20:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15609.70703125Mb; avail=472957.1953125Mb
2020-10-12 20:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081724
2020-10-12 20:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15609.6953125Mb; avail=472957.31640625Mb
2020-10-12 20:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055192
2020-10-12 20:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138914
2020-10-12 20:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15609.4765625Mb; avail=472957.4375Mb
2020-10-12 20:45:22 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.912 | nll_loss 6.7 | ppl 104 | wps 57593.8 | wpb 2300 | bsz 88.5 | num_updates 4180 | best_loss 7.912
2020-10-12 20:45:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:45:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 38 @ 4180 updates, score 7.912) (writing took 7.167164860999037 seconds)
2020-10-12 20:45:30 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 20:45:30 | INFO | train | epoch 038 | loss 7.228 | nll_loss 6.037 | ppl 65.66 | wps 15864.8 | ups 2.65 | wpb 5987.1 | bsz 235.8 | num_updates 4180 | lr 0.000195646 | gnorm 1.36 | clip 0 | train_wall 30 | wall 1657
2020-10-12 20:45:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 20:45:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 20:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15579.890625Mb; avail=472991.46484375Mb
2020-10-12 20:45:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000785
2020-10-12 20:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005619
2020-10-12 20:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15573.42578125Mb; avail=472997.59765625Mb
2020-10-12 20:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000204
2020-10-12 20:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15573.296875Mb; avail=472998.08984375Mb
2020-10-12 20:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088726
2020-10-12 20:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095288
2020-10-12 20:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15572.33203125Mb; avail=472997.84765625Mb
2020-10-12 20:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15572.1171875Mb; avail=472996.91015625Mb
2020-10-12 20:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003691
2020-10-12 20:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15572.72265625Mb; avail=472997.41796875Mb
2020-10-12 20:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000198
2020-10-12 20:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15572.59375Mb; avail=472997.3046875Mb
2020-10-12 20:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089160
2020-10-12 20:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093807
2020-10-12 20:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15572.33984375Mb; avail=472996.34765625Mb
2020-10-12 20:45:30 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 20:45:36 | INFO | train_inner | epoch 039:     20 / 110 loss=7.181, nll_loss=5.982, ppl=63.23, wps=15252.7, ups=2.58, wpb=5904.8, bsz=238.7, num_updates=4200, lr=0.00019518, gnorm=1.374, clip=0, train_wall=28, wall=1663
2020-10-12 20:46:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13117.21875Mb; avail=475462.05859375Mb
2020-10-12 20:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001966
2020-10-12 20:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13117.21875Mb; avail=475462.05859375Mb
2020-10-12 20:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080157
2020-10-12 20:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13117.21875Mb; avail=475462.05859375Mb
2020-10-12 20:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055787
2020-10-12 20:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138799
2020-10-12 20:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13117.21875Mb; avail=475462.05859375Mb
2020-10-12 20:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13117.21875Mb; avail=475462.05859375Mb
2020-10-12 20:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001313
2020-10-12 20:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13117.21875Mb; avail=475462.05859375Mb
2020-10-12 20:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079560
2020-10-12 20:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13117.14453125Mb; avail=475462.1796875Mb
2020-10-12 20:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055668
2020-10-12 20:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137307
2020-10-12 20:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13117.14453125Mb; avail=475462.1796875Mb
2020-10-12 20:46:04 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.878 | nll_loss 6.652 | ppl 100.58 | wps 57490.5 | wpb 2300 | bsz 88.5 | num_updates 4290 | best_loss 7.878
2020-10-12 20:46:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:46:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 39 @ 4290 updates, score 7.878) (writing took 4.45018960500056 seconds)
2020-10-12 20:46:08 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 20:46:08 | INFO | train | epoch 039 | loss 7.099 | nll_loss 5.886 | ppl 59.12 | wps 17003.3 | ups 2.84 | wpb 5987.1 | bsz 235.8 | num_updates 4290 | lr 0.000193122 | gnorm 1.408 | clip 0 | train_wall 30 | wall 1696
2020-10-12 20:46:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 20:46:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 20:46:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13115.60546875Mb; avail=475463.25Mb
2020-10-12 20:46:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000746
2020-10-12 20:46:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005307
2020-10-12 20:46:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13115.63671875Mb; avail=475463.12890625Mb
2020-10-12 20:46:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:46:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13115.63671875Mb; avail=475463.12890625Mb
2020-10-12 20:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.087947
2020-10-12 20:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094181
2020-10-12 20:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13115.04296875Mb; avail=475463.62109375Mb
2020-10-12 20:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13115.0703125Mb; avail=475463.5Mb
2020-10-12 20:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003697
2020-10-12 20:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13115.0703125Mb; avail=475463.5Mb
2020-10-12 20:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 20:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13115.0703125Mb; avail=475463.5Mb
2020-10-12 20:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088510
2020-10-12 20:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093108
2020-10-12 20:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13114.96875Mb; avail=475463.703125Mb
2020-10-12 20:46:09 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 20:46:12 | INFO | train_inner | epoch 040:     10 / 110 loss=7.091, nll_loss=5.875, ppl=58.71, wps=16933, ups=2.79, wpb=6068.8, bsz=237.7, num_updates=4300, lr=0.000192897, gnorm=1.392, clip=0, train_wall=28, wall=1699
2020-10-12 20:46:40 | INFO | train_inner | epoch 040:    110 / 110 loss=6.978, nll_loss=5.742, ppl=53.54, wps=21225.8, ups=3.54, wpb=5991.3, bsz=236.1, num_updates=4400, lr=0.000190693, gnorm=1.371, clip=0, train_wall=28, wall=1727
2020-10-12 20:46:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10635.19140625Mb; avail=477956.8359375Mb
2020-10-12 20:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002121
2020-10-12 20:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10635.19140625Mb; avail=477956.8359375Mb
2020-10-12 20:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081458
2020-10-12 20:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10597.80078125Mb; avail=477993.7578125Mb
2020-10-12 20:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057336
2020-10-12 20:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141728
2020-10-12 20:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10592.0703125Mb; avail=477999.8671875Mb
2020-10-12 20:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10592.0703125Mb; avail=477999.8671875Mb
2020-10-12 20:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001328
2020-10-12 20:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10592.0703125Mb; avail=477999.8671875Mb
2020-10-12 20:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082774
2020-10-12 20:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10592.0703125Mb; avail=477999.8671875Mb
2020-10-12 20:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057059
2020-10-12 20:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141942
2020-10-12 20:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10592.09765625Mb; avail=477999.74609375Mb
2020-10-12 20:46:43 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.86 | nll_loss 6.625 | ppl 98.69 | wps 57425.7 | wpb 2300 | bsz 88.5 | num_updates 4400 | best_loss 7.86
2020-10-12 20:46:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:46:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azerus_sepspm8000/O2M/checkpoint_best.pt (epoch 40 @ 4400 updates, score 7.86) (writing took 4.471269386998756 seconds)
2020-10-12 20:46:47 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 20:46:47 | INFO | train | epoch 040 | loss 6.971 | nll_loss 5.735 | ppl 53.25 | wps 17004.8 | ups 2.84 | wpb 5987.1 | bsz 235.8 | num_updates 4400 | lr 0.000190693 | gnorm 1.363 | clip 0 | train_wall 30 | wall 1735
2020-10-12 20:46:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 20:46:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 20:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10623.4765625Mb; avail=477968.02734375Mb
2020-10-12 20:46:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000830
2020-10-12 20:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006190
2020-10-12 20:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10623.4765625Mb; avail=477968.02734375Mb
2020-10-12 20:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000214
2020-10-12 20:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10623.4765625Mb; avail=477968.02734375Mb
2020-10-12 20:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090798
2020-10-12 20:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098008
2020-10-12 20:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10617.69140625Mb; avail=477973.76953125Mb
2020-10-12 20:46:47 | INFO | fairseq_cli.train | done training in 1734.9 seconds
