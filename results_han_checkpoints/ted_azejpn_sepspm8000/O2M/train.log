2020-10-12 20:17:42 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_azejpn_sepspm8000/O2M/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok='tgt', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='eng-aze,eng-jpn', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 20:17:42 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 20:17:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['aze', 'eng', 'jpn']
2020-10-12 20:17:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | [aze] dictionary: 22163 types
2020-10-12 20:17:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 22163 types
2020-10-12 20:17:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | [jpn] dictionary: 22163 types
2020-10-12 20:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 20:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16034.83984375Mb; avail=472625.57421875Mb
2020-10-12 20:17:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 20:17:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:eng-aze': 1, 'main:eng-jpn': 1}
2020-10-12 20:17:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-aze src_langtok: 22160; tgt_langtok: None
2020-10-12 20:17:42 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azejpn_sepspm8000/O2M/valid.eng-aze.eng
2020-10-12 20:17:42 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azejpn_sepspm8000/O2M/valid.eng-aze.aze
2020-10-12 20:17:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azejpn_sepspm8000/O2M/ valid eng-aze 671 examples
2020-10-12 20:17:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-jpn src_langtok: 22162; tgt_langtok: None
2020-10-12 20:17:42 | INFO | fairseq.data.data_utils | loaded 4429 examples from: fairseq/data-bin/ted_azejpn_sepspm8000/O2M/valid.eng-jpn.eng
2020-10-12 20:17:42 | INFO | fairseq.data.data_utils | loaded 4429 examples from: fairseq/data-bin/ted_azejpn_sepspm8000/O2M/valid.eng-jpn.jpn
2020-10-12 20:17:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azejpn_sepspm8000/O2M/ valid eng-jpn 4429 examples
2020-10-12 20:17:43 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22163, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22163, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=22163, bias=False)
  )
)
2020-10-12 20:17:43 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 20:17:43 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 20:17:43 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 20:17:43 | INFO | fairseq_cli.train | num. model params: 42890752 (num. trained: 42890752)
2020-10-12 20:17:47 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 20:17:47 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 20:17:47 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 20:17:47 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 20:17:47 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 20:17:47 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 20:17:47 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 20:17:47 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_last.pt
2020-10-12 20:17:47 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 20:17:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 20:17:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15376.4140625Mb; avail=473278.9453125Mb
2020-10-12 20:17:47 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 20:17:47 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:eng-aze': 1, 'main:eng-jpn': 1}
2020-10-12 20:17:47 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-aze src_langtok: 22160; tgt_langtok: None
2020-10-12 20:17:47 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azejpn_sepspm8000/O2M/train.eng-aze.eng
2020-10-12 20:17:47 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azejpn_sepspm8000/O2M/train.eng-aze.aze
2020-10-12 20:17:47 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azejpn_sepspm8000/O2M/ train eng-aze 5946 examples
2020-10-12 20:17:47 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-jpn src_langtok: 22162; tgt_langtok: None
2020-10-12 20:17:47 | INFO | fairseq.data.data_utils | loaded 19994 examples from: fairseq/data-bin/ted_azejpn_sepspm8000/O2M/train.eng-jpn.eng
2020-10-12 20:17:47 | INFO | fairseq.data.data_utils | loaded 19994 examples from: fairseq/data-bin/ted_azejpn_sepspm8000/O2M/train.eng-jpn.jpn
2020-10-12 20:17:47 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azejpn_sepspm8000/O2M/ train eng-jpn 19994 examples
2020-10-12 20:17:47 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:eng-aze', 5946), ('main:eng-jpn', 19994)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 20:17:47 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 20:17:47 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 25940
2020-10-12 20:17:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 25940; virtual dataset size 25940
2020-10-12 20:17:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 20:17:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:eng-aze': 5946, 'main:eng-jpn': 19994}; raw total size: 25940
2020-10-12 20:17:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:eng-aze': 5946, 'main:eng-jpn': 19994}; resampled total size: 25940
2020-10-12 20:17:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 20:17:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.003815
2020-10-12 20:17:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15377.625Mb; avail=473277.734375Mb
2020-10-12 20:17:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000497
2020-10-12 20:17:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004734
2020-10-12 20:17:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15377.625Mb; avail=473277.2421875Mb
2020-10-12 20:17:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000264
2020-10-12 20:17:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15377.625Mb; avail=473277.2421875Mb
2020-10-12 20:17:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092004
2020-10-12 20:17:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097789
2020-10-12 20:17:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15388.50390625Mb; avail=473266.36328125Mb
2020-10-12 20:17:47 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 20:17:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15394.4296875Mb; avail=473260.4375Mb
2020-10-12 20:17:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003659
2020-10-12 20:17:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15396.8515625Mb; avail=473258.015625Mb
2020-10-12 20:17:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 20:17:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15396.8515625Mb; avail=473258.015625Mb
2020-10-12 20:17:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089921
2020-10-12 20:17:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094501
2020-10-12 20:17:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15400.1796875Mb; avail=473254.39453125Mb
2020-10-12 20:17:47 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 20:18:16 | INFO | train_inner | epoch 001:    100 / 104 loss=14.638, nll_loss=14.579, ppl=24466.7, wps=18952, ups=3.54, wpb=5352.3, bsz=248.4, num_updates=100, lr=5.0975e-06, gnorm=2.681, clip=0, train_wall=28, wall=29
2020-10-12 20:18:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:18:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17964.8359375Mb; avail=470659.8125Mb
2020-10-12 20:18:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001713
2020-10-12 20:18:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17965.44140625Mb; avail=470659.20703125Mb
2020-10-12 20:18:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081047
2020-10-12 20:18:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17965.39453125Mb; avail=470659.44921875Mb
2020-10-12 20:18:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059117
2020-10-12 20:18:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142687
2020-10-12 20:18:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17965.2578125Mb; avail=470659.19140625Mb
2020-10-12 20:18:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17965.01953125Mb; avail=470659.796875Mb
2020-10-12 20:18:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001591
2020-10-12 20:18:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17965.01953125Mb; avail=470659.796875Mb
2020-10-12 20:18:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079484
2020-10-12 20:18:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17965.140625Mb; avail=470659.67578125Mb
2020-10-12 20:18:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056877
2020-10-12 20:18:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138720
2020-10-12 20:18:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17965.15625Mb; avail=470659.67578125Mb
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2020-10-12 20:18:20 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.906 | nll_loss 13.753 | ppl 13801.9 | wps 49646.1 | wpb 2129.5 | bsz 102 | num_updates 104
2020-10-12 20:18:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:18:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 1 @ 104 updates, score 13.906) (writing took 1.5751500690003013 seconds)
2020-10-12 20:18:21 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 20:18:21 | INFO | train | epoch 001 | loss 14.616 | nll_loss 14.554 | ppl 24055.2 | wps 16509.2 | ups 3.1 | wpb 5327.4 | bsz 249.4 | num_updates 104 | lr 5.2974e-06 | gnorm 2.673 | clip 0 | train_wall 29 | wall 35
2020-10-12 20:18:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 20:18:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 20:18:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17899.34765625Mb; avail=470707.70703125Mb
2020-10-12 20:18:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000959
2020-10-12 20:18:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005530
2020-10-12 20:18:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17899.30859375Mb; avail=470707.94921875Mb
2020-10-12 20:18:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000190
2020-10-12 20:18:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17899.30859375Mb; avail=470707.94921875Mb
2020-10-12 20:18:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090182
2020-10-12 20:18:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096635
2020-10-12 20:18:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17894.33203125Mb; avail=470713.15234375Mb
2020-10-12 20:18:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17894.22265625Mb; avail=470713.15234375Mb
2020-10-12 20:18:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003680
2020-10-12 20:18:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17894.125Mb; avail=470713.15234375Mb
2020-10-12 20:18:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-12 20:18:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17894.125Mb; avail=470713.15234375Mb
2020-10-12 20:18:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088190
2020-10-12 20:18:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.092782
2020-10-12 20:18:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17894.34765625Mb; avail=470712.91015625Mb
2020-10-12 20:18:21 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 20:18:49 | INFO | train_inner | epoch 002:     96 / 104 loss=13.777, nll_loss=13.622, ppl=12604.4, wps=16036.6, ups=3.03, wpb=5295.9, bsz=254.4, num_updates=200, lr=1.0095e-05, gnorm=1.388, clip=0, train_wall=28, wall=62
2020-10-12 20:18:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18389.0703125Mb; avail=470202.72265625Mb
2020-10-12 20:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001712
2020-10-12 20:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18389.0703125Mb; avail=470202.72265625Mb
2020-10-12 20:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075289
2020-10-12 20:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18389.1171875Mb; avail=470202.96484375Mb
2020-10-12 20:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053159
2020-10-12 20:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130973
2020-10-12 20:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18389.1171875Mb; avail=470202.96484375Mb
2020-10-12 20:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18389.1171875Mb; avail=470202.96484375Mb
2020-10-12 20:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001256
2020-10-12 20:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18389.1171875Mb; avail=470202.96484375Mb
2020-10-12 20:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076247
2020-10-12 20:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18389.13671875Mb; avail=470202.93359375Mb
2020-10-12 20:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053918
2020-10-12 20:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.132216
2020-10-12 20:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18389.05078125Mb; avail=470202.8125Mb
2020-10-12 20:18:54 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.306 | nll_loss 13.092 | ppl 8733.54 | wps 50677 | wpb 2129.5 | bsz 102 | num_updates 208 | best_loss 13.306
2020-10-12 20:18:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:19:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 2 @ 208 updates, score 13.306) (writing took 19.923115392999534 seconds)
2020-10-12 20:19:14 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 20:19:14 | INFO | train | epoch 002 | loss 13.755 | nll_loss 13.598 | ppl 12397.9 | wps 10561.1 | ups 1.98 | wpb 5327.4 | bsz 249.4 | num_updates 208 | lr 1.04948e-05 | gnorm 1.322 | clip 0 | train_wall 29 | wall 87
2020-10-12 20:19:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 20:19:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19004.3828125Mb; avail=469588.56640625Mb
2020-10-12 20:19:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000756
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005406
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19004.22265625Mb; avail=469589.17578125Mb
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000190
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19004.22265625Mb; avail=469589.17578125Mb
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091958
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098283
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18998.45703125Mb; avail=469594.953125Mb
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18998.45703125Mb; avail=469594.953125Mb
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003640
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18998.45703125Mb; avail=469594.953125Mb
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18998.45703125Mb; avail=469594.953125Mb
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089956
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094498
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18998.375Mb; avail=469594.83203125Mb
2020-10-12 20:19:14 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 20:19:40 | INFO | train_inner | epoch 003:     92 / 104 loss=13.251, nll_loss=13.04, ppl=8422.15, wps=10379.2, ups=1.95, wpb=5314.6, bsz=247.8, num_updates=300, lr=1.50925e-05, gnorm=1.095, clip=0, train_wall=28, wall=114
2020-10-12 20:19:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17891.94140625Mb; avail=470699.29296875Mb
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001696
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17891.94140625Mb; avail=470699.29296875Mb
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076879
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17884.5859375Mb; avail=470706.796875Mb
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054702
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.134089
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.97265625Mb; avail=470730.30078125Mb
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17854.4453125Mb; avail=470736.828125Mb
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001316
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.4453125Mb; avail=470736.828125Mb
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077649
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.671875Mb; avail=470736.94921875Mb
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052403
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.132127
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.66015625Mb; avail=470737.0703125Mb
2020-10-12 20:19:46 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 12.735 | nll_loss 12.447 | ppl 5585.57 | wps 50631.4 | wpb 2129.5 | bsz 102 | num_updates 312 | best_loss 12.735
2020-10-12 20:19:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:19:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 3 @ 312 updates, score 12.735) (writing took 8.856981986000392 seconds)
2020-10-12 20:19:55 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 20:19:55 | INFO | train | epoch 003 | loss 13.19 | nll_loss 12.972 | ppl 8033.43 | wps 13415.6 | ups 2.52 | wpb 5327.4 | bsz 249.4 | num_updates 312 | lr 1.56922e-05 | gnorm 1.132 | clip 0 | train_wall 29 | wall 128
2020-10-12 20:19:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 20:19:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 20:19:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18980.37109375Mb; avail=469612.3828125Mb
2020-10-12 20:19:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001036
2020-10-12 20:19:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005969
2020-10-12 20:19:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18980.9765625Mb; avail=469611.77734375Mb
2020-10-12 20:19:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-12 20:19:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18980.9765625Mb; avail=469611.77734375Mb
2020-10-12 20:19:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093317
2020-10-12 20:19:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100837
2020-10-12 20:19:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18975.3515625Mb; avail=469617.60546875Mb
2020-10-12 20:19:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18975.1953125Mb; avail=469617.7578125Mb
2020-10-12 20:19:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004405
2020-10-12 20:19:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18975.30859375Mb; avail=469617.64453125Mb
2020-10-12 20:19:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000274
2020-10-12 20:19:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18975.30859375Mb; avail=469617.765625Mb
2020-10-12 20:19:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089233
2020-10-12 20:19:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094761
2020-10-12 20:19:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18975.2265625Mb; avail=469617.9375Mb
2020-10-12 20:19:55 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 20:20:20 | INFO | train_inner | epoch 004:     88 / 104 loss=12.668, nll_loss=12.384, ppl=5343.5, wps=13073.9, ups=2.48, wpb=5279.9, bsz=245, num_updates=400, lr=2.009e-05, gnorm=1.151, clip=0, train_wall=28, wall=154
2020-10-12 20:20:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17836.53515625Mb; avail=470755.79296875Mb
2020-10-12 20:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002092
2020-10-12 20:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.6484375Mb; avail=470755.6796875Mb
2020-10-12 20:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077921
2020-10-12 20:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.453125Mb; avail=470755.6796875Mb
2020-10-12 20:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057154
2020-10-12 20:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137980
2020-10-12 20:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.62109375Mb; avail=470755.55859375Mb
2020-10-12 20:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17836.4921875Mb; avail=470755.80078125Mb
2020-10-12 20:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001166
2020-10-12 20:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.4921875Mb; avail=470755.80078125Mb
2020-10-12 20:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076882
2020-10-12 20:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.4921875Mb; avail=470755.80078125Mb
2020-10-12 20:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053898
2020-10-12 20:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.132699
2020-10-12 20:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.34765625Mb; avail=470755.921875Mb
2020-10-12 20:20:28 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 12.152 | nll_loss 11.783 | ppl 3524.98 | wps 50618.5 | wpb 2129.5 | bsz 102 | num_updates 416 | best_loss 12.152
2020-10-12 20:20:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:20:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 4 @ 416 updates, score 12.152) (writing took 6.623390598999322 seconds)
2020-10-12 20:20:34 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 20:20:34 | INFO | train | epoch 004 | loss 12.588 | nll_loss 12.291 | ppl 5011.95 | wps 14093.4 | ups 2.65 | wpb 5327.4 | bsz 249.4 | num_updates 416 | lr 2.08896e-05 | gnorm 1.152 | clip 0 | train_wall 29 | wall 168
2020-10-12 20:20:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 20:20:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 20:20:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17785.05859375Mb; avail=470807.109375Mb
2020-10-12 20:20:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000774
2020-10-12 20:20:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005412
2020-10-12 20:20:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17785.05859375Mb; avail=470807.109375Mb
2020-10-12 20:20:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 20:20:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17785.05859375Mb; avail=470807.109375Mb
2020-10-12 20:20:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089262
2020-10-12 20:20:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095596
2020-10-12 20:20:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17778.61328125Mb; avail=470813.75Mb
2020-10-12 20:20:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17778.5Mb; avail=470813.87109375Mb
2020-10-12 20:20:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003625
2020-10-12 20:20:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17778.5Mb; avail=470813.87109375Mb
2020-10-12 20:20:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 20:20:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17778.5Mb; avail=470813.87109375Mb
2020-10-12 20:20:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091147
2020-10-12 20:20:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095770
2020-10-12 20:20:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17801.46875Mb; avail=470791.03125Mb
2020-10-12 20:20:34 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 20:20:58 | INFO | train_inner | epoch 005:     84 / 104 loss=12.233, nll_loss=11.878, ppl=3764.1, wps=14052.4, ups=2.63, wpb=5334.6, bsz=243, num_updates=500, lr=2.50875e-05, gnorm=1.341, clip=0, train_wall=28, wall=192
2020-10-12 20:21:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17820.75Mb; avail=470771.07421875Mb
2020-10-12 20:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001908
2020-10-12 20:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.75Mb; avail=470771.07421875Mb
2020-10-12 20:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076030
2020-10-12 20:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.83203125Mb; avail=470771.1953125Mb
2020-10-12 20:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054630
2020-10-12 20:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.133353
2020-10-12 20:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.84375Mb; avail=470771.07421875Mb
2020-10-12 20:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17820.8203125Mb; avail=470771.1953125Mb
2020-10-12 20:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001216
2020-10-12 20:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.8203125Mb; avail=470771.1953125Mb
2020-10-12 20:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076444
2020-10-12 20:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.92578125Mb; avail=470771.1953125Mb
2020-10-12 20:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053509
2020-10-12 20:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.131922
2020-10-12 20:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17821.0859375Mb; avail=470770.58984375Mb
2020-10-12 20:21:07 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 11.824 | nll_loss 11.393 | ppl 2688.7 | wps 50278.1 | wpb 2129.5 | bsz 102 | num_updates 520 | best_loss 11.824
2020-10-12 20:21:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:21:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 5 @ 520 updates, score 11.824) (writing took 8.172673217999545 seconds)
2020-10-12 20:21:15 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 20:21:15 | INFO | train | epoch 005 | loss 12.16 | nll_loss 11.791 | ppl 3544.74 | wps 13652.3 | ups 2.56 | wpb 5327.4 | bsz 249.4 | num_updates 520 | lr 2.6087e-05 | gnorm 1.446 | clip 0 | train_wall 29 | wall 208
2020-10-12 20:21:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 20:21:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 20:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18487.46484375Mb; avail=470104.7578125Mb
2020-10-12 20:21:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000664
2020-10-12 20:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005396
2020-10-12 20:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18495.3359375Mb; avail=470097.4921875Mb
2020-10-12 20:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000203
2020-10-12 20:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18495.94140625Mb; avail=470096.88671875Mb
2020-10-12 20:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.121982
2020-10-12 20:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.128371
2020-10-12 20:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18641.3515625Mb; avail=469951.4296875Mb
2020-10-12 20:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18696.44921875Mb; avail=469895.7265625Mb
2020-10-12 20:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003771
2020-10-12 20:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18701.8984375Mb; avail=469890.8828125Mb
2020-10-12 20:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-12 20:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18702.50390625Mb; avail=469890.27734375Mb
2020-10-12 20:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091048
2020-10-12 20:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095777
2020-10-12 20:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18821.78125Mb; avail=469771.0Mb
2020-10-12 20:21:15 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 20:21:38 | INFO | train_inner | epoch 006:     80 / 104 loss=11.905, nll_loss=11.484, ppl=2865.34, wps=13676.7, ups=2.55, wpb=5365.4, bsz=257.6, num_updates=600, lr=3.0085e-05, gnorm=1.225, clip=0, train_wall=28, wall=231
2020-10-12 20:21:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:21:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17825.44921875Mb; avail=470766.10546875Mb
2020-10-12 20:21:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001974
2020-10-12 20:21:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17825.44921875Mb; avail=470766.10546875Mb
2020-10-12 20:21:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076090
2020-10-12 20:21:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17825.25Mb; avail=470766.34765625Mb
2020-10-12 20:21:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053280
2020-10-12 20:21:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.132131
2020-10-12 20:21:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17825.25Mb; avail=470766.34765625Mb
2020-10-12 20:21:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17825.25Mb; avail=470766.83984375Mb
2020-10-12 20:21:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001181
2020-10-12 20:21:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17825.25Mb; avail=470766.83984375Mb
2020-10-12 20:21:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076429
2020-10-12 20:21:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17825.65234375Mb; avail=470766.234375Mb
2020-10-12 20:21:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052801
2020-10-12 20:21:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.131179
2020-10-12 20:21:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17825.67578125Mb; avail=470766.11328125Mb
2020-10-12 20:21:47 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.584 | nll_loss 11.089 | ppl 2177.69 | wps 50359.5 | wpb 2129.5 | bsz 102 | num_updates 624 | best_loss 11.584
2020-10-12 20:21:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:21:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 6 @ 624 updates, score 11.584) (writing took 4.439839509000194 seconds)
2020-10-12 20:21:51 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 20:21:51 | INFO | train | epoch 006 | loss 11.855 | nll_loss 11.423 | ppl 2745.38 | wps 15126.2 | ups 2.84 | wpb 5327.4 | bsz 249.4 | num_updates 624 | lr 3.12844e-05 | gnorm 1.112 | clip 0 | train_wall 29 | wall 245
2020-10-12 20:21:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 20:21:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 20:21:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18600.94140625Mb; avail=469992.28125Mb
2020-10-12 20:21:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000781
2020-10-12 20:21:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005374
2020-10-12 20:21:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18600.7734375Mb; avail=469991.99609375Mb
2020-10-12 20:21:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 20:21:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18600.64453125Mb; avail=469991.27734375Mb
2020-10-12 20:21:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089892
2020-10-12 20:21:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096233
2020-10-12 20:21:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18595.38671875Mb; avail=469996.6328125Mb
2020-10-12 20:21:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18341.140625Mb; avail=470251.328125Mb
2020-10-12 20:21:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004429
2020-10-12 20:21:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18341.140625Mb; avail=470251.328125Mb
2020-10-12 20:21:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000229
2020-10-12 20:21:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18341.140625Mb; avail=470251.328125Mb
2020-10-12 20:21:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.127689
2020-10-12 20:21:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.133128
2020-10-12 20:21:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18341.171875Mb; avail=470251.0859375Mb
2020-10-12 20:21:52 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 20:22:13 | INFO | train_inner | epoch 007:     76 / 104 loss=11.688, nll_loss=11.217, ppl=2380.69, wps=15150.9, ups=2.81, wpb=5392.8, bsz=255.9, num_updates=700, lr=3.50825e-05, gnorm=1.196, clip=0, train_wall=28, wall=267
2020-10-12 20:22:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:22:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17831.6484375Mb; avail=470760.65234375Mb
2020-10-12 20:22:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001758
2020-10-12 20:22:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17831.6484375Mb; avail=470760.65234375Mb
2020-10-12 20:22:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074608
2020-10-12 20:22:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17831.6640625Mb; avail=470760.63671875Mb
2020-10-12 20:22:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054801
2020-10-12 20:22:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.131948
2020-10-12 20:22:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17831.72265625Mb; avail=470760.63671875Mb
2020-10-12 20:22:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17831.73828125Mb; avail=470760.515625Mb
2020-10-12 20:22:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001200
2020-10-12 20:22:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17831.73828125Mb; avail=470760.515625Mb
2020-10-12 20:22:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076642
2020-10-12 20:22:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17831.90234375Mb; avail=470760.2421875Mb
2020-10-12 20:22:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052545
2020-10-12 20:22:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.131148
2020-10-12 20:22:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17831.93359375Mb; avail=470760.078125Mb
2020-10-12 20:22:24 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 11.472 | nll_loss 10.949 | ppl 1977.03 | wps 50201.6 | wpb 2129.5 | bsz 102 | num_updates 728 | best_loss 11.472
2020-10-12 20:22:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:22:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 7 @ 728 updates, score 11.472) (writing took 4.434983585000737 seconds)
2020-10-12 20:22:28 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 20:22:28 | INFO | train | epoch 007 | loss 11.647 | nll_loss 11.166 | ppl 2297.43 | wps 15093.5 | ups 2.83 | wpb 5327.4 | bsz 249.4 | num_updates 728 | lr 3.64818e-05 | gnorm 1.146 | clip 0 | train_wall 29 | wall 282
2020-10-12 20:22:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 20:22:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 20:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17830.93359375Mb; avail=470760.98828125Mb
2020-10-12 20:22:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000666
2020-10-12 20:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005443
2020-10-12 20:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17832.0546875Mb; avail=470759.8671875Mb
2020-10-12 20:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17832.17578125Mb; avail=470759.74609375Mb
2020-10-12 20:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091883
2020-10-12 20:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098309
2020-10-12 20:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17828.37890625Mb; avail=470764.0625Mb
2020-10-12 20:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17828.15234375Mb; avail=470763.69921875Mb
2020-10-12 20:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003668
2020-10-12 20:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17828.1875Mb; avail=470763.578125Mb
2020-10-12 20:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 20:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17828.1875Mb; avail=470763.578125Mb
2020-10-12 20:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089788
2020-10-12 20:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094357
2020-10-12 20:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17828.328125Mb; avail=470763.45703125Mb
2020-10-12 20:22:28 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 20:22:49 | INFO | train_inner | epoch 008:     72 / 104 loss=11.557, nll_loss=11.053, ppl=2124.34, wps=14784.7, ups=2.8, wpb=5283.4, bsz=247.6, num_updates=800, lr=4.008e-05, gnorm=1.189, clip=0, train_wall=28, wall=302
2020-10-12 20:22:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17829.9453125Mb; avail=470762.1953125Mb
2020-10-12 20:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002644
2020-10-12 20:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17829.9453125Mb; avail=470762.1953125Mb
2020-10-12 20:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.106353
2020-10-12 20:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17829.953125Mb; avail=470762.1953125Mb
2020-10-12 20:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053239
2020-10-12 20:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.163103
2020-10-12 20:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17829.62890625Mb; avail=470762.55859375Mb
2020-10-12 20:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17829.55078125Mb; avail=470762.55859375Mb
2020-10-12 20:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001271
2020-10-12 20:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17829.55078125Mb; avail=470762.55859375Mb
2020-10-12 20:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075226
2020-10-12 20:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17829.85546875Mb; avail=470762.07421875Mb
2020-10-12 20:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.068155
2020-10-12 20:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.145401
2020-10-12 20:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17829.90625Mb; avail=470762.55859375Mb
2020-10-12 20:23:01 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 11.391 | nll_loss 10.851 | ppl 1846.45 | wps 50343.5 | wpb 2129.5 | bsz 102 | num_updates 832 | best_loss 11.391
2020-10-12 20:23:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:23:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 8 @ 832 updates, score 11.391) (writing took 8.714986535999742 seconds)
2020-10-12 20:23:10 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 20:23:10 | INFO | train | epoch 008 | loss 11.515 | nll_loss 11.003 | ppl 2051.63 | wps 13345.2 | ups 2.5 | wpb 5327.4 | bsz 249.4 | num_updates 832 | lr 4.16792e-05 | gnorm 1.219 | clip 0 | train_wall 29 | wall 323
2020-10-12 20:23:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 20:23:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 20:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18350.953125Mb; avail=470240.9921875Mb
2020-10-12 20:23:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000802
2020-10-12 20:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005340
2020-10-12 20:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18350.95703125Mb; avail=470240.9921875Mb
2020-10-12 20:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 20:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18350.95703125Mb; avail=470240.9921875Mb
2020-10-12 20:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092516
2020-10-12 20:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098814
2020-10-12 20:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18344.421875Mb; avail=470247.6171875Mb
2020-10-12 20:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18345.04296875Mb; avail=470246.890625Mb
2020-10-12 20:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003695
2020-10-12 20:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18345.04296875Mb; avail=470246.890625Mb
2020-10-12 20:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18345.04296875Mb; avail=470246.890625Mb
2020-10-12 20:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094279
2020-10-12 20:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099211
2020-10-12 20:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18345.89453125Mb; avail=470246.625Mb
2020-10-12 20:23:10 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 20:23:29 | INFO | train_inner | epoch 009:     68 / 104 loss=11.443, nll_loss=10.914, ppl=1929.32, wps=13144.9, ups=2.5, wpb=5254.3, bsz=246, num_updates=900, lr=4.50775e-05, gnorm=1.202, clip=0, train_wall=28, wall=342
2020-10-12 20:23:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:23:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17859.234375Mb; avail=470733.234375Mb
2020-10-12 20:23:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001784
2020-10-12 20:23:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17859.234375Mb; avail=470733.234375Mb
2020-10-12 20:23:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075801
2020-10-12 20:23:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17859.28125Mb; avail=470733.59765625Mb
2020-10-12 20:23:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052212
2020-10-12 20:23:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130581
2020-10-12 20:23:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17859.32421875Mb; avail=470733.35546875Mb
2020-10-12 20:23:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17859.3984375Mb; avail=470733.4765625Mb
2020-10-12 20:23:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001239
2020-10-12 20:23:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17859.3984375Mb; avail=470733.4765625Mb
2020-10-12 20:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076051
2020-10-12 20:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17859.75Mb; avail=470733.12109375Mb
2020-10-12 20:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052645
2020-10-12 20:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130701
2020-10-12 20:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17859.27734375Mb; avail=470733.2421875Mb
2020-10-12 20:23:42 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 11.274 | nll_loss 10.693 | ppl 1655.59 | wps 50629.9 | wpb 2129.5 | bsz 102 | num_updates 936 | best_loss 11.274
2020-10-12 20:23:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:23:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 9 @ 936 updates, score 11.274) (writing took 4.443464223999399 seconds)
2020-10-12 20:23:46 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 20:23:46 | INFO | train | epoch 009 | loss 11.423 | nll_loss 10.889 | ppl 1896.1 | wps 15146 | ups 2.84 | wpb 5327.4 | bsz 249.4 | num_updates 936 | lr 4.68766e-05 | gnorm 1.128 | clip 0 | train_wall 29 | wall 360
2020-10-12 20:23:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 20:23:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 20:23:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18807.12109375Mb; avail=469789.59765625Mb
2020-10-12 20:23:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000797
2020-10-12 20:23:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005736
2020-10-12 20:23:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18814.4375Mb; avail=469783.7734375Mb
2020-10-12 20:23:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 20:23:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18814.9140625Mb; avail=469782.44921875Mb
2020-10-12 20:23:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101345
2020-10-12 20:23:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108299
2020-10-12 20:23:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18929.00390625Mb; avail=469667.5078125Mb
2020-10-12 20:23:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18978.96875Mb; avail=469615.9609375Mb
2020-10-12 20:23:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004010
2020-10-12 20:23:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18979.1875Mb; avail=469616.8671875Mb
2020-10-12 20:23:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000245
2020-10-12 20:23:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18979.05859375Mb; avail=469616.75390625Mb
2020-10-12 20:23:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095756
2020-10-12 20:23:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100762
2020-10-12 20:23:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18973.8203125Mb; avail=469626.07421875Mb
2020-10-12 20:23:47 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 20:24:05 | INFO | train_inner | epoch 010:     64 / 104 loss=11.358, nll_loss=10.81, ppl=1795.89, wps=15096.2, ups=2.8, wpb=5390.8, bsz=255.6, num_updates=1000, lr=5.0075e-05, gnorm=1.099, clip=0, train_wall=28, wall=378
2020-10-12 20:24:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17860.5234375Mb; avail=470731.265625Mb
2020-10-12 20:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001727
2020-10-12 20:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.5234375Mb; avail=470731.265625Mb
2020-10-12 20:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078910
2020-10-12 20:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.65625Mb; avail=470731.265625Mb
2020-10-12 20:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057702
2020-10-12 20:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139147
2020-10-12 20:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.5390625Mb; avail=470731.38671875Mb
2020-10-12 20:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17860.83203125Mb; avail=470731.265625Mb
2020-10-12 20:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001278
2020-10-12 20:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.83203125Mb; avail=470731.265625Mb
2020-10-12 20:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079247
2020-10-12 20:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.8125Mb; avail=470731.265625Mb
2020-10-12 20:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059646
2020-10-12 20:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140929
2020-10-12 20:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.8125Mb; avail=470731.265625Mb
2020-10-12 20:24:18 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 11.172 | nll_loss 10.571 | ppl 1520.86 | wps 49926.4 | wpb 2129.5 | bsz 102 | num_updates 1040 | best_loss 11.172
2020-10-12 20:24:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:24:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 10 @ 1040 updates, score 11.172) (writing took 4.506426001999898 seconds)
2020-10-12 20:24:23 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 20:24:23 | INFO | train | epoch 010 | loss 11.341 | nll_loss 10.787 | ppl 1767.15 | wps 15097.4 | ups 2.83 | wpb 5327.4 | bsz 249.4 | num_updates 1040 | lr 5.2074e-05 | gnorm 1.178 | clip 0 | train_wall 29 | wall 396
2020-10-12 20:24:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 20:24:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 20:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17876.91015625Mb; avail=470715.14453125Mb
2020-10-12 20:24:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000681
2020-10-12 20:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005118
2020-10-12 20:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.91015625Mb; avail=470715.14453125Mb
2020-10-12 20:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 20:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.91015625Mb; avail=470715.14453125Mb
2020-10-12 20:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092965
2020-10-12 20:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099025
2020-10-12 20:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.90234375Mb; avail=470719.83203125Mb
2020-10-12 20:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17871.97265625Mb; avail=470719.953125Mb
2020-10-12 20:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003617
2020-10-12 20:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.97265625Mb; avail=470719.953125Mb
2020-10-12 20:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 20:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.97265625Mb; avail=470719.953125Mb
2020-10-12 20:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089701
2020-10-12 20:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094260
2020-10-12 20:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.77734375Mb; avail=470719.83203125Mb
2020-10-12 20:24:23 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 20:24:41 | INFO | train_inner | epoch 011:     60 / 104 loss=11.305, nll_loss=10.743, ppl=1713.99, wps=15027.6, ups=2.79, wpb=5387.3, bsz=241.9, num_updates=1100, lr=5.50725e-05, gnorm=1.152, clip=0, train_wall=28, wall=414
2020-10-12 20:24:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17913.75390625Mb; avail=470678.0Mb
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001438
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17913.75390625Mb; avail=470678.0Mb
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074730
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17913.69140625Mb; avail=470678.484375Mb
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052693
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129681
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17913.8203125Mb; avail=470678.2421875Mb
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17913.9296875Mb; avail=470678.12109375Mb
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001245
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17913.9296875Mb; avail=470678.12109375Mb
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075378
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17914.0390625Mb; avail=470678.0Mb
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052854
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130215
2020-10-12 20:24:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17914.0390625Mb; avail=470678.0Mb
2020-10-12 20:24:55 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 11.119 | nll_loss 10.503 | ppl 1451.59 | wps 50536.2 | wpb 2129.5 | bsz 102 | num_updates 1144 | best_loss 11.119
2020-10-12 20:24:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:25:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 11 @ 1144 updates, score 11.119) (writing took 4.432011985999452 seconds)
2020-10-12 20:25:00 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 20:25:00 | INFO | train | epoch 011 | loss 11.262 | nll_loss 10.693 | ppl 1655.73 | wps 14980.9 | ups 2.81 | wpb 5327.4 | bsz 249.4 | num_updates 1144 | lr 5.72714e-05 | gnorm 1.167 | clip 0 | train_wall 29 | wall 433
2020-10-12 20:25:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 20:25:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 20:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17876.06640625Mb; avail=470715.4609375Mb
2020-10-12 20:25:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000842
2020-10-12 20:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005510
2020-10-12 20:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.06640625Mb; avail=470715.4609375Mb
2020-10-12 20:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 20:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.06640625Mb; avail=470715.4609375Mb
2020-10-12 20:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092622
2020-10-12 20:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099093
2020-10-12 20:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.0Mb; avail=470720.91015625Mb
2020-10-12 20:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17871.0078125Mb; avail=470720.796875Mb
2020-10-12 20:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003619
2020-10-12 20:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.0078125Mb; avail=470720.796875Mb
2020-10-12 20:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 20:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.0078125Mb; avail=470720.796875Mb
2020-10-12 20:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089764
2020-10-12 20:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094290
2020-10-12 20:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17870.90234375Mb; avail=470720.67578125Mb
2020-10-12 20:25:00 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 20:25:16 | INFO | train_inner | epoch 012:     56 / 104 loss=11.206, nll_loss=10.628, ppl=1582.26, wps=14625.8, ups=2.81, wpb=5201.1, bsz=249.5, num_updates=1200, lr=6.007e-05, gnorm=1.074, clip=0, train_wall=28, wall=450
2020-10-12 20:25:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18826.34375Mb; avail=469770.4453125Mb
2020-10-12 20:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001838
2020-10-12 20:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18826.0859375Mb; avail=469770.10546875Mb
2020-10-12 20:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080381
2020-10-12 20:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18825.20703125Mb; avail=469769.27734375Mb
2020-10-12 20:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059501
2020-10-12 20:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142768
2020-10-12 20:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18825.47265625Mb; avail=469768.9765625Mb
2020-10-12 20:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18825.48046875Mb; avail=469769.43359375Mb
2020-10-12 20:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001176
2020-10-12 20:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18825.22265625Mb; avail=469769.20703125Mb
2020-10-12 20:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082555
2020-10-12 20:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18824.93359375Mb; avail=469768.37890625Mb
2020-10-12 20:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.058704
2020-10-12 20:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.143249
2020-10-12 20:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18825.00390625Mb; avail=469767.828125Mb
2020-10-12 20:25:32 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 11.019 | nll_loss 10.395 | ppl 1346.89 | wps 50316.7 | wpb 2129.5 | bsz 102 | num_updates 1248 | best_loss 11.019
2020-10-12 20:25:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:25:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 12 @ 1248 updates, score 11.019) (writing took 7.799609308000072 seconds)
2020-10-12 20:25:40 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 20:25:40 | INFO | train | epoch 012 | loss 11.176 | nll_loss 10.592 | ppl 1543.28 | wps 13798.2 | ups 2.59 | wpb 5327.4 | bsz 249.4 | num_updates 1248 | lr 6.24688e-05 | gnorm 1.036 | clip 0 | train_wall 29 | wall 473
2020-10-12 20:25:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 20:25:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 20:25:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17804.515625Mb; avail=470787.1484375Mb
2020-10-12 20:25:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000782
2020-10-12 20:25:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005335
2020-10-12 20:25:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17804.515625Mb; avail=470787.1484375Mb
2020-10-12 20:25:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000196
2020-10-12 20:25:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17804.515625Mb; avail=470787.1484375Mb
2020-10-12 20:25:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089773
2020-10-12 20:25:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096042
2020-10-12 20:25:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.40625Mb; avail=470793.4609375Mb
2020-10-12 20:25:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17798.3046875Mb; avail=470793.58203125Mb
2020-10-12 20:25:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003643
2020-10-12 20:25:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.27734375Mb; avail=470793.82421875Mb
2020-10-12 20:25:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-12 20:25:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.27734375Mb; avail=470793.82421875Mb
2020-10-12 20:25:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089011
2020-10-12 20:25:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093553
2020-10-12 20:25:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.31640625Mb; avail=470793.58203125Mb
2020-10-12 20:25:40 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 20:25:55 | INFO | train_inner | epoch 013:     52 / 104 loss=11.139, nll_loss=10.547, ppl=1496.61, wps=13778.8, ups=2.57, wpb=5363.2, bsz=245.9, num_updates=1300, lr=6.50675e-05, gnorm=1.164, clip=0, train_wall=28, wall=488
2020-10-12 20:26:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17877.83203125Mb; avail=470714.01953125Mb
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001860
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.88671875Mb; avail=470714.1328125Mb
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075267
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17883.15625Mb; avail=470708.765625Mb
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052754
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130686
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.3828125Mb; avail=470698.33984375Mb
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17895.01171875Mb; avail=470696.16015625Mb
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001209
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17895.01171875Mb; avail=470696.16015625Mb
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075979
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17895.46875Mb; avail=470696.171875Mb
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053082
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.131062
2020-10-12 20:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17895.55859375Mb; avail=470696.171875Mb
2020-10-12 20:26:12 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 10.928 | nll_loss 10.286 | ppl 1248.92 | wps 50365.9 | wpb 2129.5 | bsz 102 | num_updates 1352 | best_loss 10.928
2020-10-12 20:26:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:26:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 13 @ 1352 updates, score 10.928) (writing took 16.470973778999905 seconds)
2020-10-12 20:26:29 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 20:26:29 | INFO | train | epoch 013 | loss 11.106 | nll_loss 10.51 | ppl 1457.75 | wps 11401.3 | ups 2.14 | wpb 5327.4 | bsz 249.4 | num_updates 1352 | lr 6.76662e-05 | gnorm 1.13 | clip 0 | train_wall 29 | wall 522
2020-10-12 20:26:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 20:26:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18547.30078125Mb; avail=470045.1171875Mb
2020-10-12 20:26:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000862
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005472
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18547.30078125Mb; avail=470045.1171875Mb
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000198
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18547.30078125Mb; avail=470045.1171875Mb
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090481
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096894
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18541.4921875Mb; avail=470050.90234375Mb
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18541.390625Mb; avail=470051.0234375Mb
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003803
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18541.390625Mb; avail=470051.0234375Mb
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18541.390625Mb; avail=470051.0234375Mb
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091717
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096387
2020-10-12 20:26:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18540.90625Mb; avail=470051.875Mb
2020-10-12 20:26:29 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 20:26:42 | INFO | train_inner | epoch 014:     48 / 104 loss=11.078, nll_loss=10.477, ppl=1425.48, wps=11272.4, ups=2.11, wpb=5345.4, bsz=253.6, num_updates=1400, lr=7.0065e-05, gnorm=1.084, clip=0, train_wall=27, wall=536
2020-10-12 20:26:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:26:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17846.67578125Mb; avail=470744.9921875Mb
2020-10-12 20:26:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002700
2020-10-12 20:26:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17846.67578125Mb; avail=470744.9921875Mb
2020-10-12 20:26:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077145
2020-10-12 20:26:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17846.75Mb; avail=470744.9921875Mb
2020-10-12 20:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103139
2020-10-12 20:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.184057
2020-10-12 20:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17846.578125Mb; avail=470745.47265625Mb
2020-10-12 20:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17846.58984375Mb; avail=470745.3515625Mb
2020-10-12 20:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001244
2020-10-12 20:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17846.58984375Mb; avail=470745.3515625Mb
2020-10-12 20:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075311
2020-10-12 20:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17846.6875Mb; avail=470745.109375Mb
2020-10-12 20:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053379
2020-10-12 20:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130702
2020-10-12 20:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17846.7578125Mb; avail=470745.109375Mb
2020-10-12 20:27:01 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 10.816 | nll_loss 10.157 | ppl 1142.01 | wps 50086 | wpb 2129.5 | bsz 102 | num_updates 1456 | best_loss 10.816
2020-10-12 20:27:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:27:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 14 @ 1456 updates, score 10.816) (writing took 8.741437240000778 seconds)
2020-10-12 20:27:10 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 20:27:10 | INFO | train | epoch 014 | loss 11.006 | nll_loss 10.394 | ppl 1345.66 | wps 13495.9 | ups 2.53 | wpb 5327.4 | bsz 249.4 | num_updates 1456 | lr 7.28636e-05 | gnorm 1.16 | clip 0 | train_wall 29 | wall 563
2020-10-12 20:27:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 20:27:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 20:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18416.21875Mb; avail=470176.06640625Mb
2020-10-12 20:27:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000763
2020-10-12 20:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005448
2020-10-12 20:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18410.56640625Mb; avail=470181.71875Mb
2020-10-12 20:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18410.56640625Mb; avail=470181.71875Mb
2020-10-12 20:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091700
2020-10-12 20:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098171
2020-10-12 20:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18410.56640625Mb; avail=470181.71875Mb
2020-10-12 20:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18410.4453125Mb; avail=470181.83984375Mb
2020-10-12 20:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003777
2020-10-12 20:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18410.4453125Mb; avail=470181.83984375Mb
2020-10-12 20:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 20:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18410.4453125Mb; avail=470181.83984375Mb
2020-10-12 20:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088973
2020-10-12 20:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093668
2020-10-12 20:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18410.05078125Mb; avail=470182.32421875Mb
2020-10-12 20:27:10 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 20:27:22 | INFO | train_inner | epoch 015:     44 / 104 loss=10.971, nll_loss=10.354, ppl=1308.62, wps=13310.1, ups=2.5, wpb=5328.6, bsz=236.6, num_updates=1500, lr=7.50625e-05, gnorm=1.185, clip=0, train_wall=28, wall=576
2020-10-12 20:27:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18167.6015625Mb; avail=470423.76953125Mb
2020-10-12 20:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001858
2020-10-12 20:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18170.0234375Mb; avail=470421.953125Mb
2020-10-12 20:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077540
2020-10-12 20:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18274.39453125Mb; avail=470317.20703125Mb
2020-10-12 20:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053667
2020-10-12 20:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.133884
2020-10-12 20:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18339.78515625Mb; avail=470252.421875Mb
2020-10-12 20:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18371.890625Mb; avail=470220.2109375Mb
2020-10-12 20:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001243
2020-10-12 20:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18373.1015625Mb; avail=470218.39453125Mb
2020-10-12 20:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076143
2020-10-12 20:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18459.828125Mb; avail=470132.17578125Mb
2020-10-12 20:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052539
2020-10-12 20:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130665
2020-10-12 20:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18519.8125Mb; avail=470072.11328125Mb
2020-10-12 20:27:42 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 10.704 | nll_loss 10.034 | ppl 1048.6 | wps 50295.6 | wpb 2129.5 | bsz 102 | num_updates 1560 | best_loss 10.704
2020-10-12 20:27:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:27:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 15 @ 1560 updates, score 10.704) (writing took 4.656941561999702 seconds)
2020-10-12 20:27:47 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 20:27:47 | INFO | train | epoch 015 | loss 10.904 | nll_loss 10.277 | ppl 1240.62 | wps 14934 | ups 2.8 | wpb 5327.4 | bsz 249.4 | num_updates 1560 | lr 7.8061e-05 | gnorm 1.19 | clip 0 | train_wall 29 | wall 600
2020-10-12 20:27:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 20:27:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17811.3828125Mb; avail=470780.359375Mb
2020-10-12 20:27:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000658
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005173
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.3828125Mb; avail=470780.359375Mb
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.3828125Mb; avail=470780.359375Mb
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090822
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096926
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.7890625Mb; avail=470786.14453125Mb
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17805.72265625Mb; avail=470786.62890625Mb
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003727
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.72265625Mb; avail=470786.62890625Mb
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.72265625Mb; avail=470786.62890625Mb
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089432
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094057
2020-10-12 20:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.23046875Mb; avail=470787.12109375Mb
2020-10-12 20:27:47 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 20:27:58 | INFO | train_inner | epoch 016:     40 / 104 loss=10.832, nll_loss=10.195, ppl=1172.09, wps=14836.8, ups=2.79, wpb=5322.3, bsz=266.4, num_updates=1600, lr=8.006e-05, gnorm=1.232, clip=0, train_wall=28, wall=612
2020-10-12 20:28:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:28:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17871.84375Mb; avail=470719.23828125Mb
2020-10-12 20:28:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001830
2020-10-12 20:28:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.84375Mb; avail=470719.23828125Mb
2020-10-12 20:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075802
2020-10-12 20:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17872.21875Mb; avail=470719.23828125Mb
2020-10-12 20:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052955
2020-10-12 20:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.131356
2020-10-12 20:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17872.47265625Mb; avail=470719.1171875Mb
2020-10-12 20:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17872.53125Mb; avail=470718.875Mb
2020-10-12 20:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001232
2020-10-12 20:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17872.53125Mb; avail=470718.875Mb
2020-10-12 20:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076205
2020-10-12 20:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17872.60546875Mb; avail=470718.99609375Mb
2020-10-12 20:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052204
2020-10-12 20:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130376
2020-10-12 20:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17872.66015625Mb; avail=470718.6328125Mb
2020-10-12 20:28:19 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 10.576 | nll_loss 9.884 | ppl 945.19 | wps 50290.9 | wpb 2129.5 | bsz 102 | num_updates 1664 | best_loss 10.576
2020-10-12 20:28:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:28:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 16 @ 1664 updates, score 10.576) (writing took 4.455970804000572 seconds)
2020-10-12 20:28:23 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 20:28:23 | INFO | train | epoch 016 | loss 10.789 | nll_loss 10.144 | ppl 1131.66 | wps 15130 | ups 2.84 | wpb 5327.4 | bsz 249.4 | num_updates 1664 | lr 8.32584e-05 | gnorm 1.219 | clip 0 | train_wall 29 | wall 637
2020-10-12 20:28:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 20:28:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 20:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17859.75390625Mb; avail=470731.83203125Mb
2020-10-12 20:28:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000791
2020-10-12 20:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005646
2020-10-12 20:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17859.75390625Mb; avail=470731.83203125Mb
2020-10-12 20:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000249
2020-10-12 20:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17859.75390625Mb; avail=470731.83203125Mb
2020-10-12 20:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091330
2020-10-12 20:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097996
2020-10-12 20:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.84375Mb; avail=470737.45703125Mb
2020-10-12 20:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17853.8671875Mb; avail=470737.69921875Mb
2020-10-12 20:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003775
2020-10-12 20:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.8671875Mb; avail=470737.69921875Mb
2020-10-12 20:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000199
2020-10-12 20:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.8671875Mb; avail=470737.69921875Mb
2020-10-12 20:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090347
2020-10-12 20:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095071
2020-10-12 20:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.85546875Mb; avail=470737.94140625Mb
2020-10-12 20:28:24 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 20:28:34 | INFO | train_inner | epoch 017:     36 / 104 loss=10.76, nll_loss=10.111, ppl=1105.76, wps=14921.6, ups=2.81, wpb=5312.6, bsz=243.1, num_updates=1700, lr=8.50575e-05, gnorm=1.211, clip=0, train_wall=28, wall=647
2020-10-12 20:28:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:28:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19097.46875Mb; avail=469495.4375Mb
2020-10-12 20:28:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001667
2020-10-12 20:28:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19097.46875Mb; avail=469495.4375Mb
2020-10-12 20:28:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076303
2020-10-12 20:28:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19097.5078125Mb; avail=469495.30078125Mb
2020-10-12 20:28:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053186
2020-10-12 20:28:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.131988
2020-10-12 20:28:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19097.6171875Mb; avail=469495.26953125Mb
2020-10-12 20:28:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19097.6171875Mb; avail=469495.26953125Mb
2020-10-12 20:28:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001167
2020-10-12 20:28:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19097.6171875Mb; avail=469495.26953125Mb
2020-10-12 20:28:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075853
2020-10-12 20:28:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19097.4921875Mb; avail=469495.39453125Mb
2020-10-12 20:28:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053679
2020-10-12 20:28:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.131471
2020-10-12 20:28:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19097.4921875Mb; avail=469495.39453125Mb
2020-10-12 20:28:56 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 10.445 | nll_loss 9.724 | ppl 845.61 | wps 50406.3 | wpb 2129.5 | bsz 102 | num_updates 1768 | best_loss 10.445
2020-10-12 20:28:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:29:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 17 @ 1768 updates, score 10.445) (writing took 9.212544314999832 seconds)
2020-10-12 20:29:05 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 20:29:05 | INFO | train | epoch 017 | loss 10.671 | nll_loss 10.007 | ppl 1029.01 | wps 13325.8 | ups 2.5 | wpb 5327.4 | bsz 249.4 | num_updates 1768 | lr 8.84558e-05 | gnorm 1.169 | clip 0 | train_wall 29 | wall 678
2020-10-12 20:29:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 20:29:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 20:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17807.9921875Mb; avail=470783.65234375Mb
2020-10-12 20:29:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000812
2020-10-12 20:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005389
2020-10-12 20:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17807.9921875Mb; avail=470783.65234375Mb
2020-10-12 20:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000209
2020-10-12 20:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17807.9921875Mb; avail=470783.65234375Mb
2020-10-12 20:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089948
2020-10-12 20:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096279
2020-10-12 20:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.35546875Mb; avail=470789.4375Mb
2020-10-12 20:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17802.46484375Mb; avail=470789.31640625Mb
2020-10-12 20:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003718
2020-10-12 20:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.46484375Mb; avail=470789.31640625Mb
2020-10-12 20:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 20:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.46484375Mb; avail=470789.31640625Mb
2020-10-12 20:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090274
2020-10-12 20:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094891
2020-10-12 20:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.6796875Mb; avail=470789.07421875Mb
2020-10-12 20:29:05 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 20:29:14 | INFO | train_inner | epoch 018:     32 / 104 loss=10.648, nll_loss=9.981, ppl=1010.35, wps=13252.5, ups=2.47, wpb=5362.9, bsz=237.5, num_updates=1800, lr=9.0055e-05, gnorm=1.088, clip=0, train_wall=28, wall=688
2020-10-12 20:29:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18428.109375Mb; avail=470163.8515625Mb
2020-10-12 20:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001688
2020-10-12 20:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18428.140625Mb; avail=470163.8515625Mb
2020-10-12 20:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076883
2020-10-12 20:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18428.68359375Mb; avail=470163.47265625Mb
2020-10-12 20:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053178
2020-10-12 20:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.132556
2020-10-12 20:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18428.75Mb; avail=470163.5625Mb
2020-10-12 20:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18428.75Mb; avail=470163.5625Mb
2020-10-12 20:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001220
2020-10-12 20:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18428.75Mb; avail=470163.5625Mb
2020-10-12 20:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076073
2020-10-12 20:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18428.75Mb; avail=470163.5625Mb
2020-10-12 20:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053063
2020-10-12 20:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.131098
2020-10-12 20:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18428.75Mb; avail=470163.5625Mb
2020-10-12 20:29:37 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 10.353 | nll_loss 9.618 | ppl 785.72 | wps 49606.3 | wpb 2129.5 | bsz 102 | num_updates 1872 | best_loss 10.353
2020-10-12 20:29:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:29:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 18 @ 1872 updates, score 10.353) (writing took 9.215255853000599 seconds)
2020-10-12 20:29:47 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 20:29:47 | INFO | train | epoch 018 | loss 10.547 | nll_loss 9.864 | ppl 931.62 | wps 13333.7 | ups 2.5 | wpb 5327.4 | bsz 249.4 | num_updates 1872 | lr 9.36532e-05 | gnorm 1.16 | clip 0 | train_wall 29 | wall 720
2020-10-12 20:29:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 20:29:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 20:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17878.9609375Mb; avail=470712.984375Mb
2020-10-12 20:29:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000804
2020-10-12 20:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005453
2020-10-12 20:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17879.12890625Mb; avail=470712.6796875Mb
2020-10-12 20:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-12 20:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17879.12890625Mb; avail=470712.6796875Mb
2020-10-12 20:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090216
2020-10-12 20:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096629
2020-10-12 20:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.46484375Mb; avail=470718.09375Mb
2020-10-12 20:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17873.44140625Mb; avail=470718.3359375Mb
2020-10-12 20:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003626
2020-10-12 20:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.44140625Mb; avail=470718.3359375Mb
2020-10-12 20:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.44140625Mb; avail=470718.3359375Mb
2020-10-12 20:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091475
2020-10-12 20:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096022
2020-10-12 20:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.3828125Mb; avail=470718.3046875Mb
2020-10-12 20:29:47 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 20:29:55 | INFO | train_inner | epoch 019:     28 / 104 loss=10.496, nll_loss=9.805, ppl=894.66, wps=13094.2, ups=2.48, wpb=5283.4, bsz=261.5, num_updates=1900, lr=9.50525e-05, gnorm=1.232, clip=0, train_wall=28, wall=728
2020-10-12 20:30:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:30:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18406.0546875Mb; avail=470190.30078125Mb
2020-10-12 20:30:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001718
2020-10-12 20:30:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18406.40234375Mb; avail=470190.56640625Mb
2020-10-12 20:30:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077534
2020-10-12 20:30:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18406.171875Mb; avail=470189.75390625Mb
2020-10-12 20:30:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053973
2020-10-12 20:30:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.134053
2020-10-12 20:30:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18405.76171875Mb; avail=470188.9453125Mb
2020-10-12 20:30:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18405.71484375Mb; avail=470188.27734375Mb
2020-10-12 20:30:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001366
2020-10-12 20:30:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18405.45703125Mb; avail=470188.54296875Mb
2020-10-12 20:30:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077254
2020-10-12 20:30:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18405.859375Mb; avail=470187.30859375Mb
2020-10-12 20:30:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053252
2020-10-12 20:30:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.132645
2020-10-12 20:30:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18406.24609375Mb; avail=470186.5625Mb
2020-10-12 20:30:19 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 10.25 | nll_loss 9.504 | ppl 726.13 | wps 50058.6 | wpb 2129.5 | bsz 102 | num_updates 1976 | best_loss 10.25
2020-10-12 20:30:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:30:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 19 @ 1976 updates, score 10.25) (writing took 7.188361233000251 seconds)
2020-10-12 20:30:26 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 20:30:26 | INFO | train | epoch 019 | loss 10.434 | nll_loss 9.732 | ppl 850.64 | wps 13941.6 | ups 2.62 | wpb 5327.4 | bsz 249.4 | num_updates 1976 | lr 9.88506e-05 | gnorm 1.139 | clip 0 | train_wall 29 | wall 760
2020-10-12 20:30:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 20:30:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 20:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17854.25Mb; avail=470737.0Mb
2020-10-12 20:30:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000828
2020-10-12 20:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005374
2020-10-12 20:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.3203125Mb; avail=470736.63671875Mb
2020-10-12 20:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000206
2020-10-12 20:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.3203125Mb; avail=470736.63671875Mb
2020-10-12 20:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089976
2020-10-12 20:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096304
2020-10-12 20:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17848.9453125Mb; avail=470742.35546875Mb
2020-10-12 20:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17848.8671875Mb; avail=470742.59765625Mb
2020-10-12 20:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004367
2020-10-12 20:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17848.8671875Mb; avail=470742.59765625Mb
2020-10-12 20:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000220
2020-10-12 20:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17848.8671875Mb; avail=470742.59765625Mb
2020-10-12 20:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091923
2020-10-12 20:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097253
2020-10-12 20:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17848.94921875Mb; avail=470742.234375Mb
2020-10-12 20:30:27 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 20:30:33 | INFO | train_inner | epoch 020:     24 / 104 loss=10.405, nll_loss=9.698, ppl=830.87, wps=13705.7, ups=2.59, wpb=5286.5, bsz=242.8, num_updates=2000, lr=0.00010005, gnorm=1.141, clip=0, train_wall=28, wall=767
2020-10-12 20:30:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18543.7734375Mb; avail=470048.453125Mb
2020-10-12 20:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001731
2020-10-12 20:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18544.99609375Mb; avail=470046.515625Mb
2020-10-12 20:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075544
2020-10-12 20:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18600.66796875Mb; avail=469991.265625Mb
2020-10-12 20:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054020
2020-10-12 20:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.132099
2020-10-12 20:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18646.71484375Mb; avail=469945.0390625Mb
2020-10-12 20:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18667.47265625Mb; avail=469923.87890625Mb
2020-10-12 20:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001197
2020-10-12 20:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18668.68359375Mb; avail=469922.66796875Mb
2020-10-12 20:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075765
2020-10-12 20:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18761.3671875Mb; avail=469830.58984375Mb
2020-10-12 20:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054576
2020-10-12 20:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.132316
2020-10-12 20:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18818.328125Mb; avail=469773.0390625Mb
2020-10-12 20:30:59 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 10.158 | nll_loss 9.386 | ppl 668.84 | wps 50310.9 | wpb 2129.5 | bsz 102 | num_updates 2080 | best_loss 10.158
2020-10-12 20:30:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:31:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 20 @ 2080 updates, score 10.158) (writing took 9.807915067998692 seconds)
2020-10-12 20:31:08 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 20:31:08 | INFO | train | epoch 020 | loss 10.326 | nll_loss 9.607 | ppl 779.63 | wps 13147.8 | ups 2.47 | wpb 5327.4 | bsz 249.4 | num_updates 2080 | lr 0.000104048 | gnorm 1.179 | clip 0 | train_wall 29 | wall 802
2020-10-12 20:31:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 20:31:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 20:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18242.87890625Mb; avail=470348.55859375Mb
2020-10-12 20:31:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000808
2020-10-12 20:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005736
2020-10-12 20:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18242.87890625Mb; avail=470348.55859375Mb
2020-10-12 20:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000328
2020-10-12 20:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18242.87890625Mb; avail=470348.55859375Mb
2020-10-12 20:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.112181
2020-10-12 20:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.119439
2020-10-12 20:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18237.25390625Mb; avail=470354.3046875Mb
2020-10-12 20:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17878.078125Mb; avail=470713.48828125Mb
2020-10-12 20:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003803
2020-10-12 20:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17878.078125Mb; avail=470713.48828125Mb
2020-10-12 20:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000282
2020-10-12 20:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17878.078125Mb; avail=470713.48828125Mb
2020-10-12 20:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109490
2020-10-12 20:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114796
2020-10-12 20:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.49609375Mb; avail=470713.98046875Mb
2020-10-12 20:31:09 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 20:31:15 | INFO | train_inner | epoch 021:     20 / 104 loss=10.304, nll_loss=9.582, ppl=766.21, wps=13196.3, ups=2.43, wpb=5440.4, bsz=259.3, num_updates=2100, lr=0.000105048, gnorm=1.174, clip=0, train_wall=28, wall=808
2020-10-12 20:31:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17836.21484375Mb; avail=470755.2578125Mb
2020-10-12 20:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001885
2020-10-12 20:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.21484375Mb; avail=470755.2578125Mb
2020-10-12 20:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075246
2020-10-12 20:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.3359375Mb; avail=470755.12109375Mb
2020-10-12 20:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052861
2020-10-12 20:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130828
2020-10-12 20:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.3359375Mb; avail=470755.12109375Mb
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17836.296875Mb; avail=470755.0Mb
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001252
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.296875Mb; avail=470755.0Mb
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075751
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.296875Mb; avail=470755.0Mb
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053239
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130993
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.7578125Mb; avail=470754.7578125Mb
2020-10-12 20:31:41 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 10.095 | nll_loss 9.315 | ppl 636.95 | wps 50502.2 | wpb 2129.5 | bsz 102 | num_updates 2184 | best_loss 10.095
2020-10-12 20:31:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:31:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 21 @ 2184 updates, score 10.095) (writing took 7.000384900000427 seconds)
2020-10-12 20:31:48 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 20:31:48 | INFO | train | epoch 021 | loss 10.219 | nll_loss 9.483 | ppl 715.36 | wps 14057.4 | ups 2.64 | wpb 5327.4 | bsz 249.4 | num_updates 2184 | lr 0.000109245 | gnorm 1.208 | clip 0 | train_wall 29 | wall 841
2020-10-12 20:31:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 20:31:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 20:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17872.828125Mb; avail=470718.98828125Mb
2020-10-12 20:31:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000829
2020-10-12 20:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005333
2020-10-12 20:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17866.08984375Mb; avail=470725.6640625Mb
2020-10-12 20:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17866.08984375Mb; avail=470725.6640625Mb
2020-10-12 20:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090134
2020-10-12 20:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096403
2020-10-12 20:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.62109375Mb; avail=470726.04296875Mb
2020-10-12 20:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17865.5Mb; avail=470726.04296875Mb
2020-10-12 20:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003731
2020-10-12 20:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.5Mb; avail=470726.04296875Mb
2020-10-12 20:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 20:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.5Mb; avail=470726.04296875Mb
2020-10-12 20:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088713
2020-10-12 20:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093329
2020-10-12 20:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.71484375Mb; avail=470726.04296875Mb
2020-10-12 20:31:48 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 20:31:53 | INFO | train_inner | epoch 022:     16 / 104 loss=10.205, nll_loss=9.466, ppl=707.09, wps=13941.7, ups=2.62, wpb=5325.7, bsz=248.1, num_updates=2200, lr=0.000110045, gnorm=1.23, clip=0, train_wall=28, wall=846
2020-10-12 20:32:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:32:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17908.1171875Mb; avail=470683.140625Mb
2020-10-12 20:32:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002609
2020-10-12 20:32:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17908.1171875Mb; avail=470683.140625Mb
2020-10-12 20:32:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082052
2020-10-12 20:32:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17908.44921875Mb; avail=470683.01953125Mb
2020-10-12 20:32:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.071492
2020-10-12 20:32:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.157191
2020-10-12 20:32:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17908.61328125Mb; avail=470682.77734375Mb
2020-10-12 20:32:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17909.21875Mb; avail=470682.171875Mb
2020-10-12 20:32:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001227
2020-10-12 20:32:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17909.21875Mb; avail=470682.171875Mb
2020-10-12 20:32:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075198
2020-10-12 20:32:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17909.55859375Mb; avail=470682.171875Mb
2020-10-12 20:32:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051922
2020-10-12 20:32:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129100
2020-10-12 20:32:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17909.58984375Mb; avail=470682.29296875Mb
2020-10-12 20:32:20 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 10.013 | nll_loss 9.211 | ppl 592.57 | wps 50321.9 | wpb 2129.5 | bsz 102 | num_updates 2288 | best_loss 10.013
2020-10-12 20:32:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:32:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 22 @ 2288 updates, score 10.013) (writing took 4.87240268000096 seconds)
2020-10-12 20:32:25 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 20:32:25 | INFO | train | epoch 022 | loss 10.122 | nll_loss 9.369 | ppl 661.44 | wps 14895.9 | ups 2.8 | wpb 5327.4 | bsz 249.4 | num_updates 2288 | lr 0.000114443 | gnorm 1.248 | clip 0 | train_wall 29 | wall 878
2020-10-12 20:32:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 20:32:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 20:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19118.359375Mb; avail=469473.59375Mb
2020-10-12 20:32:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000755
2020-10-12 20:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005140
2020-10-12 20:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19123.80859375Mb; avail=469468.14453125Mb
2020-10-12 20:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 20:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19124.4140625Mb; avail=469467.5390625Mb
2020-10-12 20:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091343
2020-10-12 20:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097540
2020-10-12 20:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19158.97265625Mb; avail=469436.91796875Mb
2020-10-12 20:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19159.95703125Mb; avail=469438.88671875Mb
2020-10-12 20:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003751
2020-10-12 20:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19159.95703125Mb; avail=469438.88671875Mb
2020-10-12 20:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-12 20:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19159.95703125Mb; avail=469438.88671875Mb
2020-10-12 20:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090035
2020-10-12 20:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094712
2020-10-12 20:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19162.07421875Mb; avail=469436.390625Mb
2020-10-12 20:32:25 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 20:32:29 | INFO | train_inner | epoch 023:     12 / 104 loss=10.115, nll_loss=9.362, ppl=657.83, wps=14746.3, ups=2.79, wpb=5293.8, bsz=241.7, num_updates=2300, lr=0.000115043, gnorm=1.215, clip=0, train_wall=28, wall=882
2020-10-12 20:32:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17883.0859375Mb; avail=470708.73046875Mb
2020-10-12 20:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001838
2020-10-12 20:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17883.0859375Mb; avail=470708.73046875Mb
2020-10-12 20:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074137
2020-10-12 20:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17882.734375Mb; avail=470708.3671875Mb
2020-10-12 20:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051694
2020-10-12 20:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128457
2020-10-12 20:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17883.328125Mb; avail=470708.1015625Mb
2020-10-12 20:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17883.36328125Mb; avail=470707.859375Mb
2020-10-12 20:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001196
2020-10-12 20:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17883.36328125Mb; avail=470707.859375Mb
2020-10-12 20:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075874
2020-10-12 20:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17883.359375Mb; avail=470707.96484375Mb
2020-10-12 20:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052212
2020-10-12 20:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130065
2020-10-12 20:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17883.03125Mb; avail=470708.20703125Mb
2020-10-12 20:32:57 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 9.947 | nll_loss 9.143 | ppl 565.16 | wps 50142.7 | wpb 2129.5 | bsz 102 | num_updates 2392 | best_loss 9.947
2020-10-12 20:32:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:33:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 23 @ 2392 updates, score 9.947) (writing took 4.536205112999596 seconds)
2020-10-12 20:33:02 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 20:33:02 | INFO | train | epoch 023 | loss 10.006 | nll_loss 9.236 | ppl 603.12 | wps 15042.4 | ups 2.82 | wpb 5327.4 | bsz 249.4 | num_updates 2392 | lr 0.00011964 | gnorm 1.239 | clip 0 | train_wall 29 | wall 915
2020-10-12 20:33:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 20:33:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 20:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18495.00390625Mb; avail=470098.16796875Mb
2020-10-12 20:33:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000824
2020-10-12 20:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005277
2020-10-12 20:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18495.49609375Mb; avail=470099.15234375Mb
2020-10-12 20:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18495.49609375Mb; avail=470099.15234375Mb
2020-10-12 20:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089424
2020-10-12 20:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095732
2020-10-12 20:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18490.29296875Mb; avail=470109.27734375Mb
2020-10-12 20:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18489.8671875Mb; avail=470109.02734375Mb
2020-10-12 20:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003927
2020-10-12 20:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18490.17578125Mb; avail=470108.5Mb
2020-10-12 20:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 20:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18490.65234375Mb; avail=470108.87890625Mb
2020-10-12 20:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089824
2020-10-12 20:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094791
2020-10-12 20:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18490.33203125Mb; avail=470107.16015625Mb
2020-10-12 20:33:02 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 20:33:05 | INFO | train_inner | epoch 024:      8 / 104 loss=9.991, nll_loss=9.219, ppl=595.96, wps=14908.8, ups=2.79, wpb=5341.5, bsz=255.9, num_updates=2400, lr=0.00012004, gnorm=1.246, clip=0, train_wall=28, wall=918
2020-10-12 20:33:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18262.76171875Mb; avail=470329.11328125Mb
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001918
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18262.76171875Mb; avail=470329.11328125Mb
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074251
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18281.51171875Mb; avail=470310.625Mb
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052531
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129591
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18344.328125Mb; avail=470247.80859375Mb
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18370.12109375Mb; avail=470221.31640625Mb
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001422
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18365.44921875Mb; avail=470226.50390625Mb
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075507
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18450.1796875Mb; avail=470142.1875Mb
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052685
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130397
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18518.59765625Mb; avail=470073.76953125Mb
2020-10-12 20:33:34 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 9.858 | nll_loss 9.027 | ppl 521.85 | wps 50267.2 | wpb 2129.5 | bsz 102 | num_updates 2496 | best_loss 9.858
2020-10-12 20:33:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:33:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 24 @ 2496 updates, score 9.858) (writing took 6.192680312999073 seconds)
2020-10-12 20:33:40 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 20:33:40 | INFO | train | epoch 024 | loss 9.894 | nll_loss 9.107 | ppl 551.41 | wps 14395.3 | ups 2.7 | wpb 5327.4 | bsz 249.4 | num_updates 2496 | lr 0.000124838 | gnorm 1.202 | clip 0 | train_wall 29 | wall 954
2020-10-12 20:33:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 20:33:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 20:33:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15915.94921875Mb; avail=472687.36328125Mb
2020-10-12 20:33:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000804
2020-10-12 20:33:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006682
2020-10-12 20:33:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15926.84765625Mb; avail=472676.46484375Mb
2020-10-12 20:33:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000318
2020-10-12 20:33:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15928.05859375Mb; avail=472675.25390625Mb
2020-10-12 20:33:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.132599
2020-10-12 20:33:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.140888
2020-10-12 20:33:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16124.16796875Mb; avail=472478.92578125Mb
2020-10-12 20:33:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16146.78125Mb; avail=472456.7890625Mb
2020-10-12 20:33:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005349
2020-10-12 20:33:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16149.1171875Mb; avail=472454.3671875Mb
2020-10-12 20:33:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000331
2020-10-12 20:33:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16149.72265625Mb; avail=472453.76171875Mb
2020-10-12 20:33:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.160523
2020-10-12 20:33:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.167991
2020-10-12 20:33:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16177.2421875Mb; avail=472426.03125Mb
2020-10-12 20:33:41 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 20:33:42 | INFO | train_inner | epoch 025:      4 / 104 loss=9.887, nll_loss=9.099, ppl=548.18, wps=14298.6, ups=2.67, wpb=5365.1, bsz=250.3, num_updates=2500, lr=0.000125037, gnorm=1.205, clip=0, train_wall=28, wall=955
2020-10-12 20:34:11 | INFO | train_inner | epoch 025:    104 / 104 loss=9.787, nll_loss=8.984, ppl=506.31, wps=18581.1, ups=3.51, wpb=5289.6, bsz=249.9, num_updates=2600, lr=0.000130035, gnorm=1.354, clip=0, train_wall=28, wall=984
2020-10-12 20:34:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18494.375Mb; avail=470097.05859375Mb
2020-10-12 20:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001816
2020-10-12 20:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18496.5703125Mb; avail=470094.86328125Mb
2020-10-12 20:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075824
2020-10-12 20:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18496.25Mb; avail=470102.12890625Mb
2020-10-12 20:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054035
2020-10-12 20:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.132471
2020-10-12 20:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18498.76953125Mb; avail=470099.57421875Mb
2020-10-12 20:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18498.375Mb; avail=470099.55078125Mb
2020-10-12 20:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001235
2020-10-12 20:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18497.98828125Mb; avail=470099.2109375Mb
2020-10-12 20:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.103259
2020-10-12 20:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18498.53125Mb; avail=470097.0859375Mb
2020-10-12 20:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062701
2020-10-12 20:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.168209
2020-10-12 20:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18498.2890625Mb; avail=470095.63671875Mb
2020-10-12 20:34:13 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 9.795 | nll_loss 8.949 | ppl 494.36 | wps 50063.3 | wpb 2129.5 | bsz 102 | num_updates 2600 | best_loss 9.795
2020-10-12 20:34:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:34:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 25 @ 2600 updates, score 9.795) (writing took 5.982118582998737 seconds)
2020-10-12 20:34:19 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 20:34:19 | INFO | train | epoch 025 | loss 9.79 | nll_loss 8.987 | ppl 507.51 | wps 14330 | ups 2.69 | wpb 5327.4 | bsz 249.4 | num_updates 2600 | lr 0.000130035 | gnorm 1.349 | clip 0 | train_wall 29 | wall 992
2020-10-12 20:34:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 20:34:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17880.37890625Mb; avail=470711.3515625Mb
2020-10-12 20:34:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000825
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005312
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17880.37890625Mb; avail=470711.3515625Mb
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17880.37890625Mb; avail=470711.3515625Mb
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090746
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096979
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.44140625Mb; avail=470717.265625Mb
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17874.50390625Mb; avail=470717.39453125Mb
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003616
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.50390625Mb; avail=470717.39453125Mb
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.50390625Mb; avail=470717.39453125Mb
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.120696
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.125501
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17886.703125Mb; avail=470704.90234375Mb
2020-10-12 20:34:19 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 20:34:48 | INFO | train_inner | epoch 026:    100 / 104 loss=9.695, nll_loss=8.878, ppl=470.44, wps=14505.3, ups=2.7, wpb=5374.6, bsz=245.5, num_updates=2700, lr=0.000135032, gnorm=1.319, clip=0, train_wall=28, wall=1021
2020-10-12 20:34:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17947.80078125Mb; avail=470643.1171875Mb
2020-10-12 20:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001726
2020-10-12 20:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17947.80078125Mb; avail=470643.1171875Mb
2020-10-12 20:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080243
2020-10-12 20:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17947.453125Mb; avail=470643.359375Mb
2020-10-12 20:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.058421
2020-10-12 20:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141192
2020-10-12 20:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17947.4375Mb; avail=470643.48046875Mb
2020-10-12 20:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17947.4375Mb; avail=470643.48046875Mb
2020-10-12 20:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001284
2020-10-12 20:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17947.4375Mb; avail=470643.48046875Mb
2020-10-12 20:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081521
2020-10-12 20:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17947.46875Mb; avail=470643.84375Mb
2020-10-12 20:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059122
2020-10-12 20:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142712
2020-10-12 20:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17947.34765625Mb; avail=470643.96484375Mb
2020-10-12 20:34:51 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 9.779 | nll_loss 8.954 | ppl 495.76 | wps 50588.1 | wpb 2129.5 | bsz 102 | num_updates 2704 | best_loss 9.779
2020-10-12 20:34:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:34:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 26 @ 2704 updates, score 9.779) (writing took 6.994282544999805 seconds)
2020-10-12 20:34:58 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 20:34:58 | INFO | train | epoch 026 | loss 9.682 | nll_loss 8.862 | ppl 465.46 | wps 14151.3 | ups 2.66 | wpb 5327.4 | bsz 249.4 | num_updates 2704 | lr 0.000135232 | gnorm 1.323 | clip 0 | train_wall 29 | wall 1032
2020-10-12 20:34:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 20:34:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 20:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19067.59375Mb; avail=469524.28125Mb
2020-10-12 20:34:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000824
2020-10-12 20:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005331
2020-10-12 20:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19067.59375Mb; avail=469524.28125Mb
2020-10-12 20:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 20:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19067.59375Mb; avail=469524.28125Mb
2020-10-12 20:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093807
2020-10-12 20:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100160
2020-10-12 20:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19062.2265625Mb; avail=469529.78125Mb
2020-10-12 20:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19062.046875Mb; avail=469529.859375Mb
2020-10-12 20:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003648
2020-10-12 20:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19062.046875Mb; avail=469529.859375Mb
2020-10-12 20:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-12 20:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19062.046875Mb; avail=469529.859375Mb
2020-10-12 20:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090927
2020-10-12 20:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095483
2020-10-12 20:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19062.37109375Mb; avail=469529.73828125Mb
2020-10-12 20:34:58 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 20:35:26 | INFO | train_inner | epoch 027:     96 / 104 loss=9.538, nll_loss=8.698, ppl=415.19, wps=13843.2, ups=2.63, wpb=5260.6, bsz=256.9, num_updates=2800, lr=0.00014003, gnorm=1.316, clip=0, train_wall=27, wall=1059
2020-10-12 20:35:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17884.75390625Mb; avail=470689.09375Mb
2020-10-12 20:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001770
2020-10-12 20:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17884.73828125Mb; avail=470689.3359375Mb
2020-10-12 20:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078213
2020-10-12 20:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17884.515625Mb; avail=470689.09375Mb
2020-10-12 20:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056101
2020-10-12 20:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136970
2020-10-12 20:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17884.50390625Mb; avail=470689.2109375Mb
2020-10-12 20:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17884.50390625Mb; avail=470689.21484375Mb
2020-10-12 20:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001293
2020-10-12 20:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17884.50390625Mb; avail=470689.21484375Mb
2020-10-12 20:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078231
2020-10-12 20:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17884.42578125Mb; avail=470689.09375Mb
2020-10-12 20:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054382
2020-10-12 20:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.134716
2020-10-12 20:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17884.36328125Mb; avail=470689.3359375Mb
2020-10-12 20:35:30 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 9.667 | nll_loss 8.794 | ppl 443.74 | wps 50249.3 | wpb 2129.5 | bsz 102 | num_updates 2808 | best_loss 9.667
2020-10-12 20:35:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:35:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 27 @ 2808 updates, score 9.667) (writing took 13.559812969000632 seconds)
2020-10-12 20:35:44 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 20:35:44 | INFO | train | epoch 027 | loss 9.56 | nll_loss 8.722 | ppl 422.26 | wps 12118 | ups 2.27 | wpb 5327.4 | bsz 249.4 | num_updates 2808 | lr 0.00014043 | gnorm 1.314 | clip 0 | train_wall 29 | wall 1077
2020-10-12 20:35:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 20:35:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 20:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18451.31640625Mb; avail=470122.72265625Mb
2020-10-12 20:35:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000696
2020-10-12 20:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005110
2020-10-12 20:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18451.31640625Mb; avail=470122.72265625Mb
2020-10-12 20:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000219
2020-10-12 20:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18451.31640625Mb; avail=470122.72265625Mb
2020-10-12 20:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.114042
2020-10-12 20:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.120126
2020-10-12 20:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18445.06640625Mb; avail=470129.0546875Mb
2020-10-12 20:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18445.07421875Mb; avail=470129.0546875Mb
2020-10-12 20:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003776
2020-10-12 20:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18445.07421875Mb; avail=470129.0546875Mb
2020-10-12 20:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000202
2020-10-12 20:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18445.07421875Mb; avail=470129.0546875Mb
2020-10-12 20:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106682
2020-10-12 20:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111398
2020-10-12 20:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18444.9765625Mb; avail=470129.0546875Mb
2020-10-12 20:35:44 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 20:36:10 | INFO | train_inner | epoch 028:     92 / 104 loss=9.452, nll_loss=8.598, ppl=387.39, wps=11949.3, ups=2.23, wpb=5349.1, bsz=252, num_updates=2900, lr=0.000145028, gnorm=1.352, clip=0, train_wall=28, wall=1104
2020-10-12 20:36:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17955.58203125Mb; avail=470618.28515625Mb
2020-10-12 20:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001738
2020-10-12 20:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17955.58203125Mb; avail=470618.28515625Mb
2020-10-12 20:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075383
2020-10-12 20:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17955.62890625Mb; avail=470618.1640625Mb
2020-10-12 20:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052132
2020-10-12 20:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130036
2020-10-12 20:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17955.62890625Mb; avail=470618.1640625Mb
2020-10-12 20:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17955.69921875Mb; avail=470618.28515625Mb
2020-10-12 20:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001259
2020-10-12 20:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17955.69921875Mb; avail=470618.28515625Mb
2020-10-12 20:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075708
2020-10-12 20:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17955.3359375Mb; avail=470618.53515625Mb
2020-10-12 20:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052428
2020-10-12 20:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130131
2020-10-12 20:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17955.23046875Mb; avail=470618.65625Mb
2020-10-12 20:36:16 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 9.586 | nll_loss 8.702 | ppl 416.47 | wps 50576.1 | wpb 2129.5 | bsz 102 | num_updates 2912 | best_loss 9.586
2020-10-12 20:36:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:36:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 28 @ 2912 updates, score 9.586) (writing took 8.65582876300141 seconds)
2020-10-12 20:36:25 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 20:36:25 | INFO | train | epoch 028 | loss 9.443 | nll_loss 8.587 | ppl 384.51 | wps 13529 | ups 2.54 | wpb 5327.4 | bsz 249.4 | num_updates 2912 | lr 0.000145627 | gnorm 1.345 | clip 0 | train_wall 29 | wall 1118
2020-10-12 20:36:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 20:36:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 20:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17941.14453125Mb; avail=470632.85546875Mb
2020-10-12 20:36:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000680
2020-10-12 20:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005069
2020-10-12 20:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17941.140625Mb; avail=470632.86328125Mb
2020-10-12 20:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-12 20:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17941.140625Mb; avail=470632.86328125Mb
2020-10-12 20:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090289
2020-10-12 20:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096310
2020-10-12 20:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17935.5Mb; avail=470638.64453125Mb
2020-10-12 20:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17935.2265625Mb; avail=470639.25Mb
2020-10-12 20:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003677
2020-10-12 20:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17935.2265625Mb; avail=470639.25Mb
2020-10-12 20:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 20:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17935.2265625Mb; avail=470639.25Mb
2020-10-12 20:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.088861
2020-10-12 20:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093471
2020-10-12 20:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17935.578125Mb; avail=470638.765625Mb
2020-10-12 20:36:25 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 20:36:50 | INFO | train_inner | epoch 029:     88 / 104 loss=9.335, nll_loss=8.463, ppl=352.91, wps=13348.6, ups=2.51, wpb=5319.6, bsz=248.5, num_updates=3000, lr=0.000150025, gnorm=1.3, clip=0, train_wall=28, wall=1144
2020-10-12 20:36:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:36:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18461.73046875Mb; avail=470112.0390625Mb
2020-10-12 20:36:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001874
2020-10-12 20:36:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18461.6171875Mb; avail=470112.0390625Mb
2020-10-12 20:36:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076086
2020-10-12 20:36:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18461.4296875Mb; avail=470112.921875Mb
2020-10-12 20:36:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052820
2020-10-12 20:36:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.131557
2020-10-12 20:36:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18461.546875Mb; avail=470112.78515625Mb
2020-10-12 20:36:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18461.546875Mb; avail=470112.78515625Mb
2020-10-12 20:36:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001182
2020-10-12 20:36:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18461.546875Mb; avail=470112.78515625Mb
2020-10-12 20:36:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074699
2020-10-12 20:36:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18461.3359375Mb; avail=470112.890625Mb
2020-10-12 20:36:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052663
2020-10-12 20:36:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129278
2020-10-12 20:36:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18461.6953125Mb; avail=470112.73828125Mb
2020-10-12 20:36:57 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 9.554 | nll_loss 8.666 | ppl 406.1 | wps 50295.1 | wpb 2129.5 | bsz 102 | num_updates 3016 | best_loss 9.554
2020-10-12 20:36:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:37:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 29 @ 3016 updates, score 9.554) (writing took 5.213681954999629 seconds)
2020-10-12 20:37:03 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 20:37:03 | INFO | train | epoch 029 | loss 9.32 | nll_loss 8.445 | ppl 348.54 | wps 14709.6 | ups 2.76 | wpb 5327.4 | bsz 249.4 | num_updates 3016 | lr 0.000150825 | gnorm 1.297 | clip 0 | train_wall 29 | wall 1156
2020-10-12 20:37:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 20:37:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 20:37:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17895.10546875Mb; avail=470678.08203125Mb
2020-10-12 20:37:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000655
2020-10-12 20:37:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005088
2020-10-12 20:37:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17895.10546875Mb; avail=470678.08203125Mb
2020-10-12 20:37:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 20:37:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17895.10546875Mb; avail=470678.08203125Mb
2020-10-12 20:37:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091833
2020-10-12 20:37:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097954
2020-10-12 20:37:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17889.26171875Mb; avail=470683.9296875Mb
2020-10-12 20:37:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17889.26171875Mb; avail=470683.9296875Mb
2020-10-12 20:37:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003705
2020-10-12 20:37:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17889.26171875Mb; avail=470683.9296875Mb
2020-10-12 20:37:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 20:37:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17889.26171875Mb; avail=470683.9296875Mb
2020-10-12 20:37:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092043
2020-10-12 20:37:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096668
2020-10-12 20:37:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17889.73046875Mb; avail=470683.80859375Mb
2020-10-12 20:37:03 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 20:37:27 | INFO | train_inner | epoch 030:     84 / 104 loss=9.237, nll_loss=8.349, ppl=326.06, wps=14636.1, ups=2.74, wpb=5348.8, bsz=246.1, num_updates=3100, lr=0.000155023, gnorm=1.409, clip=0, train_wall=28, wall=1180
2020-10-12 20:37:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17903.2890625Mb; avail=470671.265625Mb
2020-10-12 20:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001886
2020-10-12 20:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17903.2890625Mb; avail=470671.265625Mb
2020-10-12 20:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076626
2020-10-12 20:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17903.28125Mb; avail=470671.5078125Mb
2020-10-12 20:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052901
2020-10-12 20:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.132224
2020-10-12 20:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17903.51171875Mb; avail=470671.265625Mb
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17903.51171875Mb; avail=470671.265625Mb
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001239
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17903.51171875Mb; avail=470671.265625Mb
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075966
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17903.55078125Mb; avail=470671.265625Mb
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052859
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130839
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17903.92578125Mb; avail=470670.66015625Mb
2020-10-12 20:37:35 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 9.505 | nll_loss 8.596 | ppl 386.96 | wps 50495.6 | wpb 2129.5 | bsz 102 | num_updates 3120 | best_loss 9.505
2020-10-12 20:37:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:37:39 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 30 @ 3120 updates, score 9.505) (writing took 4.454657125999802 seconds)
2020-10-12 20:37:39 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 20:37:39 | INFO | train | epoch 030 | loss 9.212 | nll_loss 8.32 | ppl 319.59 | wps 15072.3 | ups 2.83 | wpb 5327.4 | bsz 249.4 | num_updates 3120 | lr 0.000156022 | gnorm 1.429 | clip 0 | train_wall 29 | wall 1193
2020-10-12 20:37:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 20:37:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 20:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17929.2421875Mb; avail=470644.3046875Mb
2020-10-12 20:37:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000702
2020-10-12 20:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005241
2020-10-12 20:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17929.2421875Mb; avail=470644.3046875Mb
2020-10-12 20:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000199
2020-10-12 20:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17929.2421875Mb; avail=470644.3046875Mb
2020-10-12 20:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089493
2020-10-12 20:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095696
2020-10-12 20:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17923.671875Mb; avail=470649.59765625Mb
2020-10-12 20:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17923.765625Mb; avail=470649.71875Mb
2020-10-12 20:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003693
2020-10-12 20:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17923.765625Mb; avail=470649.71875Mb
2020-10-12 20:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 20:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17923.765625Mb; avail=470649.71875Mb
2020-10-12 20:37:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089984
2020-10-12 20:37:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094584
2020-10-12 20:37:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17923.71875Mb; avail=470649.35546875Mb
2020-10-12 20:37:40 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 20:38:02 | INFO | train_inner | epoch 031:     80 / 104 loss=9.096, nll_loss=8.186, ppl=291.28, wps=14928.4, ups=2.82, wpb=5296.9, bsz=250.1, num_updates=3200, lr=0.00016002, gnorm=1.389, clip=0, train_wall=28, wall=1216
2020-10-12 20:38:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18510.703125Mb; avail=470063.5546875Mb
2020-10-12 20:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001916
2020-10-12 20:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18510.703125Mb; avail=470063.5546875Mb
2020-10-12 20:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079926
2020-10-12 20:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18510.59765625Mb; avail=470063.46484375Mb
2020-10-12 20:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.058394
2020-10-12 20:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141038
2020-10-12 20:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18510.5Mb; avail=470063.46484375Mb
2020-10-12 20:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18510.578125Mb; avail=470063.5859375Mb
2020-10-12 20:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001747
2020-10-12 20:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18510.578125Mb; avail=470063.5859375Mb
2020-10-12 20:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082972
2020-10-12 20:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18510.58984375Mb; avail=470063.5859375Mb
2020-10-12 20:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063423
2020-10-12 20:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.149098
2020-10-12 20:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18510.58203125Mb; avail=470063.5859375Mb
2020-10-12 20:38:12 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 9.439 | nll_loss 8.51 | ppl 364.45 | wps 49797.1 | wpb 2129.5 | bsz 102 | num_updates 3224 | best_loss 9.439
2020-10-12 20:38:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:38:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 31 @ 3224 updates, score 9.439) (writing took 11.275052908000362 seconds)
2020-10-12 20:38:23 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 20:38:23 | INFO | train | epoch 031 | loss 9.083 | nll_loss 8.17 | ppl 288.1 | wps 12732.8 | ups 2.39 | wpb 5327.4 | bsz 249.4 | num_updates 3224 | lr 0.000161219 | gnorm 1.374 | clip 0 | train_wall 29 | wall 1236
2020-10-12 20:38:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 20:38:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 20:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17807.9609375Mb; avail=470746.7265625Mb
2020-10-12 20:38:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000664
2020-10-12 20:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005217
2020-10-12 20:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17807.7421875Mb; avail=470746.99609375Mb
2020-10-12 20:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000188
2020-10-12 20:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17807.7421875Mb; avail=470746.99609375Mb
2020-10-12 20:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090304
2020-10-12 20:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096445
2020-10-12 20:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17801.703125Mb; avail=470753.2421875Mb
2020-10-12 20:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17801.58203125Mb; avail=470753.36328125Mb
2020-10-12 20:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003683
2020-10-12 20:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17801.58203125Mb; avail=470753.36328125Mb
2020-10-12 20:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 20:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17801.58203125Mb; avail=470753.36328125Mb
2020-10-12 20:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089277
2020-10-12 20:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093857
2020-10-12 20:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.265625Mb; avail=470752.39453125Mb
2020-10-12 20:38:23 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 20:38:44 | INFO | train_inner | epoch 032:     76 / 104 loss=8.989, nll_loss=8.062, ppl=267.24, wps=12493.4, ups=2.37, wpb=5277.1, bsz=245.9, num_updates=3300, lr=0.000165018, gnorm=1.404, clip=0, train_wall=27, wall=1258
2020-10-12 20:38:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:38:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19330.39453125Mb; avail=469225.21484375Mb
2020-10-12 20:38:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001832
2020-10-12 20:38:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19330.39453125Mb; avail=469225.21484375Mb
2020-10-12 20:38:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075666
2020-10-12 20:38:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19330.39453125Mb; avail=469225.21484375Mb
2020-10-12 20:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053276
2020-10-12 20:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.131625
2020-10-12 20:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19330.39453125Mb; avail=469225.21484375Mb
2020-10-12 20:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19329.98828125Mb; avail=469225.3359375Mb
2020-10-12 20:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001221
2020-10-12 20:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19329.96875Mb; avail=469225.21484375Mb
2020-10-12 20:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075245
2020-10-12 20:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19330.43359375Mb; avail=469225.09375Mb
2020-10-12 20:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053261
2020-10-12 20:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130470
2020-10-12 20:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19330.67578125Mb; avail=469224.8515625Mb
2020-10-12 20:38:55 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 9.396 | nll_loss 8.461 | ppl 352.45 | wps 50386.9 | wpb 2129.5 | bsz 102 | num_updates 3328 | best_loss 9.396
2020-10-12 20:38:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:39:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 32 @ 3328 updates, score 9.396) (writing took 13.831066308999652 seconds)
2020-10-12 20:39:09 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 20:39:09 | INFO | train | epoch 032 | loss 8.967 | nll_loss 8.035 | ppl 262.37 | wps 12067.6 | ups 2.27 | wpb 5327.4 | bsz 249.4 | num_updates 3328 | lr 0.000166417 | gnorm 1.426 | clip 0 | train_wall 29 | wall 1282
2020-10-12 20:39:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 20:39:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 20:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18348.18359375Mb; avail=470212.59375Mb
2020-10-12 20:39:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000705
2020-10-12 20:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005208
2020-10-12 20:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18347.8984375Mb; avail=470213.15234375Mb
2020-10-12 20:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000209
2020-10-12 20:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18347.76953125Mb; avail=470213.16015625Mb
2020-10-12 20:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090438
2020-10-12 20:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096652
2020-10-12 20:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18342.28515625Mb; avail=470217.5078125Mb
2020-10-12 20:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18342.33984375Mb; avail=470217.11328125Mb
2020-10-12 20:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004568
2020-10-12 20:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18342.25390625Mb; avail=470216.69921875Mb
2020-10-12 20:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000255
2020-10-12 20:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18342.73046875Mb; avail=470217.078125Mb
2020-10-12 20:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089982
2020-10-12 20:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095606
2020-10-12 20:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18342.74609375Mb; avail=470215.48828125Mb
2020-10-12 20:39:09 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 20:39:30 | INFO | train_inner | epoch 033:     72 / 104 loss=8.889, nll_loss=7.946, ppl=246.59, wps=12056.1, ups=2.21, wpb=5445.9, bsz=263.3, num_updates=3400, lr=0.000170015, gnorm=1.415, clip=0, train_wall=28, wall=1303
2020-10-12 20:39:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17852.9375Mb; avail=470700.98046875Mb
2020-10-12 20:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001792
2020-10-12 20:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.9375Mb; avail=470700.98046875Mb
2020-10-12 20:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074315
2020-10-12 20:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.02734375Mb; avail=470701.22265625Mb
2020-10-12 20:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052225
2020-10-12 20:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129122
2020-10-12 20:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.953125Mb; avail=470701.22265625Mb
2020-10-12 20:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17852.93359375Mb; avail=470701.2421875Mb
2020-10-12 20:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001229
2020-10-12 20:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.93359375Mb; avail=470701.2421875Mb
2020-10-12 20:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074734
2020-10-12 20:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.89453125Mb; avail=470701.48046875Mb
2020-10-12 20:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052405
2020-10-12 20:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129184
2020-10-12 20:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.984375Mb; avail=470701.1171875Mb
2020-10-12 20:39:41 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 9.351 | nll_loss 8.394 | ppl 336.28 | wps 50667.9 | wpb 2129.5 | bsz 102 | num_updates 3432 | best_loss 9.351
2020-10-12 20:39:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:39:52 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 33 @ 3432 updates, score 9.351) (writing took 11.233760938999694 seconds)
2020-10-12 20:39:52 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 20:39:52 | INFO | train | epoch 033 | loss 8.838 | nll_loss 7.886 | ppl 236.58 | wps 12718.1 | ups 2.39 | wpb 5327.4 | bsz 249.4 | num_updates 3432 | lr 0.000171614 | gnorm 1.409 | clip 0 | train_wall 29 | wall 1326
2020-10-12 20:39:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 20:39:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 20:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18934.96875Mb; avail=469620.16796875Mb
2020-10-12 20:39:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000850
2020-10-12 20:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005381
2020-10-12 20:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18934.96875Mb; avail=469620.16796875Mb
2020-10-12 20:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000200
2020-10-12 20:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18934.96875Mb; avail=469620.16796875Mb
2020-10-12 20:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090610
2020-10-12 20:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096922
2020-10-12 20:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18929.63671875Mb; avail=469625.4140625Mb
2020-10-12 20:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18929.64453125Mb; avail=469625.29296875Mb
2020-10-12 20:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003726
2020-10-12 20:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18929.64453125Mb; avail=469625.29296875Mb
2020-10-12 20:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 20:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18929.64453125Mb; avail=469625.29296875Mb
2020-10-12 20:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091775
2020-10-12 20:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096442
2020-10-12 20:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18929.609375Mb; avail=469625.33984375Mb
2020-10-12 20:39:53 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 20:40:12 | INFO | train_inner | epoch 034:     68 / 104 loss=8.745, nll_loss=7.778, ppl=219.54, wps=12545.4, ups=2.38, wpb=5271.4, bsz=243, num_updates=3500, lr=0.000175013, gnorm=1.494, clip=0, train_wall=27, wall=1345
2020-10-12 20:40:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:40:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18438.4921875Mb; avail=470116.0703125Mb
2020-10-12 20:40:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001838
2020-10-12 20:40:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18438.4921875Mb; avail=470116.0703125Mb
2020-10-12 20:40:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075373
2020-10-12 20:40:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18438.5078125Mb; avail=470116.0703125Mb
2020-10-12 20:40:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055078
2020-10-12 20:40:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.133111
2020-10-12 20:40:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18438.62890625Mb; avail=470115.94921875Mb
2020-10-12 20:40:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18438.546875Mb; avail=470115.94921875Mb
2020-10-12 20:40:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001282
2020-10-12 20:40:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18438.578125Mb; avail=470115.828125Mb
2020-10-12 20:40:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.097687
2020-10-12 20:40:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18438.1484375Mb; avail=470116.44140625Mb
2020-10-12 20:40:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054289
2020-10-12 20:40:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.154057
2020-10-12 20:40:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18438.06640625Mb; avail=470116.3203125Mb
2020-10-12 20:40:25 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 9.322 | nll_loss 8.365 | ppl 329.75 | wps 49676.1 | wpb 2129.5 | bsz 102 | num_updates 3536 | best_loss 9.322
2020-10-12 20:40:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:40:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 34 @ 3536 updates, score 9.322) (writing took 4.953711783000472 seconds)
2020-10-12 20:40:30 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 20:40:30 | INFO | train | epoch 034 | loss 8.725 | nll_loss 7.754 | ppl 215.87 | wps 14868 | ups 2.79 | wpb 5327.4 | bsz 249.4 | num_updates 3536 | lr 0.000176812 | gnorm 1.505 | clip 0 | train_wall 29 | wall 1363
2020-10-12 20:40:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 20:40:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 20:40:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19004.8046875Mb; avail=469558.1640625Mb
2020-10-12 20:40:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000787
2020-10-12 20:40:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005308
2020-10-12 20:40:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19003.4453125Mb; avail=469557.8671875Mb
2020-10-12 20:40:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 20:40:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19004.52734375Mb; avail=469558.01953125Mb
2020-10-12 20:40:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090286
2020-10-12 20:40:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096660
2020-10-12 20:40:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19003.41796875Mb; avail=469554.1328125Mb
2020-10-12 20:40:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19003.91015625Mb; avail=469553.6171875Mb
2020-10-12 20:40:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003981
2020-10-12 20:40:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19004.421875Mb; avail=469553.640625Mb
2020-10-12 20:40:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000206
2020-10-12 20:40:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19004.1640625Mb; avail=469553.4140625Mb
2020-10-12 20:40:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090282
2020-10-12 20:40:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095283
2020-10-12 20:40:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18997.6484375Mb; avail=469557.71875Mb
2020-10-12 20:40:30 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 20:40:48 | INFO | train_inner | epoch 035:     64 / 104 loss=8.647, nll_loss=7.664, ppl=202.75, wps=14289.3, ups=2.76, wpb=5183.2, bsz=239.2, num_updates=3600, lr=0.00018001, gnorm=1.524, clip=0, train_wall=28, wall=1381
2020-10-12 20:40:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:40:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17863.65625Mb; avail=470689.80078125Mb
2020-10-12 20:40:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001719
2020-10-12 20:40:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.65625Mb; avail=470689.80078125Mb
2020-10-12 20:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077436
2020-10-12 20:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.625Mb; avail=470690.04296875Mb
2020-10-12 20:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053836
2020-10-12 20:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.133828
2020-10-12 20:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.7734375Mb; avail=470690.1640625Mb
2020-10-12 20:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17863.7734375Mb; avail=470690.1640625Mb
2020-10-12 20:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001295
2020-10-12 20:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.7734375Mb; avail=470690.1640625Mb
2020-10-12 20:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077432
2020-10-12 20:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.8671875Mb; avail=470661.14453125Mb
2020-10-12 20:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053181
2020-10-12 20:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.132703
2020-10-12 20:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17960.07421875Mb; avail=470593.9375Mb
2020-10-12 20:41:02 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 9.249 | nll_loss 8.256 | ppl 305.69 | wps 50163.3 | wpb 2129.5 | bsz 102 | num_updates 3640 | best_loss 9.249
2020-10-12 20:41:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:41:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 35 @ 3640 updates, score 9.249) (writing took 6.698380820000239 seconds)
2020-10-12 20:41:09 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 20:41:09 | INFO | train | epoch 035 | loss 8.602 | nll_loss 7.611 | ppl 195.54 | wps 14150.8 | ups 2.66 | wpb 5327.4 | bsz 249.4 | num_updates 3640 | lr 0.000182009 | gnorm 1.509 | clip 0 | train_wall 29 | wall 1402
2020-10-12 20:41:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 20:41:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17808.16015625Mb; avail=470745.87890625Mb
2020-10-12 20:41:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000737
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005115
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.16015625Mb; avail=470745.87890625Mb
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000196
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.16015625Mb; avail=470745.87890625Mb
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090466
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096520
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.7890625Mb; avail=470751.29296875Mb
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17802.8203125Mb; avail=470751.05078125Mb
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003733
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.8203125Mb; avail=470751.05078125Mb
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.8203125Mb; avail=470751.05078125Mb
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089757
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094413
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.50390625Mb; avail=470751.7109375Mb
2020-10-12 20:41:09 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 20:41:26 | INFO | train_inner | epoch 036:     60 / 104 loss=8.55, nll_loss=7.55, ppl=187.4, wps=14317.3, ups=2.61, wpb=5490.7, bsz=253.1, num_updates=3700, lr=0.000185008, gnorm=1.447, clip=0, train_wall=28, wall=1420
2020-10-12 20:41:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17978.28125Mb; avail=470575.484375Mb
2020-10-12 20:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001691
2020-10-12 20:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17978.28125Mb; avail=470575.484375Mb
2020-10-12 20:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075126
2020-10-12 20:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17978.43359375Mb; avail=470575.2421875Mb
2020-10-12 20:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052566
2020-10-12 20:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130158
2020-10-12 20:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17978.4140625Mb; avail=470575.36328125Mb
2020-10-12 20:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17978.26171875Mb; avail=470575.7265625Mb
2020-10-12 20:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001226
2020-10-12 20:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17978.26171875Mb; avail=470575.7265625Mb
2020-10-12 20:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075342
2020-10-12 20:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17978.24609375Mb; avail=470575.7265625Mb
2020-10-12 20:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053078
2020-10-12 20:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130393
2020-10-12 20:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17978.28125Mb; avail=470575.484375Mb
2020-10-12 20:41:42 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 9.224 | nll_loss 8.222 | ppl 298.64 | wps 50274.8 | wpb 2129.5 | bsz 102 | num_updates 3744 | best_loss 9.224
2020-10-12 20:41:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:41:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 36 @ 3744 updates, score 9.224) (writing took 15.373032856999998 seconds)
2020-10-12 20:41:57 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 20:41:57 | INFO | train | epoch 036 | loss 8.468 | nll_loss 7.455 | ppl 175.48 | wps 11488.2 | ups 2.16 | wpb 5327.4 | bsz 249.4 | num_updates 3744 | lr 0.000187206 | gnorm 1.474 | clip 0 | train_wall 29 | wall 1450
2020-10-12 20:41:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 20:41:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 20:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19135.90234375Mb; avail=469419.04296875Mb
2020-10-12 20:41:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000648
2020-10-12 20:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004973
2020-10-12 20:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19135.90234375Mb; avail=469419.04296875Mb
2020-10-12 20:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19135.90234375Mb; avail=469419.04296875Mb
2020-10-12 20:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089795
2020-10-12 20:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095690
2020-10-12 20:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18576.73828125Mb; avail=469979.77734375Mb
2020-10-12 20:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18576.671875Mb; avail=469983.31640625Mb
2020-10-12 20:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003799
2020-10-12 20:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18576.1796875Mb; avail=469984.546875Mb
2020-10-12 20:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 20:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18575.6875Mb; avail=469984.0546875Mb
2020-10-12 20:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091816
2020-10-12 20:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096508
2020-10-12 20:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18576.06640625Mb; avail=469985.6796875Mb
2020-10-12 20:41:57 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 20:42:13 | INFO | train_inner | epoch 037:     56 / 104 loss=8.404, nll_loss=7.381, ppl=166.66, wps=11299.6, ups=2.13, wpb=5304.5, bsz=245.5, num_updates=3800, lr=0.000190005, gnorm=1.506, clip=0, train_wall=28, wall=1467
2020-10-12 20:42:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17989.8515625Mb; avail=470564.47265625Mb
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001713
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17989.8515625Mb; avail=470564.47265625Mb
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075345
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17989.9609375Mb; avail=470564.47265625Mb
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053105
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130997
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17989.9609375Mb; avail=470564.46875Mb
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17989.875Mb; avail=470564.34765625Mb
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001319
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17989.875Mb; avail=470564.34765625Mb
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075281
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17990.22265625Mb; avail=470563.98828125Mb
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052045
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129405
2020-10-12 20:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17990.203125Mb; avail=470563.98828125Mb
2020-10-12 20:42:29 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 9.207 | nll_loss 8.203 | ppl 294.61 | wps 50044.9 | wpb 2129.5 | bsz 102 | num_updates 3848 | best_loss 9.207
2020-10-12 20:42:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:42:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 37 @ 3848 updates, score 9.207) (writing took 8.571780545000365 seconds)
2020-10-12 20:42:38 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 20:42:38 | INFO | train | epoch 037 | loss 8.348 | nll_loss 7.314 | ppl 159.15 | wps 13550.1 | ups 2.54 | wpb 5327.4 | bsz 249.4 | num_updates 3848 | lr 0.000192404 | gnorm 1.494 | clip 0 | train_wall 29 | wall 1491
2020-10-12 20:42:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 20:42:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 20:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18469.234375Mb; avail=470086.1640625Mb
2020-10-12 20:42:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000806
2020-10-12 20:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005364
2020-10-12 20:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18469.234375Mb; avail=470086.1640625Mb
2020-10-12 20:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000209
2020-10-12 20:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18469.234375Mb; avail=470086.1640625Mb
2020-10-12 20:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091326
2020-10-12 20:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097662
2020-10-12 20:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18463.29296875Mb; avail=470091.8984375Mb
2020-10-12 20:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18463.29296875Mb; avail=470091.8984375Mb
2020-10-12 20:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003940
2020-10-12 20:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18463.29296875Mb; avail=470091.8984375Mb
2020-10-12 20:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000227
2020-10-12 20:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18463.29296875Mb; avail=470091.8984375Mb
2020-10-12 20:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091675
2020-10-12 20:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096592
2020-10-12 20:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18463.38671875Mb; avail=470091.53515625Mb
2020-10-12 20:42:38 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 20:42:53 | INFO | train_inner | epoch 038:     52 / 104 loss=8.257, nll_loss=7.209, ppl=147.92, wps=13468.9, ups=2.51, wpb=5359.3, bsz=257.2, num_updates=3900, lr=0.000195003, gnorm=1.485, clip=0, train_wall=28, wall=1506
2020-10-12 20:43:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18651.96484375Mb; avail=469902.23046875Mb
2020-10-12 20:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001973
2020-10-12 20:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18651.96484375Mb; avail=469902.23046875Mb
2020-10-12 20:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076111
2020-10-12 20:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18651.93359375Mb; avail=469902.44140625Mb
2020-10-12 20:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052489
2020-10-12 20:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.131374
2020-10-12 20:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18651.91015625Mb; avail=469902.5625Mb
2020-10-12 20:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18651.921875Mb; avail=469902.44140625Mb
2020-10-12 20:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001168
2020-10-12 20:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18651.921875Mb; avail=469902.44140625Mb
2020-10-12 20:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077041
2020-10-12 20:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18651.95703125Mb; avail=469902.3203125Mb
2020-10-12 20:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051643
2020-10-12 20:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130597
2020-10-12 20:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18651.95703125Mb; avail=469902.3203125Mb
2020-10-12 20:43:10 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 9.176 | nll_loss 8.167 | ppl 287.38 | wps 49815.7 | wpb 2129.5 | bsz 102 | num_updates 3952 | best_loss 9.176
2020-10-12 20:43:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:43:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 38 @ 3952 updates, score 9.176) (writing took 9.231428892999247 seconds)
2020-10-12 20:43:20 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 20:43:20 | INFO | train | epoch 038 | loss 8.221 | nll_loss 7.166 | ppl 143.6 | wps 13280.6 | ups 2.49 | wpb 5327.4 | bsz 249.4 | num_updates 3952 | lr 0.000197601 | gnorm 1.528 | clip 0 | train_wall 29 | wall 1533
2020-10-12 20:43:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 20:43:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 20:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19030.875Mb; avail=469524.5390625Mb
2020-10-12 20:43:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000840
2020-10-12 20:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005679
2020-10-12 20:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19030.875Mb; avail=469524.5390625Mb
2020-10-12 20:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000208
2020-10-12 20:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19030.875Mb; avail=469524.5390625Mb
2020-10-12 20:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090396
2020-10-12 20:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097143
2020-10-12 20:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19024.66796875Mb; avail=469530.78125Mb
2020-10-12 20:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19024.546875Mb; avail=469530.90234375Mb
2020-10-12 20:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003719
2020-10-12 20:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19024.4921875Mb; avail=469530.78125Mb
2020-10-12 20:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 20:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19024.5390625Mb; avail=469530.78125Mb
2020-10-12 20:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091154
2020-10-12 20:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095907
2020-10-12 20:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19024.546875Mb; avail=469530.90234375Mb
2020-10-12 20:43:20 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 20:43:33 | INFO | train_inner | epoch 039:     48 / 104 loss=8.16, nll_loss=7.095, ppl=136.68, wps=13164.9, ups=2.47, wpb=5323.7, bsz=247.8, num_updates=4000, lr=0.0002, gnorm=1.518, clip=0, train_wall=28, wall=1547
2020-10-12 20:43:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15493.46875Mb; avail=473073.8359375Mb
2020-10-12 20:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001996
2020-10-12 20:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15494.07421875Mb; avail=473073.23046875Mb
2020-10-12 20:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073692
2020-10-12 20:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15494.08203125Mb; avail=473072.625Mb
2020-10-12 20:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054968
2020-10-12 20:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.131454
2020-10-12 20:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15493.890625Mb; avail=473073.109375Mb
2020-10-12 20:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15494.09375Mb; avail=473072.74609375Mb
2020-10-12 20:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001159
2020-10-12 20:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15494.09375Mb; avail=473072.74609375Mb
2020-10-12 20:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078055
2020-10-12 20:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15494.23046875Mb; avail=473072.625Mb
2020-10-12 20:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053023
2020-10-12 20:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.133022
2020-10-12 20:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15494.23046875Mb; avail=473072.625Mb
2020-10-12 20:43:52 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 9.132 | nll_loss 8.097 | ppl 273.73 | wps 50367.6 | wpb 2129.5 | bsz 102 | num_updates 4056 | best_loss 9.132
2020-10-12 20:43:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:43:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 39 @ 4056 updates, score 9.132) (writing took 4.4973818239996035 seconds)
2020-10-12 20:43:57 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 20:43:57 | INFO | train | epoch 039 | loss 8.084 | nll_loss 7.007 | ppl 128.58 | wps 14969 | ups 2.81 | wpb 5327.4 | bsz 249.4 | num_updates 4056 | lr 0.000198615 | gnorm 1.499 | clip 0 | train_wall 29 | wall 1570
2020-10-12 20:43:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 20:43:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15611.28515625Mb; avail=472954.62109375Mb
2020-10-12 20:43:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000865
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005623
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15618.14453125Mb; avail=472948.32421875Mb
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000215
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15618.75Mb; avail=472947.71875Mb
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092415
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099072
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15748.08984375Mb; avail=472818.56640625Mb
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15797.6015625Mb; avail=472768.91796875Mb
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004778
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15803.65625Mb; avail=472762.86328125Mb
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15804.26171875Mb; avail=472762.2578125Mb
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089084
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094772
2020-10-12 20:43:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15908.2578125Mb; avail=472658.23828125Mb
2020-10-12 20:43:57 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 20:44:09 | INFO | train_inner | epoch 040:     44 / 104 loss=8.034, nll_loss=6.948, ppl=123.47, wps=14947.7, ups=2.78, wpb=5382.2, bsz=245.9, num_updates=4100, lr=0.000197546, gnorm=1.486, clip=0, train_wall=28, wall=1583
2020-10-12 20:44:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15537.34375Mb; avail=473029.4453125Mb
2020-10-12 20:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001943
2020-10-12 20:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15537.34375Mb; avail=473029.4453125Mb
2020-10-12 20:44:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075852
2020-10-12 20:44:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15537.4609375Mb; avail=473029.32421875Mb
2020-10-12 20:44:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053692
2020-10-12 20:44:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.132327
2020-10-12 20:44:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15537.4609375Mb; avail=473029.32421875Mb
2020-10-12 20:44:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15538.06640625Mb; avail=473028.71875Mb
2020-10-12 20:44:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001182
2020-10-12 20:44:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15538.06640625Mb; avail=473028.71875Mb
2020-10-12 20:44:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075728
2020-10-12 20:44:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15537.9375Mb; avail=473028.71875Mb
2020-10-12 20:44:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053074
2020-10-12 20:44:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130767
2020-10-12 20:44:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15538.03515625Mb; avail=473028.83984375Mb
2020-10-12 20:44:29 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 9.114 | nll_loss 8.064 | ppl 267.64 | wps 50227.5 | wpb 2129.5 | bsz 102 | num_updates 4160 | best_loss 9.114
2020-10-12 20:44:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:44:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/O2M/checkpoint_best.pt (epoch 40 @ 4160 updates, score 9.114) (writing took 4.453758374000245 seconds)
2020-10-12 20:44:33 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 20:44:33 | INFO | train | epoch 040 | loss 7.947 | nll_loss 6.847 | ppl 115.11 | wps 15036.1 | ups 2.82 | wpb 5327.4 | bsz 249.4 | num_updates 4160 | lr 0.000196116 | gnorm 1.512 | clip 0 | train_wall 29 | wall 1607
2020-10-12 20:44:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 20:44:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 20:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15521.66015625Mb; avail=473045.03515625Mb
2020-10-12 20:44:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000974
2020-10-12 20:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005674
2020-10-12 20:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15521.16796875Mb; avail=473045.52734375Mb
2020-10-12 20:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 20:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15521.16796875Mb; avail=473045.52734375Mb
2020-10-12 20:44:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089184
2020-10-12 20:44:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095778
2020-10-12 20:44:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15514.95703125Mb; avail=473051.7265625Mb
2020-10-12 20:44:34 | INFO | fairseq_cli.train | done training in 1606.8 seconds
