2020-10-12 19:48:24 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_azejpn_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='aze-eng,jpn-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 19:48:24 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 19:48:24 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['aze', 'eng', 'jpn']
2020-10-12 19:48:25 | INFO | fairseq.data.multilingual.multilingual_data_manager | [aze] dictionary: 22163 types
2020-10-12 19:48:25 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 22163 types
2020-10-12 19:48:25 | INFO | fairseq.data.multilingual.multilingual_data_manager | [jpn] dictionary: 22163 types
2020-10-12 19:48:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 19:48:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10906.8203125Mb; avail=477902.99609375Mb
2020-10-12 19:48:25 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 19:48:25 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:aze-eng': 1, 'main:jpn-eng': 1}
2020-10-12 19:48:25 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:25 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azejpn_sepspm8000/M2O/valid.aze-eng.aze
2020-10-12 19:48:25 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azejpn_sepspm8000/M2O/valid.aze-eng.eng
2020-10-12 19:48:25 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azejpn_sepspm8000/M2O/ valid aze-eng 671 examples
2020-10-12 19:48:25 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:jpn-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:25 | INFO | fairseq.data.data_utils | loaded 4429 examples from: fairseq/data-bin/ted_azejpn_sepspm8000/M2O/valid.jpn-eng.jpn
2020-10-12 19:48:25 | INFO | fairseq.data.data_utils | loaded 4429 examples from: fairseq/data-bin/ted_azejpn_sepspm8000/M2O/valid.jpn-eng.eng
2020-10-12 19:48:25 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azejpn_sepspm8000/M2O/ valid jpn-eng 4429 examples
2020-10-12 19:48:25 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22163, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22163, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=22163, bias=False)
  )
)
2020-10-12 19:48:25 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 19:48:25 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 19:48:25 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 19:48:25 | INFO | fairseq_cli.train | num. model params: 42890752 (num. trained: 42890752)
2020-10-12 19:48:29 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 19:48:29 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 19:48:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 19:48:29 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 19:48:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 19:48:29 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 19:48:29 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 19:48:29 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_last.pt
2020-10-12 19:48:29 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 19:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 19:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13071.515625Mb; avail=475728.23046875Mb
2020-10-12 19:48:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 19:48:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:aze-eng': 1, 'main:jpn-eng': 1}
2020-10-12 19:48:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:29 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azejpn_sepspm8000/M2O/train.aze-eng.aze
2020-10-12 19:48:29 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azejpn_sepspm8000/M2O/train.aze-eng.eng
2020-10-12 19:48:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azejpn_sepspm8000/M2O/ train aze-eng 5946 examples
2020-10-12 19:48:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:jpn-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:29 | INFO | fairseq.data.data_utils | loaded 19994 examples from: fairseq/data-bin/ted_azejpn_sepspm8000/M2O/train.jpn-eng.jpn
2020-10-12 19:48:29 | INFO | fairseq.data.data_utils | loaded 19994 examples from: fairseq/data-bin/ted_azejpn_sepspm8000/M2O/train.jpn-eng.eng
2020-10-12 19:48:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azejpn_sepspm8000/M2O/ train jpn-eng 19994 examples
2020-10-12 19:48:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:aze-eng', 5946), ('main:jpn-eng', 19994)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 19:48:29 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 19:48:29 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 25940
2020-10-12 19:48:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 25940; virtual dataset size 25940
2020-10-12 19:48:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 19:48:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:aze-eng': 5946, 'main:jpn-eng': 19994}; raw total size: 25940
2020-10-12 19:48:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:aze-eng': 5946, 'main:jpn-eng': 19994}; resampled total size: 25940
2020-10-12 19:48:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 19:48:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.004776
2020-10-12 19:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13072.65625Mb; avail=475727.29296875Mb
2020-10-12 19:48:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000545
2020-10-12 19:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005344
2020-10-12 19:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13072.07421875Mb; avail=475727.78515625Mb
2020-10-12 19:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000188
2020-10-12 19:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13072.07421875Mb; avail=475727.78515625Mb
2020-10-12 19:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097604
2020-10-12 19:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103899
2020-10-12 19:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13072.2890625Mb; avail=475727.6796875Mb
2020-10-12 19:48:29 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 19:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13073.05078125Mb; avail=475726.72265625Mb
2020-10-12 19:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003650
2020-10-12 19:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13073.05078125Mb; avail=475726.72265625Mb
2020-10-12 19:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-12 19:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13073.05078125Mb; avail=475726.72265625Mb
2020-10-12 19:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094811
2020-10-12 19:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099386
2020-10-12 19:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13073.609375Mb; avail=475726.171875Mb
2020-10-12 19:48:29 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 19:48:59 | INFO | train_inner | epoch 001:    100 / 112 loss=14.098, nll_loss=13.98, ppl=16161.9, wps=21482, ups=3.48, wpb=6197.3, bsz=232, num_updates=100, lr=5.0975e-06, gnorm=4.298, clip=0, train_wall=28, wall=30
2020-10-12 19:49:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:49:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17996.14453125Mb; avail=470725.5546875Mb
2020-10-12 19:49:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001923
2020-10-12 19:49:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17996.14453125Mb; avail=470725.5546875Mb
2020-10-12 19:49:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074690
2020-10-12 19:49:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17996.109375Mb; avail=470725.67578125Mb
2020-10-12 19:49:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.058722
2020-10-12 19:49:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136164
2020-10-12 19:49:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17996.16796875Mb; avail=470725.43359375Mb
2020-10-12 19:49:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17996.85546875Mb; avail=470724.94921875Mb
2020-10-12 19:49:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001205
2020-10-12 19:49:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17996.86328125Mb; avail=470724.828125Mb
2020-10-12 19:49:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073301
2020-10-12 19:49:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17996.9140625Mb; avail=470724.70703125Mb
2020-10-12 19:49:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059133
2020-10-12 19:49:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.134393
2020-10-12 19:49:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17996.6953125Mb; avail=470725.0703125Mb
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2020-10-12 19:49:05 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.5 | nll_loss 12.191 | ppl 4677.07 | wps 56909.6 | wpb 2236.6 | bsz 85 | num_updates 112
2020-10-12 19:49:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:49:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 1 @ 112 updates, score 12.5) (writing took 1.5778143440002168 seconds)
2020-10-12 19:49:07 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 19:49:07 | INFO | train | epoch 001 | loss 13.977 | nll_loss 13.845 | ppl 14718.8 | wps 19071.8 | ups 3.06 | wpb 6246.1 | bsz 231.6 | num_updates 112 | lr 5.6972e-06 | gnorm 4.101 | clip 0 | train_wall 32 | wall 38
2020-10-12 19:49:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 19:49:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 19:49:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17903.76953125Mb; avail=470801.63671875Mb
2020-10-12 19:49:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000851
2020-10-12 19:49:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005781
2020-10-12 19:49:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17903.78125Mb; avail=470801.515625Mb
2020-10-12 19:49:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000198
2020-10-12 19:49:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17903.78125Mb; avail=470801.515625Mb
2020-10-12 19:49:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099000
2020-10-12 19:49:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105822
2020-10-12 19:49:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17898.296875Mb; avail=470806.62890625Mb
2020-10-12 19:49:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17898.26953125Mb; avail=470806.7578125Mb
2020-10-12 19:49:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003732
2020-10-12 19:49:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17898.26953125Mb; avail=470806.7578125Mb
2020-10-12 19:49:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 19:49:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17898.26953125Mb; avail=470806.7578125Mb
2020-10-12 19:49:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096745
2020-10-12 19:49:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101402
2020-10-12 19:49:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17898.35546875Mb; avail=470807.12109375Mb
2020-10-12 19:49:07 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 19:49:33 | INFO | train_inner | epoch 002:     88 / 112 loss=12.488, nll_loss=12.186, ppl=4659.97, wps=18888.3, ups=2.97, wpb=6354.3, bsz=238.4, num_updates=200, lr=1.0095e-05, gnorm=2.08, clip=0, train_wall=28, wall=63
2020-10-12 19:49:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17750.15625Mb; avail=470924.6484375Mb
2020-10-12 19:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001814
2020-10-12 19:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17750.15625Mb; avail=470924.6484375Mb
2020-10-12 19:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073112
2020-10-12 19:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17750.15234375Mb; avail=470924.04296875Mb
2020-10-12 19:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060504
2020-10-12 19:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136261
2020-10-12 19:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17750.19921875Mb; avail=470923.99609375Mb
2020-10-12 19:49:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17750.19921875Mb; avail=470923.99609375Mb
2020-10-12 19:49:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001346
2020-10-12 19:49:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17750.19921875Mb; avail=470923.99609375Mb
2020-10-12 19:49:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075040
2020-10-12 19:49:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17750.515625Mb; avail=470923.64453125Mb
2020-10-12 19:49:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060479
2020-10-12 19:49:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137670
2020-10-12 19:49:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17750.64453125Mb; avail=470923.515625Mb
2020-10-12 19:49:42 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.641 | nll_loss 11.236 | ppl 2411.67 | wps 56634.9 | wpb 2236.6 | bsz 85 | num_updates 224 | best_loss 11.641
2020-10-12 19:49:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:49:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 2 @ 224 updates, score 11.641) (writing took 15.864671164000356 seconds)
2020-10-12 19:49:58 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 19:49:58 | INFO | train | epoch 002 | loss 12.329 | nll_loss 12.009 | ppl 4120.21 | wps 13628.4 | ups 2.18 | wpb 6246.1 | bsz 231.6 | num_updates 224 | lr 1.12944e-05 | gnorm 1.973 | clip 0 | train_wall 31 | wall 89
2020-10-12 19:49:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 19:49:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 19:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18488.375Mb; avail=470186.015625Mb
2020-10-12 19:49:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001042
2020-10-12 19:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006203
2020-10-12 19:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18488.390625Mb; avail=470186.0Mb
2020-10-12 19:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000249
2020-10-12 19:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18488.390625Mb; avail=470186.0Mb
2020-10-12 19:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109906
2020-10-12 19:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.117206
2020-10-12 19:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18482.19921875Mb; avail=470192.0Mb
2020-10-12 19:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18482.16796875Mb; avail=470192.2265625Mb
2020-10-12 19:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004028
2020-10-12 19:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18482.16796875Mb; avail=470192.2265625Mb
2020-10-12 19:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000233
2020-10-12 19:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18482.16796875Mb; avail=470192.2265625Mb
2020-10-12 19:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.110291
2020-10-12 19:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115690
2020-10-12 19:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18482.609375Mb; avail=470191.78515625Mb
2020-10-12 19:49:58 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 19:50:20 | INFO | train_inner | epoch 003:     76 / 112 loss=11.675, nll_loss=11.279, ppl=2485.42, wps=13286.2, ups=2.09, wpb=6351, bsz=230.2, num_updates=300, lr=1.50925e-05, gnorm=1.705, clip=0, train_wall=28, wall=111
2020-10-12 19:50:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:50:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17904.9296875Mb; avail=470767.83984375Mb
2020-10-12 19:50:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001761
2020-10-12 19:50:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17904.9296875Mb; avail=470767.83984375Mb
2020-10-12 19:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072754
2020-10-12 19:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17906.1015625Mb; avail=470767.28515625Mb
2020-10-12 19:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059540
2020-10-12 19:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.134878
2020-10-12 19:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17906.640625Mb; avail=470766.91015625Mb
2020-10-12 19:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17906.671875Mb; avail=470766.7890625Mb
2020-10-12 19:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001304
2020-10-12 19:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17906.671875Mb; avail=470766.91015625Mb
2020-10-12 19:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073745
2020-10-12 19:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17906.73046875Mb; avail=470766.7890625Mb
2020-10-12 19:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059316
2020-10-12 19:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.135121
2020-10-12 19:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17906.8515625Mb; avail=470766.1875Mb
2020-10-12 19:50:33 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.525 | nll_loss 9.965 | ppl 999.48 | wps 56356.5 | wpb 2236.6 | bsz 85 | num_updates 336 | best_loss 10.525
2020-10-12 19:50:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:50:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 3 @ 336 updates, score 10.525) (writing took 8.98620462999952 seconds)
2020-10-12 19:50:42 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 19:50:42 | INFO | train | epoch 003 | loss 11.398 | nll_loss 10.969 | ppl 2004.26 | wps 15816.4 | ups 2.53 | wpb 6246.1 | bsz 231.6 | num_updates 336 | lr 1.68916e-05 | gnorm 1.692 | clip 0 | train_wall 31 | wall 133
2020-10-12 19:50:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 19:50:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 19:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18400.6796875Mb; avail=470273.70703125Mb
2020-10-12 19:50:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000864
2020-10-12 19:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005335
2020-10-12 19:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18400.6796875Mb; avail=470273.70703125Mb
2020-10-12 19:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000197
2020-10-12 19:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18400.6796875Mb; avail=470273.70703125Mb
2020-10-12 19:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096318
2020-10-12 19:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102682
2020-10-12 19:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18395.07421875Mb; avail=470279.3359375Mb
2020-10-12 19:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18395.11328125Mb; avail=470279.09375Mb
2020-10-12 19:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003700
2020-10-12 19:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18395.11328125Mb; avail=470279.09375Mb
2020-10-12 19:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000197
2020-10-12 19:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18395.11328125Mb; avail=470279.09375Mb
2020-10-12 19:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096950
2020-10-12 19:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101670
2020-10-12 19:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18395.0546875Mb; avail=470279.578125Mb
2020-10-12 19:50:42 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 19:51:01 | INFO | train_inner | epoch 004:     64 / 112 loss=10.578, nll_loss=10.036, ppl=1050.07, wps=15024.5, ups=2.45, wpb=6138.1, bsz=232.3, num_updates=400, lr=2.009e-05, gnorm=1.675, clip=0, train_wall=28, wall=152
2020-10-12 19:51:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:51:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17818.5546875Mb; avail=470854.78515625Mb
2020-10-12 19:51:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001754
2020-10-12 19:51:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.5546875Mb; avail=470854.78515625Mb
2020-10-12 19:51:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074884
2020-10-12 19:51:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.484375Mb; avail=470854.6640625Mb
2020-10-12 19:51:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059627
2020-10-12 19:51:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137078
2020-10-12 19:51:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.55859375Mb; avail=470854.54296875Mb
2020-10-12 19:51:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17818.69140625Mb; avail=470854.78515625Mb
2020-10-12 19:51:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001243
2020-10-12 19:51:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.69140625Mb; avail=470854.78515625Mb
2020-10-12 19:51:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074884
2020-10-12 19:51:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.69140625Mb; avail=470854.78515625Mb
2020-10-12 19:51:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061483
2020-10-12 19:51:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138627
2020-10-12 19:51:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.60546875Mb; avail=470854.6640625Mb
2020-10-12 19:51:18 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.494 | nll_loss 8.72 | ppl 421.61 | wps 55217.1 | wpb 2236.6 | bsz 85 | num_updates 448 | best_loss 9.494
2020-10-12 19:51:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:51:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 4 @ 448 updates, score 9.494) (writing took 4.47969981299957 seconds)
2020-10-12 19:51:23 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 19:51:23 | INFO | train | epoch 004 | loss 10.162 | nll_loss 9.55 | ppl 749.43 | wps 17319.3 | ups 2.77 | wpb 6246.1 | bsz 231.6 | num_updates 448 | lr 2.24888e-05 | gnorm 1.583 | clip 0 | train_wall 32 | wall 173
2020-10-12 19:51:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 19:51:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 19:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17785.57421875Mb; avail=470887.51953125Mb
2020-10-12 19:51:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000783
2020-10-12 19:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005884
2020-10-12 19:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17785.57421875Mb; avail=470887.51953125Mb
2020-10-12 19:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000283
2020-10-12 19:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17785.59765625Mb; avail=470887.3984375Mb
2020-10-12 19:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098174
2020-10-12 19:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105113
2020-10-12 19:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17779.9375Mb; avail=470893.4296875Mb
2020-10-12 19:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17779.3671875Mb; avail=470893.921875Mb
2020-10-12 19:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003662
2020-10-12 19:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17779.3671875Mb; avail=470893.80078125Mb
2020-10-12 19:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 19:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17779.296875Mb; avail=470893.80078125Mb
2020-10-12 19:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098804
2020-10-12 19:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103639
2020-10-12 19:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17779.25390625Mb; avail=470893.4375Mb
2020-10-12 19:51:23 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 19:51:38 | INFO | train_inner | epoch 005:     52 / 112 loss=9.707, nll_loss=9.005, ppl=513.93, wps=16844.2, ups=2.7, wpb=6240.3, bsz=229.2, num_updates=500, lr=2.50875e-05, gnorm=1.315, clip=0, train_wall=28, wall=189
2020-10-12 19:51:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17765.06640625Mb; avail=470908.18359375Mb
2020-10-12 19:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002002
2020-10-12 19:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17765.671875Mb; avail=470907.578125Mb
2020-10-12 19:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074872
2020-10-12 19:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17765.05859375Mb; avail=470907.70703125Mb
2020-10-12 19:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059728
2020-10-12 19:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137433
2020-10-12 19:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17765.125Mb; avail=470907.70703125Mb
2020-10-12 19:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17765.125Mb; avail=470907.70703125Mb
2020-10-12 19:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001140
2020-10-12 19:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17765.125Mb; avail=470907.70703125Mb
2020-10-12 19:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073977
2020-10-12 19:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17765.15625Mb; avail=470907.46484375Mb
2020-10-12 19:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059856
2020-10-12 19:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.135734
2020-10-12 19:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17765.15625Mb; avail=470907.46484375Mb
2020-10-12 19:51:59 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.108 | nll_loss 8.226 | ppl 299.36 | wps 56643.4 | wpb 2236.6 | bsz 85 | num_updates 560 | best_loss 9.108
2020-10-12 19:51:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:52:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 5 @ 560 updates, score 9.108) (writing took 4.4603381970000555 seconds)
2020-10-12 19:52:03 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 19:52:03 | INFO | train | epoch 005 | loss 9.451 | nll_loss 8.693 | ppl 413.79 | wps 17296.6 | ups 2.77 | wpb 6246.1 | bsz 231.6 | num_updates 560 | lr 2.8086e-05 | gnorm 1.286 | clip 0 | train_wall 32 | wall 214
2020-10-12 19:52:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 19:52:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17769.4140625Mb; avail=470903.765625Mb
2020-10-12 19:52:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000953
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005653
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17769.4140625Mb; avail=470903.765625Mb
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000214
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17769.4140625Mb; avail=470903.765625Mb
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099069
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105743
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17763.6640625Mb; avail=470909.62890625Mb
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17763.5Mb; avail=470909.87109375Mb
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003737
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17763.5Mb; avail=470909.87109375Mb
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000211
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17763.5Mb; avail=470909.87109375Mb
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095597
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100310
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17763.48828125Mb; avail=470909.87890625Mb
2020-10-12 19:52:03 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 19:52:15 | INFO | train_inner | epoch 006:     40 / 112 loss=9.311, nll_loss=8.515, ppl=365.76, wps=16570, ups=2.72, wpb=6086.1, bsz=217.2, num_updates=600, lr=3.0085e-05, gnorm=1.299, clip=0, train_wall=28, wall=226
2020-10-12 19:52:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:52:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17773.16796875Mb; avail=470899.88671875Mb
2020-10-12 19:52:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001983
2020-10-12 19:52:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17773.16796875Mb; avail=470899.88671875Mb
2020-10-12 19:52:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075764
2020-10-12 19:52:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17773.96875Mb; avail=470899.28125Mb
2020-10-12 19:52:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059203
2020-10-12 19:52:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137830
2020-10-12 19:52:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17774.01171875Mb; avail=470899.0390625Mb
2020-10-12 19:52:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17773.98046875Mb; avail=470899.0390625Mb
2020-10-12 19:52:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001248
2020-10-12 19:52:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17773.98046875Mb; avail=470899.0390625Mb
2020-10-12 19:52:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072600
2020-10-12 19:52:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17774.109375Mb; avail=470898.91796875Mb
2020-10-12 19:52:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.058493
2020-10-12 19:52:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.133144
2020-10-12 19:52:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17774.19140625Mb; avail=470898.67578125Mb
2020-10-12 19:52:39 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.939 | nll_loss 8.011 | ppl 257.88 | wps 56156.3 | wpb 2236.6 | bsz 85 | num_updates 672 | best_loss 8.939
2020-10-12 19:52:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:52:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 6 @ 672 updates, score 8.939) (writing took 4.4654077569994115 seconds)
2020-10-12 19:52:43 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 19:52:43 | INFO | train | epoch 006 | loss 9.19 | nll_loss 8.354 | ppl 327.24 | wps 17303.6 | ups 2.77 | wpb 6246.1 | bsz 231.6 | num_updates 672 | lr 3.36832e-05 | gnorm 1.365 | clip 0 | train_wall 32 | wall 254
2020-10-12 19:52:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 19:52:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 19:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18524.21875Mb; avail=470156.23046875Mb
2020-10-12 19:52:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001104
2020-10-12 19:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005987
2020-10-12 19:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18524.25390625Mb; avail=470155.62890625Mb
2020-10-12 19:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000209
2020-10-12 19:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18524.125Mb; avail=470155.515625Mb
2020-10-12 19:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098628
2020-10-12 19:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105693
2020-10-12 19:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18518.65234375Mb; avail=470160.87890625Mb
2020-10-12 19:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18517.82421875Mb; avail=470160.421875Mb
2020-10-12 19:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004000
2020-10-12 19:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18518.21875Mb; avail=470160.51171875Mb
2020-10-12 19:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-12 19:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18518.08984375Mb; avail=470160.28515625Mb
2020-10-12 19:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098322
2020-10-12 19:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103433
2020-10-12 19:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18517.953125Mb; avail=470157.98828125Mb
2020-10-12 19:52:44 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 19:52:52 | INFO | train_inner | epoch 007:     28 / 112 loss=9.131, nll_loss=8.279, ppl=310.58, wps=17198.6, ups=2.7, wpb=6380.3, bsz=252.5, num_updates=700, lr=3.50825e-05, gnorm=1.454, clip=0, train_wall=28, wall=263
2020-10-12 19:53:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17982.8515625Mb; avail=470690.671875Mb
2020-10-12 19:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002931
2020-10-12 19:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17983.3671875Mb; avail=470690.06640625Mb
2020-10-12 19:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076771
2020-10-12 19:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17983.34375Mb; avail=470690.19140625Mb
2020-10-12 19:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.065416
2020-10-12 19:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.146179
2020-10-12 19:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17983.45703125Mb; avail=470690.078125Mb
2020-10-12 19:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17983.71484375Mb; avail=470689.8359375Mb
2020-10-12 19:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001250
2020-10-12 19:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17983.71484375Mb; avail=470689.8359375Mb
2020-10-12 19:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083770
2020-10-12 19:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17983.51171875Mb; avail=470690.19921875Mb
2020-10-12 19:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.065962
2020-10-12 19:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.151814
2020-10-12 19:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17983.6171875Mb; avail=470690.078125Mb
2020-10-12 19:53:19 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.8 | nll_loss 7.832 | ppl 227.85 | wps 55797.6 | wpb 2236.6 | bsz 85 | num_updates 784 | best_loss 8.8
2020-10-12 19:53:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:53:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 7 @ 784 updates, score 8.8) (writing took 7.010490005000065 seconds)
2020-10-12 19:53:26 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 19:53:26 | INFO | train | epoch 007 | loss 9.034 | nll_loss 8.16 | ppl 286 | wps 16355.6 | ups 2.62 | wpb 6246.1 | bsz 231.6 | num_updates 784 | lr 3.92804e-05 | gnorm 1.349 | clip 0 | train_wall 32 | wall 297
2020-10-12 19:53:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 19:53:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 19:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17788.6953125Mb; avail=470884.19921875Mb
2020-10-12 19:53:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000924
2020-10-12 19:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005700
2020-10-12 19:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17788.57421875Mb; avail=470884.3203125Mb
2020-10-12 19:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 19:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17788.57421875Mb; avail=470884.3203125Mb
2020-10-12 19:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097092
2020-10-12 19:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103798
2020-10-12 19:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17781.61328125Mb; avail=470890.76953125Mb
2020-10-12 19:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17782.19921875Mb; avail=470890.890625Mb
2020-10-12 19:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003860
2020-10-12 19:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17782.19921875Mb; avail=470890.890625Mb
2020-10-12 19:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 19:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17782.19921875Mb; avail=470890.890625Mb
2020-10-12 19:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097801
2020-10-12 19:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102621
2020-10-12 19:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17782.08984375Mb; avail=470890.890625Mb
2020-10-12 19:53:26 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 19:53:31 | INFO | train_inner | epoch 008:     16 / 112 loss=9.027, nll_loss=8.15, ppl=284.11, wps=15933.7, ups=2.55, wpb=6256.6, bsz=218.2, num_updates=800, lr=4.008e-05, gnorm=1.37, clip=0, train_wall=28, wall=302
2020-10-12 19:53:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17838.7109375Mb; avail=470833.390625Mb
2020-10-12 19:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001887
2020-10-12 19:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.31640625Mb; avail=470832.78515625Mb
2020-10-12 19:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074208
2020-10-12 19:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.53515625Mb; avail=470832.6328125Mb
2020-10-12 19:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059538
2020-10-12 19:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136460
2020-10-12 19:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.92578125Mb; avail=470832.4140625Mb
2020-10-12 19:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17839.7734375Mb; avail=470832.05078125Mb
2020-10-12 19:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001252
2020-10-12 19:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.7734375Mb; avail=470832.05078125Mb
2020-10-12 19:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073074
2020-10-12 19:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17840.15625Mb; avail=470831.80859375Mb
2020-10-12 19:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059840
2020-10-12 19:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.134933
2020-10-12 19:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17840.234375Mb; avail=470831.9375Mb
2020-10-12 19:54:02 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.597 | nll_loss 7.598 | ppl 193.73 | wps 54393.1 | wpb 2236.6 | bsz 85 | num_updates 896 | best_loss 8.597
2020-10-12 19:54:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:54:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 8 @ 896 updates, score 8.597) (writing took 12.214400591999947 seconds)
2020-10-12 19:54:15 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 19:54:15 | INFO | train | epoch 008 | loss 8.865 | nll_loss 7.964 | ppl 249.62 | wps 14490.2 | ups 2.32 | wpb 6246.1 | bsz 231.6 | num_updates 896 | lr 4.48776e-05 | gnorm 1.388 | clip 0 | train_wall 32 | wall 345
2020-10-12 19:54:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 19:54:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 19:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18351.3671875Mb; avail=470321.55859375Mb
2020-10-12 19:54:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001164
2020-10-12 19:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006606
2020-10-12 19:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18351.3671875Mb; avail=470321.55859375Mb
2020-10-12 19:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000256
2020-10-12 19:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18351.3671875Mb; avail=470321.55859375Mb
2020-10-12 19:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.110024
2020-10-12 19:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.117765
2020-10-12 19:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18346.0078125Mb; avail=470326.8125Mb
2020-10-12 19:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18346.1796875Mb; avail=470326.5546875Mb
2020-10-12 19:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004206
2020-10-12 19:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18346.1796875Mb; avail=470326.5546875Mb
2020-10-12 19:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 19:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18346.1796875Mb; avail=470326.5546875Mb
2020-10-12 19:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.108527
2020-10-12 19:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113763
2020-10-12 19:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18346.21484375Mb; avail=470327.0Mb
2020-10-12 19:54:15 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 19:54:16 | INFO | train_inner | epoch 009:      4 / 112 loss=8.856, nll_loss=7.953, ppl=247.83, wps=13947.5, ups=2.23, wpb=6248.1, bsz=236.7, num_updates=900, lr=4.50775e-05, gnorm=1.373, clip=0, train_wall=28, wall=347
2020-10-12 19:54:45 | INFO | train_inner | epoch 009:    104 / 112 loss=8.648, nll_loss=7.715, ppl=210.13, wps=21195.7, ups=3.44, wpb=6158.4, bsz=227.6, num_updates=1000, lr=5.0075e-05, gnorm=1.404, clip=0, train_wall=28, wall=376
2020-10-12 19:54:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17793.42578125Mb; avail=470879.6875Mb
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001959
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17793.42578125Mb; avail=470879.6875Mb
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073079
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17793.64453125Mb; avail=470879.5703125Mb
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059894
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.135807
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17793.53515625Mb; avail=470879.5703125Mb
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17793.4140625Mb; avail=470879.69140625Mb
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001355
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17793.4140625Mb; avail=470879.69140625Mb
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074678
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17793.734375Mb; avail=470879.5703125Mb
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060793
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137633
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17793.71875Mb; avail=470879.5703125Mb
2020-10-12 19:54:50 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.427 | nll_loss 7.418 | ppl 170.99 | wps 56349.7 | wpb 2236.6 | bsz 85 | num_updates 1008 | best_loss 8.427
2020-10-12 19:54:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:54:59 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 9 @ 1008 updates, score 8.427) (writing took 8.76828650600055 seconds)
2020-10-12 19:54:59 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 19:54:59 | INFO | train | epoch 009 | loss 8.695 | nll_loss 7.768 | ppl 218 | wps 15691.2 | ups 2.51 | wpb 6246.1 | bsz 231.6 | num_updates 1008 | lr 5.04748e-05 | gnorm 1.399 | clip 0 | train_wall 32 | wall 390
2020-10-12 19:54:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 19:54:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 19:54:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17814.375Mb; avail=470858.328125Mb
2020-10-12 19:54:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000886
2020-10-12 19:54:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006138
2020-10-12 19:54:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17814.359375Mb; avail=470858.69140625Mb
2020-10-12 19:54:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000213
2020-10-12 19:54:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17814.359375Mb; avail=470858.69140625Mb
2020-10-12 19:54:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098505
2020-10-12 19:54:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105808
2020-10-12 19:54:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.4453125Mb; avail=470864.2421875Mb
2020-10-12 19:54:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17808.37109375Mb; avail=470864.60546875Mb
2020-10-12 19:54:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003718
2020-10-12 19:54:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.28125Mb; avail=470864.60546875Mb
2020-10-12 19:54:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 19:54:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.28125Mb; avail=470864.60546875Mb
2020-10-12 19:54:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096080
2020-10-12 19:54:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100726
2020-10-12 19:54:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.63671875Mb; avail=470864.2421875Mb
2020-10-12 19:54:59 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 19:55:26 | INFO | train_inner | epoch 010:     92 / 112 loss=8.575, nll_loss=7.63, ppl=198.04, wps=15560.3, ups=2.44, wpb=6374.1, bsz=242.8, num_updates=1100, lr=5.50725e-05, gnorm=1.538, clip=0, train_wall=28, wall=417
2020-10-12 19:55:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17837.95703125Mb; avail=470834.796875Mb
2020-10-12 19:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001804
2020-10-12 19:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.95703125Mb; avail=470834.796875Mb
2020-10-12 19:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074338
2020-10-12 19:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.15625Mb; avail=470835.40234375Mb
2020-10-12 19:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062784
2020-10-12 19:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139845
2020-10-12 19:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.15625Mb; avail=470835.40234375Mb
2020-10-12 19:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17838.0859375Mb; avail=470835.28125Mb
2020-10-12 19:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001207
2020-10-12 19:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.0859375Mb; avail=470835.28125Mb
2020-10-12 19:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073342
2020-10-12 19:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.15625Mb; avail=470835.40234375Mb
2020-10-12 19:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061655
2020-10-12 19:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137023
2020-10-12 19:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.1875Mb; avail=470835.265625Mb
2020-10-12 19:55:35 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.285 | nll_loss 7.259 | ppl 153.18 | wps 55182.9 | wpb 2236.6 | bsz 85 | num_updates 1120 | best_loss 8.285
2020-10-12 19:55:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:55:39 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 10 @ 1120 updates, score 8.285) (writing took 4.467117601000609 seconds)
2020-10-12 19:55:39 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 19:55:39 | INFO | train | epoch 010 | loss 8.565 | nll_loss 7.619 | ppl 196.61 | wps 17467.2 | ups 2.8 | wpb 6246.1 | bsz 231.6 | num_updates 1120 | lr 5.6072e-05 | gnorm 1.512 | clip 0 | train_wall 31 | wall 430
2020-10-12 19:55:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 19:55:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 19:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17804.38671875Mb; avail=470868.5390625Mb
2020-10-12 19:55:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000786
2020-10-12 19:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005931
2020-10-12 19:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17804.3984375Mb; avail=470868.5390625Mb
2020-10-12 19:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-12 19:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17804.3984375Mb; avail=470868.5390625Mb
2020-10-12 19:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099066
2020-10-12 19:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106071
2020-10-12 19:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.5078125Mb; avail=470874.37109375Mb
2020-10-12 19:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17798.4296875Mb; avail=470874.7421875Mb
2020-10-12 19:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003641
2020-10-12 19:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.4296875Mb; avail=470874.7421875Mb
2020-10-12 19:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 19:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.4296875Mb; avail=470874.7421875Mb
2020-10-12 19:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095906
2020-10-12 19:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100477
2020-10-12 19:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.0546875Mb; avail=470875.03515625Mb
2020-10-12 19:55:39 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 19:56:03 | INFO | train_inner | epoch 011:     80 / 112 loss=8.471, nll_loss=7.512, ppl=182.53, wps=16757.3, ups=2.69, wpb=6232.4, bsz=223.7, num_updates=1200, lr=6.007e-05, gnorm=1.338, clip=0, train_wall=29, wall=454
2020-10-12 19:56:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:56:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17892.36328125Mb; avail=470780.49609375Mb
2020-10-12 19:56:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002967
2020-10-12 19:56:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.45703125Mb; avail=470780.49609375Mb
2020-10-12 19:56:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073009
2020-10-12 19:56:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.21484375Mb; avail=470780.73828125Mb
2020-10-12 19:56:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060294
2020-10-12 19:56:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137086
2020-10-12 19:56:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.16796875Mb; avail=470780.98046875Mb
2020-10-12 19:56:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17892.16796875Mb; avail=470780.98046875Mb
2020-10-12 19:56:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001261
2020-10-12 19:56:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.7421875Mb; avail=470780.40625Mb
2020-10-12 19:56:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072592
2020-10-12 19:56:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.80078125Mb; avail=470780.04296875Mb
2020-10-12 19:56:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059127
2020-10-12 19:56:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.133778
2020-10-12 19:56:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.71875Mb; avail=470780.40625Mb
2020-10-12 19:56:15 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.197 | nll_loss 7.165 | ppl 143.51 | wps 56321.3 | wpb 2236.6 | bsz 85 | num_updates 1232 | best_loss 8.197
2020-10-12 19:56:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:56:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 11 @ 1232 updates, score 8.197) (writing took 4.466805543000191 seconds)
2020-10-12 19:56:20 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 19:56:20 | INFO | train | epoch 011 | loss 8.425 | nll_loss 7.461 | ppl 176.19 | wps 17213.7 | ups 2.76 | wpb 6246.1 | bsz 231.6 | num_updates 1232 | lr 6.16692e-05 | gnorm 1.363 | clip 0 | train_wall 32 | wall 471
2020-10-12 19:56:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 19:56:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 19:56:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17854.0234375Mb; avail=470818.671875Mb
2020-10-12 19:56:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001014
2020-10-12 19:56:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005869
2020-10-12 19:56:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.04296875Mb; avail=470818.55078125Mb
2020-10-12 19:56:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000246
2020-10-12 19:56:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.04296875Mb; avail=470818.55078125Mb
2020-10-12 19:56:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098592
2020-10-12 19:56:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105649
2020-10-12 19:56:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17848.57421875Mb; avail=470824.3828125Mb
2020-10-12 19:56:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17848.60546875Mb; avail=470824.26171875Mb
2020-10-12 19:56:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004715
2020-10-12 19:56:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17848.63671875Mb; avail=470824.140625Mb
2020-10-12 19:56:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000213
2020-10-12 19:56:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17848.63671875Mb; avail=470824.140625Mb
2020-10-12 19:56:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095999
2020-10-12 19:56:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101757
2020-10-12 19:56:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17848.59765625Mb; avail=470824.3828125Mb
2020-10-12 19:56:20 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 19:56:40 | INFO | train_inner | epoch 012:     68 / 112 loss=8.324, nll_loss=7.346, ppl=162.67, wps=16557.5, ups=2.73, wpb=6065.4, bsz=222.7, num_updates=1300, lr=6.50675e-05, gnorm=1.327, clip=0, train_wall=28, wall=491
2020-10-12 19:56:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:56:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17877.66796875Mb; avail=470795.6171875Mb
2020-10-12 19:56:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001735
2020-10-12 19:56:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.66796875Mb; avail=470795.6171875Mb
2020-10-12 19:56:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073636
2020-10-12 19:56:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.47265625Mb; avail=470795.375Mb
2020-10-12 19:56:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059535
2020-10-12 19:56:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.135725
2020-10-12 19:56:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.3984375Mb; avail=470795.49609375Mb
2020-10-12 19:56:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17877.3984375Mb; avail=470795.49609375Mb
2020-10-12 19:56:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001368
2020-10-12 19:56:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17878.01953125Mb; avail=470794.76953125Mb
2020-10-12 19:56:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074081
2020-10-12 19:56:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.9140625Mb; avail=470794.76953125Mb
2020-10-12 19:56:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060639
2020-10-12 19:56:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136856
2020-10-12 19:56:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.83203125Mb; avail=470795.1015625Mb
2020-10-12 19:56:56 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.102 | nll_loss 7.055 | ppl 132.96 | wps 56193.8 | wpb 2236.6 | bsz 85 | num_updates 1344 | best_loss 8.102
2020-10-12 19:56:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:57:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 12 @ 1344 updates, score 8.102) (writing took 4.506247147000067 seconds)
2020-10-12 19:57:00 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 19:57:00 | INFO | train | epoch 012 | loss 8.302 | nll_loss 7.321 | ppl 159.85 | wps 17305.6 | ups 2.77 | wpb 6246.1 | bsz 231.6 | num_updates 1344 | lr 6.72664e-05 | gnorm 1.331 | clip 0 | train_wall 32 | wall 511
2020-10-12 19:57:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 19:57:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 19:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17860.16796875Mb; avail=470812.4296875Mb
2020-10-12 19:57:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000700
2020-10-12 19:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005773
2020-10-12 19:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.16796875Mb; avail=470812.4296875Mb
2020-10-12 19:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 19:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.16796875Mb; avail=470812.4296875Mb
2020-10-12 19:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099740
2020-10-12 19:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106617
2020-10-12 19:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.7265625Mb; avail=470818.86328125Mb
2020-10-12 19:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17853.8046875Mb; avail=470819.10546875Mb
2020-10-12 19:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003646
2020-10-12 19:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.828125Mb; avail=470818.984375Mb
2020-10-12 19:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 19:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.828125Mb; avail=470818.984375Mb
2020-10-12 19:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098201
2020-10-12 19:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103195
2020-10-12 19:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.02734375Mb; avail=470819.1171875Mb
2020-10-12 19:57:00 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 19:57:17 | INFO | train_inner | epoch 013:     56 / 112 loss=8.314, nll_loss=7.334, ppl=161.3, wps=16817.3, ups=2.68, wpb=6273.2, bsz=238.2, num_updates=1400, lr=7.0065e-05, gnorm=1.391, clip=0, train_wall=29, wall=528
2020-10-12 19:57:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:57:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18223.8671875Mb; avail=470448.765625Mb
2020-10-12 19:57:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002481
2020-10-12 19:57:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18223.8828125Mb; avail=470448.64453125Mb
2020-10-12 19:57:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073457
2020-10-12 19:57:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18223.953125Mb; avail=470448.765625Mb
2020-10-12 19:57:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.058463
2020-10-12 19:57:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.135518
2020-10-12 19:57:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18224.0625Mb; avail=470448.64453125Mb
2020-10-12 19:57:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18224.375Mb; avail=470448.71875Mb
2020-10-12 19:57:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001199
2020-10-12 19:57:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18224.375Mb; avail=470448.71875Mb
2020-10-12 19:57:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073393
2020-10-12 19:57:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17825.2734375Mb; avail=470847.765625Mb
2020-10-12 19:57:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059143
2020-10-12 19:57:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.134562
2020-10-12 19:57:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17819.1015625Mb; avail=470854.08984375Mb
2020-10-12 19:57:36 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.985 | nll_loss 6.91 | ppl 120.29 | wps 56236.1 | wpb 2236.6 | bsz 85 | num_updates 1456 | best_loss 7.985
2020-10-12 19:57:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:57:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 13 @ 1456 updates, score 7.985) (writing took 4.48817532799967 seconds)
2020-10-12 19:57:41 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 19:57:41 | INFO | train | epoch 013 | loss 8.195 | nll_loss 7.197 | ppl 146.77 | wps 17156.9 | ups 2.75 | wpb 6246.1 | bsz 231.6 | num_updates 1456 | lr 7.28636e-05 | gnorm 1.379 | clip 0 | train_wall 32 | wall 552
2020-10-12 19:57:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 19:57:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 19:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17849.12890625Mb; avail=470823.73828125Mb
2020-10-12 19:57:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000972
2020-10-12 19:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006003
2020-10-12 19:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.109375Mb; avail=470823.98046875Mb
2020-10-12 19:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000229
2020-10-12 19:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17848.6171875Mb; avail=470824.47265625Mb
2020-10-12 19:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096002
2020-10-12 19:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103004
2020-10-12 19:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17843.25Mb; avail=470830.015625Mb
2020-10-12 19:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17842.7578125Mb; avail=470830.5078125Mb
2020-10-12 19:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003689
2020-10-12 19:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.7578125Mb; avail=470830.5078125Mb
2020-10-12 19:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 19:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.7578125Mb; avail=470830.5078125Mb
2020-10-12 19:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097403
2020-10-12 19:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102023
2020-10-12 19:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.87109375Mb; avail=470830.38671875Mb
2020-10-12 19:57:41 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 19:57:54 | INFO | train_inner | epoch 014:     44 / 112 loss=8.087, nll_loss=7.074, ppl=134.74, wps=17208.9, ups=2.72, wpb=6321.2, bsz=232.7, num_updates=1500, lr=7.50625e-05, gnorm=1.356, clip=0, train_wall=28, wall=565
2020-10-12 19:58:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:58:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17860.078125Mb; avail=470815.23828125Mb
2020-10-12 19:58:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002141
2020-10-12 19:58:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.17578125Mb; avail=470815.125Mb
2020-10-12 19:58:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073189
2020-10-12 19:58:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17859.765625Mb; avail=470822.359375Mb
2020-10-12 19:58:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060745
2020-10-12 19:58:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136939
2020-10-12 19:58:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17859.8046875Mb; avail=470822.12890625Mb
2020-10-12 19:58:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17859.6875Mb; avail=470821.8984375Mb
2020-10-12 19:58:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001162
2020-10-12 19:58:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17859.4296875Mb; avail=470821.671875Mb
2020-10-12 19:58:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073273
2020-10-12 19:58:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.94140625Mb; avail=470821.17578125Mb
2020-10-12 19:58:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059663
2020-10-12 19:58:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.134871
2020-10-12 19:58:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17859.40625Mb; avail=470820.94140625Mb
2020-10-12 19:58:17 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.897 | nll_loss 6.813 | ppl 112.4 | wps 56365.9 | wpb 2236.6 | bsz 85 | num_updates 1568 | best_loss 7.897
2020-10-12 19:58:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:58:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 14 @ 1568 updates, score 7.897) (writing took 4.466299980000258 seconds)
2020-10-12 19:58:21 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 19:58:21 | INFO | train | epoch 014 | loss 8.098 | nll_loss 7.087 | ppl 135.92 | wps 17465 | ups 2.8 | wpb 6246.1 | bsz 231.6 | num_updates 1568 | lr 7.84608e-05 | gnorm 1.333 | clip 0 | train_wall 31 | wall 592
2020-10-12 19:58:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 19:58:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 19:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17845.046875Mb; avail=470827.89453125Mb
2020-10-12 19:58:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000992
2020-10-12 19:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006383
2020-10-12 19:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17845.046875Mb; avail=470827.89453125Mb
2020-10-12 19:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 19:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17845.046875Mb; avail=470827.89453125Mb
2020-10-12 19:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099245
2020-10-12 19:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106690
2020-10-12 19:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.07421875Mb; avail=470833.8671875Mb
2020-10-12 19:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17838.36328125Mb; avail=470834.359375Mb
2020-10-12 19:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003692
2020-10-12 19:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.36328125Mb; avail=470834.359375Mb
2020-10-12 19:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 19:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.36328125Mb; avail=470834.359375Mb
2020-10-12 19:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098436
2020-10-12 19:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103331
2020-10-12 19:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.7265625Mb; avail=470834.23828125Mb
2020-10-12 19:58:21 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 19:58:31 | INFO | train_inner | epoch 015:     32 / 112 loss=8.11, nll_loss=7.1, ppl=137.19, wps=17072.6, ups=2.72, wpb=6278.1, bsz=234, num_updates=1600, lr=8.006e-05, gnorm=1.303, clip=0, train_wall=28, wall=602
2020-10-12 19:58:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:58:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17834.37109375Mb; avail=470838.11328125Mb
2020-10-12 19:58:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002028
2020-10-12 19:58:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.37109375Mb; avail=470838.11328125Mb
2020-10-12 19:58:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074712
2020-10-12 19:58:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.62890625Mb; avail=470837.87109375Mb
2020-10-12 19:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060818
2020-10-12 19:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138379
2020-10-12 19:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.62890625Mb; avail=470837.87109375Mb
2020-10-12 19:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17834.6640625Mb; avail=470837.75Mb
2020-10-12 19:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001167
2020-10-12 19:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.6640625Mb; avail=470837.75Mb
2020-10-12 19:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074978
2020-10-12 19:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.37109375Mb; avail=470838.11328125Mb
2020-10-12 19:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059899
2020-10-12 19:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136818
2020-10-12 19:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.37109375Mb; avail=470838.11328125Mb
2020-10-12 19:58:57 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.803 | nll_loss 6.698 | ppl 103.81 | wps 56009.7 | wpb 2236.6 | bsz 85 | num_updates 1680 | best_loss 7.803
2020-10-12 19:58:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:59:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 15 @ 1680 updates, score 7.803) (writing took 4.465958523000154 seconds)
2020-10-12 19:59:02 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 19:59:02 | INFO | train | epoch 015 | loss 8.003 | nll_loss 6.977 | ppl 125.99 | wps 17238.3 | ups 2.76 | wpb 6246.1 | bsz 231.6 | num_updates 1680 | lr 8.4058e-05 | gnorm 1.37 | clip 0 | train_wall 32 | wall 633
2020-10-12 19:59:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 19:59:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 19:59:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17823.59765625Mb; avail=470848.98046875Mb
2020-10-12 19:59:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000696
2020-10-12 19:59:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005488
2020-10-12 19:59:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.59765625Mb; avail=470848.98046875Mb
2020-10-12 19:59:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000199
2020-10-12 19:59:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.59765625Mb; avail=470848.98046875Mb
2020-10-12 19:59:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098259
2020-10-12 19:59:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104782
2020-10-12 19:59:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.79296875Mb; avail=470854.7578125Mb
2020-10-12 19:59:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17817.8984375Mb; avail=470854.7578125Mb
2020-10-12 19:59:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003772
2020-10-12 19:59:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.8984375Mb; avail=470854.7578125Mb
2020-10-12 19:59:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 19:59:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.8984375Mb; avail=470854.7578125Mb
2020-10-12 19:59:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095854
2020-10-12 19:59:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100566
2020-10-12 19:59:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.9140625Mb; avail=470854.63671875Mb
2020-10-12 19:59:02 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 19:59:08 | INFO | train_inner | epoch 016:     20 / 112 loss=7.961, nll_loss=6.929, ppl=121.86, wps=16753.9, ups=2.7, wpb=6200.1, bsz=235.2, num_updates=1700, lr=8.50575e-05, gnorm=1.374, clip=0, train_wall=28, wall=639
2020-10-12 19:59:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17787.29296875Mb; avail=470885.2578125Mb
2020-10-12 19:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002066
2020-10-12 19:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17787.79296875Mb; avail=470884.7578125Mb
2020-10-12 19:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074742
2020-10-12 19:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17788.015625Mb; avail=470884.6484375Mb
2020-10-12 19:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059801
2020-10-12 19:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137476
2020-10-12 19:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17788.08984375Mb; avail=470884.40625Mb
2020-10-12 19:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17788.078125Mb; avail=470884.40625Mb
2020-10-12 19:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001263
2020-10-12 19:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17788.078125Mb; avail=470884.40625Mb
2020-10-12 19:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073575
2020-10-12 19:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17788.0625Mb; avail=470884.52734375Mb
2020-10-12 19:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060109
2020-10-12 19:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.135756
2020-10-12 19:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.71875Mb; avail=470872.578125Mb
2020-10-12 19:59:37 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.765 | nll_loss 6.668 | ppl 101.69 | wps 56101 | wpb 2236.6 | bsz 85 | num_updates 1792 | best_loss 7.765
2020-10-12 19:59:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:59:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 16 @ 1792 updates, score 7.765) (writing took 8.796796394000012 seconds)
2020-10-12 19:59:46 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 19:59:46 | INFO | train | epoch 016 | loss 7.906 | nll_loss 6.867 | ppl 116.71 | wps 15682.5 | ups 2.51 | wpb 6246.1 | bsz 231.6 | num_updates 1792 | lr 8.96552e-05 | gnorm 1.248 | clip 0 | train_wall 32 | wall 677
2020-10-12 19:59:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 19:59:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 19:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18185.1171875Mb; avail=470487.9453125Mb
2020-10-12 19:59:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000738
2020-10-12 19:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005493
2020-10-12 19:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18191.171875Mb; avail=470481.28515625Mb
2020-10-12 19:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000206
2020-10-12 19:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18192.3828125Mb; avail=470480.6796875Mb
2020-10-12 19:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.118878
2020-10-12 19:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.125452
2020-10-12 19:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18325.3203125Mb; avail=470347.1953125Mb
2020-10-12 19:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18348.28125Mb; avail=470325.2734375Mb
2020-10-12 19:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003790
2020-10-12 19:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18348.7734375Mb; avail=470325.765625Mb
2020-10-12 19:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 19:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18348.28125Mb; avail=470325.765625Mb
2020-10-12 19:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096181
2020-10-12 19:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101066
2020-10-12 19:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18348.953125Mb; avail=470331.1640625Mb
2020-10-12 19:59:47 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 19:59:49 | INFO | train_inner | epoch 017:      8 / 112 loss=7.91, nll_loss=6.87, ppl=117, wps=15168.8, ups=2.43, wpb=6247.4, bsz=223.6, num_updates=1800, lr=9.0055e-05, gnorm=1.264, clip=0, train_wall=28, wall=680
2020-10-12 20:00:18 | INFO | train_inner | epoch 017:    108 / 112 loss=7.808, nll_loss=6.755, ppl=107.99, wps=21358.3, ups=3.43, wpb=6235.3, bsz=233.4, num_updates=1900, lr=9.50525e-05, gnorm=1.226, clip=0, train_wall=28, wall=709
2020-10-12 20:00:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:00:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17810.9140625Mb; avail=470861.875Mb
2020-10-12 20:00:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002066
2020-10-12 20:00:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.51953125Mb; avail=470861.26953125Mb
2020-10-12 20:00:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073168
2020-10-12 20:00:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.609375Mb; avail=470861.14453125Mb
2020-10-12 20:00:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061685
2020-10-12 20:00:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137772
2020-10-12 20:00:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.77734375Mb; avail=470861.14453125Mb
2020-10-12 20:00:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17811.77734375Mb; avail=470861.14453125Mb
2020-10-12 20:00:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001149
2020-10-12 20:00:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.77734375Mb; avail=470861.14453125Mb
2020-10-12 20:00:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072766
2020-10-12 20:00:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.8828125Mb; avail=470861.265625Mb
2020-10-12 20:00:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059896
2020-10-12 20:00:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.134676
2020-10-12 20:00:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.9765625Mb; avail=470860.87109375Mb
2020-10-12 20:00:22 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.695 | nll_loss 6.572 | ppl 95.13 | wps 56050.5 | wpb 2236.6 | bsz 85 | num_updates 1904 | best_loss 7.695
2020-10-12 20:00:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:00:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 17 @ 1904 updates, score 7.695) (writing took 4.4757450689994585 seconds)
2020-10-12 20:00:27 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 20:00:27 | INFO | train | epoch 017 | loss 7.817 | nll_loss 6.765 | ppl 108.76 | wps 17345.2 | ups 2.78 | wpb 6246.1 | bsz 231.6 | num_updates 1904 | lr 9.52524e-05 | gnorm 1.245 | clip 0 | train_wall 32 | wall 717
2020-10-12 20:00:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 20:00:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 20:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18212.89453125Mb; avail=470460.6015625Mb
2020-10-12 20:00:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000822
2020-10-12 20:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006299
2020-10-12 20:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18219.5546875Mb; avail=470453.94140625Mb
2020-10-12 20:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-12 20:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18219.5546875Mb; avail=470453.3359375Mb
2020-10-12 20:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097768
2020-10-12 20:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105148
2020-10-12 20:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18329.26171875Mb; avail=470344.0625Mb
2020-10-12 20:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18349.734375Mb; avail=470324.08203125Mb
2020-10-12 20:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003710
2020-10-12 20:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18349.79296875Mb; avail=470325.0703125Mb
2020-10-12 20:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 20:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18349.79296875Mb; avail=470324.578125Mb
2020-10-12 20:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097420
2020-10-12 20:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102170
2020-10-12 20:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18350.609375Mb; avail=470330.609375Mb
2020-10-12 20:00:27 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 20:00:55 | INFO | train_inner | epoch 018:     96 / 112 loss=7.743, nll_loss=6.679, ppl=102.49, wps=16970, ups=2.72, wpb=6229.4, bsz=222.7, num_updates=2000, lr=0.00010005, gnorm=1.341, clip=0, train_wall=28, wall=746
2020-10-12 20:01:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17860.08984375Mb; avail=470813.609375Mb
2020-10-12 20:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001763
2020-10-12 20:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.08984375Mb; avail=470813.609375Mb
2020-10-12 20:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074659
2020-10-12 20:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.27734375Mb; avail=470813.00390625Mb
2020-10-12 20:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059285
2020-10-12 20:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136549
2020-10-12 20:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.27734375Mb; avail=470813.00390625Mb
2020-10-12 20:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17860.27734375Mb; avail=470813.00390625Mb
2020-10-12 20:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001226
2020-10-12 20:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.27734375Mb; avail=470813.00390625Mb
2020-10-12 20:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074821
2020-10-12 20:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.4296875Mb; avail=470812.88671875Mb
2020-10-12 20:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059184
2020-10-12 20:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136031
2020-10-12 20:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.4296875Mb; avail=470812.88671875Mb
2020-10-12 20:01:02 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.616 | nll_loss 6.474 | ppl 88.88 | wps 55650.5 | wpb 2236.6 | bsz 85 | num_updates 2016 | best_loss 7.616
2020-10-12 20:01:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:01:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 18 @ 2016 updates, score 7.616) (writing took 4.478378441000132 seconds)
2020-10-12 20:01:07 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 20:01:07 | INFO | train | epoch 018 | loss 7.748 | nll_loss 6.685 | ppl 102.88 | wps 17326.8 | ups 2.77 | wpb 6246.1 | bsz 231.6 | num_updates 2016 | lr 0.00010085 | gnorm 1.304 | clip 0 | train_wall 32 | wall 758
2020-10-12 20:01:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 20:01:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 20:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18383.39453125Mb; avail=470296.76953125Mb
2020-10-12 20:01:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000702
2020-10-12 20:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006405
2020-10-12 20:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18383.79296875Mb; avail=470296.66015625Mb
2020-10-12 20:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000227
2020-10-12 20:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18384.26953125Mb; avail=470297.0390625Mb
2020-10-12 20:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100111
2020-10-12 20:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107755
2020-10-12 20:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18378.75Mb; avail=470300.6328125Mb
2020-10-12 20:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18378.6875Mb; avail=470300.06640625Mb
2020-10-12 20:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003943
2020-10-12 20:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18378.50390625Mb; avail=470300.14453125Mb
2020-10-12 20:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000240
2020-10-12 20:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18378.375Mb; avail=470299.42578125Mb
2020-10-12 20:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098472
2020-10-12 20:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103544
2020-10-12 20:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18378.1640625Mb; avail=470297.703125Mb
2020-10-12 20:01:07 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 20:01:32 | INFO | train_inner | epoch 019:     84 / 112 loss=7.673, nll_loss=6.6, ppl=97.02, wps=17091.9, ups=2.71, wpb=6317.1, bsz=246.8, num_updates=2100, lr=0.000105048, gnorm=1.245, clip=0, train_wall=28, wall=783
2020-10-12 20:01:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18200.4140625Mb; avail=470477.30078125Mb
2020-10-12 20:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001717
2020-10-12 20:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18200.76171875Mb; avail=470477.07421875Mb
2020-10-12 20:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075703
2020-10-12 20:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18200.2421875Mb; avail=470476.359375Mb
2020-10-12 20:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060486
2020-10-12 20:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139104
2020-10-12 20:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18200.15625Mb; avail=470475.53515625Mb
2020-10-12 20:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18200.9140625Mb; avail=470475.203125Mb
2020-10-12 20:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001467
2020-10-12 20:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18200.1640625Mb; avail=470475.22265625Mb
2020-10-12 20:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074209
2020-10-12 20:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18200.44140625Mb; avail=470474.56640625Mb
2020-10-12 20:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061202
2020-10-12 20:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137791
2020-10-12 20:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18200.94921875Mb; avail=470473.16015625Mb
2020-10-12 20:01:43 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.577 | nll_loss 6.437 | ppl 86.67 | wps 55086.8 | wpb 2236.6 | bsz 85 | num_updates 2128 | best_loss 7.577
2020-10-12 20:01:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:01:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 19 @ 2128 updates, score 7.577) (writing took 4.530140629999551 seconds)
2020-10-12 20:01:47 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 20:01:47 | INFO | train | epoch 019 | loss 7.664 | nll_loss 6.588 | ppl 96.22 | wps 17363.5 | ups 2.78 | wpb 6246.1 | bsz 231.6 | num_updates 2128 | lr 0.000106447 | gnorm 1.254 | clip 0 | train_wall 31 | wall 798
2020-10-12 20:01:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 20:01:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 20:01:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17920.34375Mb; avail=470752.41796875Mb
2020-10-12 20:01:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000832
2020-10-12 20:01:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005857
2020-10-12 20:01:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17926.98046875Mb; avail=470745.39453125Mb
2020-10-12 20:01:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000240
2020-10-12 20:01:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17927.5859375Mb; avail=470744.7890625Mb
2020-10-12 20:01:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099040
2020-10-12 20:01:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105966
2020-10-12 20:01:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18047.73828125Mb; avail=470625.390625Mb
2020-10-12 20:01:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18079.22265625Mb; avail=470593.30078125Mb
2020-10-12 20:01:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003756
2020-10-12 20:01:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18084.5625Mb; avail=470588.578125Mb
2020-10-12 20:01:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 20:01:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18085.16796875Mb; avail=470587.97265625Mb
2020-10-12 20:01:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095626
2020-10-12 20:01:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100300
2020-10-12 20:01:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18202.6484375Mb; avail=470470.390625Mb
2020-10-12 20:01:47 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 20:02:08 | INFO | train_inner | epoch 020:     72 / 112 loss=7.636, nll_loss=6.557, ppl=94.13, wps=17014.7, ups=2.75, wpb=6193.8, bsz=234.7, num_updates=2200, lr=0.000110045, gnorm=1.295, clip=0, train_wall=28, wall=819
2020-10-12 20:02:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18174.62109375Mb; avail=470505.19921875Mb
2020-10-12 20:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001854
2020-10-12 20:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18174.62109375Mb; avail=470505.4453125Mb
2020-10-12 20:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074370
2020-10-12 20:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18174.328125Mb; avail=470505.37109375Mb
2020-10-12 20:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060879
2020-10-12 20:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137933
2020-10-12 20:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18174.62109375Mb; avail=470504.39453125Mb
2020-10-12 20:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18175.51953125Mb; avail=470503.671875Mb
2020-10-12 20:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001262
2020-10-12 20:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18175.26171875Mb; avail=470503.4453125Mb
2020-10-12 20:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073471
2020-10-12 20:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18174.47265625Mb; avail=470502.90234375Mb
2020-10-12 20:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061740
2020-10-12 20:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137266
2020-10-12 20:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18174.9765625Mb; avail=470502.62109375Mb
2020-10-12 20:02:23 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.507 | nll_loss 6.345 | ppl 81.29 | wps 53911.9 | wpb 2236.6 | bsz 85 | num_updates 2240 | best_loss 7.507
2020-10-12 20:02:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:02:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 20 @ 2240 updates, score 7.507) (writing took 4.536767579000298 seconds)
2020-10-12 20:02:27 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 20:02:27 | INFO | train | epoch 020 | loss 7.588 | nll_loss 6.502 | ppl 90.61 | wps 17522.9 | ups 2.81 | wpb 6246.1 | bsz 231.6 | num_updates 2240 | lr 0.000112044 | gnorm 1.256 | clip 0 | train_wall 31 | wall 838
2020-10-12 20:02:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 20:02:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 20:02:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17797.984375Mb; avail=470874.41796875Mb
2020-10-12 20:02:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000918
2020-10-12 20:02:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005779
2020-10-12 20:02:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17797.984375Mb; avail=470874.41796875Mb
2020-10-12 20:02:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000237
2020-10-12 20:02:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17797.984375Mb; avail=470874.41796875Mb
2020-10-12 20:02:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098648
2020-10-12 20:02:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105482
2020-10-12 20:02:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17791.85546875Mb; avail=470880.7109375Mb
2020-10-12 20:02:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17791.83984375Mb; avail=470880.83203125Mb
2020-10-12 20:02:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003679
2020-10-12 20:02:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17791.83984375Mb; avail=470880.83203125Mb
2020-10-12 20:02:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 20:02:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17791.81640625Mb; avail=470880.953125Mb
2020-10-12 20:02:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096463
2020-10-12 20:02:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101139
2020-10-12 20:02:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17789.43359375Mb; avail=470883.4140625Mb
2020-10-12 20:02:27 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 20:02:45 | INFO | train_inner | epoch 021:     60 / 112 loss=7.545, nll_loss=6.451, ppl=87.51, wps=17399, ups=2.72, wpb=6399.6, bsz=220.5, num_updates=2300, lr=0.000115043, gnorm=1.174, clip=0, train_wall=28, wall=856
2020-10-12 20:03:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17817.48046875Mb; avail=470862.046875Mb
2020-10-12 20:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002043
2020-10-12 20:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.69921875Mb; avail=470861.1015625Mb
2020-10-12 20:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074028
2020-10-12 20:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.8203125Mb; avail=470860.59375Mb
2020-10-12 20:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061250
2020-10-12 20:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138174
2020-10-12 20:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.140625Mb; avail=470859.78515625Mb
2020-10-12 20:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17817.89453125Mb; avail=470859.0703125Mb
2020-10-12 20:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001151
2020-10-12 20:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.37109375Mb; avail=470859.3359375Mb
2020-10-12 20:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074159
2020-10-12 20:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.79296875Mb; avail=470858.08203125Mb
2020-10-12 20:03:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062023
2020-10-12 20:03:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138135
2020-10-12 20:03:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.40234375Mb; avail=470857.40625Mb
2020-10-12 20:03:03 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.499 | nll_loss 6.344 | ppl 81.23 | wps 53533.1 | wpb 2236.6 | bsz 85 | num_updates 2352 | best_loss 7.499
2020-10-12 20:03:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:03:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 21 @ 2352 updates, score 7.499) (writing took 4.485499341999457 seconds)
2020-10-12 20:03:08 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 20:03:08 | INFO | train | epoch 021 | loss 7.51 | nll_loss 6.412 | ppl 85.18 | wps 17284.9 | ups 2.77 | wpb 6246.1 | bsz 231.6 | num_updates 2352 | lr 0.000117641 | gnorm 1.251 | clip 0 | train_wall 32 | wall 879
2020-10-12 20:03:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 20:03:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 20:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18170.76953125Mb; avail=470502.1640625Mb
2020-10-12 20:03:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000997
2020-10-12 20:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006436
2020-10-12 20:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18178.03515625Mb; avail=470495.50390625Mb
2020-10-12 20:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000230
2020-10-12 20:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18178.640625Mb; avail=470494.8984375Mb
2020-10-12 20:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097872
2020-10-12 20:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105342
2020-10-12 20:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18288.24609375Mb; avail=470385.51953125Mb
2020-10-12 20:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18314.88671875Mb; avail=470358.2734375Mb
2020-10-12 20:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003744
2020-10-12 20:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18319.73046875Mb; avail=470354.03515625Mb
2020-10-12 20:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 20:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18320.3359375Mb; avail=470353.4296875Mb
2020-10-12 20:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096244
2020-10-12 20:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101010
2020-10-12 20:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18372.94921875Mb; avail=470305.85546875Mb
2020-10-12 20:03:08 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 20:03:22 | INFO | train_inner | epoch 022:     48 / 112 loss=7.509, nll_loss=6.411, ppl=85.07, wps=16562.7, ups=2.71, wpb=6109.6, bsz=223.2, num_updates=2400, lr=0.00012004, gnorm=1.258, clip=0, train_wall=28, wall=893
2020-10-12 20:03:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17817.46484375Mb; avail=470860.91796875Mb
2020-10-12 20:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002074
2020-10-12 20:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.078125Mb; avail=470861.0703125Mb
2020-10-12 20:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074652
2020-10-12 20:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.86328125Mb; avail=470860.08984375Mb
2020-10-12 20:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059614
2020-10-12 20:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137180
2020-10-12 20:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.87890625Mb; avail=470859.24609375Mb
2020-10-12 20:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17817.46484375Mb; avail=470859.7421875Mb
2020-10-12 20:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001191
2020-10-12 20:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.3359375Mb; avail=470859.515625Mb
2020-10-12 20:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075866
2020-10-12 20:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.73828125Mb; avail=470858.6640625Mb
2020-10-12 20:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060048
2020-10-12 20:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137904
2020-10-12 20:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.82421875Mb; avail=470858.04296875Mb
2020-10-12 20:03:44 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.436 | nll_loss 6.271 | ppl 77.22 | wps 56600.3 | wpb 2236.6 | bsz 85 | num_updates 2464 | best_loss 7.436
2020-10-12 20:03:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:03:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 22 @ 2464 updates, score 7.436) (writing took 4.460746920999554 seconds)
2020-10-12 20:03:48 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 20:03:48 | INFO | train | epoch 022 | loss 7.433 | nll_loss 6.324 | ppl 80.1 | wps 17331.7 | ups 2.77 | wpb 6246.1 | bsz 231.6 | num_updates 2464 | lr 0.000123238 | gnorm 1.233 | clip 0 | train_wall 32 | wall 919
2020-10-12 20:03:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 20:03:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 20:03:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17921.08984375Mb; avail=470751.34375Mb
2020-10-12 20:03:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001078
2020-10-12 20:03:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005748
2020-10-12 20:03:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17928.3671875Mb; avail=470743.95703125Mb
2020-10-12 20:03:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 20:03:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17928.97265625Mb; avail=470743.3515625Mb
2020-10-12 20:03:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097462
2020-10-12 20:03:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104262
2020-10-12 20:03:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18041.41796875Mb; avail=470631.15625Mb
2020-10-12 20:03:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18072.375Mb; avail=470601.00390625Mb
2020-10-12 20:03:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003794
2020-10-12 20:03:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18077.82421875Mb; avail=470595.5546875Mb
2020-10-12 20:03:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 20:03:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18080.4609375Mb; avail=470592.8984375Mb
2020-10-12 20:03:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095210
2020-10-12 20:03:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101300
2020-10-12 20:03:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18200.2890625Mb; avail=470472.7734375Mb
2020-10-12 20:03:48 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 20:03:59 | INFO | train_inner | epoch 023:     36 / 112 loss=7.367, nll_loss=6.249, ppl=76.05, wps=17076.4, ups=2.71, wpb=6310.3, bsz=244.8, num_updates=2500, lr=0.000125037, gnorm=1.251, clip=0, train_wall=28, wall=930
2020-10-12 20:04:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17836.2734375Mb; avail=470843.59375Mb
2020-10-12 20:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002156
2020-10-12 20:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.88671875Mb; avail=470843.25390625Mb
2020-10-12 20:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074023
2020-10-12 20:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.03125Mb; avail=470851.3125Mb
2020-10-12 20:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060985
2020-10-12 20:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138260
2020-10-12 20:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17894.31640625Mb; avail=470784.37890625Mb
2020-10-12 20:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17913.8671875Mb; avail=470763.93359375Mb
2020-10-12 20:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001322
2020-10-12 20:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17916.16015625Mb; avail=470762.49609375Mb
2020-10-12 20:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073815
2020-10-12 20:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18004.07421875Mb; avail=470674.1953125Mb
2020-10-12 20:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060615
2020-10-12 20:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136795
2020-10-12 20:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18074.66796875Mb; avail=470601.8125Mb
2020-10-12 20:04:24 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.399 | nll_loss 6.237 | ppl 75.43 | wps 56269.2 | wpb 2236.6 | bsz 85 | num_updates 2576 | best_loss 7.399
2020-10-12 20:04:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:04:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 23 @ 2576 updates, score 7.399) (writing took 13.706530659000236 seconds)
2020-10-12 20:04:37 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 20:04:37 | INFO | train | epoch 023 | loss 7.361 | nll_loss 6.24 | ppl 75.59 | wps 14133.7 | ups 2.26 | wpb 6246.1 | bsz 231.6 | num_updates 2576 | lr 0.000128836 | gnorm 1.234 | clip 0 | train_wall 32 | wall 968
2020-10-12 20:04:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 20:04:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 20:04:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18238.89453125Mb; avail=470448.59765625Mb
2020-10-12 20:04:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000849
2020-10-12 20:04:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005821
2020-10-12 20:04:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18115.86328125Mb; avail=470570.1796875Mb
2020-10-12 20:04:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000205
2020-10-12 20:04:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18108.1015625Mb; avail=470577.44921875Mb
2020-10-12 20:04:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096369
2020-10-12 20:04:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103264
2020-10-12 20:04:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18050.3828125Mb; avail=470637.04296875Mb
2020-10-12 20:04:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18045.58984375Mb; avail=470642.61328125Mb
2020-10-12 20:04:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003893
2020-10-12 20:04:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18045.1484375Mb; avail=470641.5Mb
2020-10-12 20:04:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 20:04:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18045.01953125Mb; avail=470642.484375Mb
2020-10-12 20:04:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096165
2020-10-12 20:04:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101341
2020-10-12 20:04:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18046.19921875Mb; avail=470639.8984375Mb
2020-10-12 20:04:38 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 20:04:45 | INFO | train_inner | epoch 024:     24 / 112 loss=7.325, nll_loss=6.199, ppl=73.46, wps=13392.9, ups=2.19, wpb=6128.1, bsz=222.7, num_updates=2600, lr=0.000130035, gnorm=1.22, clip=0, train_wall=28, wall=975
2020-10-12 20:05:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18113.84765625Mb; avail=470558.8203125Mb
2020-10-12 20:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002113
2020-10-12 20:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18113.8046875Mb; avail=470559.0625Mb
2020-10-12 20:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078033
2020-10-12 20:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18113.8125Mb; avail=470559.0625Mb
2020-10-12 20:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.071603
2020-10-12 20:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.152657
2020-10-12 20:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18113.6875Mb; avail=470559.05859375Mb
2020-10-12 20:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18113.63671875Mb; avail=470559.30078125Mb
2020-10-12 20:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001149
2020-10-12 20:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18113.63671875Mb; avail=470559.30078125Mb
2020-10-12 20:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073345
2020-10-12 20:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18114.0390625Mb; avail=470558.90625Mb
2020-10-12 20:05:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060615
2020-10-12 20:05:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.135890
2020-10-12 20:05:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18113.96875Mb; avail=470558.78515625Mb
2020-10-12 20:05:13 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.353 | nll_loss 6.162 | ppl 71.62 | wps 56552.8 | wpb 2236.6 | bsz 85 | num_updates 2688 | best_loss 7.353
2020-10-12 20:05:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:05:17 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 24 @ 2688 updates, score 7.353) (writing took 4.467670375000125 seconds)
2020-10-12 20:05:17 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 20:05:17 | INFO | train | epoch 024 | loss 7.28 | nll_loss 6.147 | ppl 70.87 | wps 17498.4 | ups 2.8 | wpb 6246.1 | bsz 231.6 | num_updates 2688 | lr 0.000134433 | gnorm 1.222 | clip 0 | train_wall 31 | wall 1008
2020-10-12 20:05:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 20:05:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 20:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18083.50390625Mb; avail=470589.28515625Mb
2020-10-12 20:05:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000994
2020-10-12 20:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005806
2020-10-12 20:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18083.50390625Mb; avail=470589.28515625Mb
2020-10-12 20:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000188
2020-10-12 20:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18083.50390625Mb; avail=470589.28515625Mb
2020-10-12 20:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099477
2020-10-12 20:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106236
2020-10-12 20:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18070.45703125Mb; avail=470602.8203125Mb
2020-10-12 20:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18070.52734375Mb; avail=470602.8203125Mb
2020-10-12 20:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003718
2020-10-12 20:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18070.52734375Mb; avail=470602.8203125Mb
2020-10-12 20:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 20:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18070.52734375Mb; avail=470602.8203125Mb
2020-10-12 20:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096625
2020-10-12 20:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101264
2020-10-12 20:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18152.640625Mb; avail=470520.1015625Mb
2020-10-12 20:05:18 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 20:05:21 | INFO | train_inner | epoch 025:     12 / 112 loss=7.286, nll_loss=6.153, ppl=71.17, wps=17153.5, ups=2.72, wpb=6305.7, bsz=236.9, num_updates=2700, lr=0.000135032, gnorm=1.225, clip=0, train_wall=28, wall=1012
2020-10-12 20:05:50 | INFO | train_inner | epoch 025:    112 / 112 loss=7.197, nll_loss=6.052, ppl=66.33, wps=21798.3, ups=3.48, wpb=6258.8, bsz=232.2, num_updates=2800, lr=0.00014003, gnorm=1.271, clip=0, train_wall=28, wall=1041
2020-10-12 20:05:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17839.140625Mb; avail=470833.765625Mb
2020-10-12 20:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002027
2020-10-12 20:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.74609375Mb; avail=470833.16015625Mb
2020-10-12 20:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074687
2020-10-12 20:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.80859375Mb; avail=470832.796875Mb
2020-10-12 20:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060362
2020-10-12 20:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137990
2020-10-12 20:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.87109375Mb; avail=470818.0234375Mb
2020-10-12 20:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17855.09765625Mb; avail=470817.796875Mb
2020-10-12 20:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001269
2020-10-12 20:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17855.09765625Mb; avail=470817.796875Mb
2020-10-12 20:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073479
2020-10-12 20:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.2734375Mb; avail=470811.7265625Mb
2020-10-12 20:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.058561
2020-10-12 20:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.134141
2020-10-12 20:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.2734375Mb; avail=470811.7265625Mb
2020-10-12 20:05:53 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.33 | nll_loss 6.156 | ppl 71.32 | wps 56339.7 | wpb 2236.6 | bsz 85 | num_updates 2800 | best_loss 7.33
2020-10-12 20:05:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:05:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 25 @ 2800 updates, score 7.33) (writing took 4.837650804000077 seconds)
2020-10-12 20:05:58 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 20:05:58 | INFO | train | epoch 025 | loss 7.209 | nll_loss 6.065 | ppl 66.95 | wps 17389.6 | ups 2.78 | wpb 6246.1 | bsz 231.6 | num_updates 2800 | lr 0.00014003 | gnorm 1.265 | clip 0 | train_wall 31 | wall 1049
2020-10-12 20:05:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 20:05:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18461.125Mb; avail=470212.390625Mb
2020-10-12 20:05:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001083
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006213
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18469.6015625Mb; avail=470203.9140625Mb
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000196
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18470.8125Mb; avail=470202.09765625Mb
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.111496
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.118895
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18617.78125Mb; avail=470054.90234375Mb
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18652.49609375Mb; avail=470020.875Mb
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004375
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18658.55078125Mb; avail=470014.8203125Mb
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18659.15625Mb; avail=470014.21484375Mb
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.108998
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114430
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18796.5Mb; avail=469876.86328125Mb
2020-10-12 20:05:58 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 20:06:27 | INFO | train_inner | epoch 026:    100 / 112 loss=7.163, nll_loss=6.012, ppl=64.55, wps=17266.8, ups=2.71, wpb=6361.4, bsz=229.4, num_updates=2900, lr=0.000145028, gnorm=1.204, clip=0, train_wall=28, wall=1078
2020-10-12 20:06:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17885.15625Mb; avail=470787.60546875Mb
2020-10-12 20:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001986
2020-10-12 20:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17885.15625Mb; avail=470787.60546875Mb
2020-10-12 20:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075638
2020-10-12 20:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17885.17578125Mb; avail=470787.84765625Mb
2020-10-12 20:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063066
2020-10-12 20:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141559
2020-10-12 20:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17885.1875Mb; avail=470787.84765625Mb
2020-10-12 20:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17885.1328125Mb; avail=470787.7265625Mb
2020-10-12 20:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001228
2020-10-12 20:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17885.1328125Mb; avail=470787.7265625Mb
2020-10-12 20:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074421
2020-10-12 20:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17885.046875Mb; avail=470788.21875Mb
2020-10-12 20:06:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062022
2020-10-12 20:06:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138485
2020-10-12 20:06:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17885.1484375Mb; avail=470788.09765625Mb
2020-10-12 20:06:33 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.38 | nll_loss 6.196 | ppl 73.3 | wps 56308 | wpb 2236.6 | bsz 85 | num_updates 2912 | best_loss 7.33
2020-10-12 20:06:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:06:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_last.pt (epoch 26 @ 2912 updates, score 7.38) (writing took 2.231264210000518 seconds)
2020-10-12 20:06:35 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 20:06:35 | INFO | train | epoch 026 | loss 7.133 | nll_loss 5.978 | ppl 63.02 | wps 18635.5 | ups 2.98 | wpb 6246.1 | bsz 231.6 | num_updates 2912 | lr 0.000145627 | gnorm 1.212 | clip 0 | train_wall 31 | wall 1086
2020-10-12 20:06:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 20:06:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 20:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17891.95703125Mb; avail=470780.671875Mb
2020-10-12 20:06:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000800
2020-10-12 20:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005461
2020-10-12 20:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17891.48828125Mb; avail=470781.04296875Mb
2020-10-12 20:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000188
2020-10-12 20:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17891.48828125Mb; avail=470781.04296875Mb
2020-10-12 20:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097916
2020-10-12 20:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104483
2020-10-12 20:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17886.05078125Mb; avail=470786.69921875Mb
2020-10-12 20:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17885.9609375Mb; avail=470786.69921875Mb
2020-10-12 20:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003730
2020-10-12 20:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17885.9609375Mb; avail=470786.69921875Mb
2020-10-12 20:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17885.9609375Mb; avail=470786.69921875Mb
2020-10-12 20:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096819
2020-10-12 20:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101509
2020-10-12 20:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17886.08203125Mb; avail=470786.578125Mb
2020-10-12 20:06:35 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 20:07:02 | INFO | train_inner | epoch 027:     88 / 112 loss=7.058, nll_loss=5.891, ppl=59.33, wps=17786.2, ups=2.89, wpb=6159.7, bsz=235.8, num_updates=3000, lr=0.000150025, gnorm=1.287, clip=0, train_wall=28, wall=1112
2020-10-12 20:07:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:07:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17917.45703125Mb; avail=470754.86328125Mb
2020-10-12 20:07:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001876
2020-10-12 20:07:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17917.45703125Mb; avail=470754.86328125Mb
2020-10-12 20:07:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074779
2020-10-12 20:07:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17917.609375Mb; avail=470755.10546875Mb
2020-10-12 20:07:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062451
2020-10-12 20:07:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139966
2020-10-12 20:07:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17917.703125Mb; avail=470755.2265625Mb
2020-10-12 20:07:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17917.58203125Mb; avail=470755.34765625Mb
2020-10-12 20:07:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001288
2020-10-12 20:07:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17917.58203125Mb; avail=470755.34765625Mb
2020-10-12 20:07:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072615
2020-10-12 20:07:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17917.5390625Mb; avail=470754.984375Mb
2020-10-12 20:07:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060768
2020-10-12 20:07:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.135457
2020-10-12 20:07:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17917.56640625Mb; avail=470755.34765625Mb
2020-10-12 20:07:11 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.283 | nll_loss 6.083 | ppl 67.78 | wps 55904.9 | wpb 2236.6 | bsz 85 | num_updates 3024 | best_loss 7.283
2020-10-12 20:07:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:07:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 27 @ 3024 updates, score 7.283) (writing took 4.462536517000444 seconds)
2020-10-12 20:07:16 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 20:07:16 | INFO | train | epoch 027 | loss 7.069 | nll_loss 5.903 | ppl 59.82 | wps 17269.1 | ups 2.76 | wpb 6246.1 | bsz 231.6 | num_updates 3024 | lr 0.000151224 | gnorm 1.282 | clip 0 | train_wall 32 | wall 1127
2020-10-12 20:07:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 20:07:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 20:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17882.015625Mb; avail=470790.671875Mb
2020-10-12 20:07:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000696
2020-10-12 20:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006136
2020-10-12 20:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17882.21484375Mb; avail=470790.671875Mb
2020-10-12 20:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 20:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17882.21484375Mb; avail=470790.671875Mb
2020-10-12 20:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095069
2020-10-12 20:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102168
2020-10-12 20:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17875.97265625Mb; avail=470796.71484375Mb
2020-10-12 20:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17876.03515625Mb; avail=470796.8359375Mb
2020-10-12 20:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003716
2020-10-12 20:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.03515625Mb; avail=470796.8359375Mb
2020-10-12 20:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000188
2020-10-12 20:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.03515625Mb; avail=470796.8359375Mb
2020-10-12 20:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096748
2020-10-12 20:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101719
2020-10-12 20:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.07421875Mb; avail=470796.8125Mb
2020-10-12 20:07:16 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 20:07:38 | INFO | train_inner | epoch 028:     76 / 112 loss=6.992, nll_loss=5.815, ppl=56.31, wps=16884.8, ups=2.74, wpb=6157.9, bsz=231, num_updates=3100, lr=0.000155023, gnorm=1.218, clip=0, train_wall=28, wall=1149
2020-10-12 20:07:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:07:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17893.56640625Mb; avail=470778.7421875Mb
2020-10-12 20:07:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001746
2020-10-12 20:07:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.56640625Mb; avail=470778.7421875Mb
2020-10-12 20:07:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073927
2020-10-12 20:07:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.484375Mb; avail=470778.7421875Mb
2020-10-12 20:07:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062056
2020-10-12 20:07:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138547
2020-10-12 20:07:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.57421875Mb; avail=470778.86328125Mb
2020-10-12 20:07:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17893.57421875Mb; avail=470778.86328125Mb
2020-10-12 20:07:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001187
2020-10-12 20:07:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.66015625Mb; avail=470778.86328125Mb
2020-10-12 20:07:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074333
2020-10-12 20:07:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.76953125Mb; avail=470778.7421875Mb
2020-10-12 20:07:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060972
2020-10-12 20:07:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137261
2020-10-12 20:07:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.796875Mb; avail=470778.5Mb
2020-10-12 20:07:51 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.24 | nll_loss 6.036 | ppl 65.62 | wps 56509.5 | wpb 2236.6 | bsz 85 | num_updates 3136 | best_loss 7.24
2020-10-12 20:07:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:08:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 28 @ 3136 updates, score 7.24) (writing took 10.97292820199982 seconds)
2020-10-12 20:08:02 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 20:08:02 | INFO | train | epoch 028 | loss 6.979 | nll_loss 5.8 | ppl 55.72 | wps 15041.6 | ups 2.41 | wpb 6246.1 | bsz 231.6 | num_updates 3136 | lr 0.000156822 | gnorm 1.197 | clip 0 | train_wall 31 | wall 1173
2020-10-12 20:08:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 20:08:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 20:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17830.8828125Mb; avail=470848.6484375Mb
2020-10-12 20:08:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000885
2020-10-12 20:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006016
2020-10-12 20:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17832.203125Mb; avail=470847.2421875Mb
2020-10-12 20:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000205
2020-10-12 20:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17831.46875Mb; avail=470847.734375Mb
2020-10-12 20:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096024
2020-10-12 20:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103064
2020-10-12 20:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.32421875Mb; avail=470830.9609375Mb
2020-10-12 20:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17878.07421875Mb; avail=470799.9609375Mb
2020-10-12 20:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003675
2020-10-12 20:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17882.40234375Mb; avail=470796.5Mb
2020-10-12 20:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000216
2020-10-12 20:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17882.38671875Mb; avail=470795.78125Mb
2020-10-12 20:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095665
2020-10-12 20:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100310
2020-10-12 20:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17993.4375Mb; avail=470684.1328125Mb
2020-10-12 20:08:02 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 20:08:21 | INFO | train_inner | epoch 029:     64 / 112 loss=6.907, nll_loss=5.716, ppl=52.57, wps=14560.7, ups=2.3, wpb=6325.3, bsz=242.2, num_updates=3200, lr=0.00016002, gnorm=1.27, clip=0, train_wall=28, wall=1192
2020-10-12 20:08:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17826.27734375Mb; avail=470846.68359375Mb
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001757
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.27734375Mb; avail=470846.68359375Mb
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074159
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.58984375Mb; avail=470834.5390625Mb
2020-10-12 20:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060989
2020-10-12 20:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137737
2020-10-12 20:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.89453125Mb; avail=470830.55078125Mb
2020-10-12 20:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17843.25Mb; avail=470829.703125Mb
2020-10-12 20:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001222
2020-10-12 20:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17843.25Mb; avail=470829.703125Mb
2020-10-12 20:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080264
2020-10-12 20:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17843.08203125Mb; avail=470829.4609375Mb
2020-10-12 20:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.094246
2020-10-12 20:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.176660
2020-10-12 20:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17843.46484375Mb; avail=470828.85546875Mb
2020-10-12 20:08:38 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.207 | nll_loss 5.982 | ppl 63.19 | wps 54242.2 | wpb 2236.6 | bsz 85 | num_updates 3248 | best_loss 7.207
2020-10-12 20:08:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:08:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 29 @ 3248 updates, score 7.207) (writing took 6.913257144000454 seconds)
2020-10-12 20:08:45 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 20:08:45 | INFO | train | epoch 029 | loss 6.914 | nll_loss 5.724 | ppl 52.85 | wps 16280 | ups 2.61 | wpb 6246.1 | bsz 231.6 | num_updates 3248 | lr 0.000162419 | gnorm 1.295 | clip 0 | train_wall 32 | wall 1216
2020-10-12 20:08:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 20:08:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 20:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17808.96875Mb; avail=470863.86328125Mb
2020-10-12 20:08:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000809
2020-10-12 20:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005402
2020-10-12 20:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.96875Mb; avail=470863.86328125Mb
2020-10-12 20:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000198
2020-10-12 20:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.96875Mb; avail=470863.86328125Mb
2020-10-12 20:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097756
2020-10-12 20:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104202
2020-10-12 20:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.93359375Mb; avail=470869.890625Mb
2020-10-12 20:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17802.984375Mb; avail=470869.76953125Mb
2020-10-12 20:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005517
2020-10-12 20:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.875Mb; avail=470869.76953125Mb
2020-10-12 20:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 20:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.890625Mb; avail=470869.6484375Mb
2020-10-12 20:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096885
2020-10-12 20:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103361
2020-10-12 20:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.6484375Mb; avail=470870.375Mb
2020-10-12 20:08:45 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 20:09:01 | INFO | train_inner | epoch 030:     52 / 112 loss=6.886, nll_loss=5.691, ppl=51.68, wps=15866.9, ups=2.53, wpb=6282.8, bsz=232.6, num_updates=3300, lr=0.000165018, gnorm=1.217, clip=0, train_wall=28, wall=1232
2020-10-12 20:09:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17845.89453125Mb; avail=470826.62109375Mb
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002031
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17845.89453125Mb; avail=470826.62109375Mb
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073600
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17845.890625Mb; avail=470826.62109375Mb
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059713
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136176
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17845.86328125Mb; avail=470826.7421875Mb
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17845.86328125Mb; avail=470826.7421875Mb
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001090
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17845.86328125Mb; avail=470826.7421875Mb
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074311
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17845.8359375Mb; avail=470826.953125Mb
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059007
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.135178
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17845.31640625Mb; avail=470827.07421875Mb
2020-10-12 20:09:21 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.222 | nll_loss 6.001 | ppl 64.05 | wps 53686.4 | wpb 2236.6 | bsz 85 | num_updates 3360 | best_loss 7.207
2020-10-12 20:09:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:09:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_last.pt (epoch 30 @ 3360 updates, score 7.222) (writing took 3.3025496580003164 seconds)
2020-10-12 20:09:24 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 20:09:24 | INFO | train | epoch 030 | loss 6.825 | nll_loss 5.622 | ppl 49.24 | wps 17873.3 | ups 2.86 | wpb 6246.1 | bsz 231.6 | num_updates 3360 | lr 0.000168016 | gnorm 1.223 | clip 0 | train_wall 31 | wall 1255
2020-10-12 20:09:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 20:09:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 20:09:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17811.6953125Mb; avail=470860.8984375Mb
2020-10-12 20:09:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000816
2020-10-12 20:09:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005502
2020-10-12 20:09:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.828125Mb; avail=470860.65625Mb
2020-10-12 20:09:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000198
2020-10-12 20:09:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.828125Mb; avail=470860.65625Mb
2020-10-12 20:09:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098175
2020-10-12 20:09:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104892
2020-10-12 20:09:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17806.1875Mb; avail=470866.68359375Mb
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17806.1875Mb; avail=470866.68359375Mb
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004089
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17806.1875Mb; avail=470866.68359375Mb
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000237
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17806.1875Mb; avail=470866.68359375Mb
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095825
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100904
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17806.0859375Mb; avail=470866.44921875Mb
2020-10-12 20:09:25 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 20:09:36 | INFO | train_inner | epoch 031:     40 / 112 loss=6.807, nll_loss=5.6, ppl=48.51, wps=17614.6, ups=2.83, wpb=6216.8, bsz=210.4, num_updates=3400, lr=0.000170015, gnorm=1.298, clip=0, train_wall=28, wall=1267
2020-10-12 20:09:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17849.3984375Mb; avail=470822.546875Mb
2020-10-12 20:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002017
2020-10-12 20:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.3984375Mb; avail=470822.546875Mb
2020-10-12 20:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074743
2020-10-12 20:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.1640625Mb; avail=470822.703125Mb
2020-10-12 20:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059874
2020-10-12 20:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137471
2020-10-12 20:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.23046875Mb; avail=470822.82421875Mb
2020-10-12 20:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17854.6796875Mb; avail=470816.76953125Mb
2020-10-12 20:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001279
2020-10-12 20:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17855.890625Mb; avail=470816.1640625Mb
2020-10-12 20:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074674
2020-10-12 20:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17869.2109375Mb; avail=470802.84375Mb
2020-10-12 20:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059978
2020-10-12 20:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136744
2020-10-12 20:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17869.234375Mb; avail=470802.6015625Mb
2020-10-12 20:10:00 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.19 | nll_loss 5.966 | ppl 62.49 | wps 56505.1 | wpb 2236.6 | bsz 85 | num_updates 3472 | best_loss 7.19
2020-10-12 20:10:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:10:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 31 @ 3472 updates, score 7.19) (writing took 4.465621480999289 seconds)
2020-10-12 20:10:05 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 20:10:05 | INFO | train | epoch 031 | loss 6.774 | nll_loss 5.562 | ppl 47.23 | wps 17393.1 | ups 2.78 | wpb 6246.1 | bsz 231.6 | num_updates 3472 | lr 0.000173613 | gnorm 1.358 | clip 0 | train_wall 32 | wall 1295
2020-10-12 20:10:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 20:10:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 20:10:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18151.015625Mb; avail=470521.24609375Mb
2020-10-12 20:10:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000920
2020-10-12 20:10:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006426
2020-10-12 20:10:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18153.29296875Mb; avail=470518.265625Mb
2020-10-12 20:10:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000203
2020-10-12 20:10:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18154.50390625Mb; avail=470517.66015625Mb
2020-10-12 20:10:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101931
2020-10-12 20:10:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109773
2020-10-12 20:10:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18276.35546875Mb; avail=470396.17578125Mb
2020-10-12 20:10:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18303.5234375Mb; avail=470368.2890625Mb
2020-10-12 20:10:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003794
2020-10-12 20:10:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18308.3671875Mb; avail=470363.4453125Mb
2020-10-12 20:10:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000206
2020-10-12 20:10:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18309.578125Mb; avail=470362.83984375Mb
2020-10-12 20:10:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095607
2020-10-12 20:10:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100402
2020-10-12 20:10:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18438.61328125Mb; avail=470233.02734375Mb
2020-10-12 20:10:05 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 20:10:13 | INFO | train_inner | epoch 032:     28 / 112 loss=6.756, nll_loss=5.54, ppl=46.54, wps=16857.2, ups=2.72, wpb=6192.9, bsz=235.3, num_updates=3500, lr=0.000175013, gnorm=1.315, clip=0, train_wall=28, wall=1304
2020-10-12 20:10:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18077.63671875Mb; avail=470594.58203125Mb
2020-10-12 20:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.005537
2020-10-12 20:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18077.63671875Mb; avail=470594.58203125Mb
2020-10-12 20:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074287
2020-10-12 20:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18077.6953125Mb; avail=470594.21875Mb
2020-10-12 20:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060331
2020-10-12 20:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140986
2020-10-12 20:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18077.56640625Mb; avail=470594.4609375Mb
2020-10-12 20:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18077.4375Mb; avail=470594.703125Mb
2020-10-12 20:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001163
2020-10-12 20:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18077.4375Mb; avail=470594.703125Mb
2020-10-12 20:10:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073711
2020-10-12 20:10:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18077.4609375Mb; avail=470594.4609375Mb
2020-10-12 20:10:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060025
2020-10-12 20:10:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.135669
2020-10-12 20:10:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18077.3515625Mb; avail=470594.58203125Mb
2020-10-12 20:10:40 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.178 | nll_loss 5.954 | ppl 62 | wps 56380.2 | wpb 2236.6 | bsz 85 | num_updates 3584 | best_loss 7.178
2020-10-12 20:10:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:10:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 32 @ 3584 updates, score 7.178) (writing took 4.462932288000047 seconds)
2020-10-12 20:10:45 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 20:10:45 | INFO | train | epoch 032 | loss 6.681 | nll_loss 5.453 | ppl 43.82 | wps 17512.3 | ups 2.8 | wpb 6246.1 | bsz 231.6 | num_updates 3584 | lr 0.00017921 | gnorm 1.284 | clip 0 | train_wall 31 | wall 1335
2020-10-12 20:10:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 20:10:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 20:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18367.90234375Mb; avail=470303.80078125Mb
2020-10-12 20:10:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000806
2020-10-12 20:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005904
2020-10-12 20:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18367.53125Mb; avail=470304.171875Mb
2020-10-12 20:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18368.0234375Mb; avail=470304.171875Mb
2020-10-12 20:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098764
2020-10-12 20:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105830
2020-10-12 20:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18363.8203125Mb; avail=470314.63671875Mb
2020-10-12 20:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18364.13671875Mb; avail=470314.61328125Mb
2020-10-12 20:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003947
2020-10-12 20:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18363.953125Mb; avail=470315.4296875Mb
2020-10-12 20:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18363.82421875Mb; avail=470314.7109375Mb
2020-10-12 20:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095883
2020-10-12 20:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100891
2020-10-12 20:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18364.359375Mb; avail=470313.14453125Mb
2020-10-12 20:10:45 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 20:10:50 | INFO | train_inner | epoch 033:     16 / 112 loss=6.657, nll_loss=5.425, ppl=42.97, wps=17253.6, ups=2.74, wpb=6291.5, bsz=238.4, num_updates=3600, lr=0.00018001, gnorm=1.298, clip=0, train_wall=28, wall=1340
2020-10-12 20:11:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17872.6328125Mb; avail=470805.35546875Mb
2020-10-12 20:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002038
2020-10-12 20:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17872.98046875Mb; avail=470805.015625Mb
2020-10-12 20:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073678
2020-10-12 20:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.578125Mb; avail=470803.765625Mb
2020-10-12 20:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060227
2020-10-12 20:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136781
2020-10-12 20:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17872.9765625Mb; avail=470803.2890625Mb
2020-10-12 20:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17872.8046875Mb; avail=470803.05078125Mb
2020-10-12 20:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001210
2020-10-12 20:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.03125Mb; avail=470802.33984375Mb
2020-10-12 20:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074272
2020-10-12 20:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.125Mb; avail=470801.85546875Mb
2020-10-12 20:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060090
2020-10-12 20:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136361
2020-10-12 20:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.140625Mb; avail=470800.65625Mb
2020-10-12 20:11:20 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.145 | nll_loss 5.895 | ppl 59.5 | wps 54900.5 | wpb 2236.6 | bsz 85 | num_updates 3696 | best_loss 7.145
2020-10-12 20:11:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:11:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 33 @ 3696 updates, score 7.145) (writing took 6.80025530399962 seconds)
2020-10-12 20:11:27 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 20:11:27 | INFO | train | epoch 033 | loss 6.604 | nll_loss 5.364 | ppl 41.19 | wps 16351.4 | ups 2.62 | wpb 6246.1 | bsz 231.6 | num_updates 3696 | lr 0.000184808 | gnorm 1.273 | clip 0 | train_wall 32 | wall 1378
2020-10-12 20:11:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 20:11:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 20:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17878.87890625Mb; avail=470793.875Mb
2020-10-12 20:11:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000866
2020-10-12 20:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005983
2020-10-12 20:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17878.87890625Mb; avail=470793.875Mb
2020-10-12 20:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000248
2020-10-12 20:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17878.87890625Mb; avail=470793.875Mb
2020-10-12 20:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.110413
2020-10-12 20:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.117663
2020-10-12 20:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17872.8828125Mb; avail=470799.77734375Mb
2020-10-12 20:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17872.6953125Mb; avail=470799.65625Mb
2020-10-12 20:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004454
2020-10-12 20:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17872.6328125Mb; avail=470799.8984375Mb
2020-10-12 20:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000322
2020-10-12 20:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17872.6328125Mb; avail=470799.8984375Mb
2020-10-12 20:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109440
2020-10-12 20:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115044
2020-10-12 20:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17872.55078125Mb; avail=470800.15234375Mb
2020-10-12 20:11:28 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 20:11:29 | INFO | train_inner | epoch 034:      4 / 112 loss=6.61, nll_loss=5.371, ppl=41.39, wps=15769.8, ups=2.55, wpb=6188.2, bsz=226.4, num_updates=3700, lr=0.000185008, gnorm=1.282, clip=0, train_wall=28, wall=1380
2020-10-12 20:11:58 | INFO | train_inner | epoch 034:    104 / 112 loss=6.507, nll_loss=5.253, ppl=38.12, wps=21803.6, ups=3.47, wpb=6292.4, bsz=238.9, num_updates=3800, lr=0.000190005, gnorm=1.312, clip=0, train_wall=28, wall=1409
2020-10-12 20:12:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17892.5078125Mb; avail=470780.4375Mb
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002730
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.5078125Mb; avail=470780.4375Mb
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.103199
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.5Mb; avail=470780.31640625Mb
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061733
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.168814
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.6015625Mb; avail=470779.2265625Mb
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17893.6015625Mb; avail=470779.2265625Mb
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001185
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.6015625Mb; avail=470779.2265625Mb
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074858
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.6015625Mb; avail=470779.2265625Mb
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061970
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138793
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.48046875Mb; avail=470779.34765625Mb
2020-10-12 20:12:03 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.185 | nll_loss 5.944 | ppl 61.56 | wps 56430.3 | wpb 2236.6 | bsz 85 | num_updates 3808 | best_loss 7.145
2020-10-12 20:12:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:12:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_last.pt (epoch 34 @ 3808 updates, score 7.185) (writing took 4.939444087999618 seconds)
2020-10-12 20:12:08 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 20:12:08 | INFO | train | epoch 034 | loss 6.524 | nll_loss 5.271 | ppl 38.62 | wps 17276.1 | ups 2.77 | wpb 6246.1 | bsz 231.6 | num_updates 3808 | lr 0.000190405 | gnorm 1.314 | clip 0 | train_wall 31 | wall 1419
2020-10-12 20:12:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 20:12:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 20:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17882.0390625Mb; avail=470791.03515625Mb
2020-10-12 20:12:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000914
2020-10-12 20:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005647
2020-10-12 20:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17881.0546875Mb; avail=470792.01953125Mb
2020-10-12 20:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17881.0546875Mb; avail=470792.01953125Mb
2020-10-12 20:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098331
2020-10-12 20:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105253
2020-10-12 20:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.68359375Mb; avail=470797.8828125Mb
2020-10-12 20:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17874.77734375Mb; avail=470798.00390625Mb
2020-10-12 20:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003850
2020-10-12 20:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.77734375Mb; avail=470798.00390625Mb
2020-10-12 20:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000207
2020-10-12 20:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.77734375Mb; avail=470798.00390625Mb
2020-10-12 20:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097868
2020-10-12 20:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102704
2020-10-12 20:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.796875Mb; avail=470797.98828125Mb
2020-10-12 20:12:08 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 20:12:35 | INFO | train_inner | epoch 035:     92 / 112 loss=6.463, nll_loss=5.199, ppl=36.75, wps=16648.1, ups=2.69, wpb=6199.2, bsz=224.5, num_updates=3900, lr=0.000195003, gnorm=1.281, clip=0, train_wall=28, wall=1446
2020-10-12 20:12:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:12:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17931.93359375Mb; avail=470740.1484375Mb
2020-10-12 20:12:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001982
2020-10-12 20:12:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17931.93359375Mb; avail=470740.1484375Mb
2020-10-12 20:12:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079590
2020-10-12 20:12:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17931.71484375Mb; avail=470740.6484375Mb
2020-10-12 20:12:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.066978
2020-10-12 20:12:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.149422
2020-10-12 20:12:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17930.99609375Mb; avail=470741.140625Mb
2020-10-12 20:12:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17930.99609375Mb; avail=470741.140625Mb
2020-10-12 20:12:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001242
2020-10-12 20:12:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17930.99609375Mb; avail=470741.140625Mb
2020-10-12 20:12:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082208
2020-10-12 20:12:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17932.265625Mb; avail=470740.01171875Mb
2020-10-12 20:12:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.066273
2020-10-12 20:12:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.150560
2020-10-12 20:12:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17932.21875Mb; avail=470739.76953125Mb
2020-10-12 20:12:44 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.118 | nll_loss 5.867 | ppl 58.34 | wps 56198.4 | wpb 2236.6 | bsz 85 | num_updates 3920 | best_loss 7.118
2020-10-12 20:12:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:12:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 35 @ 3920 updates, score 7.118) (writing took 11.079245791000176 seconds)
2020-10-12 20:12:55 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 20:12:55 | INFO | train | epoch 035 | loss 6.447 | nll_loss 5.182 | ppl 36.29 | wps 14907.9 | ups 2.39 | wpb 6246.1 | bsz 231.6 | num_updates 3920 | lr 0.000196002 | gnorm 1.276 | clip 0 | train_wall 32 | wall 1466
2020-10-12 20:12:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 20:12:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 20:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18572.14453125Mb; avail=470099.953125Mb
2020-10-12 20:12:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000818
2020-10-12 20:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005683
2020-10-12 20:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18572.265625Mb; avail=470099.953125Mb
2020-10-12 20:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000198
2020-10-12 20:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18572.265625Mb; avail=470099.953125Mb
2020-10-12 20:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096350
2020-10-12 20:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103008
2020-10-12 20:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18566.64453125Mb; avail=470105.98828125Mb
2020-10-12 20:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18565.73046875Mb; avail=470107.40625Mb
2020-10-12 20:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003793
2020-10-12 20:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18565.73046875Mb; avail=470107.40625Mb
2020-10-12 20:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000207
2020-10-12 20:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18565.73046875Mb; avail=470107.40625Mb
2020-10-12 20:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096289
2020-10-12 20:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101053
2020-10-12 20:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18565.67578125Mb; avail=470107.44140625Mb
2020-10-12 20:12:55 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 20:13:18 | INFO | train_inner | epoch 036:     80 / 112 loss=6.38, nll_loss=5.104, ppl=34.4, wps=14452.7, ups=2.31, wpb=6270, bsz=234.7, num_updates=4000, lr=0.0002, gnorm=1.259, clip=0, train_wall=28, wall=1489
2020-10-12 20:13:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:13:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18517.67578125Mb; avail=470155.1796875Mb
2020-10-12 20:13:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001830
2020-10-12 20:13:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18517.67578125Mb; avail=470155.1796875Mb
2020-10-12 20:13:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075436
2020-10-12 20:13:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18517.67578125Mb; avail=470155.1796875Mb
2020-10-12 20:13:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062347
2020-10-12 20:13:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140561
2020-10-12 20:13:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18517.83203125Mb; avail=470154.81640625Mb
2020-10-12 20:13:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18517.83203125Mb; avail=470154.81640625Mb
2020-10-12 20:13:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001314
2020-10-12 20:13:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18517.83203125Mb; avail=470154.81640625Mb
2020-10-12 20:13:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075022
2020-10-12 20:13:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18517.9921875Mb; avail=470154.4375Mb
2020-10-12 20:13:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060726
2020-10-12 20:13:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138332
2020-10-12 20:13:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18518.95703125Mb; avail=470153.71875Mb
2020-10-12 20:13:30 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.146 | nll_loss 5.887 | ppl 59.19 | wps 56283 | wpb 2236.6 | bsz 85 | num_updates 4032 | best_loss 7.118
2020-10-12 20:13:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:13:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_last.pt (epoch 36 @ 4032 updates, score 7.146) (writing took 4.0393633319999935 seconds)
2020-10-12 20:13:34 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 20:13:34 | INFO | train | epoch 036 | loss 6.355 | nll_loss 5.074 | ppl 33.69 | wps 17616.2 | ups 2.82 | wpb 6246.1 | bsz 231.6 | num_updates 4032 | lr 0.000199205 | gnorm 1.271 | clip 0 | train_wall 31 | wall 1505
2020-10-12 20:13:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 20:13:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 20:13:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17831.515625Mb; avail=470840.91015625Mb
2020-10-12 20:13:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000701
2020-10-12 20:13:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005483
2020-10-12 20:13:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17831.515625Mb; avail=470840.91015625Mb
2020-10-12 20:13:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000197
2020-10-12 20:13:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17831.515625Mb; avail=470840.91015625Mb
2020-10-12 20:13:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095812
2020-10-12 20:13:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102303
2020-10-12 20:13:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17825.97265625Mb; avail=470846.7109375Mb
2020-10-12 20:13:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17825.97265625Mb; avail=470846.7109375Mb
2020-10-12 20:13:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003782
2020-10-12 20:13:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17825.97265625Mb; avail=470846.7109375Mb
2020-10-12 20:13:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000216
2020-10-12 20:13:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17825.97265625Mb; avail=470846.7109375Mb
2020-10-12 20:13:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095548
2020-10-12 20:13:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100303
2020-10-12 20:13:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17825.9921875Mb; avail=470846.7109375Mb
2020-10-12 20:13:35 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 20:13:54 | INFO | train_inner | epoch 037:     68 / 112 loss=6.282, nll_loss=4.99, ppl=31.77, wps=17351.7, ups=2.77, wpb=6260.9, bsz=230.2, num_updates=4100, lr=0.000197546, gnorm=1.26, clip=0, train_wall=28, wall=1525
2020-10-12 20:14:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:14:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16118.3046875Mb; avail=472565.41015625Mb
2020-10-12 20:14:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001845
2020-10-12 20:14:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16121.07421875Mb; avail=472563.859375Mb
2020-10-12 20:14:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078346
2020-10-12 20:14:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16171.4609375Mb; avail=472512.0390625Mb
2020-10-12 20:14:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061376
2020-10-12 20:14:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142643
2020-10-12 20:14:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16201.05859375Mb; avail=472479.15234375Mb
2020-10-12 20:14:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16211.56640625Mb; avail=472467.87890625Mb
2020-10-12 20:14:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001321
2020-10-12 20:14:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16214.46484375Mb; avail=472465.72265625Mb
2020-10-12 20:14:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073391
2020-10-12 20:14:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16253.328125Mb; avail=472426.19140625Mb
2020-10-12 20:14:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060448
2020-10-12 20:14:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.135936
2020-10-12 20:14:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16287.58984375Mb; avail=472391.6015625Mb
2020-10-12 20:14:10 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.097 | nll_loss 5.826 | ppl 56.73 | wps 55677.1 | wpb 2236.6 | bsz 85 | num_updates 4144 | best_loss 7.097
2020-10-12 20:14:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:14:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 37 @ 4144 updates, score 7.097) (writing took 4.487833418000264 seconds)
2020-10-12 20:14:15 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 20:14:15 | INFO | train | epoch 037 | loss 6.274 | nll_loss 4.979 | ppl 31.53 | wps 17455.8 | ups 2.79 | wpb 6246.1 | bsz 231.6 | num_updates 4144 | lr 0.000196494 | gnorm 1.282 | clip 0 | train_wall 31 | wall 1545
2020-10-12 20:14:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 20:14:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 20:14:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17884.9453125Mb; avail=470787.34375Mb
2020-10-12 20:14:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000865
2020-10-12 20:14:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005539
2020-10-12 20:14:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17884.9453125Mb; avail=470787.34375Mb
2020-10-12 20:14:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000203
2020-10-12 20:14:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17884.9453125Mb; avail=470787.34375Mb
2020-10-12 20:14:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097710
2020-10-12 20:14:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104205
2020-10-12 20:14:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17879.046875Mb; avail=470793.2265625Mb
2020-10-12 20:14:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17878.6328125Mb; avail=470793.96875Mb
2020-10-12 20:14:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003949
2020-10-12 20:14:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17878.64453125Mb; avail=470793.84765625Mb
2020-10-12 20:14:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 20:14:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17878.64453125Mb; avail=470793.84765625Mb
2020-10-12 20:14:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095695
2020-10-12 20:14:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100590
2020-10-12 20:14:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17878.7578125Mb; avail=470793.84765625Mb
2020-10-12 20:14:15 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 20:14:31 | INFO | train_inner | epoch 038:     56 / 112 loss=6.195, nll_loss=4.889, ppl=29.62, wps=16835.6, ups=2.74, wpb=6149.3, bsz=235.9, num_updates=4200, lr=0.00019518, gnorm=1.25, clip=0, train_wall=28, wall=1562
2020-10-12 20:14:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:14:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17926.01171875Mb; avail=470746.71484375Mb
2020-10-12 20:14:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002035
2020-10-12 20:14:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17926.05078125Mb; avail=470746.59375Mb
2020-10-12 20:14:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073681
2020-10-12 20:14:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17925.7421875Mb; avail=470746.59375Mb
2020-10-12 20:14:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060468
2020-10-12 20:14:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137007
2020-10-12 20:14:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17925.703125Mb; avail=470746.8359375Mb
2020-10-12 20:14:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17925.703125Mb; avail=470746.8359375Mb
2020-10-12 20:14:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001136
2020-10-12 20:14:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17925.703125Mb; avail=470746.8359375Mb
2020-10-12 20:14:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074660
2020-10-12 20:14:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17925.6796875Mb; avail=470747.078125Mb
2020-10-12 20:14:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059723
2020-10-12 20:14:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136285
2020-10-12 20:14:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17925.6796875Mb; avail=470747.078125Mb
2020-10-12 20:14:50 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.109 | nll_loss 5.836 | ppl 57.12 | wps 56412.9 | wpb 2236.6 | bsz 85 | num_updates 4256 | best_loss 7.097
2020-10-12 20:14:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:14:52 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_last.pt (epoch 38 @ 4256 updates, score 7.109) (writing took 2.329875334999997 seconds)
2020-10-12 20:14:52 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 20:14:52 | INFO | train | epoch 038 | loss 6.174 | nll_loss 4.864 | ppl 29.12 | wps 18560.6 | ups 2.97 | wpb 6246.1 | bsz 231.6 | num_updates 4256 | lr 0.000193892 | gnorm 1.22 | clip 0 | train_wall 31 | wall 1583
2020-10-12 20:14:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 20:14:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 20:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17910.75Mb; avail=470761.66015625Mb
2020-10-12 20:14:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000843
2020-10-12 20:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005513
2020-10-12 20:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17910.75Mb; avail=470761.66015625Mb
2020-10-12 20:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000213
2020-10-12 20:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17910.75Mb; avail=470761.66015625Mb
2020-10-12 20:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097302
2020-10-12 20:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103831
2020-10-12 20:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17903.87109375Mb; avail=470768.75Mb
2020-10-12 20:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17903.37890625Mb; avail=470769.2421875Mb
2020-10-12 20:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003710
2020-10-12 20:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17903.37890625Mb; avail=470769.2421875Mb
2020-10-12 20:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 20:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17903.37890625Mb; avail=470769.2421875Mb
2020-10-12 20:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096068
2020-10-12 20:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100697
2020-10-12 20:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17903.47265625Mb; avail=470769.12109375Mb
2020-10-12 20:14:52 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 20:15:06 | INFO | train_inner | epoch 039:     44 / 112 loss=6.185, nll_loss=4.875, ppl=29.34, wps=18171.4, ups=2.88, wpb=6304, bsz=226.6, num_updates=4300, lr=0.000192897, gnorm=1.288, clip=0, train_wall=28, wall=1596
2020-10-12 20:15:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:15:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16614.06640625Mb; avail=472069.90625Mb
2020-10-12 20:15:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002191
2020-10-12 20:15:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16546.63671875Mb; avail=472125.5234375Mb
2020-10-12 20:15:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073679
2020-10-12 20:15:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15626.34375Mb; avail=473045.578125Mb
2020-10-12 20:15:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060235
2020-10-12 20:15:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137005
2020-10-12 20:15:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15626.65625Mb; avail=473045.32421875Mb
2020-10-12 20:15:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15622.71875Mb; avail=473049.26171875Mb
2020-10-12 20:15:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001212
2020-10-12 20:15:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15622.234375Mb; avail=473049.74609375Mb
2020-10-12 20:15:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072916
2020-10-12 20:15:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15601.96875Mb; avail=473070.01171875Mb
2020-10-12 20:15:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060431
2020-10-12 20:15:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.135331
2020-10-12 20:15:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15600.953125Mb; avail=473071.02734375Mb
2020-10-12 20:15:28 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.085 | nll_loss 5.795 | ppl 55.54 | wps 55394.2 | wpb 2236.6 | bsz 85 | num_updates 4368 | best_loss 7.085
2020-10-12 20:15:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:15:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_best.pt (epoch 39 @ 4368 updates, score 7.085) (writing took 4.465734753999641 seconds)
2020-10-12 20:15:33 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 20:15:33 | INFO | train | epoch 039 | loss 6.099 | nll_loss 4.775 | ppl 27.38 | wps 17286 | ups 2.77 | wpb 6246.1 | bsz 231.6 | num_updates 4368 | lr 0.00019139 | gnorm 1.292 | clip 0 | train_wall 32 | wall 1624
2020-10-12 20:15:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 20:15:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 20:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17833.34375Mb; avail=470839.53515625Mb
2020-10-12 20:15:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000887
2020-10-12 20:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005553
2020-10-12 20:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.58203125Mb; avail=470835.296875Mb
2020-10-12 20:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000190
2020-10-12 20:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.1875Mb; avail=470834.69140625Mb
2020-10-12 20:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098510
2020-10-12 20:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105018
2020-10-12 20:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17884.2265625Mb; avail=470788.640625Mb
2020-10-12 20:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17896.125Mb; avail=470776.7421875Mb
2020-10-12 20:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003713
2020-10-12 20:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17896.73046875Mb; avail=470775.53125Mb
2020-10-12 20:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 20:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17897.94140625Mb; avail=470774.92578125Mb
2020-10-12 20:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096231
2020-10-12 20:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100861
2020-10-12 20:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17948.96875Mb; avail=470724.06640625Mb
2020-10-12 20:15:33 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 20:15:42 | INFO | train_inner | epoch 040:     32 / 112 loss=6.059, nll_loss=4.729, ppl=26.52, wps=16895.8, ups=2.73, wpb=6199.9, bsz=224.9, num_updates=4400, lr=0.000190693, gnorm=1.273, clip=0, train_wall=28, wall=1633
2020-10-12 20:16:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:16:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17881.42578125Mb; avail=470790.90234375Mb
2020-10-12 20:16:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002012
2020-10-12 20:16:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17882.63671875Mb; avail=470790.296875Mb
2020-10-12 20:16:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.086421
2020-10-12 20:16:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17928.5390625Mb; avail=470744.37109375Mb
2020-10-12 20:16:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.068932
2020-10-12 20:16:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.158541
2020-10-12 20:16:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17977.2890625Mb; avail=470695.5390625Mb
2020-10-12 20:16:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17981.53515625Mb; avail=470691.1796875Mb
2020-10-12 20:16:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001158
2020-10-12 20:16:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17981.53515625Mb; avail=470691.1796875Mb
2020-10-12 20:16:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076652
2020-10-12 20:16:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18028.98046875Mb; avail=470643.921875Mb
2020-10-12 20:16:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.082005
2020-10-12 20:16:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.160596
2020-10-12 20:16:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18036.0078125Mb; avail=470637.078125Mb
2020-10-12 20:16:08 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.095 | nll_loss 5.799 | ppl 55.67 | wps 53639.9 | wpb 2236.6 | bsz 85 | num_updates 4480 | best_loss 7.085
2020-10-12 20:16:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:16:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azejpn_sepspm8000/M2O/checkpoint_last.pt (epoch 40 @ 4480 updates, score 7.095) (writing took 2.234046589999707 seconds)
2020-10-12 20:16:11 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 20:16:11 | INFO | train | epoch 040 | loss 6.017 | nll_loss 4.679 | ppl 25.62 | wps 18407.5 | ups 2.95 | wpb 6246.1 | bsz 231.6 | num_updates 4480 | lr 0.000188982 | gnorm 1.292 | clip 0 | train_wall 31 | wall 1662
2020-10-12 20:16:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 20:16:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 20:16:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17946.78125Mb; avail=470725.7890625Mb
2020-10-12 20:16:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000774
2020-10-12 20:16:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005115
2020-10-12 20:16:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17946.78125Mb; avail=470725.7890625Mb
2020-10-12 20:16:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:16:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17945.796875Mb; avail=470728.25Mb
2020-10-12 20:16:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097020
2020-10-12 20:16:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103187
2020-10-12 20:16:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17939.66796875Mb; avail=470732.6796875Mb
2020-10-12 20:16:11 | INFO | fairseq_cli.train | done training in 1661.6 seconds
