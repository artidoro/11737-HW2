2020-10-11 22:25:00 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_belukr_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='bel-eng,ukr-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_belukr_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-11 22:25:00 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-11 22:25:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['bel', 'eng', 'ukr']
2020-10-11 22:25:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bel] dictionary: 21683 types
2020-10-11 22:25:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 21683 types
2020-10-11 22:25:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | [ukr] dictionary: 21683 types
2020-10-11 22:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-11 22:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:25:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-11 22:25:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:bel-eng': 1, 'main:ukr-eng': 1}
2020-10-11 22:25:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-11 22:25:00 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belukr_sepspm8000/M2O/valid.bel-eng.bel
2020-10-11 22:25:00 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belukr_sepspm8000/M2O/valid.bel-eng.eng
2020-10-11 22:25:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belukr_sepspm8000/M2O/ valid bel-eng 248 examples
2020-10-11 22:25:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:ukr-eng src_langtok: None; tgt_langtok: None
2020-10-11 22:25:00 | INFO | fairseq.data.data_utils | loaded 3060 examples from: fairseq/data-bin/ted_belukr_sepspm8000/M2O/valid.ukr-eng.ukr
2020-10-11 22:25:00 | INFO | fairseq.data.data_utils | loaded 3060 examples from: fairseq/data-bin/ted_belukr_sepspm8000/M2O/valid.ukr-eng.eng
2020-10-11 22:25:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belukr_sepspm8000/M2O/ valid ukr-eng 3060 examples
2020-10-11 22:25:01 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21683, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21683, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=21683, bias=False)
  )
)
2020-10-11 22:25:01 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-11 22:25:01 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-11 22:25:01 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-11 22:25:01 | INFO | fairseq_cli.train | num. model params: 42644992 (num. trained: 42644992)
2020-10-11 22:25:03 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-11 22:25:03 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-11 22:25:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-11 22:25:03 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.732 GB ; name = GeForce RTX 2080 Ti                     
2020-10-11 22:25:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-11 22:25:03 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-11 22:25:03 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-11 22:25:03 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_last.pt
2020-10-11 22:25:03 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-11 22:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-11 22:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:25:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-11 22:25:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:bel-eng': 1, 'main:ukr-eng': 1}
2020-10-11 22:25:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-11 22:25:03 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belukr_sepspm8000/M2O/train.bel-eng.bel
2020-10-11 22:25:03 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belukr_sepspm8000/M2O/train.bel-eng.eng
2020-10-11 22:25:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belukr_sepspm8000/M2O/ train bel-eng 4509 examples
2020-10-11 22:25:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:ukr-eng src_langtok: None; tgt_langtok: None
2020-10-11 22:25:03 | INFO | fairseq.data.data_utils | loaded 39988 examples from: fairseq/data-bin/ted_belukr_sepspm8000/M2O/train.ukr-eng.ukr
2020-10-11 22:25:03 | INFO | fairseq.data.data_utils | loaded 39988 examples from: fairseq/data-bin/ted_belukr_sepspm8000/M2O/train.ukr-eng.eng
2020-10-11 22:25:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belukr_sepspm8000/M2O/ train ukr-eng 39988 examples
2020-10-11 22:25:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:bel-eng', 4509), ('main:ukr-eng', 39988)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-11 22:25:03 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-11 22:25:03 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 44497
2020-10-11 22:25:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 44497; virtual dataset size 44497
2020-10-11 22:25:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-11 22:25:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:bel-eng': 4509, 'main:ukr-eng': 39988}; raw total size: 44497
2020-10-11 22:25:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:bel-eng': 4509, 'main:ukr-eng': 39988}; resampled total size: 44497
2020-10-11 22:25:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-11 22:25:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.003091
2020-10-11 22:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:25:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000413
2020-10-11 22:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005216
2020-10-11 22:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-11 22:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103455
2020-10-11 22:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109220
2020-10-11 22:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:25:03 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-11 22:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004217
2020-10-11 22:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-11 22:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102428
2020-10-11 22:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107160
2020-10-11 22:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:25:03 | INFO | fairseq.trainer | begin training epoch 1
2020-10-11 22:25:33 | INFO | train_inner | epoch 001:    100 / 173 loss=14.135, nll_loss=14.022, ppl=16641.4, wps=22771.3, ups=3.37, wpb=6747.6, bsz=262.7, num_updates=100, lr=5.0975e-06, gnorm=4.783, clip=0, train_wall=29, wall=30
2020-10-11 22:25:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000668
2020-10-11 22:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027236
2020-10-11 22:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022190
2020-10-11 22:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050436
2020-10-11 22:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000578
2020-10-11 22:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027450
2020-10-11 22:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022481
2020-10-11 22:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050838
2020-10-11 22:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
/home/han/Documents/11737-hw/assign2/fairseq/fairseq/utils.py:340: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2020-10-11 22:25:57 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.859 | nll_loss 11.473 | ppl 2842.65 | wps 52744 | wpb 2417.7 | bsz 91.9 | num_updates 173
2020-10-11 22:25:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:25:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 1 @ 173 updates, score 11.859) (writing took 0.9458258800004842 seconds)
2020-10-11 22:25:58 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-11 22:25:58 | INFO | train | epoch 001 | loss 13.462 | nll_loss 13.273 | ppl 9896.5 | wps 21491.6 | ups 3.18 | wpb 6755.6 | bsz 257.2 | num_updates 173 | lr 8.74568e-06 | gnorm 3.667 | clip 0 | train_wall 51 | wall 55
2020-10-11 22:25:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-11 22:25:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-11 22:25:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:25:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000556
2020-10-11 22:25:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005660
2020-10-11 22:25:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:25:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-11 22:25:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:25:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104802
2020-10-11 22:25:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110994
2020-10-11 22:25:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:25:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:25:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004397
2020-10-11 22:25:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:25:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-11 22:25:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:25:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103841
2020-10-11 22:25:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108752
2020-10-11 22:25:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:25:58 | INFO | fairseq.trainer | begin training epoch 2
2020-10-11 22:26:06 | INFO | train_inner | epoch 002:     27 / 173 loss=12.414, nll_loss=12.103, ppl=4399.8, wps=20366, ups=3.01, wpb=6764.9, bsz=265.5, num_updates=200, lr=1.0095e-05, gnorm=2.095, clip=0, train_wall=30, wall=63
2020-10-11 22:26:37 | INFO | train_inner | epoch 002:    127 / 173 loss=11.586, nll_loss=11.179, ppl=2318.36, wps=22028.4, ups=3.29, wpb=6695.7, bsz=238, num_updates=300, lr=1.50925e-05, gnorm=1.536, clip=0, train_wall=30, wall=94
2020-10-11 22:26:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.006885
2020-10-11 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027546
2020-10-11 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021893
2020-10-11 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.056670
2020-10-11 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000558
2020-10-11 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027778
2020-10-11 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021855
2020-10-11 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050507
2020-10-11 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:26:52 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.365 | nll_loss 9.771 | ppl 873.79 | wps 51726.1 | wpb 2417.7 | bsz 91.9 | num_updates 346 | best_loss 10.365
2020-10-11 22:26:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:26:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 2 @ 346 updates, score 10.365) (writing took 1.4851219279953511 seconds)
2020-10-11 22:26:54 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-11 22:26:54 | INFO | train | epoch 002 | loss 11.477 | nll_loss 11.056 | ppl 2128.64 | wps 20752.1 | ups 3.07 | wpb 6755.6 | bsz 257.2 | num_updates 346 | lr 1.73914e-05 | gnorm 1.644 | clip 0 | train_wall 52 | wall 111
2020-10-11 22:26:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-11 22:26:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-11 22:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:26:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000574
2020-10-11 22:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005917
2020-10-11 22:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000246
2020-10-11 22:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105166
2020-10-11 22:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111674
2020-10-11 22:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004282
2020-10-11 22:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-11 22:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103130
2020-10-11 22:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107913
2020-10-11 22:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:26:54 | INFO | fairseq.trainer | begin training epoch 3
2020-10-11 22:27:11 | INFO | train_inner | epoch 003:     54 / 173 loss=10.567, nll_loss=10.024, ppl=1040.91, wps=20016.9, ups=2.91, wpb=6885, bsz=263.2, num_updates=400, lr=2.009e-05, gnorm=1.675, clip=0, train_wall=30, wall=128
2020-10-11 22:27:42 | INFO | train_inner | epoch 003:    154 / 173 loss=9.674, nll_loss=8.97, ppl=501.61, wps=21573.8, ups=3.26, wpb=6620.5, bsz=248.2, num_updates=500, lr=2.50875e-05, gnorm=1.319, clip=0, train_wall=30, wall=159
2020-10-11 22:27:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000680
2020-10-11 22:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027484
2020-10-11 22:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021762
2020-10-11 22:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050257
2020-10-11 22:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000572
2020-10-11 22:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027042
2020-10-11 22:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021539
2020-10-11 22:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049485
2020-10-11 22:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:27:49 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.166 | nll_loss 8.315 | ppl 318.36 | wps 51175.1 | wpb 2417.7 | bsz 91.9 | num_updates 519 | best_loss 9.166
2020-10-11 22:27:49 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:27:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 3 @ 519 updates, score 9.166) (writing took 1.9176119189942256 seconds)
2020-10-11 22:27:51 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-11 22:27:51 | INFO | train | epoch 003 | loss 9.848 | nll_loss 9.176 | ppl 578.47 | wps 20352.6 | ups 3.01 | wpb 6755.6 | bsz 257.2 | num_updates 519 | lr 2.6037e-05 | gnorm 1.431 | clip 0 | train_wall 52 | wall 169
2020-10-11 22:27:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-11 22:27:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-11 22:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:27:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000451
2020-10-11 22:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005323
2020-10-11 22:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 22:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.107424
2020-10-11 22:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113259
2020-10-11 22:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004209
2020-10-11 22:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-11 22:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104238
2020-10-11 22:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108933
2020-10-11 22:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:27:52 | INFO | fairseq.trainer | begin training epoch 4
2020-10-11 22:28:17 | INFO | train_inner | epoch 004:     81 / 173 loss=9.319, nll_loss=8.525, ppl=368.33, wps=19409.9, ups=2.85, wpb=6820, bsz=255.5, num_updates=600, lr=3.0085e-05, gnorm=1.253, clip=0, train_wall=31, wall=194
2020-10-11 22:28:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000684
2020-10-11 22:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027852
2020-10-11 22:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022029
2020-10-11 22:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050904
2020-10-11 22:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000536
2020-10-11 22:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027815
2020-10-11 22:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022152
2020-10-11 22:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050836
2020-10-11 22:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:28:48 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.864 | nll_loss 7.926 | ppl 243.25 | wps 50686.1 | wpb 2417.7 | bsz 91.9 | num_updates 692 | best_loss 8.864
2020-10-11 22:28:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:28:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 4 @ 692 updates, score 8.864) (writing took 1.9139603799994802 seconds)
2020-10-11 22:28:49 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-11 22:28:49 | INFO | train | epoch 004 | loss 9.211 | nll_loss 8.386 | ppl 334.49 | wps 20126.6 | ups 2.98 | wpb 6755.6 | bsz 257.2 | num_updates 692 | lr 3.46827e-05 | gnorm 1.358 | clip 0 | train_wall 53 | wall 227
2020-10-11 22:28:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-11 22:28:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-11 22:28:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:28:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000438
2020-10-11 22:28:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005217
2020-10-11 22:28:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:28:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-11 22:28:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106745
2020-10-11 22:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112455
2020-10-11 22:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004193
2020-10-11 22:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-11 22:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102888
2020-10-11 22:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107585
2020-10-11 22:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:28:50 | INFO | fairseq.trainer | begin training epoch 5
2020-10-11 22:28:52 | INFO | train_inner | epoch 005:      8 / 173 loss=9.14, nll_loss=8.293, ppl=313.76, wps=18962.5, ups=2.83, wpb=6709.3, bsz=259.7, num_updates=700, lr=3.50825e-05, gnorm=1.486, clip=0, train_wall=31, wall=229
2020-10-11 22:29:24 | INFO | train_inner | epoch 005:    108 / 173 loss=9.006, nll_loss=8.132, ppl=280.52, wps=21598.7, ups=3.18, wpb=6797.4, bsz=264.4, num_updates=800, lr=4.008e-05, gnorm=1.556, clip=0, train_wall=31, wall=261
2020-10-11 22:29:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:29:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:29:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000620
2020-10-11 22:29:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:29:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027657
2020-10-11 22:29:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:29:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022087
2020-10-11 22:29:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050695
2020-10-11 22:29:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:29:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:29:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000562
2020-10-11 22:29:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:29:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028013
2020-10-11 22:29:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:29:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022361
2020-10-11 22:29:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051255
2020-10-11 22:29:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:29:46 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.716 | nll_loss 7.73 | ppl 212.28 | wps 49506.9 | wpb 2417.7 | bsz 91.9 | num_updates 865 | best_loss 8.716
2020-10-11 22:29:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:29:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 5 @ 865 updates, score 8.716) (writing took 1.693570406991057 seconds)
2020-10-11 22:29:48 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-11 22:29:48 | INFO | train | epoch 005 | loss 8.968 | nll_loss 8.088 | ppl 272.11 | wps 20012 | ups 2.96 | wpb 6755.6 | bsz 257.2 | num_updates 865 | lr 4.33284e-05 | gnorm 1.581 | clip 0 | train_wall 54 | wall 285
2020-10-11 22:29:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-11 22:29:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-11 22:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:29:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000458
2020-10-11 22:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005304
2020-10-11 22:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 22:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101011
2020-10-11 22:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106832
2020-10-11 22:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004254
2020-10-11 22:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 22:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098320
2020-10-11 22:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103077
2020-10-11 22:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:29:48 | INFO | fairseq.trainer | begin training epoch 6
2020-10-11 22:29:59 | INFO | train_inner | epoch 006:     35 / 173 loss=8.894, nll_loss=8.001, ppl=256.25, wps=19011.3, ups=2.81, wpb=6763.3, bsz=258.8, num_updates=900, lr=4.50775e-05, gnorm=1.577, clip=0, train_wall=31, wall=297
2020-10-11 22:30:31 | INFO | train_inner | epoch 006:    135 / 173 loss=8.699, nll_loss=7.778, ppl=219.55, wps=21253.5, ups=3.13, wpb=6782.2, bsz=251.1, num_updates=1000, lr=5.0075e-05, gnorm=1.337, clip=0, train_wall=31, wall=328
2020-10-11 22:30:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002155
2020-10-11 22:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027483
2020-10-11 22:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022281
2020-10-11 22:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.052273
2020-10-11 22:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000538
2020-10-11 22:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027706
2020-10-11 22:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022105
2020-10-11 22:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050671
2020-10-11 22:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:30:45 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.336 | nll_loss 7.325 | ppl 160.38 | wps 49057.5 | wpb 2417.7 | bsz 91.9 | num_updates 1038 | best_loss 8.336
2020-10-11 22:30:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:30:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 6 @ 1038 updates, score 8.336) (writing took 1.9360599170031492 seconds)
2020-10-11 22:30:47 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-11 22:30:47 | INFO | train | epoch 006 | loss 8.704 | nll_loss 7.785 | ppl 220.49 | wps 19687.2 | ups 2.91 | wpb 6755.6 | bsz 257.2 | num_updates 1038 | lr 5.19741e-05 | gnorm 1.396 | clip 0 | train_wall 54 | wall 345
2020-10-11 22:30:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-11 22:30:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-11 22:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:30:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000437
2020-10-11 22:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005258
2020-10-11 22:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 22:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103291
2020-10-11 22:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109113
2020-10-11 22:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004318
2020-10-11 22:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-11 22:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101467
2020-10-11 22:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106294
2020-10-11 22:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:30:47 | INFO | fairseq.trainer | begin training epoch 7
2020-10-11 22:31:07 | INFO | train_inner | epoch 007:     62 / 173 loss=8.565, nll_loss=7.625, ppl=197.41, wps=18531.8, ups=2.77, wpb=6701.9, bsz=264.2, num_updates=1100, lr=5.50725e-05, gnorm=1.405, clip=0, train_wall=31, wall=365
2020-10-11 22:31:40 | INFO | train_inner | epoch 007:    162 / 173 loss=8.426, nll_loss=7.467, ppl=176.97, wps=20852.1, ups=3.09, wpb=6752.6, bsz=253.7, num_updates=1200, lr=6.007e-05, gnorm=1.414, clip=0, train_wall=32, wall=397
2020-10-11 22:31:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000740
2020-10-11 22:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027863
2020-10-11 22:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022155
2020-10-11 22:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051102
2020-10-11 22:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000580
2020-10-11 22:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027293
2020-10-11 22:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021995
2020-10-11 22:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050188
2020-10-11 22:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:31:45 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.108 | nll_loss 7.054 | ppl 132.91 | wps 48389.8 | wpb 2417.7 | bsz 91.9 | num_updates 1211 | best_loss 8.108
2020-10-11 22:31:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:31:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 7 @ 1211 updates, score 8.108) (writing took 1.494717794994358 seconds)
2020-10-11 22:31:47 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-11 22:31:47 | INFO | train | epoch 007 | loss 8.47 | nll_loss 7.517 | ppl 183.18 | wps 19642.6 | ups 2.91 | wpb 6755.6 | bsz 257.2 | num_updates 1211 | lr 6.06197e-05 | gnorm 1.396 | clip 0 | train_wall 55 | wall 404
2020-10-11 22:31:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-11 22:31:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-11 22:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:31:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000529
2020-10-11 22:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005873
2020-10-11 22:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000287
2020-10-11 22:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.110089
2020-10-11 22:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.116609
2020-10-11 22:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004343
2020-10-11 22:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-11 22:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102776
2020-10-11 22:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107626
2020-10-11 22:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:31:47 | INFO | fairseq.trainer | begin training epoch 8
2020-10-11 22:32:16 | INFO | train_inner | epoch 008:     89 / 173 loss=8.328, nll_loss=7.355, ppl=163.75, wps=18800, ups=2.75, wpb=6825.2, bsz=260, num_updates=1300, lr=6.50675e-05, gnorm=1.356, clip=0, train_wall=32, wall=433
2020-10-11 22:32:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000731
2020-10-11 22:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028350
2020-10-11 22:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022351
2020-10-11 22:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051769
2020-10-11 22:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000573
2020-10-11 22:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028364
2020-10-11 22:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021799
2020-10-11 22:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051064
2020-10-11 22:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:32:45 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.919 | nll_loss 6.843 | ppl 114.8 | wps 48008.9 | wpb 2417.7 | bsz 91.9 | num_updates 1384 | best_loss 7.919
2020-10-11 22:32:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:32:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 8 @ 1384 updates, score 7.919) (writing took 1.8048975620040437 seconds)
2020-10-11 22:32:47 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-11 22:32:47 | INFO | train | epoch 008 | loss 8.267 | nll_loss 7.287 | ppl 156.13 | wps 19373.9 | ups 2.87 | wpb 6755.6 | bsz 257.2 | num_updates 1384 | lr 6.92654e-05 | gnorm 1.384 | clip 0 | train_wall 55 | wall 464
2020-10-11 22:32:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-11 22:32:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-11 22:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:32:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000469
2020-10-11 22:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005294
2020-10-11 22:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-11 22:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104264
2020-10-11 22:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110065
2020-10-11 22:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004220
2020-10-11 22:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-11 22:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102011
2020-10-11 22:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106725
2020-10-11 22:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:32:47 | INFO | fairseq.trainer | begin training epoch 9
2020-10-11 22:32:52 | INFO | train_inner | epoch 009:     16 / 173 loss=8.212, nll_loss=7.223, ppl=149.38, wps=18327.4, ups=2.74, wpb=6693.3, bsz=251, num_updates=1400, lr=7.0065e-05, gnorm=1.431, clip=0, train_wall=32, wall=470
2020-10-11 22:33:25 | INFO | train_inner | epoch 009:    116 / 173 loss=8.078, nll_loss=7.07, ppl=134.34, wps=20682.4, ups=3.06, wpb=6761.9, bsz=269.9, num_updates=1500, lr=7.50625e-05, gnorm=1.324, clip=0, train_wall=32, wall=503
2020-10-11 22:33:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:33:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:33:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000743
2020-10-11 22:33:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:33:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028052
2020-10-11 22:33:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:33:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022473
2020-10-11 22:33:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051616
2020-10-11 22:33:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:33:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:33:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000581
2020-10-11 22:33:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:33:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027623
2020-10-11 22:33:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:33:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022516
2020-10-11 22:33:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051042
2020-10-11 22:33:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:33:46 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.735 | nll_loss 6.625 | ppl 98.67 | wps 47820.7 | wpb 2417.7 | bsz 91.9 | num_updates 1557 | best_loss 7.735
2020-10-11 22:33:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:33:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 9 @ 1557 updates, score 7.735) (writing took 1.9094498139893403 seconds)
2020-10-11 22:33:48 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-11 22:33:48 | INFO | train | epoch 009 | loss 8.09 | nll_loss 7.083 | ppl 135.6 | wps 19275 | ups 2.85 | wpb 6755.6 | bsz 257.2 | num_updates 1557 | lr 7.79111e-05 | gnorm 1.351 | clip 0 | train_wall 55 | wall 525
2020-10-11 22:33:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-11 22:33:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-11 22:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:33:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000486
2020-10-11 22:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005317
2020-10-11 22:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-11 22:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104735
2020-10-11 22:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110549
2020-10-11 22:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004212
2020-10-11 22:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-11 22:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102695
2020-10-11 22:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107403
2020-10-11 22:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:33:48 | INFO | fairseq.trainer | begin training epoch 10
2020-10-11 22:34:02 | INFO | train_inner | epoch 010:     43 / 173 loss=8.033, nll_loss=7.017, ppl=129.55, wps=18417.4, ups=2.73, wpb=6754.2, bsz=246.6, num_updates=1600, lr=8.006e-05, gnorm=1.472, clip=0, train_wall=32, wall=539
2020-10-11 22:34:35 | INFO | train_inner | epoch 010:    143 / 173 loss=7.929, nll_loss=6.899, ppl=119.34, wps=20357.2, ups=3.04, wpb=6701.9, bsz=262.2, num_updates=1700, lr=8.50575e-05, gnorm=1.243, clip=0, train_wall=32, wall=572
2020-10-11 22:34:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000676
2020-10-11 22:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027256
2020-10-11 22:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022025
2020-10-11 22:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050297
2020-10-11 22:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000573
2020-10-11 22:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027246
2020-10-11 22:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021910
2020-10-11 22:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050055
2020-10-11 22:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:34:47 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.623 | nll_loss 6.496 | ppl 90.25 | wps 47111.1 | wpb 2417.7 | bsz 91.9 | num_updates 1730 | best_loss 7.623
2020-10-11 22:34:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:34:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 10 @ 1730 updates, score 7.623) (writing took 1.9538904210057808 seconds)
2020-10-11 22:34:49 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-11 22:34:49 | INFO | train | epoch 010 | loss 7.944 | nll_loss 6.916 | ppl 120.74 | wps 19158.1 | ups 2.84 | wpb 6755.6 | bsz 257.2 | num_updates 1730 | lr 8.65568e-05 | gnorm 1.332 | clip 0 | train_wall 56 | wall 586
2020-10-11 22:34:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-11 22:34:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-11 22:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:34:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000535
2020-10-11 22:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005443
2020-10-11 22:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-11 22:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103881
2020-10-11 22:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109836
2020-10-11 22:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004299
2020-10-11 22:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-11 22:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101468
2020-10-11 22:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106280
2020-10-11 22:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:34:49 | INFO | fairseq.trainer | begin training epoch 11
2020-10-11 22:35:12 | INFO | train_inner | epoch 011:     70 / 173 loss=7.835, nll_loss=6.791, ppl=110.77, wps=18513.2, ups=2.68, wpb=6898.9, bsz=260.2, num_updates=1800, lr=9.0055e-05, gnorm=1.236, clip=0, train_wall=32, wall=609
2020-10-11 22:35:45 | INFO | train_inner | epoch 011:    170 / 173 loss=7.828, nll_loss=6.781, ppl=110, wps=20243.5, ups=3.03, wpb=6679.7, bsz=252.2, num_updates=1900, lr=9.50525e-05, gnorm=1.217, clip=0, train_wall=32, wall=642
2020-10-11 22:35:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000714
2020-10-11 22:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027809
2020-10-11 22:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021533
2020-10-11 22:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050389
2020-10-11 22:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000575
2020-10-11 22:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027913
2020-10-11 22:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021472
2020-10-11 22:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050281
2020-10-11 22:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:35:48 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.513 | nll_loss 6.365 | ppl 82.42 | wps 46947.8 | wpb 2417.7 | bsz 91.9 | num_updates 1903 | best_loss 7.513
2020-10-11 22:35:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:35:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 11 @ 1903 updates, score 7.513) (writing took 1.8353025399992475 seconds)
2020-10-11 22:35:50 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-11 22:35:50 | INFO | train | epoch 011 | loss 7.803 | nll_loss 6.753 | ppl 107.86 | wps 19117 | ups 2.83 | wpb 6755.6 | bsz 257.2 | num_updates 1903 | lr 9.52024e-05 | gnorm 1.234 | clip 0 | train_wall 56 | wall 647
2020-10-11 22:35:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-11 22:35:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-11 22:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:35:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000497
2020-10-11 22:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005299
2020-10-11 22:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 22:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105259
2020-10-11 22:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111072
2020-10-11 22:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004402
2020-10-11 22:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-11 22:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101607
2020-10-11 22:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106538
2020-10-11 22:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:35:50 | INFO | fairseq.trainer | begin training epoch 12
2020-10-11 22:36:22 | INFO | train_inner | epoch 012:     97 / 173 loss=7.682, nll_loss=6.614, ppl=97.96, wps=18446.1, ups=2.7, wpb=6839.5, bsz=266, num_updates=2000, lr=0.00010005, gnorm=1.341, clip=0, train_wall=32, wall=679
2020-10-11 22:36:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:36:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:36:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000684
2020-10-11 22:36:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:36:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027370
2020-10-11 22:36:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:36:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021862
2020-10-11 22:36:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050255
2020-10-11 22:36:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:36:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:36:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000575
2020-10-11 22:36:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:36:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027633
2020-10-11 22:36:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:36:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022212
2020-10-11 22:36:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050742
2020-10-11 22:36:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:36:49 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.424 | nll_loss 6.261 | ppl 76.69 | wps 47151.8 | wpb 2417.7 | bsz 91.9 | num_updates 2076 | best_loss 7.424
2020-10-11 22:36:49 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:36:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 12 @ 2076 updates, score 7.424) (writing took 3.322107120999135 seconds)
2020-10-11 22:36:53 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-11 22:36:53 | INFO | train | epoch 012 | loss 7.69 | nll_loss 6.623 | ppl 98.57 | wps 18622.4 | ups 2.76 | wpb 6755.6 | bsz 257.2 | num_updates 2076 | lr 0.000103848 | gnorm 1.294 | clip 0 | train_wall 56 | wall 710
2020-10-11 22:36:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-11 22:36:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-11 22:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:36:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000526
2020-10-11 22:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005481
2020-10-11 22:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-11 22:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103481
2020-10-11 22:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109477
2020-10-11 22:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004287
2020-10-11 22:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 22:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102525
2020-10-11 22:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107312
2020-10-11 22:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:36:53 | INFO | fairseq.trainer | begin training epoch 13
2020-10-11 22:37:01 | INFO | train_inner | epoch 013:     24 / 173 loss=7.689, nll_loss=6.622, ppl=98.47, wps=17248.9, ups=2.59, wpb=6659.8, bsz=240.5, num_updates=2100, lr=0.000105048, gnorm=1.249, clip=0, train_wall=32, wall=718
2020-10-11 22:37:34 | INFO | train_inner | epoch 013:    124 / 173 loss=7.582, nll_loss=6.499, ppl=90.47, wps=20472.5, ups=3.04, wpb=6743.3, bsz=261.9, num_updates=2200, lr=0.000110045, gnorm=1.211, clip=0, train_wall=32, wall=751
2020-10-11 22:37:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000677
2020-10-11 22:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028093
2020-10-11 22:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022246
2020-10-11 22:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051359
2020-10-11 22:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000578
2020-10-11 22:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028519
2020-10-11 22:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021931
2020-10-11 22:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051354
2020-10-11 22:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:37:52 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.334 | nll_loss 6.166 | ppl 71.78 | wps 47050.8 | wpb 2417.7 | bsz 91.9 | num_updates 2249 | best_loss 7.334
2020-10-11 22:37:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:37:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 13 @ 2249 updates, score 7.334) (writing took 1.509344415011583 seconds)
2020-10-11 22:37:53 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-11 22:37:53 | INFO | train | epoch 013 | loss 7.571 | nll_loss 6.486 | ppl 89.65 | wps 19200.9 | ups 2.84 | wpb 6755.6 | bsz 257.2 | num_updates 2249 | lr 0.000112494 | gnorm 1.241 | clip 0 | train_wall 56 | wall 771
2020-10-11 22:37:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-11 22:37:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-11 22:37:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:37:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000565
2020-10-11 22:37:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005881
2020-10-11 22:37:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:37:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-11 22:37:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101898
2020-10-11 22:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108280
2020-10-11 22:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004223
2020-10-11 22:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-11 22:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098554
2020-10-11 22:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103265
2020-10-11 22:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:37:54 | INFO | fairseq.trainer | begin training epoch 14
2020-10-11 22:38:11 | INFO | train_inner | epoch 014:     51 / 173 loss=7.505, nll_loss=6.411, ppl=85.09, wps=18420.9, ups=2.7, wpb=6816, bsz=269.4, num_updates=2300, lr=0.000115043, gnorm=1.303, clip=0, train_wall=33, wall=788
2020-10-11 22:38:44 | INFO | train_inner | epoch 014:    151 / 173 loss=7.432, nll_loss=6.327, ppl=80.27, wps=20265.5, ups=3.03, wpb=6697, bsz=252.2, num_updates=2400, lr=0.00012004, gnorm=1.201, clip=0, train_wall=32, wall=821
2020-10-11 22:38:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000757
2020-10-11 22:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028185
2020-10-11 22:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022186
2020-10-11 22:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051467
2020-10-11 22:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000547
2020-10-11 22:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027123
2020-10-11 22:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022089
2020-10-11 22:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050079
2020-10-11 22:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:38:53 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.228 | nll_loss 6.052 | ppl 66.33 | wps 47064.9 | wpb 2417.7 | bsz 91.9 | num_updates 2422 | best_loss 7.228
2020-10-11 22:38:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:38:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 14 @ 2422 updates, score 7.228) (writing took 1.928890388997388 seconds)
2020-10-11 22:38:55 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-11 22:38:55 | INFO | train | epoch 014 | loss 7.469 | nll_loss 6.368 | ppl 82.62 | wps 19002.1 | ups 2.81 | wpb 6755.6 | bsz 257.2 | num_updates 2422 | lr 0.000121139 | gnorm 1.242 | clip 0 | train_wall 56 | wall 832
2020-10-11 22:38:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-11 22:38:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-11 22:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:38:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000454
2020-10-11 22:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005982
2020-10-11 22:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000196
2020-10-11 22:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.111121
2020-10-11 22:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.117679
2020-10-11 22:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004347
2020-10-11 22:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-11 22:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102626
2020-10-11 22:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107502
2020-10-11 22:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:38:55 | INFO | fairseq.trainer | begin training epoch 15
2020-10-11 22:39:21 | INFO | train_inner | epoch 015:     78 / 173 loss=7.405, nll_loss=6.295, ppl=78.53, wps=18302.8, ups=2.68, wpb=6840.3, bsz=255.2, num_updates=2500, lr=0.000125037, gnorm=1.211, clip=0, train_wall=33, wall=858
2020-10-11 22:39:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000761
2020-10-11 22:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027551
2020-10-11 22:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022652
2020-10-11 22:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051309
2020-10-11 22:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000547
2020-10-11 22:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028144
2020-10-11 22:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022231
2020-10-11 22:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051245
2020-10-11 22:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:39:55 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.178 | nll_loss 5.996 | ppl 63.83 | wps 47014.3 | wpb 2417.7 | bsz 91.9 | num_updates 2595 | best_loss 7.178
2020-10-11 22:39:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:39:56 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 15 @ 2595 updates, score 7.178) (writing took 1.4888295020064106 seconds)
2020-10-11 22:39:56 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-11 22:39:56 | INFO | train | epoch 015 | loss 7.361 | nll_loss 6.246 | ppl 75.88 | wps 19125.4 | ups 2.83 | wpb 6755.6 | bsz 257.2 | num_updates 2595 | lr 0.000129785 | gnorm 1.19 | clip 0 | train_wall 56 | wall 893
2020-10-11 22:39:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-11 22:39:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-11 22:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:39:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000598
2020-10-11 22:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006177
2020-10-11 22:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000197
2020-10-11 22:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103083
2020-10-11 22:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109830
2020-10-11 22:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004261
2020-10-11 22:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-11 22:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101136
2020-10-11 22:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105898
2020-10-11 22:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:39:56 | INFO | fairseq.trainer | begin training epoch 16
2020-10-11 22:39:58 | INFO | train_inner | epoch 016:      5 / 173 loss=7.36, nll_loss=6.244, ppl=75.78, wps=18113.4, ups=2.72, wpb=6671.1, bsz=252, num_updates=2600, lr=0.000130035, gnorm=1.18, clip=0, train_wall=32, wall=895
2020-10-11 22:40:31 | INFO | train_inner | epoch 016:    105 / 173 loss=7.254, nll_loss=6.122, ppl=69.64, wps=20250.1, ups=3.04, wpb=6659.2, bsz=249.1, num_updates=2700, lr=0.000135032, gnorm=1.181, clip=0, train_wall=32, wall=928
2020-10-11 22:40:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000738
2020-10-11 22:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027413
2020-10-11 22:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022493
2020-10-11 22:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050984
2020-10-11 22:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000547
2020-10-11 22:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027318
2020-10-11 22:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022145
2020-10-11 22:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050331
2020-10-11 22:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:40:56 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.077 | nll_loss 5.864 | ppl 58.24 | wps 46632.1 | wpb 2417.7 | bsz 91.9 | num_updates 2768 | best_loss 7.077
2020-10-11 22:40:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:40:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 16 @ 2768 updates, score 7.077) (writing took 2.3163503109972226 seconds)
2020-10-11 22:40:58 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-11 22:40:58 | INFO | train | epoch 016 | loss 7.258 | nll_loss 6.127 | ppl 69.88 | wps 18845.2 | ups 2.79 | wpb 6755.6 | bsz 257.2 | num_updates 2768 | lr 0.000138431 | gnorm 1.169 | clip 0 | train_wall 56 | wall 955
2020-10-11 22:40:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-11 22:40:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-11 22:40:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:40:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000563
2020-10-11 22:40:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006082
2020-10-11 22:40:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:40:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-11 22:40:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:40:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.114174
2020-10-11 22:40:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.120878
2020-10-11 22:40:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:40:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:40:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004299
2020-10-11 22:40:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:40:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-11 22:40:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:40:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102394
2020-10-11 22:40:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107182
2020-10-11 22:40:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:40:58 | INFO | fairseq.trainer | begin training epoch 17
2020-10-11 22:41:09 | INFO | train_inner | epoch 017:     32 / 173 loss=7.188, nll_loss=6.047, ppl=66.11, wps=17775.6, ups=2.64, wpb=6739.5, bsz=274.5, num_updates=2800, lr=0.00014003, gnorm=1.141, clip=0, train_wall=33, wall=966
2020-10-11 22:41:42 | INFO | train_inner | epoch 017:    132 / 173 loss=7.179, nll_loss=6.035, ppl=65.57, wps=20483.9, ups=2.99, wpb=6845.1, bsz=264.6, num_updates=2900, lr=0.000145028, gnorm=1.257, clip=0, train_wall=33, wall=999
2020-10-11 22:41:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000742
2020-10-11 22:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027697
2020-10-11 22:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021997
2020-10-11 22:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050804
2020-10-11 22:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000581
2020-10-11 22:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027835
2020-10-11 22:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021707
2020-10-11 22:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050446
2020-10-11 22:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:41:58 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.999 | nll_loss 5.767 | ppl 54.46 | wps 47202.6 | wpb 2417.7 | bsz 91.9 | num_updates 2941 | best_loss 6.999
2020-10-11 22:41:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:41:59 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 17 @ 2941 updates, score 6.999) (writing took 1.4896194059983827 seconds)
2020-10-11 22:41:59 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-11 22:41:59 | INFO | train | epoch 017 | loss 7.168 | nll_loss 6.022 | ppl 64.98 | wps 19116.3 | ups 2.83 | wpb 6755.6 | bsz 257.2 | num_updates 2941 | lr 0.000147076 | gnorm 1.222 | clip 0 | train_wall 56 | wall 1017
2020-10-11 22:41:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-11 22:41:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-11 22:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:41:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000650
2020-10-11 22:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006019
2020-10-11 22:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-11 22:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.107767
2020-10-11 22:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114399
2020-10-11 22:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004396
2020-10-11 22:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-11 22:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104136
2020-10-11 22:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109042
2020-10-11 22:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:41:59 | INFO | fairseq.trainer | begin training epoch 18
2020-10-11 22:42:19 | INFO | train_inner | epoch 018:     59 / 173 loss=7.125, nll_loss=5.972, ppl=62.75, wps=18463.5, ups=2.72, wpb=6794.1, bsz=242.7, num_updates=3000, lr=0.000150025, gnorm=1.172, clip=0, train_wall=32, wall=1036
2020-10-11 22:42:52 | INFO | train_inner | epoch 018:    159 / 173 loss=7.043, nll_loss=5.879, ppl=58.86, wps=20272.6, ups=3, wpb=6760.5, bsz=264.2, num_updates=3100, lr=0.000155023, gnorm=1.18, clip=0, train_wall=33, wall=1070
2020-10-11 22:42:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000687
2020-10-11 22:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027741
2020-10-11 22:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021918
2020-10-11 22:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050677
2020-10-11 22:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000570
2020-10-11 22:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.026952
2020-10-11 22:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021968
2020-10-11 22:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049807
2020-10-11 22:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:42:59 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.92 | nll_loss 5.676 | ppl 51.12 | wps 47003.1 | wpb 2417.7 | bsz 91.9 | num_updates 3114 | best_loss 6.92
2020-10-11 22:42:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:43:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 18 @ 3114 updates, score 6.92) (writing took 1.832321816997137 seconds)
2020-10-11 22:43:01 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-11 22:43:01 | INFO | train | epoch 018 | loss 7.051 | nll_loss 5.888 | ppl 59.23 | wps 18973.6 | ups 2.81 | wpb 6755.6 | bsz 257.2 | num_updates 3114 | lr 0.000155722 | gnorm 1.175 | clip 0 | train_wall 56 | wall 1078
2020-10-11 22:43:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-11 22:43:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-11 22:43:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:43:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000463
2020-10-11 22:43:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005321
2020-10-11 22:43:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:43:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-11 22:43:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:43:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102520
2020-10-11 22:43:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108349
2020-10-11 22:43:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:43:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:43:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004406
2020-10-11 22:43:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:43:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 22:43:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:43:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101884
2020-10-11 22:43:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106798
2020-10-11 22:43:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:43:01 | INFO | fairseq.trainer | begin training epoch 19
2020-10-11 22:43:29 | INFO | train_inner | epoch 019:     86 / 173 loss=6.962, nll_loss=5.787, ppl=55.22, wps=18015.7, ups=2.69, wpb=6706.5, bsz=254.4, num_updates=3200, lr=0.00016002, gnorm=1.17, clip=0, train_wall=33, wall=1107
2020-10-11 22:43:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:43:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:43:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000710
2020-10-11 22:43:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028017
2020-10-11 22:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022559
2020-10-11 22:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051626
2020-10-11 22:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000554
2020-10-11 22:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027778
2020-10-11 22:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022084
2020-10-11 22:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050732
2020-10-11 22:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:44:00 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.795 | nll_loss 5.519 | ppl 45.86 | wps 46644 | wpb 2417.7 | bsz 91.9 | num_updates 3287 | best_loss 6.795
2020-10-11 22:44:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:44:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 19 @ 3287 updates, score 6.795) (writing took 1.4829230570030631 seconds)
2020-10-11 22:44:02 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-11 22:44:02 | INFO | train | epoch 019 | loss 6.926 | nll_loss 5.745 | ppl 53.61 | wps 19102.9 | ups 2.83 | wpb 6755.6 | bsz 257.2 | num_updates 3287 | lr 0.000164368 | gnorm 1.139 | clip 0 | train_wall 56 | wall 1139
2020-10-11 22:44:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-11 22:44:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-11 22:44:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:44:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000574
2020-10-11 22:44:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005690
2020-10-11 22:44:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:44:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-11 22:44:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:44:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104156
2020-10-11 22:44:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110432
2020-10-11 22:44:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:44:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:44:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004229
2020-10-11 22:44:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:44:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 22:44:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:44:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102454
2020-10-11 22:44:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107182
2020-10-11 22:44:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:44:02 | INFO | fairseq.trainer | begin training epoch 20
2020-10-11 22:44:07 | INFO | train_inner | epoch 020:     13 / 173 loss=6.911, nll_loss=5.727, ppl=52.96, wps=18324.8, ups=2.7, wpb=6795.3, bsz=250.9, num_updates=3300, lr=0.000165018, gnorm=1.133, clip=0, train_wall=33, wall=1144
2020-10-11 22:44:40 | INFO | train_inner | epoch 020:    113 / 173 loss=6.834, nll_loss=5.638, ppl=49.81, wps=20269, ups=3.01, wpb=6724.6, bsz=250, num_updates=3400, lr=0.000170015, gnorm=1.2, clip=0, train_wall=33, wall=1177
2020-10-11 22:45:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000678
2020-10-11 22:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027951
2020-10-11 22:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022214
2020-10-11 22:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051175
2020-10-11 22:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000580
2020-10-11 22:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027863
2020-10-11 22:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022220
2020-10-11 22:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050981
2020-10-11 22:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:45:02 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.714 | nll_loss 5.428 | ppl 43.04 | wps 46835.7 | wpb 2417.7 | bsz 91.9 | num_updates 3460 | best_loss 6.714
2020-10-11 22:45:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:45:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 20 @ 3460 updates, score 6.714) (writing took 1.4876383780065225 seconds)
2020-10-11 22:45:03 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-11 22:45:03 | INFO | train | epoch 020 | loss 6.814 | nll_loss 5.615 | ppl 49.02 | wps 19059 | ups 2.82 | wpb 6755.6 | bsz 257.2 | num_updates 3460 | lr 0.000173013 | gnorm 1.185 | clip 0 | train_wall 57 | wall 1201
2020-10-11 22:45:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-11 22:45:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-11 22:45:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:45:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000621
2020-10-11 22:45:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006074
2020-10-11 22:45:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:45:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 22:45:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:45:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109141
2020-10-11 22:45:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115747
2020-10-11 22:45:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:45:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:45:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004403
2020-10-11 22:45:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:45:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-11 22:45:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102201
2020-10-11 22:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107104
2020-10-11 22:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:45:04 | INFO | fairseq.trainer | begin training epoch 21
2020-10-11 22:45:17 | INFO | train_inner | epoch 021:     40 / 173 loss=6.764, nll_loss=5.558, ppl=47.1, wps=18341.4, ups=2.69, wpb=6807.9, bsz=263.2, num_updates=3500, lr=0.000175013, gnorm=1.148, clip=0, train_wall=33, wall=1214
2020-10-11 22:45:50 | INFO | train_inner | epoch 021:    140 / 173 loss=6.718, nll_loss=5.504, ppl=45.39, wps=20188.7, ups=3.02, wpb=6690.6, bsz=235.6, num_updates=3600, lr=0.00018001, gnorm=1.171, clip=0, train_wall=33, wall=1247
2020-10-11 22:46:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000683
2020-10-11 22:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027946
2020-10-11 22:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022252
2020-10-11 22:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051214
2020-10-11 22:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000585
2020-10-11 22:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028275
2020-10-11 22:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022171
2020-10-11 22:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051353
2020-10-11 22:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:46:03 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.643 | nll_loss 5.325 | ppl 40.09 | wps 46751 | wpb 2417.7 | bsz 91.9 | num_updates 3633 | best_loss 6.643
2020-10-11 22:46:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:46:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 21 @ 3633 updates, score 6.643) (writing took 1.8527218130038818 seconds)
2020-10-11 22:46:05 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-11 22:46:05 | INFO | train | epoch 021 | loss 6.696 | nll_loss 5.48 | ppl 44.62 | wps 18925.3 | ups 2.8 | wpb 6755.6 | bsz 257.2 | num_updates 3633 | lr 0.000181659 | gnorm 1.188 | clip 0 | train_wall 57 | wall 1262
2020-10-11 22:46:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-11 22:46:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-11 22:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:46:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000493
2020-10-11 22:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005303
2020-10-11 22:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-11 22:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103114
2020-10-11 22:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108914
2020-10-11 22:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004287
2020-10-11 22:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-11 22:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101083
2020-10-11 22:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105861
2020-10-11 22:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:46:05 | INFO | fairseq.trainer | begin training epoch 22
2020-10-11 22:46:28 | INFO | train_inner | epoch 022:     67 / 173 loss=6.576, nll_loss=5.342, ppl=40.56, wps=18164.3, ups=2.66, wpb=6837, bsz=282.2, num_updates=3700, lr=0.000185008, gnorm=1.232, clip=0, train_wall=33, wall=1285
2020-10-11 22:47:01 | INFO | train_inner | epoch 022:    167 / 173 loss=6.553, nll_loss=5.314, ppl=39.77, wps=20169.6, ups=2.99, wpb=6739.6, bsz=261.9, num_updates=3800, lr=0.000190005, gnorm=1.163, clip=0, train_wall=33, wall=1318
2020-10-11 22:47:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000656
2020-10-11 22:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027388
2020-10-11 22:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022387
2020-10-11 22:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050766
2020-10-11 22:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000514
2020-10-11 22:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027512
2020-10-11 22:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022617
2020-10-11 22:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050958
2020-10-11 22:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:47:05 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.523 | nll_loss 5.198 | ppl 36.72 | wps 46964.3 | wpb 2417.7 | bsz 91.9 | num_updates 3806 | best_loss 6.523
2020-10-11 22:47:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:47:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 22 @ 3806 updates, score 6.523) (writing took 1.5260376730002463 seconds)
2020-10-11 22:47:07 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-11 22:47:07 | INFO | train | epoch 022 | loss 6.577 | nll_loss 5.342 | ppl 40.56 | wps 19000.7 | ups 2.81 | wpb 6755.6 | bsz 257.2 | num_updates 3806 | lr 0.000190305 | gnorm 1.176 | clip 0 | train_wall 57 | wall 1324
2020-10-11 22:47:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-11 22:47:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-11 22:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:47:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000573
2020-10-11 22:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006025
2020-10-11 22:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-11 22:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109753
2020-10-11 22:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.116350
2020-10-11 22:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004223
2020-10-11 22:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-11 22:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099882
2020-10-11 22:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104597
2020-10-11 22:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:47:07 | INFO | fairseq.trainer | begin training epoch 23
2020-10-11 22:47:38 | INFO | train_inner | epoch 023:     94 / 173 loss=6.458, nll_loss=5.206, ppl=36.92, wps=18108.6, ups=2.71, wpb=6686.1, bsz=255.9, num_updates=3900, lr=0.000195003, gnorm=1.181, clip=0, train_wall=33, wall=1355
2020-10-11 22:48:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000752
2020-10-11 22:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027010
2020-10-11 22:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:48:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021721
2020-10-11 22:48:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049818
2020-10-11 22:48:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:48:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:48:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000570
2020-10-11 22:48:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:48:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027370
2020-10-11 22:48:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:48:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021912
2020-10-11 22:48:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050169
2020-10-11 22:48:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:48:06 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.467 | nll_loss 5.116 | ppl 34.69 | wps 46609.6 | wpb 2417.7 | bsz 91.9 | num_updates 3979 | best_loss 6.467
2020-10-11 22:48:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:48:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 23 @ 3979 updates, score 6.467) (writing took 1.9305008619994624 seconds)
2020-10-11 22:48:08 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-11 22:48:08 | INFO | train | epoch 023 | loss 6.445 | nll_loss 5.19 | ppl 36.5 | wps 18901.1 | ups 2.8 | wpb 6755.6 | bsz 257.2 | num_updates 3979 | lr 0.000198951 | gnorm 1.182 | clip 0 | train_wall 57 | wall 1386
2020-10-11 22:48:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-11 22:48:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-11 22:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:48:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000522
2020-10-11 22:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005421
2020-10-11 22:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000202
2020-10-11 22:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102617
2020-10-11 22:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108588
2020-10-11 22:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004263
2020-10-11 22:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-11 22:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100539
2020-10-11 22:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105287
2020-10-11 22:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:48:09 | INFO | fairseq.trainer | begin training epoch 24
2020-10-11 22:48:16 | INFO | train_inner | epoch 024:     21 / 173 loss=6.424, nll_loss=5.164, ppl=35.86, wps=18085.9, ups=2.66, wpb=6798.2, bsz=250.3, num_updates=4000, lr=0.0002, gnorm=1.17, clip=0, train_wall=33, wall=1393
2020-10-11 22:48:49 | INFO | train_inner | epoch 024:    121 / 173 loss=6.326, nll_loss=5.052, ppl=33.17, wps=20230.9, ups=3, wpb=6733.1, bsz=259, num_updates=4100, lr=0.000197546, gnorm=1.22, clip=0, train_wall=33, wall=1426
2020-10-11 22:49:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:49:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:49:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000744
2020-10-11 22:49:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:49:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027722
2020-10-11 22:49:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:49:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022123
2020-10-11 22:49:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050933
2020-10-11 22:49:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:49:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:49:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000567
2020-10-11 22:49:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:49:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027584
2020-10-11 22:49:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:49:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022146
2020-10-11 22:49:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050623
2020-10-11 22:49:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:49:08 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.337 | nll_loss 4.961 | ppl 31.15 | wps 46728 | wpb 2417.7 | bsz 91.9 | num_updates 4152 | best_loss 6.337
2020-10-11 22:49:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:49:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 24 @ 4152 updates, score 6.337) (writing took 2.369879813006264 seconds)
2020-10-11 22:49:11 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-11 22:49:11 | INFO | train | epoch 024 | loss 6.321 | nll_loss 5.045 | ppl 33.02 | wps 18739.8 | ups 2.77 | wpb 6755.6 | bsz 257.2 | num_updates 4152 | lr 0.000196305 | gnorm 1.195 | clip 0 | train_wall 57 | wall 1448
2020-10-11 22:49:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-11 22:49:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-11 22:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:49:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000453
2020-10-11 22:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005609
2020-10-11 22:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000203
2020-10-11 22:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109606
2020-10-11 22:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115781
2020-10-11 22:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004408
2020-10-11 22:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-11 22:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102037
2020-10-11 22:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106959
2020-10-11 22:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:49:11 | INFO | fairseq.trainer | begin training epoch 25
2020-10-11 22:49:27 | INFO | train_inner | epoch 025:     48 / 173 loss=6.227, nll_loss=4.939, ppl=30.67, wps=17858.5, ups=2.61, wpb=6829.5, bsz=269.4, num_updates=4200, lr=0.00019518, gnorm=1.17, clip=0, train_wall=33, wall=1464
2020-10-11 22:50:00 | INFO | train_inner | epoch 025:    148 / 173 loss=6.209, nll_loss=4.916, ppl=30.19, wps=20165.9, ups=3.01, wpb=6707.1, bsz=246.6, num_updates=4300, lr=0.000192897, gnorm=1.14, clip=0, train_wall=33, wall=1498
2020-10-11 22:50:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:50:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:50:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000742
2020-10-11 22:50:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:50:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027099
2020-10-11 22:50:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:50:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022423
2020-10-11 22:50:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050610
2020-10-11 22:50:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:50:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:50:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000570
2020-10-11 22:50:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:50:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027944
2020-10-11 22:50:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:50:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022073
2020-10-11 22:50:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050913
2020-10-11 22:50:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:50:11 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.261 | nll_loss 4.87 | ppl 29.24 | wps 47014.1 | wpb 2417.7 | bsz 91.9 | num_updates 4325 | best_loss 6.261
2020-10-11 22:50:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:50:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 25 @ 4325 updates, score 6.261) (writing took 1.4883814039931167 seconds)
2020-10-11 22:50:12 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-11 22:50:12 | INFO | train | epoch 025 | loss 6.169 | nll_loss 4.87 | ppl 29.24 | wps 19042.4 | ups 2.82 | wpb 6755.6 | bsz 257.2 | num_updates 4325 | lr 0.000192339 | gnorm 1.148 | clip 0 | train_wall 57 | wall 1510
2020-10-11 22:50:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-11 22:50:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-11 22:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:50:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000552
2020-10-11 22:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005698
2020-10-11 22:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-11 22:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102332
2020-10-11 22:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108612
2020-10-11 22:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004271
2020-10-11 22:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 22:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102308
2020-10-11 22:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107079
2020-10-11 22:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:50:12 | INFO | fairseq.trainer | begin training epoch 26
2020-10-11 22:50:37 | INFO | train_inner | epoch 026:     75 / 173 loss=6.092, nll_loss=4.781, ppl=27.49, wps=18198.5, ups=2.7, wpb=6748.7, bsz=253.8, num_updates=4400, lr=0.000190693, gnorm=1.245, clip=0, train_wall=33, wall=1535
2020-10-11 22:51:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000706
2020-10-11 22:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027625
2020-10-11 22:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022541
2020-10-11 22:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051205
2020-10-11 22:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000566
2020-10-11 22:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028212
2020-10-11 22:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022292
2020-10-11 22:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051389
2020-10-11 22:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:51:12 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.192 | nll_loss 4.775 | ppl 27.37 | wps 46759 | wpb 2417.7 | bsz 91.9 | num_updates 4498 | best_loss 6.192
2020-10-11 22:51:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:51:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 26 @ 4498 updates, score 6.192) (writing took 1.9451422579877544 seconds)
2020-10-11 22:51:14 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-11 22:51:14 | INFO | train | epoch 026 | loss 6.07 | nll_loss 4.755 | ppl 26.99 | wps 18882.5 | ups 2.8 | wpb 6755.6 | bsz 257.2 | num_updates 4498 | lr 0.000188604 | gnorm 1.242 | clip 0 | train_wall 57 | wall 1571
2020-10-11 22:51:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-11 22:51:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-11 22:51:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:51:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000556
2020-10-11 22:51:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005650
2020-10-11 22:51:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:51:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-11 22:51:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:51:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105649
2020-10-11 22:51:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111864
2020-10-11 22:51:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:51:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:51:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004422
2020-10-11 22:51:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:51:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-11 22:51:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:51:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101584
2020-10-11 22:51:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106526
2020-10-11 22:51:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:51:14 | INFO | fairseq.trainer | begin training epoch 27
2020-10-11 22:51:15 | INFO | train_inner | epoch 027:      2 / 173 loss=6.056, nll_loss=4.738, ppl=26.69, wps=18022, ups=2.67, wpb=6761.8, bsz=259.9, num_updates=4500, lr=0.000188562, gnorm=1.207, clip=0, train_wall=33, wall=1572
2020-10-11 22:51:48 | INFO | train_inner | epoch 027:    102 / 173 loss=5.922, nll_loss=4.585, ppl=24, wps=20308.2, ups=3, wpb=6763.7, bsz=261.7, num_updates=4600, lr=0.000186501, gnorm=1.091, clip=0, train_wall=33, wall=1606
2020-10-11 22:52:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:52:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:52:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000682
2020-10-11 22:52:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:52:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027743
2020-10-11 22:52:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:52:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022080
2020-10-11 22:52:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050836
2020-10-11 22:52:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:52:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:52:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000600
2020-10-11 22:52:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:52:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028226
2020-10-11 22:52:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:52:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022440
2020-10-11 22:52:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051587
2020-10-11 22:52:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:52:14 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.118 | nll_loss 4.701 | ppl 26.01 | wps 47191.3 | wpb 2417.7 | bsz 91.9 | num_updates 4671 | best_loss 6.118
2020-10-11 22:52:14 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:52:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 27 @ 4671 updates, score 6.118) (writing took 1.484328728998662 seconds)
2020-10-11 22:52:15 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-11 22:52:15 | INFO | train | epoch 027 | loss 5.928 | nll_loss 4.59 | ppl 24.08 | wps 19019.9 | ups 2.82 | wpb 6755.6 | bsz 257.2 | num_updates 4671 | lr 0.000185078 | gnorm 1.127 | clip 0 | train_wall 57 | wall 1633
2020-10-11 22:52:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-11 22:52:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-11 22:52:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:52:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000467
2020-10-11 22:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005903
2020-10-11 22:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000254
2020-10-11 22:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104967
2020-10-11 22:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111462
2020-10-11 22:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004339
2020-10-11 22:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-11 22:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099916
2020-10-11 22:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104765
2020-10-11 22:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:52:16 | INFO | fairseq.trainer | begin training epoch 28
2020-10-11 22:52:25 | INFO | train_inner | epoch 028:     29 / 173 loss=5.862, nll_loss=4.513, ppl=22.84, wps=18329, ups=2.7, wpb=6792.7, bsz=261.4, num_updates=4700, lr=0.000184506, gnorm=1.156, clip=0, train_wall=33, wall=1643
2020-10-11 22:52:59 | INFO | train_inner | epoch 028:    129 / 173 loss=5.863, nll_loss=4.514, ppl=22.85, wps=20081.6, ups=3, wpb=6698.9, bsz=250.3, num_updates=4800, lr=0.000182574, gnorm=1.276, clip=0, train_wall=33, wall=1676
2020-10-11 22:53:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000695
2020-10-11 22:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028190
2020-10-11 22:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022275
2020-10-11 22:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051516
2020-10-11 22:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000572
2020-10-11 22:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028024
2020-10-11 22:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022220
2020-10-11 22:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051135
2020-10-11 22:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:53:16 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.066 | nll_loss 4.621 | ppl 24.6 | wps 46410.3 | wpb 2417.7 | bsz 91.9 | num_updates 4844 | best_loss 6.066
2020-10-11 22:53:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:53:17 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 28 @ 4844 updates, score 6.066) (writing took 1.4842692140082363 seconds)
2020-10-11 22:53:17 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-11 22:53:17 | INFO | train | epoch 028 | loss 5.828 | nll_loss 4.474 | ppl 22.22 | wps 18996.3 | ups 2.81 | wpb 6755.6 | bsz 257.2 | num_updates 4844 | lr 0.000181743 | gnorm 1.204 | clip 0 | train_wall 57 | wall 1694
2020-10-11 22:53:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-11 22:53:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-11 22:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:53:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000524
2020-10-11 22:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005974
2020-10-11 22:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-11 22:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.107062
2020-10-11 22:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113563
2020-10-11 22:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004341
2020-10-11 22:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-11 22:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101406
2020-10-11 22:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106258
2020-10-11 22:53:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:53:17 | INFO | fairseq.trainer | begin training epoch 29
2020-10-11 22:53:36 | INFO | train_inner | epoch 029:     56 / 173 loss=5.751, nll_loss=4.385, ppl=20.89, wps=18253.2, ups=2.68, wpb=6819.5, bsz=257.9, num_updates=4900, lr=0.000180702, gnorm=1.092, clip=0, train_wall=33, wall=1713
2020-10-11 22:54:09 | INFO | train_inner | epoch 029:    156 / 173 loss=5.726, nll_loss=4.354, ppl=20.45, wps=20009.2, ups=3, wpb=6672.1, bsz=257.3, num_updates=5000, lr=0.000178885, gnorm=1.216, clip=0, train_wall=33, wall=1747
2020-10-11 22:54:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000741
2020-10-11 22:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028239
2020-10-11 22:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022383
2020-10-11 22:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051696
2020-10-11 22:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000514
2020-10-11 22:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027879
2020-10-11 22:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022074
2020-10-11 22:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050784
2020-10-11 22:54:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:54:17 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.026 | nll_loss 4.568 | ppl 23.71 | wps 47010.4 | wpb 2417.7 | bsz 91.9 | num_updates 5017 | best_loss 6.026
2020-10-11 22:54:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:54:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 29 @ 5017 updates, score 6.026) (writing took 1.6972054880025098 seconds)
2020-10-11 22:54:19 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-11 22:54:19 | INFO | train | epoch 029 | loss 5.711 | nll_loss 4.338 | ppl 20.23 | wps 18907.1 | ups 2.8 | wpb 6755.6 | bsz 257.2 | num_updates 5017 | lr 0.000178582 | gnorm 1.149 | clip 0 | train_wall 57 | wall 1756
2020-10-11 22:54:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-11 22:54:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-11 22:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:54:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000437
2020-10-11 22:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005410
2020-10-11 22:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000220
2020-10-11 22:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102596
2020-10-11 22:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108564
2020-10-11 22:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004310
2020-10-11 22:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-11 22:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102522
2020-10-11 22:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107361
2020-10-11 22:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:54:19 | INFO | fairseq.trainer | begin training epoch 30
2020-10-11 22:54:47 | INFO | train_inner | epoch 030:     83 / 173 loss=5.633, nll_loss=4.248, ppl=19, wps=18208.9, ups=2.68, wpb=6785, bsz=257.9, num_updates=5100, lr=0.000177123, gnorm=1.217, clip=0, train_wall=33, wall=1784
2020-10-11 22:55:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000768
2020-10-11 22:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028188
2020-10-11 22:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022232
2020-10-11 22:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051523
2020-10-11 22:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000549
2020-10-11 22:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028023
2020-10-11 22:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022213
2020-10-11 22:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051101
2020-10-11 22:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:55:19 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.982 | nll_loss 4.515 | ppl 22.87 | wps 46491.2 | wpb 2417.7 | bsz 91.9 | num_updates 5190 | best_loss 5.982
2020-10-11 22:55:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:55:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 30 @ 5190 updates, score 5.982) (writing took 1.9331915749935433 seconds)
2020-10-11 22:55:21 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-11 22:55:21 | INFO | train | epoch 030 | loss 5.627 | nll_loss 4.24 | ppl 18.89 | wps 18857.2 | ups 2.79 | wpb 6755.6 | bsz 257.2 | num_updates 5190 | lr 0.000175581 | gnorm 1.223 | clip 0 | train_wall 57 | wall 1818
2020-10-11 22:55:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-11 22:55:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-11 22:55:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:55:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000555
2020-10-11 22:55:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005581
2020-10-11 22:55:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:55:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-11 22:55:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:55:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105162
2020-10-11 22:55:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111233
2020-10-11 22:55:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:55:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:55:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004396
2020-10-11 22:55:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:55:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-11 22:55:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:55:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102494
2020-10-11 22:55:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107391
2020-10-11 22:55:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:55:21 | INFO | fairseq.trainer | begin training epoch 31
2020-10-11 22:55:24 | INFO | train_inner | epoch 031:     10 / 173 loss=5.611, nll_loss=4.22, ppl=18.63, wps=17776.3, ups=2.66, wpb=6676.9, bsz=254.4, num_updates=5200, lr=0.000175412, gnorm=1.173, clip=0, train_wall=33, wall=1822
2020-10-11 22:55:58 | INFO | train_inner | epoch 031:    110 / 173 loss=5.567, nll_loss=4.17, ppl=18, wps=20280.8, ups=3, wpb=6765.9, bsz=239.8, num_updates=5300, lr=0.000173749, gnorm=1.215, clip=0, train_wall=33, wall=1855
2020-10-11 22:56:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:56:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:56:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.006905
2020-10-11 22:56:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:56:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028809
2020-10-11 22:56:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:56:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022289
2020-10-11 22:56:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.058357
2020-10-11 22:56:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:56:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:56:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000551
2020-10-11 22:56:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:56:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027999
2020-10-11 22:56:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:56:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022268
2020-10-11 22:56:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051139
2020-10-11 22:56:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:56:21 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 5.973 | nll_loss 4.497 | ppl 22.58 | wps 46664.2 | wpb 2417.7 | bsz 91.9 | num_updates 5363 | best_loss 5.973
2020-10-11 22:56:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:56:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 31 @ 5363 updates, score 5.973) (writing took 1.9121476519940188 seconds)
2020-10-11 22:56:23 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-11 22:56:23 | INFO | train | epoch 031 | loss 5.529 | nll_loss 4.125 | ppl 17.45 | wps 18855 | ups 2.79 | wpb 6755.6 | bsz 257.2 | num_updates 5363 | lr 0.000172725 | gnorm 1.208 | clip 0 | train_wall 57 | wall 1880
2020-10-11 22:56:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-11 22:56:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-11 22:56:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:56:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000516
2020-10-11 22:56:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005398
2020-10-11 22:56:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:56:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-11 22:56:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:56:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101793
2020-10-11 22:56:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107686
2020-10-11 22:56:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:56:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:56:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004272
2020-10-11 22:56:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:56:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-11 22:56:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:56:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100853
2020-10-11 22:56:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105624
2020-10-11 22:56:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:56:23 | INFO | fairseq.trainer | begin training epoch 32
2020-10-11 22:56:35 | INFO | train_inner | epoch 032:     37 / 173 loss=5.475, nll_loss=4.061, ppl=16.7, wps=18175.4, ups=2.65, wpb=6862.1, bsz=269.8, num_updates=5400, lr=0.000172133, gnorm=1.185, clip=0, train_wall=33, wall=1893
2020-10-11 22:57:09 | INFO | train_inner | epoch 032:    137 / 173 loss=5.452, nll_loss=4.035, ppl=16.4, wps=20043, ups=2.99, wpb=6706.3, bsz=258.4, num_updates=5500, lr=0.000170561, gnorm=1.201, clip=0, train_wall=33, wall=1926
2020-10-11 22:57:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000691
2020-10-11 22:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028162
2020-10-11 22:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022000
2020-10-11 22:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051189
2020-10-11 22:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000548
2020-10-11 22:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027537
2020-10-11 22:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022339
2020-10-11 22:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050749
2020-10-11 22:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:57:23 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.917 | nll_loss 4.432 | ppl 21.58 | wps 47190 | wpb 2417.7 | bsz 91.9 | num_updates 5536 | best_loss 5.917
2020-10-11 22:57:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:57:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 32 @ 5536 updates, score 5.917) (writing took 1.793273783987388 seconds)
2020-10-11 22:57:25 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-11 22:57:25 | INFO | train | epoch 032 | loss 5.437 | nll_loss 4.018 | ppl 16.2 | wps 18902.5 | ups 2.8 | wpb 6755.6 | bsz 257.2 | num_updates 5536 | lr 0.000170005 | gnorm 1.165 | clip 0 | train_wall 57 | wall 1942
2020-10-11 22:57:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-11 22:57:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-11 22:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:57:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000515
2020-10-11 22:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005370
2020-10-11 22:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-11 22:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104423
2020-10-11 22:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110293
2020-10-11 22:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004341
2020-10-11 22:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-11 22:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099931
2020-10-11 22:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104765
2020-10-11 22:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:57:25 | INFO | fairseq.trainer | begin training epoch 33
2020-10-11 22:57:46 | INFO | train_inner | epoch 033:     64 / 173 loss=5.341, nll_loss=3.908, ppl=15.01, wps=18256.9, ups=2.67, wpb=6834.9, bsz=269.7, num_updates=5600, lr=0.000169031, gnorm=1.145, clip=0, train_wall=33, wall=1964
2020-10-11 22:58:20 | INFO | train_inner | epoch 033:    164 / 173 loss=5.37, nll_loss=3.938, ppl=15.32, wps=20150.4, ups=3, wpb=6716.6, bsz=252, num_updates=5700, lr=0.000167542, gnorm=1.17, clip=0, train_wall=33, wall=1997
2020-10-11 22:58:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000694
2020-10-11 22:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027916
2020-10-11 22:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022631
2020-10-11 22:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051572
2020-10-11 22:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000567
2020-10-11 22:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027544
2020-10-11 22:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022630
2020-10-11 22:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051063
2020-10-11 22:58:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:58:25 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 5.898 | nll_loss 4.393 | ppl 21 | wps 46696.6 | wpb 2417.7 | bsz 91.9 | num_updates 5709 | best_loss 5.898
2020-10-11 22:58:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:58:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 33 @ 5709 updates, score 5.898) (writing took 1.4897797809971962 seconds)
2020-10-11 22:58:26 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-11 22:58:26 | INFO | train | epoch 033 | loss 5.357 | nll_loss 3.924 | ppl 15.18 | wps 19003.5 | ups 2.81 | wpb 6755.6 | bsz 257.2 | num_updates 5709 | lr 0.000167409 | gnorm 1.169 | clip 0 | train_wall 57 | wall 2003
2020-10-11 22:58:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-11 22:58:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-11 22:58:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:58:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000567
2020-10-11 22:58:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006011
2020-10-11 22:58:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:58:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000197
2020-10-11 22:58:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:58:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106841
2020-10-11 22:58:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113389
2020-10-11 22:58:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:58:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:58:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004291
2020-10-11 22:58:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:58:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-11 22:58:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:58:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100696
2020-10-11 22:58:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105476
2020-10-11 22:58:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:58:26 | INFO | fairseq.trainer | begin training epoch 34
2020-10-11 22:58:57 | INFO | train_inner | epoch 034:     91 / 173 loss=5.282, nll_loss=3.838, ppl=14.3, wps=18087.6, ups=2.69, wpb=6723.9, bsz=261.3, num_updates=5800, lr=0.000166091, gnorm=1.257, clip=0, train_wall=33, wall=2034
2020-10-11 22:59:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000691
2020-10-11 22:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027697
2020-10-11 22:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022596
2020-10-11 22:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051325
2020-10-11 22:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000577
2020-10-11 22:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027918
2020-10-11 22:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022548
2020-10-11 22:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051362
2020-10-11 22:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:59:26 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 5.908 | nll_loss 4.404 | ppl 21.17 | wps 46295.1 | wpb 2417.7 | bsz 91.9 | num_updates 5882 | best_loss 5.898
2020-10-11 22:59:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:59:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_last.pt (epoch 34 @ 5882 updates, score 5.908) (writing took 1.1817718999955105 seconds)
2020-10-11 22:59:27 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-11 22:59:27 | INFO | train | epoch 034 | loss 5.286 | nll_loss 3.84 | ppl 14.33 | wps 19073.6 | ups 2.82 | wpb 6755.6 | bsz 257.2 | num_updates 5882 | lr 0.000164929 | gnorm 1.218 | clip 0 | train_wall 57 | wall 2065
2020-10-11 22:59:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-11 22:59:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-11 22:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:59:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000488
2020-10-11 22:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005464
2020-10-11 22:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 22:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:59:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105506
2020-10-11 22:59:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111470
2020-10-11 22:59:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:59:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:59:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004341
2020-10-11 22:59:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:59:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 22:59:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:59:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104145
2020-10-11 22:59:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108991
2020-10-11 22:59:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:59:28 | INFO | fairseq.trainer | begin training epoch 35
2020-10-11 22:59:34 | INFO | train_inner | epoch 035:     18 / 173 loss=5.305, nll_loss=3.861, ppl=14.53, wps=18278.7, ups=2.71, wpb=6740.2, bsz=244.3, num_updates=5900, lr=0.000164677, gnorm=1.176, clip=0, train_wall=33, wall=2071
2020-10-11 23:00:07 | INFO | train_inner | epoch 035:    118 / 173 loss=5.209, nll_loss=3.751, ppl=13.46, wps=20183.7, ups=2.99, wpb=6750.1, bsz=256.9, num_updates=6000, lr=0.000163299, gnorm=1.177, clip=0, train_wall=33, wall=2104
2020-10-11 23:00:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000693
2020-10-11 23:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027910
2020-10-11 23:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022302
2020-10-11 23:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051244
2020-10-11 23:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000566
2020-10-11 23:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027902
2020-10-11 23:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022440
2020-10-11 23:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051231
2020-10-11 23:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:00:28 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 5.877 | nll_loss 4.363 | ppl 20.58 | wps 46176 | wpb 2417.7 | bsz 91.9 | num_updates 6055 | best_loss 5.877
2020-10-11 23:00:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:00:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 35 @ 6055 updates, score 5.877) (writing took 1.49208088699379 seconds)
2020-10-11 23:00:29 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-11 23:00:29 | INFO | train | epoch 035 | loss 5.208 | nll_loss 3.75 | ppl 13.46 | wps 18927.5 | ups 2.8 | wpb 6755.6 | bsz 257.2 | num_updates 6055 | lr 0.000162556 | gnorm 1.186 | clip 0 | train_wall 57 | wall 2126
2020-10-11 23:00:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-11 23:00:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-11 23:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:00:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000658
2020-10-11 23:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006000
2020-10-11 23:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000248
2020-10-11 23:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105491
2020-10-11 23:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112103
2020-10-11 23:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004249
2020-10-11 23:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000164
2020-10-11 23:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103293
2020-10-11 23:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108028
2020-10-11 23:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:00:29 | INFO | fairseq.trainer | begin training epoch 36
2020-10-11 23:00:45 | INFO | train_inner | epoch 036:     45 / 173 loss=5.149, nll_loss=3.682, ppl=12.84, wps=18336.7, ups=2.67, wpb=6866.9, bsz=268.1, num_updates=6100, lr=0.000161955, gnorm=1.134, clip=0, train_wall=33, wall=2142
2020-10-11 23:01:18 | INFO | train_inner | epoch 036:    145 / 173 loss=5.126, nll_loss=3.654, ppl=12.59, wps=19954.6, ups=3, wpb=6661.1, bsz=259.3, num_updates=6200, lr=0.000160644, gnorm=1.2, clip=0, train_wall=33, wall=2175
2020-10-11 23:01:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000624
2020-10-11 23:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027307
2020-10-11 23:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021936
2020-10-11 23:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050195
2020-10-11 23:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000517
2020-10-11 23:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027452
2020-10-11 23:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022064
2020-10-11 23:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050350
2020-10-11 23:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:01:29 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 5.814 | nll_loss 4.287 | ppl 19.53 | wps 46703.9 | wpb 2417.7 | bsz 91.9 | num_updates 6228 | best_loss 5.814
2020-10-11 23:01:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:01:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 36 @ 6228 updates, score 5.814) (writing took 1.4999459649989149 seconds)
2020-10-11 23:01:31 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-11 23:01:31 | INFO | train | epoch 036 | loss 5.131 | nll_loss 3.66 | ppl 12.64 | wps 18939.6 | ups 2.8 | wpb 6755.6 | bsz 257.2 | num_updates 6228 | lr 0.000160282 | gnorm 1.145 | clip 0 | train_wall 57 | wall 2188
2020-10-11 23:01:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-11 23:01:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-11 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:01:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000438
2020-10-11 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005941
2020-10-11 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000250
2020-10-11 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105616
2020-10-11 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112141
2020-10-11 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004310
2020-10-11 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-11 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102233
2020-10-11 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107048
2020-10-11 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:01:31 | INFO | fairseq.trainer | begin training epoch 37
2020-10-11 23:01:55 | INFO | train_inner | epoch 037:     72 / 173 loss=5.085, nll_loss=3.607, ppl=12.18, wps=18075, ups=2.69, wpb=6718, bsz=244.1, num_updates=6300, lr=0.000159364, gnorm=1.155, clip=0, train_wall=33, wall=2212
2020-10-11 23:02:29 | INFO | train_inner | epoch 037:    172 / 173 loss=5.089, nll_loss=3.609, ppl=12.2, wps=20288.5, ups=2.98, wpb=6801.9, bsz=262.9, num_updates=6400, lr=0.000158114, gnorm=1.174, clip=0, train_wall=33, wall=2246
2020-10-11 23:02:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:02:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:02:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000748
2020-10-11 23:02:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:02:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028047
2020-10-11 23:02:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:02:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022470
2020-10-11 23:02:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051610
2020-10-11 23:02:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:02:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:02:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000546
2020-10-11 23:02:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:02:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027770
2020-10-11 23:02:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:02:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022127
2020-10-11 23:02:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050768
2020-10-11 23:02:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:02:31 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 5.884 | nll_loss 4.367 | ppl 20.63 | wps 46737.1 | wpb 2417.7 | bsz 91.9 | num_updates 6401 | best_loss 5.814
2020-10-11 23:02:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:02:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_last.pt (epoch 37 @ 6401 updates, score 5.884) (writing took 1.0348879720113473 seconds)
2020-10-11 23:02:32 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-11 23:02:32 | INFO | train | epoch 037 | loss 5.066 | nll_loss 3.583 | ppl 11.99 | wps 19119.1 | ups 2.83 | wpb 6755.6 | bsz 257.2 | num_updates 6401 | lr 0.000158102 | gnorm 1.183 | clip 0 | train_wall 57 | wall 2249
2020-10-11 23:02:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-11 23:02:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-11 23:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:02:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000543
2020-10-11 23:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005670
2020-10-11 23:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-11 23:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103827
2020-10-11 23:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110013
2020-10-11 23:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004329
2020-10-11 23:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000209
2020-10-11 23:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103460
2020-10-11 23:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108339
2020-10-11 23:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:02:32 | INFO | fairseq.trainer | begin training epoch 38
2020-10-11 23:03:05 | INFO | train_inner | epoch 038:     99 / 173 loss=4.957, nll_loss=3.458, ppl=10.99, wps=18541.8, ups=2.72, wpb=6808.7, bsz=270.3, num_updates=6500, lr=0.000156893, gnorm=1.143, clip=0, train_wall=33, wall=2283
2020-10-11 23:03:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000769
2020-10-11 23:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027457
2020-10-11 23:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022519
2020-10-11 23:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051086
2020-10-11 23:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000544
2020-10-11 23:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028070
2020-10-11 23:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022424
2020-10-11 23:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051382
2020-10-11 23:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:03:32 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 5.793 | nll_loss 4.258 | ppl 19.13 | wps 46188 | wpb 2417.7 | bsz 91.9 | num_updates 6574 | best_loss 5.793
2020-10-11 23:03:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:03:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 38 @ 6574 updates, score 5.793) (writing took 1.9120225429942366 seconds)
2020-10-11 23:03:34 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-11 23:03:34 | INFO | train | epoch 038 | loss 4.994 | nll_loss 3.5 | ppl 11.31 | wps 18828 | ups 2.79 | wpb 6755.6 | bsz 257.2 | num_updates 6574 | lr 0.000156007 | gnorm 1.144 | clip 0 | train_wall 57 | wall 2311
2020-10-11 23:03:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-11 23:03:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-11 23:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:03:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000833
2020-10-11 23:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006580
2020-10-11 23:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-11 23:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106369
2020-10-11 23:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113477
2020-10-11 23:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004347
2020-10-11 23:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-11 23:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101276
2020-10-11 23:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106121
2020-10-11 23:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:03:34 | INFO | fairseq.trainer | begin training epoch 39
2020-10-11 23:03:43 | INFO | train_inner | epoch 039:     26 / 173 loss=5.017, nll_loss=3.525, ppl=11.51, wps=17795.7, ups=2.66, wpb=6699.9, bsz=239.4, num_updates=6600, lr=0.0001557, gnorm=1.159, clip=0, train_wall=33, wall=2320
2020-10-11 23:04:16 | INFO | train_inner | epoch 039:    126 / 173 loss=4.955, nll_loss=3.452, ppl=10.95, wps=20238.2, ups=3, wpb=6751.7, bsz=257.7, num_updates=6700, lr=0.000154533, gnorm=1.242, clip=0, train_wall=33, wall=2354
2020-10-11 23:04:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000748
2020-10-11 23:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027736
2020-10-11 23:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022447
2020-10-11 23:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051273
2020-10-11 23:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000552
2020-10-11 23:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027565
2020-10-11 23:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022278
2020-10-11 23:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050715
2020-10-11 23:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:04:34 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 5.778 | nll_loss 4.241 | ppl 18.92 | wps 46431.2 | wpb 2417.7 | bsz 91.9 | num_updates 6747 | best_loss 5.778
2020-10-11 23:04:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:04:36 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 39 @ 6747 updates, score 5.778) (writing took 1.8149307380081154 seconds)
2020-10-11 23:04:36 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-11 23:04:36 | INFO | train | epoch 039 | loss 4.944 | nll_loss 3.44 | ppl 10.85 | wps 18881.1 | ups 2.79 | wpb 6755.6 | bsz 257.2 | num_updates 6747 | lr 0.000153994 | gnorm 1.195 | clip 0 | train_wall 57 | wall 2373
2020-10-11 23:04:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-11 23:04:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-11 23:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:04:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000514
2020-10-11 23:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005432
2020-10-11 23:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-11 23:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102614
2020-10-11 23:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108541
2020-10-11 23:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004336
2020-10-11 23:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-11 23:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101217
2020-10-11 23:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106046
2020-10-11 23:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:04:36 | INFO | fairseq.trainer | begin training epoch 40
2020-10-11 23:04:54 | INFO | train_inner | epoch 040:     53 / 173 loss=4.88, nll_loss=3.367, ppl=10.32, wps=18099.5, ups=2.66, wpb=6795.9, bsz=270.4, num_updates=6800, lr=0.000153393, gnorm=1.14, clip=0, train_wall=33, wall=2391
2020-10-11 23:05:27 | INFO | train_inner | epoch 040:    153 / 173 loss=4.876, nll_loss=3.361, ppl=10.27, wps=20055.7, ups=2.99, wpb=6708, bsz=253.6, num_updates=6900, lr=0.000152277, gnorm=1.154, clip=0, train_wall=33, wall=2425
2020-10-11 23:05:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000698
2020-10-11 23:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027904
2020-10-11 23:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.022279
2020-10-11 23:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051218
2020-10-11 23:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000547
2020-10-11 23:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027174
2020-10-11 23:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021995
2020-10-11 23:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050032
2020-10-11 23:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:05:36 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 5.744 | nll_loss 4.201 | ppl 18.39 | wps 46414.2 | wpb 2417.7 | bsz 91.9 | num_updates 6920 | best_loss 5.744
2020-10-11 23:05:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:05:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/M2O/checkpoint_best.pt (epoch 40 @ 6920 updates, score 5.744) (writing took 1.848426130003645 seconds)
2020-10-11 23:05:38 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-11 23:05:38 | INFO | train | epoch 040 | loss 4.876 | nll_loss 3.362 | ppl 10.28 | wps 18888.2 | ups 2.8 | wpb 6755.6 | bsz 257.2 | num_updates 6920 | lr 0.000152057 | gnorm 1.158 | clip 0 | train_wall 57 | wall 2435
2020-10-11 23:05:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-11 23:05:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-11 23:05:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:05:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000580
2020-10-11 23:05:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005607
2020-10-11 23:05:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:05:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-11 23:05:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:05:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102852
2020-10-11 23:05:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108960
2020-10-11 23:05:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:05:38 | INFO | fairseq_cli.train | done training in 2435.2 seconds
