2020-10-11 23:08:12 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_belukr_sepspm8000/O2M/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok='tgt', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='eng-bel,eng-ukr', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_belukr_sepspm8000/O2M/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-11 23:08:12 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-11 23:08:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['bel', 'eng', 'ukr']
2020-10-11 23:08:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bel] dictionary: 21683 types
2020-10-11 23:08:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 21683 types
2020-10-11 23:08:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | [ukr] dictionary: 21683 types
2020-10-11 23:08:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-11 23:08:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:08:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-11 23:08:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:eng-bel': 1, 'main:eng-ukr': 1}
2020-10-11 23:08:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-bel src_langtok: 21680; tgt_langtok: None
2020-10-11 23:08:12 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belukr_sepspm8000/O2M/valid.eng-bel.eng
2020-10-11 23:08:12 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belukr_sepspm8000/O2M/valid.eng-bel.bel
2020-10-11 23:08:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belukr_sepspm8000/O2M/ valid eng-bel 248 examples
2020-10-11 23:08:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-ukr src_langtok: 21682; tgt_langtok: None
2020-10-11 23:08:12 | INFO | fairseq.data.data_utils | loaded 3060 examples from: fairseq/data-bin/ted_belukr_sepspm8000/O2M/valid.eng-ukr.eng
2020-10-11 23:08:12 | INFO | fairseq.data.data_utils | loaded 3060 examples from: fairseq/data-bin/ted_belukr_sepspm8000/O2M/valid.eng-ukr.ukr
2020-10-11 23:08:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belukr_sepspm8000/O2M/ valid eng-ukr 3060 examples
2020-10-11 23:08:13 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21683, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21683, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=21683, bias=False)
  )
)
2020-10-11 23:08:13 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-11 23:08:13 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-11 23:08:13 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-11 23:08:13 | INFO | fairseq_cli.train | num. model params: 42644992 (num. trained: 42644992)
2020-10-11 23:08:15 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-11 23:08:15 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-11 23:08:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-11 23:08:15 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.732 GB ; name = GeForce RTX 2080 Ti                     
2020-10-11 23:08:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-11 23:08:15 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-11 23:08:15 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-11 23:08:15 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_last.pt
2020-10-11 23:08:15 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-11 23:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-11 23:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:08:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-11 23:08:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:eng-bel': 1, 'main:eng-ukr': 1}
2020-10-11 23:08:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-bel src_langtok: 21680; tgt_langtok: None
2020-10-11 23:08:15 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belukr_sepspm8000/O2M/train.eng-bel.eng
2020-10-11 23:08:15 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belukr_sepspm8000/O2M/train.eng-bel.bel
2020-10-11 23:08:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belukr_sepspm8000/O2M/ train eng-bel 4509 examples
2020-10-11 23:08:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-ukr src_langtok: 21682; tgt_langtok: None
2020-10-11 23:08:15 | INFO | fairseq.data.data_utils | loaded 39988 examples from: fairseq/data-bin/ted_belukr_sepspm8000/O2M/train.eng-ukr.eng
2020-10-11 23:08:15 | INFO | fairseq.data.data_utils | loaded 39988 examples from: fairseq/data-bin/ted_belukr_sepspm8000/O2M/train.eng-ukr.ukr
2020-10-11 23:08:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belukr_sepspm8000/O2M/ train eng-ukr 39988 examples
2020-10-11 23:08:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:eng-bel', 4509), ('main:eng-ukr', 39988)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-11 23:08:15 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-11 23:08:15 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 44497
2020-10-11 23:08:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 44497; virtual dataset size 44497
2020-10-11 23:08:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-11 23:08:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:eng-bel': 4509, 'main:eng-ukr': 39988}; raw total size: 44497
2020-10-11 23:08:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:eng-bel': 4509, 'main:eng-ukr': 39988}; resampled total size: 44497
2020-10-11 23:08:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-11 23:08:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.003127
2020-10-11 23:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:08:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000413
2020-10-11 23:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005175
2020-10-11 23:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-11 23:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096947
2020-10-11 23:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102648
2020-10-11 23:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:08:15 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-11 23:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004191
2020-10-11 23:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-11 23:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096437
2020-10-11 23:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101139
2020-10-11 23:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:08:15 | INFO | fairseq.trainer | begin training epoch 1
2020-10-11 23:08:45 | INFO | train_inner | epoch 001:    100 / 178 loss=14.186, nll_loss=14.078, ppl=17288.3, wps=22321.8, ups=3.35, wpb=6668.9, bsz=259.1, num_updates=100, lr=5.0975e-06, gnorm=4.389, clip=0, train_wall=30, wall=30
2020-10-11 23:09:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:09:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:09:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000694
2020-10-11 23:09:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:09:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027475
2020-10-11 23:09:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021671
2020-10-11 23:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050181
2020-10-11 23:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000545
2020-10-11 23:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027390
2020-10-11 23:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.020714
2020-10-11 23:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.048975
2020-10-11 23:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
/home/han/Documents/11737-hw/assign2/fairseq/fairseq/utils.py:340: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2020-10-11 23:09:10 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.445 | nll_loss 12.112 | ppl 4427.94 | wps 48824.2 | wpb 2226.9 | bsz 84.8 | num_updates 178
2020-10-11 23:09:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:09:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 1 @ 178 updates, score 12.445) (writing took 0.9473458529973868 seconds)
2020-10-11 23:09:11 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-11 23:09:11 | INFO | train | epoch 001 | loss 13.667 | nll_loss 13.496 | ppl 11557.1 | wps 20646.7 | ups 3.17 | wpb 6509.7 | bsz 250 | num_updates 178 | lr 8.99555e-06 | gnorm 3.277 | clip 0 | train_wall 53 | wall 57
2020-10-11 23:09:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-11 23:09:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-11 23:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:09:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000560
2020-10-11 23:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005544
2020-10-11 23:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-11 23:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096505
2020-10-11 23:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102565
2020-10-11 23:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004241
2020-10-11 23:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-11 23:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097752
2020-10-11 23:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102486
2020-10-11 23:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:09:12 | INFO | fairseq.trainer | begin training epoch 2
2020-10-11 23:09:18 | INFO | train_inner | epoch 002:     22 / 178 loss=12.901, nll_loss=12.639, ppl=6379.63, wps=19412.8, ups=3.01, wpb=6459.5, bsz=251.3, num_updates=200, lr=1.0095e-05, gnorm=1.76, clip=0, train_wall=30, wall=64
2020-10-11 23:09:49 | INFO | train_inner | epoch 002:    122 / 178 loss=12.247, nll_loss=11.912, ppl=3852.87, wps=20766.8, ups=3.27, wpb=6350.7, bsz=240.2, num_updates=300, lr=1.50925e-05, gnorm=1.519, clip=0, train_wall=30, wall=94
2020-10-11 23:10:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000669
2020-10-11 23:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027544
2020-10-11 23:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:10:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021262
2020-10-11 23:10:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049810
2020-10-11 23:10:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:10:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:10:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000544
2020-10-11 23:10:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:10:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028038
2020-10-11 23:10:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:10:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.020843
2020-10-11 23:10:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049746
2020-10-11 23:10:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:10:08 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.226 | nll_loss 10.736 | ppl 1704.99 | wps 47295.1 | wpb 2226.9 | bsz 84.8 | num_updates 356 | best_loss 11.226
2020-10-11 23:10:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:10:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 2 @ 356 updates, score 11.226) (writing took 2.220440157005214 seconds)
2020-10-11 23:10:11 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-11 23:10:11 | INFO | train | epoch 002 | loss 12.144 | nll_loss 11.797 | ppl 3557.78 | wps 19525.7 | ups 3 | wpb 6509.7 | bsz 250 | num_updates 356 | lr 1.78911e-05 | gnorm 1.51 | clip 0 | train_wall 54 | wall 116
2020-10-11 23:10:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-11 23:10:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-11 23:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:10:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000675
2020-10-11 23:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006232
2020-10-11 23:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-11 23:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103126
2020-10-11 23:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109881
2020-10-11 23:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004209
2020-10-11 23:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-11 23:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096211
2020-10-11 23:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100912
2020-10-11 23:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:10:11 | INFO | fairseq.trainer | begin training epoch 3
2020-10-11 23:10:25 | INFO | train_inner | epoch 003:     44 / 178 loss=11.554, nll_loss=11.131, ppl=2242.76, wps=18375.9, ups=2.79, wpb=6578.4, bsz=248.9, num_updates=400, lr=2.009e-05, gnorm=1.566, clip=0, train_wall=31, wall=130
2020-10-11 23:10:57 | INFO | train_inner | epoch 003:    144 / 178 loss=10.907, nll_loss=10.374, ppl=1327.1, wps=20428.5, ups=3.15, wpb=6487.5, bsz=251.3, num_updates=500, lr=2.50875e-05, gnorm=1.466, clip=0, train_wall=31, wall=162
2020-10-11 23:11:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:11:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:11:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000683
2020-10-11 23:11:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:11:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027900
2020-10-11 23:11:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:11:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021456
2020-10-11 23:11:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050375
2020-10-11 23:11:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:11:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:11:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000548
2020-10-11 23:11:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:11:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027171
2020-10-11 23:11:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:11:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021421
2020-10-11 23:11:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049464
2020-10-11 23:11:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:11:09 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.298 | nll_loss 9.627 | ppl 790.86 | wps 45985.2 | wpb 2226.9 | bsz 84.8 | num_updates 534 | best_loss 10.298
2020-10-11 23:11:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:11:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 3 @ 534 updates, score 10.298) (writing took 1.496278822000022 seconds)
2020-10-11 23:11:11 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-11 23:11:11 | INFO | train | epoch 003 | loss 10.957 | nll_loss 10.431 | ppl 1380.86 | wps 19233.4 | ups 2.95 | wpb 6509.7 | bsz 250 | num_updates 534 | lr 2.67866e-05 | gnorm 1.428 | clip 0 | train_wall 56 | wall 176
2020-10-11 23:11:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-11 23:11:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-11 23:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:11:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000571
2020-10-11 23:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005999
2020-10-11 23:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 23:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099732
2020-10-11 23:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106246
2020-10-11 23:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004265
2020-10-11 23:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-11 23:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098233
2020-10-11 23:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102995
2020-10-11 23:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:11:11 | INFO | fairseq.trainer | begin training epoch 4
2020-10-11 23:11:32 | INFO | train_inner | epoch 004:     66 / 178 loss=10.551, nll_loss=9.939, ppl=981.45, wps=18273.4, ups=2.79, wpb=6544.7, bsz=251.2, num_updates=600, lr=3.0085e-05, gnorm=1.197, clip=0, train_wall=32, wall=198
2020-10-11 23:12:05 | INFO | train_inner | epoch 004:    166 / 178 loss=10.372, nll_loss=9.712, ppl=838.89, wps=20183.3, ups=3.08, wpb=6562.6, bsz=251.2, num_updates=700, lr=3.50825e-05, gnorm=1.172, clip=0, train_wall=32, wall=230
2020-10-11 23:12:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.006849
2020-10-11 23:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028511
2020-10-11 23:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021262
2020-10-11 23:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.056960
2020-10-11 23:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000560
2020-10-11 23:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027749
2020-10-11 23:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021153
2020-10-11 23:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049780
2020-10-11 23:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:12:11 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.025 | nll_loss 9.275 | ppl 619.34 | wps 45206.8 | wpb 2226.9 | bsz 84.8 | num_updates 712 | best_loss 10.025
2020-10-11 23:12:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:12:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 4 @ 712 updates, score 10.025) (writing took 1.8350188199983677 seconds)
2020-10-11 23:12:13 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-11 23:12:13 | INFO | train | epoch 004 | loss 10.415 | nll_loss 9.767 | ppl 871.52 | wps 18811.3 | ups 2.89 | wpb 6509.7 | bsz 250 | num_updates 712 | lr 3.56822e-05 | gnorm 1.243 | clip 0 | train_wall 57 | wall 238
2020-10-11 23:12:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-11 23:12:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-11 23:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:12:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000554
2020-10-11 23:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005551
2020-10-11 23:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 23:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099167
2020-10-11 23:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105216
2020-10-11 23:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004468
2020-10-11 23:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-11 23:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097652
2020-10-11 23:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102632
2020-10-11 23:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:12:13 | INFO | fairseq.trainer | begin training epoch 5
2020-10-11 23:12:41 | INFO | train_inner | epoch 005:     88 / 178 loss=10.216, nll_loss=9.522, ppl=735.31, wps=17571.4, ups=2.76, wpb=6376.4, bsz=236.1, num_updates=800, lr=4.008e-05, gnorm=1.227, clip=0, train_wall=32, wall=267
2020-10-11 23:13:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000717
2020-10-11 23:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028579
2020-10-11 23:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021535
2020-10-11 23:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051177
2020-10-11 23:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000527
2020-10-11 23:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028369
2020-10-11 23:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021050
2020-10-11 23:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050275
2020-10-11 23:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:13:13 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.895 | nll_loss 9.123 | ppl 557.45 | wps 44892.2 | wpb 2226.9 | bsz 84.8 | num_updates 890 | best_loss 9.895
2020-10-11 23:13:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:13:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 5 @ 890 updates, score 9.895) (writing took 1.8355662649992155 seconds)
2020-10-11 23:13:15 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-11 23:13:15 | INFO | train | epoch 005 | loss 10.189 | nll_loss 9.489 | ppl 718.64 | wps 18605.3 | ups 2.86 | wpb 6509.7 | bsz 250 | num_updates 890 | lr 4.45778e-05 | gnorm 1.21 | clip 0 | train_wall 57 | wall 300
2020-10-11 23:13:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-11 23:13:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-11 23:13:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:13:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000437
2020-10-11 23:13:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005358
2020-10-11 23:13:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:13:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-11 23:13:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:13:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099200
2020-10-11 23:13:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105078
2020-10-11 23:13:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:13:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:13:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004279
2020-10-11 23:13:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:13:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-11 23:13:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:13:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097187
2020-10-11 23:13:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101972
2020-10-11 23:13:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:13:15 | INFO | fairseq.trainer | begin training epoch 6
2020-10-11 23:13:18 | INFO | train_inner | epoch 006:     10 / 178 loss=10.162, nll_loss=9.455, ppl=702, wps=17668.2, ups=2.69, wpb=6566.5, bsz=260.7, num_updates=900, lr=4.50775e-05, gnorm=1.3, clip=0, train_wall=33, wall=304
2020-10-11 23:13:51 | INFO | train_inner | epoch 006:    110 / 178 loss=10.069, nll_loss=9.345, ppl=650.34, wps=19941.6, ups=3.07, wpb=6499.9, bsz=235.6, num_updates=1000, lr=5.0075e-05, gnorm=1.15, clip=0, train_wall=32, wall=336
2020-10-11 23:14:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000740
2020-10-11 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027880
2020-10-11 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021460
2020-10-11 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050429
2020-10-11 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000538
2020-10-11 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:14:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027652
2020-10-11 23:14:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:14:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021445
2020-10-11 23:14:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049961
2020-10-11 23:14:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:14:16 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.654 | nll_loss 8.842 | ppl 459.04 | wps 44390 | wpb 2226.9 | bsz 84.8 | num_updates 1068 | best_loss 9.654
2020-10-11 23:14:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:14:17 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 6 @ 1068 updates, score 9.654) (writing took 1.836982523003826 seconds)
2020-10-11 23:14:17 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-11 23:14:17 | INFO | train | epoch 006 | loss 10.02 | nll_loss 9.29 | ppl 625.89 | wps 18520.6 | ups 2.85 | wpb 6509.7 | bsz 250 | num_updates 1068 | lr 5.34733e-05 | gnorm 1.199 | clip 0 | train_wall 58 | wall 363
2020-10-11 23:14:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-11 23:14:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-11 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:14:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000433
2020-10-11 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005352
2020-10-11 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000225
2020-10-11 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100000
2020-10-11 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105907
2020-10-11 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:14:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004247
2020-10-11 23:14:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:14:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 23:14:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:14:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098017
2020-10-11 23:14:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102762
2020-10-11 23:14:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:14:18 | INFO | fairseq.trainer | begin training epoch 7
2020-10-11 23:14:28 | INFO | train_inner | epoch 007:     32 / 178 loss=9.912, nll_loss=9.167, ppl=574.73, wps=17542, ups=2.69, wpb=6514.6, bsz=266, num_updates=1100, lr=5.50725e-05, gnorm=1.145, clip=0, train_wall=32, wall=373
2020-10-11 23:15:01 | INFO | train_inner | epoch 007:    132 / 178 loss=9.84, nll_loss=9.082, ppl=542.06, wps=19628.2, ups=3.07, wpb=6396.4, bsz=251.5, num_updates=1200, lr=6.007e-05, gnorm=1.197, clip=0, train_wall=32, wall=406
2020-10-11 23:15:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000723
2020-10-11 23:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027698
2020-10-11 23:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021435
2020-10-11 23:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050200
2020-10-11 23:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000559
2020-10-11 23:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027942
2020-10-11 23:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021081
2020-10-11 23:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049936
2020-10-11 23:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:15:18 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.51 | nll_loss 8.668 | ppl 406.85 | wps 44966.2 | wpb 2226.9 | bsz 84.8 | num_updates 1246 | best_loss 9.51
2020-10-11 23:15:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:15:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 7 @ 1246 updates, score 9.51) (writing took 1.494905090003158 seconds)
2020-10-11 23:15:20 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-11 23:15:20 | INFO | train | epoch 007 | loss 9.857 | nll_loss 9.101 | ppl 549.12 | wps 18620.1 | ups 2.86 | wpb 6509.7 | bsz 250 | num_updates 1246 | lr 6.23689e-05 | gnorm 1.167 | clip 0 | train_wall 58 | wall 425
2020-10-11 23:15:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-11 23:15:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-11 23:15:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:15:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000537
2020-10-11 23:15:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006009
2020-10-11 23:15:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:15:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 23:15:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:15:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100534
2020-10-11 23:15:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107060
2020-10-11 23:15:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:15:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:15:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004293
2020-10-11 23:15:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:15:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 23:15:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:15:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098220
2020-10-11 23:15:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103013
2020-10-11 23:15:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:15:20 | INFO | fairseq.trainer | begin training epoch 8
2020-10-11 23:15:38 | INFO | train_inner | epoch 008:     54 / 178 loss=9.816, nll_loss=9.051, ppl=530.54, wps=17968.4, ups=2.7, wpb=6645.2, bsz=250.2, num_updates=1300, lr=6.50675e-05, gnorm=1.296, clip=0, train_wall=33, wall=443
2020-10-11 23:16:10 | INFO | train_inner | epoch 008:    154 / 178 loss=9.713, nll_loss=8.932, ppl=488.5, wps=19647.5, ups=3.04, wpb=6454.7, bsz=242, num_updates=1400, lr=7.0065e-05, gnorm=1.098, clip=0, train_wall=32, wall=476
2020-10-11 23:16:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:16:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:16:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000681
2020-10-11 23:16:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:16:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027966
2020-10-11 23:16:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:16:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021329
2020-10-11 23:16:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050320
2020-10-11 23:16:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:16:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:16:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000559
2020-10-11 23:16:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028165
2020-10-11 23:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.020941
2020-10-11 23:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049987
2020-10-11 23:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:16:21 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.381 | nll_loss 8.511 | ppl 364.84 | wps 44155.8 | wpb 2226.9 | bsz 84.8 | num_updates 1424 | best_loss 9.381
2020-10-11 23:16:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:16:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 8 @ 1424 updates, score 9.381) (writing took 1.4955940840009134 seconds)
2020-10-11 23:16:22 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-11 23:16:22 | INFO | train | epoch 008 | loss 9.714 | nll_loss 8.933 | ppl 488.79 | wps 18559.8 | ups 2.85 | wpb 6509.7 | bsz 250 | num_updates 1424 | lr 7.12644e-05 | gnorm 1.183 | clip 0 | train_wall 58 | wall 487
2020-10-11 23:16:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-11 23:16:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-11 23:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:16:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000490
2020-10-11 23:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005408
2020-10-11 23:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 23:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098537
2020-10-11 23:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104458
2020-10-11 23:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004237
2020-10-11 23:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 23:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097110
2020-10-11 23:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101849
2020-10-11 23:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:16:22 | INFO | fairseq.trainer | begin training epoch 9
2020-10-11 23:16:47 | INFO | train_inner | epoch 009:     76 / 178 loss=9.615, nll_loss=8.819, ppl=451.49, wps=17776.3, ups=2.72, wpb=6534.2, bsz=250.8, num_updates=1500, lr=7.50625e-05, gnorm=1.103, clip=0, train_wall=32, wall=513
2020-10-11 23:17:20 | INFO | train_inner | epoch 009:    176 / 178 loss=9.519, nll_loss=8.707, ppl=417.84, wps=19801.8, ups=3.03, wpb=6530.7, bsz=254.2, num_updates=1600, lr=8.006e-05, gnorm=1.222, clip=0, train_wall=33, wall=546
2020-10-11 23:17:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:17:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:17:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000676
2020-10-11 23:17:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:17:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028247
2020-10-11 23:17:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:17:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021564
2020-10-11 23:17:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050828
2020-10-11 23:17:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:17:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:17:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000571
2020-10-11 23:17:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:17:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028164
2020-10-11 23:17:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:17:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021326
2020-10-11 23:17:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050388
2020-10-11 23:17:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:17:23 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 9.155 | nll_loss 8.271 | ppl 308.86 | wps 44250.7 | wpb 2226.9 | bsz 84.8 | num_updates 1602 | best_loss 9.155
2020-10-11 23:17:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:17:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 9 @ 1602 updates, score 9.155) (writing took 1.494661131000612 seconds)
2020-10-11 23:17:24 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-11 23:17:24 | INFO | train | epoch 009 | loss 9.559 | nll_loss 8.754 | ppl 431.64 | wps 18563.5 | ups 2.85 | wpb 6509.7 | bsz 250 | num_updates 1602 | lr 8.016e-05 | gnorm 1.17 | clip 0 | train_wall 58 | wall 550
2020-10-11 23:17:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-11 23:17:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-11 23:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:17:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000508
2020-10-11 23:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005563
2020-10-11 23:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-11 23:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096920
2020-10-11 23:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103021
2020-10-11 23:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004420
2020-10-11 23:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-11 23:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099275
2020-10-11 23:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104206
2020-10-11 23:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:17:25 | INFO | fairseq.trainer | begin training epoch 10
2020-10-11 23:17:57 | INFO | train_inner | epoch 010:     98 / 178 loss=9.445, nll_loss=8.623, ppl=394.17, wps=17878.3, ups=2.7, wpb=6620.2, bsz=263.1, num_updates=1700, lr=8.50575e-05, gnorm=1.401, clip=0, train_wall=33, wall=583
2020-10-11 23:18:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:18:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:18:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000671
2020-10-11 23:18:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:18:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028740
2020-10-11 23:18:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:18:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021449
2020-10-11 23:18:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.051201
2020-10-11 23:18:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:18:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:18:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000536
2020-10-11 23:18:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:18:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027185
2020-10-11 23:18:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:18:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021336
2020-10-11 23:18:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049372
2020-10-11 23:18:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:18:25 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.978 | nll_loss 8.058 | ppl 266.55 | wps 44495 | wpb 2226.9 | bsz 84.8 | num_updates 1780 | best_loss 8.978
2020-10-11 23:18:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:18:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 10 @ 1780 updates, score 8.978) (writing took 1.511715804997948 seconds)
2020-10-11 23:18:27 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-11 23:18:27 | INFO | train | epoch 010 | loss 9.399 | nll_loss 8.57 | ppl 380.1 | wps 18523.3 | ups 2.85 | wpb 6509.7 | bsz 250 | num_updates 1780 | lr 8.90555e-05 | gnorm 1.294 | clip 0 | train_wall 58 | wall 612
2020-10-11 23:18:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-11 23:18:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-11 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:18:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000549
2020-10-11 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005585
2020-10-11 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000199
2020-10-11 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099526
2020-10-11 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105638
2020-10-11 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004228
2020-10-11 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-11 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096954
2020-10-11 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101675
2020-10-11 23:18:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:18:27 | INFO | fairseq.trainer | begin training epoch 11
2020-10-11 23:18:34 | INFO | train_inner | epoch 011:     20 / 178 loss=9.324, nll_loss=8.484, ppl=358.02, wps=17535.3, ups=2.73, wpb=6419.2, bsz=241, num_updates=1800, lr=9.0055e-05, gnorm=1.166, clip=0, train_wall=32, wall=619
2020-10-11 23:19:07 | INFO | train_inner | epoch 011:    120 / 178 loss=9.239, nll_loss=8.388, ppl=335.04, wps=19675.1, ups=3.01, wpb=6546.9, bsz=255.6, num_updates=1900, lr=9.50525e-05, gnorm=1.168, clip=0, train_wall=33, wall=652
2020-10-11 23:19:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000611
2020-10-11 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027275
2020-10-11 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021422
2020-10-11 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049641
2020-10-11 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000505
2020-10-11 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027742
2020-10-11 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021354
2020-10-11 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049921
2020-10-11 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:19:28 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.875 | nll_loss 7.934 | ppl 244.52 | wps 43994.4 | wpb 2226.9 | bsz 84.8 | num_updates 1958 | best_loss 8.875
2020-10-11 23:19:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:19:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 11 @ 1958 updates, score 8.875) (writing took 1.4902372630022 seconds)
2020-10-11 23:19:30 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-11 23:19:30 | INFO | train | epoch 011 | loss 9.202 | nll_loss 8.345 | ppl 325.13 | wps 18512.4 | ups 2.84 | wpb 6509.7 | bsz 250 | num_updates 1958 | lr 9.79511e-05 | gnorm 1.171 | clip 0 | train_wall 58 | wall 675
2020-10-11 23:19:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-11 23:19:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-11 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:19:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000515
2020-10-11 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005908
2020-10-11 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-11 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102856
2020-10-11 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109370
2020-10-11 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004273
2020-10-11 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096665
2020-10-11 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101435
2020-10-11 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:19:30 | INFO | fairseq.trainer | begin training epoch 12
2020-10-11 23:19:44 | INFO | train_inner | epoch 012:     42 / 178 loss=9.123, nll_loss=8.253, ppl=305.12, wps=17656.4, ups=2.73, wpb=6471.7, bsz=239.9, num_updates=2000, lr=0.00010005, gnorm=1.261, clip=0, train_wall=32, wall=689
2020-10-11 23:20:17 | INFO | train_inner | epoch 012:    142 / 178 loss=9.062, nll_loss=8.183, ppl=290.59, wps=19820.9, ups=3.04, wpb=6523, bsz=247.1, num_updates=2100, lr=0.000105048, gnorm=1.203, clip=0, train_wall=32, wall=722
2020-10-11 23:20:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:20:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:20:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000733
2020-10-11 23:20:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:20:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028035
2020-10-11 23:20:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:20:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.020998
2020-10-11 23:20:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050105
2020-10-11 23:20:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:20:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:20:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000526
2020-10-11 23:20:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:20:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027748
2020-10-11 23:20:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:20:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.020894
2020-10-11 23:20:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049487
2020-10-11 23:20:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:20:31 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.684 | nll_loss 7.719 | ppl 210.67 | wps 44241.5 | wpb 2226.9 | bsz 84.8 | num_updates 2136 | best_loss 8.684
2020-10-11 23:20:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:20:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 12 @ 2136 updates, score 8.684) (writing took 1.821274161004112 seconds)
2020-10-11 23:20:32 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-11 23:20:32 | INFO | train | epoch 012 | loss 9.051 | nll_loss 8.17 | ppl 287.92 | wps 18437.7 | ups 2.83 | wpb 6509.7 | bsz 250 | num_updates 2136 | lr 0.000106847 | gnorm 1.246 | clip 0 | train_wall 58 | wall 738
2020-10-11 23:20:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-11 23:20:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-11 23:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:20:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000460
2020-10-11 23:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005324
2020-10-11 23:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-11 23:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:20:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098997
2020-10-11 23:20:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104818
2020-10-11 23:20:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:20:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:20:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004249
2020-10-11 23:20:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:20:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-11 23:20:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:20:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097365
2020-10-11 23:20:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102105
2020-10-11 23:20:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:20:33 | INFO | fairseq.trainer | begin training epoch 13
2020-10-11 23:20:54 | INFO | train_inner | epoch 013:     64 / 178 loss=8.938, nll_loss=8.039, ppl=263.05, wps=17883.2, ups=2.69, wpb=6648.3, bsz=263.1, num_updates=2200, lr=0.000110045, gnorm=1.192, clip=0, train_wall=33, wall=759
2020-10-11 23:21:27 | INFO | train_inner | epoch 013:    164 / 178 loss=8.859, nll_loss=7.946, ppl=246.65, wps=19306.4, ups=3.04, wpb=6343.1, bsz=237, num_updates=2300, lr=0.000115043, gnorm=1.148, clip=0, train_wall=32, wall=792
2020-10-11 23:21:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:21:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:21:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000738
2020-10-11 23:21:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:21:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028086
2020-10-11 23:21:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:21:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021499
2020-10-11 23:21:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050673
2020-10-11 23:21:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:21:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:21:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000539
2020-10-11 23:21:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:21:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027932
2020-10-11 23:21:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:21:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021687
2020-10-11 23:21:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050491
2020-10-11 23:21:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:21:33 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.534 | nll_loss 7.544 | ppl 186.68 | wps 43855.2 | wpb 2226.9 | bsz 84.8 | num_updates 2314 | best_loss 8.534
2020-10-11 23:21:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:21:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 13 @ 2314 updates, score 8.534) (writing took 1.6990554299991345 seconds)
2020-10-11 23:21:35 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-11 23:21:35 | INFO | train | epoch 013 | loss 8.887 | nll_loss 7.979 | ppl 252.33 | wps 18480.9 | ups 2.84 | wpb 6509.7 | bsz 250 | num_updates 2314 | lr 0.000115742 | gnorm 1.163 | clip 0 | train_wall 58 | wall 801
2020-10-11 23:21:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-11 23:21:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-11 23:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:21:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000498
2020-10-11 23:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005387
2020-10-11 23:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-11 23:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098801
2020-10-11 23:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104702
2020-10-11 23:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004362
2020-10-11 23:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 23:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097653
2020-10-11 23:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102519
2020-10-11 23:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:21:35 | INFO | fairseq.trainer | begin training epoch 14
2020-10-11 23:22:03 | INFO | train_inner | epoch 014:     86 / 178 loss=8.753, nll_loss=7.825, ppl=226.7, wps=17363.5, ups=2.72, wpb=6377.7, bsz=261.4, num_updates=2400, lr=0.00012004, gnorm=1.184, clip=0, train_wall=32, wall=829
2020-10-11 23:22:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000732
2020-10-11 23:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027755
2020-10-11 23:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021432
2020-10-11 23:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050261
2020-10-11 23:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000531
2020-10-11 23:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027736
2020-10-11 23:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021561
2020-10-11 23:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050149
2020-10-11 23:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:22:36 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.403 | nll_loss 7.393 | ppl 168.04 | wps 43996.4 | wpb 2226.9 | bsz 84.8 | num_updates 2492 | best_loss 8.403
2020-10-11 23:22:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:22:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 14 @ 2492 updates, score 8.403) (writing took 1.8252609930059407 seconds)
2020-10-11 23:22:38 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-11 23:22:38 | INFO | train | epoch 014 | loss 8.744 | nll_loss 7.814 | ppl 224.98 | wps 18461.7 | ups 2.84 | wpb 6509.7 | bsz 250 | num_updates 2492 | lr 0.000124638 | gnorm 1.186 | clip 0 | train_wall 58 | wall 863
2020-10-11 23:22:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-11 23:22:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-11 23:22:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:22:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000465
2020-10-11 23:22:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005395
2020-10-11 23:22:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:22:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-11 23:22:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:22:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098644
2020-10-11 23:22:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104560
2020-10-11 23:22:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:22:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:22:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004268
2020-10-11 23:22:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:22:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-11 23:22:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:22:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093480
2020-10-11 23:22:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098253
2020-10-11 23:22:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:22:38 | INFO | fairseq.trainer | begin training epoch 15
2020-10-11 23:22:41 | INFO | train_inner | epoch 015:      8 / 178 loss=8.739, nll_loss=7.806, ppl=223.78, wps=17798.7, ups=2.68, wpb=6651, bsz=240.6, num_updates=2500, lr=0.000125037, gnorm=1.17, clip=0, train_wall=33, wall=866
2020-10-11 23:23:14 | INFO | train_inner | epoch 015:    108 / 178 loss=8.628, nll_loss=7.68, ppl=205.05, wps=20008.3, ups=3.01, wpb=6640.9, bsz=256.8, num_updates=2600, lr=0.000130035, gnorm=1.205, clip=0, train_wall=33, wall=899
2020-10-11 23:23:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000673
2020-10-11 23:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027899
2020-10-11 23:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021430
2020-10-11 23:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050345
2020-10-11 23:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000558
2020-10-11 23:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027397
2020-10-11 23:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021451
2020-10-11 23:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049737
2020-10-11 23:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:23:39 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.37 | nll_loss 7.341 | ppl 162.07 | wps 44186.3 | wpb 2226.9 | bsz 84.8 | num_updates 2670 | best_loss 8.37
2020-10-11 23:23:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:23:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 15 @ 2670 updates, score 8.37) (writing took 1.940912270991248 seconds)
2020-10-11 23:23:41 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-11 23:23:41 | INFO | train | epoch 015 | loss 8.604 | nll_loss 7.651 | ppl 201.02 | wps 18404.6 | ups 2.83 | wpb 6509.7 | bsz 250 | num_updates 2670 | lr 0.000133533 | gnorm 1.202 | clip 0 | train_wall 58 | wall 926
2020-10-11 23:23:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-11 23:23:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-11 23:23:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:23:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000435
2020-10-11 23:23:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005384
2020-10-11 23:23:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:23:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-11 23:23:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:23:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100331
2020-10-11 23:23:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106239
2020-10-11 23:23:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:23:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:23:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004409
2020-10-11 23:23:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:23:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-11 23:23:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:23:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096505
2020-10-11 23:23:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101440
2020-10-11 23:23:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:23:41 | INFO | fairseq.trainer | begin training epoch 16
2020-10-11 23:23:51 | INFO | train_inner | epoch 016:     30 / 178 loss=8.523, nll_loss=7.557, ppl=188.35, wps=17226.7, ups=2.69, wpb=6406.2, bsz=258.3, num_updates=2700, lr=0.000135032, gnorm=1.204, clip=0, train_wall=32, wall=937
2020-10-11 23:24:24 | INFO | train_inner | epoch 016:    130 / 178 loss=8.477, nll_loss=7.504, ppl=181.51, wps=19642.8, ups=3.06, wpb=6427, bsz=242.9, num_updates=2800, lr=0.00014003, gnorm=1.159, clip=0, train_wall=32, wall=969
2020-10-11 23:24:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000690
2020-10-11 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027267
2020-10-11 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021219
2020-10-11 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049523
2020-10-11 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000566
2020-10-11 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028225
2020-10-11 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021099
2020-10-11 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050206
2020-10-11 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:24:42 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.156 | nll_loss 7.096 | ppl 136.77 | wps 44627.2 | wpb 2226.9 | bsz 84.8 | num_updates 2848 | best_loss 8.156
2020-10-11 23:24:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:24:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 16 @ 2848 updates, score 8.156) (writing took 1.4896674390038243 seconds)
2020-10-11 23:24:43 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-11 23:24:43 | INFO | train | epoch 016 | loss 8.457 | nll_loss 7.481 | ppl 178.65 | wps 18513.7 | ups 2.84 | wpb 6509.7 | bsz 250 | num_updates 2848 | lr 0.000142429 | gnorm 1.143 | clip 0 | train_wall 58 | wall 989
2020-10-11 23:24:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-11 23:24:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-11 23:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:24:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000531
2020-10-11 23:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006000
2020-10-11 23:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 23:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101765
2020-10-11 23:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108274
2020-10-11 23:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004362
2020-10-11 23:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 23:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097769
2020-10-11 23:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102639
2020-10-11 23:24:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:24:44 | INFO | fairseq.trainer | begin training epoch 17
2020-10-11 23:25:01 | INFO | train_inner | epoch 017:     52 / 178 loss=8.372, nll_loss=7.383, ppl=166.88, wps=17933, ups=2.7, wpb=6649.6, bsz=263.6, num_updates=2900, lr=0.000145028, gnorm=1.178, clip=0, train_wall=33, wall=1006
2020-10-11 23:25:34 | INFO | train_inner | epoch 017:    152 / 178 loss=8.295, nll_loss=7.292, ppl=156.75, wps=19618.1, ups=3.04, wpb=6443.9, bsz=244.4, num_updates=3000, lr=0.000150025, gnorm=1.2, clip=0, train_wall=32, wall=1039
2020-10-11 23:25:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:25:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:25:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000675
2020-10-11 23:25:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:25:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028342
2020-10-11 23:25:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:25:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021524
2020-10-11 23:25:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050887
2020-10-11 23:25:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:25:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:25:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000554
2020-10-11 23:25:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:25:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027565
2020-10-11 23:25:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:25:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021587
2020-10-11 23:25:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050027
2020-10-11 23:25:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:25:45 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 8.029 | nll_loss 6.932 | ppl 122.08 | wps 44317.5 | wpb 2226.9 | bsz 84.8 | num_updates 3026 | best_loss 8.029
2020-10-11 23:25:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:25:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 17 @ 3026 updates, score 8.029) (writing took 1.7140021759987576 seconds)
2020-10-11 23:25:46 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-11 23:25:46 | INFO | train | epoch 017 | loss 8.311 | nll_loss 7.311 | ppl 158.85 | wps 18453.5 | ups 2.83 | wpb 6509.7 | bsz 250 | num_updates 3026 | lr 0.000151324 | gnorm 1.203 | clip 0 | train_wall 58 | wall 1052
2020-10-11 23:25:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-11 23:25:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-11 23:25:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:25:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000431
2020-10-11 23:25:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005340
2020-10-11 23:25:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:25:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 23:25:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:25:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098895
2020-10-11 23:25:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104751
2020-10-11 23:25:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:25:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:25:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004339
2020-10-11 23:25:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:25:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-11 23:25:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:25:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098599
2020-10-11 23:25:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103445
2020-10-11 23:25:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:25:46 | INFO | fairseq.trainer | begin training epoch 18
2020-10-11 23:26:11 | INFO | train_inner | epoch 018:     74 / 178 loss=8.197, nll_loss=7.181, ppl=145.1, wps=17449.3, ups=2.7, wpb=6467.1, bsz=241.2, num_updates=3100, lr=0.000155023, gnorm=1.221, clip=0, train_wall=33, wall=1076
2020-10-11 23:26:44 | INFO | train_inner | epoch 018:    174 / 178 loss=8.154, nll_loss=7.129, ppl=140, wps=19744.2, ups=3.03, wpb=6523.1, bsz=246.4, num_updates=3200, lr=0.00016002, gnorm=1.224, clip=0, train_wall=33, wall=1109
2020-10-11 23:26:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000727
2020-10-11 23:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028365
2020-10-11 23:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021183
2020-10-11 23:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050621
2020-10-11 23:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000548
2020-10-11 23:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028486
2020-10-11 23:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021385
2020-10-11 23:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050744
2020-10-11 23:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:26:47 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.907 | nll_loss 6.795 | ppl 111.01 | wps 45226.3 | wpb 2226.9 | bsz 84.8 | num_updates 3204 | best_loss 7.907
2020-10-11 23:26:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:26:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 18 @ 3204 updates, score 7.907) (writing took 1.9396371449984144 seconds)
2020-10-11 23:26:49 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-11 23:26:49 | INFO | train | epoch 018 | loss 8.152 | nll_loss 7.128 | ppl 139.91 | wps 18407.7 | ups 2.83 | wpb 6509.7 | bsz 250 | num_updates 3204 | lr 0.00016022 | gnorm 1.238 | clip 0 | train_wall 58 | wall 1115
2020-10-11 23:26:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-11 23:26:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-11 23:26:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:26:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000706
2020-10-11 23:26:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006839
2020-10-11 23:26:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:26:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000327
2020-10-11 23:26:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:26:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.113513
2020-10-11 23:26:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.121060
2020-10-11 23:26:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:26:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:26:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004286
2020-10-11 23:26:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:26:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 23:26:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:26:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097410
2020-10-11 23:26:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102196
2020-10-11 23:26:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:26:49 | INFO | fairseq.trainer | begin training epoch 19
2020-10-11 23:27:21 | INFO | train_inner | epoch 019:     96 / 178 loss=7.982, nll_loss=6.935, ppl=122.32, wps=17430.8, ups=2.68, wpb=6499.7, bsz=256, num_updates=3300, lr=0.000165018, gnorm=1.149, clip=0, train_wall=33, wall=1147
2020-10-11 23:27:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000739
2020-10-11 23:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027664
2020-10-11 23:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021219
2020-10-11 23:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049971
2020-10-11 23:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000542
2020-10-11 23:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028123
2020-10-11 23:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.020801
2020-10-11 23:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049786
2020-10-11 23:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:27:50 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.752 | nll_loss 6.606 | ppl 97.4 | wps 44302.2 | wpb 2226.9 | bsz 84.8 | num_updates 3382 | best_loss 7.752
2020-10-11 23:27:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:27:52 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 19 @ 3382 updates, score 7.752) (writing took 1.5362141969962977 seconds)
2020-10-11 23:27:52 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-11 23:27:52 | INFO | train | epoch 019 | loss 7.969 | nll_loss 6.918 | ppl 120.94 | wps 18506.6 | ups 2.84 | wpb 6509.7 | bsz 250 | num_updates 3382 | lr 0.000169115 | gnorm 1.171 | clip 0 | train_wall 58 | wall 1177
2020-10-11 23:27:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-11 23:27:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-11 23:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:27:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000594
2020-10-11 23:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006032
2020-10-11 23:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-11 23:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101099
2020-10-11 23:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107721
2020-10-11 23:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007651
2020-10-11 23:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-11 23:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095573
2020-10-11 23:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103754
2020-10-11 23:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:27:52 | INFO | fairseq.trainer | begin training epoch 20
2020-10-11 23:27:58 | INFO | train_inner | epoch 020:     18 / 178 loss=7.947, nll_loss=6.89, ppl=118.61, wps=17562, ups=2.73, wpb=6435.1, bsz=235, num_updates=3400, lr=0.000170015, gnorm=1.205, clip=0, train_wall=32, wall=1183
2020-10-11 23:28:31 | INFO | train_inner | epoch 020:    118 / 178 loss=7.812, nll_loss=6.736, ppl=106.6, wps=19788.4, ups=3.02, wpb=6557.3, bsz=243.7, num_updates=3500, lr=0.000175013, gnorm=1.202, clip=0, train_wall=33, wall=1216
2020-10-11 23:28:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000734
2020-10-11 23:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027932
2020-10-11 23:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.020997
2020-10-11 23:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050003
2020-10-11 23:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000525
2020-10-11 23:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027761
2020-10-11 23:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021114
2020-10-11 23:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049721
2020-10-11 23:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:28:53 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.639 | nll_loss 6.459 | ppl 87.99 | wps 44019.5 | wpb 2226.9 | bsz 84.8 | num_updates 3560 | best_loss 7.639
2020-10-11 23:28:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:28:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 20 @ 3560 updates, score 7.639) (writing took 1.3978422410000348 seconds)
2020-10-11 23:28:54 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-11 23:28:54 | INFO | train | epoch 020 | loss 7.802 | nll_loss 6.725 | ppl 105.79 | wps 18517.7 | ups 2.84 | wpb 6509.7 | bsz 250 | num_updates 3560 | lr 0.000178011 | gnorm 1.22 | clip 0 | train_wall 58 | wall 1240
2020-10-11 23:28:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-11 23:28:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-11 23:28:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:28:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000555
2020-10-11 23:28:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006005
2020-10-11 23:28:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:28:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 23:28:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:28:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102723
2020-10-11 23:28:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109329
2020-10-11 23:28:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.011531
2020-10-11 23:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000205
2020-10-11 23:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096204
2020-10-11 23:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108313
2020-10-11 23:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:28:55 | INFO | fairseq.trainer | begin training epoch 21
2020-10-11 23:29:08 | INFO | train_inner | epoch 021:     40 / 178 loss=7.727, nll_loss=6.638, ppl=99.58, wps=17716, ups=2.71, wpb=6539.8, bsz=259.1, num_updates=3600, lr=0.00018001, gnorm=1.219, clip=0, train_wall=33, wall=1253
2020-10-11 23:29:41 | INFO | train_inner | epoch 021:    140 / 178 loss=7.661, nll_loss=6.559, ppl=94.29, wps=19932.3, ups=3.02, wpb=6590.6, bsz=237.6, num_updates=3700, lr=0.000185008, gnorm=1.257, clip=0, train_wall=33, wall=1286
2020-10-11 23:29:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:29:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:29:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000720
2020-10-11 23:29:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:29:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028164
2020-10-11 23:29:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:29:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021368
2020-10-11 23:29:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050593
2020-10-11 23:29:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:29:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:29:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000549
2020-10-11 23:29:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:29:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028351
2020-10-11 23:29:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:29:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.020966
2020-10-11 23:29:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050187
2020-10-11 23:29:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:29:55 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.557 | nll_loss 6.351 | ppl 81.65 | wps 44470.1 | wpb 2226.9 | bsz 84.8 | num_updates 3738 | best_loss 7.557
2020-10-11 23:29:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:29:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 21 @ 3738 updates, score 7.557) (writing took 1.8165621609950904 seconds)
2020-10-11 23:29:57 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-11 23:29:57 | INFO | train | epoch 021 | loss 7.629 | nll_loss 6.524 | ppl 92 | wps 18414 | ups 2.83 | wpb 6509.7 | bsz 250 | num_updates 3738 | lr 0.000186907 | gnorm 1.271 | clip 0 | train_wall 58 | wall 1303
2020-10-11 23:29:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-11 23:29:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-11 23:29:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:29:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000437
2020-10-11 23:29:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005336
2020-10-11 23:29:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:29:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-11 23:29:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:29:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101351
2020-10-11 23:29:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107184
2020-10-11 23:29:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:29:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:29:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004272
2020-10-11 23:29:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:29:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-11 23:29:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:29:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096781
2020-10-11 23:29:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101547
2020-10-11 23:29:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:29:58 | INFO | fairseq.trainer | begin training epoch 22
2020-10-11 23:30:18 | INFO | train_inner | epoch 022:     62 / 178 loss=7.476, nll_loss=6.348, ppl=81.46, wps=17464.7, ups=2.71, wpb=6451.5, bsz=267.7, num_updates=3800, lr=0.000190005, gnorm=1.272, clip=0, train_wall=32, wall=1323
2020-10-11 23:30:51 | INFO | train_inner | epoch 022:    162 / 178 loss=7.451, nll_loss=6.317, ppl=79.73, wps=19612.5, ups=3.03, wpb=6470.3, bsz=247, num_updates=3900, lr=0.000195003, gnorm=1.225, clip=0, train_wall=33, wall=1356
2020-10-11 23:30:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.006905
2020-10-11 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027893
2020-10-11 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021267
2020-10-11 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.056416
2020-10-11 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000601
2020-10-11 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028252
2020-10-11 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021742
2020-10-11 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050920
2020-10-11 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:30:58 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.293 | nll_loss 6.051 | ppl 66.3 | wps 44463.1 | wpb 2226.9 | bsz 84.8 | num_updates 3916 | best_loss 7.293
2020-10-11 23:30:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:31:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 22 @ 3916 updates, score 7.293) (writing took 1.48658607499965 seconds)
2020-10-11 23:31:00 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-11 23:31:00 | INFO | train | epoch 022 | loss 7.448 | nll_loss 6.314 | ppl 79.55 | wps 18530.8 | ups 2.85 | wpb 6509.7 | bsz 250 | num_updates 3916 | lr 0.000195802 | gnorm 1.213 | clip 0 | train_wall 58 | wall 1365
2020-10-11 23:31:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-11 23:31:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-11 23:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:31:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000474
2020-10-11 23:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006029
2020-10-11 23:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000242
2020-10-11 23:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098895
2020-10-11 23:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105506
2020-10-11 23:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004252
2020-10-11 23:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-11 23:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095205
2020-10-11 23:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099947
2020-10-11 23:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:31:00 | INFO | fairseq.trainer | begin training epoch 23
2020-10-11 23:31:28 | INFO | train_inner | epoch 023:     84 / 178 loss=7.284, nll_loss=6.125, ppl=69.8, wps=17599.1, ups=2.71, wpb=6483.9, bsz=261, num_updates=4000, lr=0.0002, gnorm=1.258, clip=0, train_wall=33, wall=1393
2020-10-11 23:31:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000683
2020-10-11 23:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028148
2020-10-11 23:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021574
2020-10-11 23:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050744
2020-10-11 23:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000559
2020-10-11 23:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028308
2020-10-11 23:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021014
2020-10-11 23:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050200
2020-10-11 23:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:32:01 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.215 | nll_loss 5.963 | ppl 62.39 | wps 44129.5 | wpb 2226.9 | bsz 84.8 | num_updates 4094 | best_loss 7.215
2020-10-11 23:32:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:32:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 23 @ 4094 updates, score 7.215) (writing took 1.8263004569889745 seconds)
2020-10-11 23:32:03 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-11 23:32:03 | INFO | train | epoch 023 | loss 7.279 | nll_loss 6.117 | ppl 69.4 | wps 18405.3 | ups 2.83 | wpb 6509.7 | bsz 250 | num_updates 4094 | lr 0.000197691 | gnorm 1.25 | clip 0 | train_wall 58 | wall 1428
2020-10-11 23:32:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-11 23:32:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-11 23:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:32:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000434
2020-10-11 23:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005362
2020-10-11 23:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-11 23:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095665
2020-10-11 23:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101537
2020-10-11 23:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004283
2020-10-11 23:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 23:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096924
2020-10-11 23:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101755
2020-10-11 23:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:32:03 | INFO | fairseq.trainer | begin training epoch 24
2020-10-11 23:32:05 | INFO | train_inner | epoch 024:      6 / 178 loss=7.281, nll_loss=6.117, ppl=69.39, wps=17637.8, ups=2.68, wpb=6580.4, bsz=240.7, num_updates=4100, lr=0.000197546, gnorm=1.236, clip=0, train_wall=33, wall=1430
2020-10-11 23:32:38 | INFO | train_inner | epoch 024:    106 / 178 loss=7.107, nll_loss=5.919, ppl=60.5, wps=19712.4, ups=3.05, wpb=6453, bsz=233.9, num_updates=4200, lr=0.00019518, gnorm=1.175, clip=0, train_wall=32, wall=1463
2020-10-11 23:33:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000612
2020-10-11 23:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027717
2020-10-11 23:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.020790
2020-10-11 23:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049445
2020-10-11 23:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000499
2020-10-11 23:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027275
2020-10-11 23:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021308
2020-10-11 23:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049396
2020-10-11 23:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:33:04 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.076 | nll_loss 5.785 | ppl 55.15 | wps 44190.2 | wpb 2226.9 | bsz 84.8 | num_updates 4272 | best_loss 7.076
2020-10-11 23:33:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:33:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 24 @ 4272 updates, score 7.076) (writing took 1.4827098850073526 seconds)
2020-10-11 23:33:05 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-11 23:33:05 | INFO | train | epoch 024 | loss 7.101 | nll_loss 5.911 | ppl 60.15 | wps 18557.5 | ups 2.85 | wpb 6509.7 | bsz 250 | num_updates 4272 | lr 0.000193528 | gnorm 1.242 | clip 0 | train_wall 58 | wall 1491
2020-10-11 23:33:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-11 23:33:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-11 23:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:33:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000439
2020-10-11 23:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005845
2020-10-11 23:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-11 23:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099711
2020-10-11 23:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106064
2020-10-11 23:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004365
2020-10-11 23:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 23:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098536
2020-10-11 23:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103402
2020-10-11 23:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:33:05 | INFO | fairseq.trainer | begin training epoch 25
2020-10-11 23:33:15 | INFO | train_inner | epoch 025:     28 / 178 loss=7.034, nll_loss=5.832, ppl=56.97, wps=17669.9, ups=2.71, wpb=6514.8, bsz=269.4, num_updates=4300, lr=0.000192897, gnorm=1.298, clip=0, train_wall=33, wall=1500
2020-10-11 23:33:48 | INFO | train_inner | epoch 025:    128 / 178 loss=6.942, nll_loss=5.726, ppl=52.92, wps=19814.4, ups=3.02, wpb=6552, bsz=249.8, num_updates=4400, lr=0.000190693, gnorm=1.218, clip=0, train_wall=33, wall=1533
2020-10-11 23:34:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000738
2020-10-11 23:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027995
2020-10-11 23:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021727
2020-10-11 23:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050806
2020-10-11 23:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000534
2020-10-11 23:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027804
2020-10-11 23:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021980
2020-10-11 23:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050636
2020-10-11 23:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:34:06 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.041 | nll_loss 5.725 | ppl 52.9 | wps 44188.6 | wpb 2226.9 | bsz 84.8 | num_updates 4450 | best_loss 7.041
2020-10-11 23:34:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:34:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 25 @ 4450 updates, score 7.041) (writing took 1.4882860890065785 seconds)
2020-10-11 23:34:08 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-11 23:34:08 | INFO | train | epoch 025 | loss 6.928 | nll_loss 5.709 | ppl 52.32 | wps 18528 | ups 2.85 | wpb 6509.7 | bsz 250 | num_updates 4450 | lr 0.000189618 | gnorm 1.223 | clip 0 | train_wall 58 | wall 1553
2020-10-11 23:34:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-11 23:34:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-11 23:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:34:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000628
2020-10-11 23:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006130
2020-10-11 23:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000257
2020-10-11 23:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103638
2020-10-11 23:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110357
2020-10-11 23:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004272
2020-10-11 23:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-11 23:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097696
2020-10-11 23:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102477
2020-10-11 23:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:34:08 | INFO | fairseq.trainer | begin training epoch 26
2020-10-11 23:34:24 | INFO | train_inner | epoch 026:     50 / 178 loss=6.853, nll_loss=5.623, ppl=49.27, wps=17407.8, ups=2.74, wpb=6361.1, bsz=234.2, num_updates=4500, lr=0.000188562, gnorm=1.275, clip=0, train_wall=32, wall=1570
2020-10-11 23:34:58 | INFO | train_inner | epoch 026:    150 / 178 loss=6.777, nll_loss=5.532, ppl=46.26, wps=19992.6, ups=2.98, wpb=6700.8, bsz=273.8, num_updates=4600, lr=0.000186501, gnorm=1.23, clip=0, train_wall=33, wall=1603
2020-10-11 23:35:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000730
2020-10-11 23:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027502
2020-10-11 23:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021977
2020-10-11 23:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050550
2020-10-11 23:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000529
2020-10-11 23:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027693
2020-10-11 23:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021970
2020-10-11 23:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050518
2020-10-11 23:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:35:09 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.891 | nll_loss 5.554 | ppl 46.97 | wps 44218.1 | wpb 2226.9 | bsz 84.8 | num_updates 4628 | best_loss 6.891
2020-10-11 23:35:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:35:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 26 @ 4628 updates, score 6.891) (writing took 1.5166753959929338 seconds)
2020-10-11 23:35:10 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-11 23:35:10 | INFO | train | epoch 026 | loss 6.78 | nll_loss 5.536 | ppl 46.39 | wps 18482 | ups 2.84 | wpb 6509.7 | bsz 250 | num_updates 4628 | lr 0.000185936 | gnorm 1.267 | clip 0 | train_wall 58 | wall 1616
2020-10-11 23:35:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-11 23:35:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-11 23:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:35:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000445
2020-10-11 23:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005493
2020-10-11 23:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-11 23:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:35:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096584
2020-10-11 23:35:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102598
2020-10-11 23:35:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:35:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:35:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004269
2020-10-11 23:35:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:35:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-11 23:35:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:35:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098611
2020-10-11 23:35:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103381
2020-10-11 23:35:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:35:11 | INFO | fairseq.trainer | begin training epoch 27
2020-10-11 23:35:34 | INFO | train_inner | epoch 027:     72 / 178 loss=6.665, nll_loss=5.403, ppl=42.32, wps=17617, ups=2.74, wpb=6434.7, bsz=235.1, num_updates=4700, lr=0.000184506, gnorm=1.226, clip=0, train_wall=32, wall=1640
2020-10-11 23:36:07 | INFO | train_inner | epoch 027:    172 / 178 loss=6.639, nll_loss=5.371, ppl=41.38, wps=19646.8, ups=3.02, wpb=6495.5, bsz=252.8, num_updates=4800, lr=0.000182574, gnorm=1.269, clip=0, train_wall=33, wall=1673
2020-10-11 23:36:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:36:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:36:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000730
2020-10-11 23:36:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:36:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027907
2020-10-11 23:36:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:36:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021228
2020-10-11 23:36:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050204
2020-10-11 23:36:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:36:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:36:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000555
2020-10-11 23:36:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:36:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027499
2020-10-11 23:36:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:36:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021473
2020-10-11 23:36:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049853
2020-10-11 23:36:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:36:12 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.845 | nll_loss 5.493 | ppl 45.04 | wps 44115.5 | wpb 2226.9 | bsz 84.8 | num_updates 4806 | best_loss 6.845
2020-10-11 23:36:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:36:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 27 @ 4806 updates, score 6.845) (writing took 1.492173228005413 seconds)
2020-10-11 23:36:13 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-11 23:36:13 | INFO | train | epoch 027 | loss 6.638 | nll_loss 5.371 | ppl 41.39 | wps 18517.4 | ups 2.84 | wpb 6509.7 | bsz 250 | num_updates 4806 | lr 0.00018246 | gnorm 1.245 | clip 0 | train_wall 58 | wall 1678
2020-10-11 23:36:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-11 23:36:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-11 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:36:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000548
2020-10-11 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005866
2020-10-11 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-11 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099475
2020-10-11 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105933
2020-10-11 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004372
2020-10-11 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-11 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096278
2020-10-11 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101165
2020-10-11 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:36:13 | INFO | fairseq.trainer | begin training epoch 28
2020-10-11 23:36:44 | INFO | train_inner | epoch 028:     94 / 178 loss=6.522, nll_loss=5.237, ppl=37.7, wps=17663.4, ups=2.71, wpb=6524.4, bsz=255.5, num_updates=4900, lr=0.000180702, gnorm=1.269, clip=0, train_wall=33, wall=1710
2020-10-11 23:37:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000676
2020-10-11 23:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027293
2020-10-11 23:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021254
2020-10-11 23:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049563
2020-10-11 23:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000545
2020-10-11 23:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027231
2020-10-11 23:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021370
2020-10-11 23:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049463
2020-10-11 23:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:37:14 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.697 | nll_loss 5.327 | ppl 40.14 | wps 44196.9 | wpb 2226.9 | bsz 84.8 | num_updates 4984 | best_loss 6.697
2020-10-11 23:37:14 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:37:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 28 @ 4984 updates, score 6.697) (writing took 1.979533383011585 seconds)
2020-10-11 23:37:16 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-11 23:37:16 | INFO | train | epoch 028 | loss 6.507 | nll_loss 5.218 | ppl 37.21 | wps 18379.1 | ups 2.82 | wpb 6509.7 | bsz 250 | num_updates 4984 | lr 0.000179172 | gnorm 1.235 | clip 0 | train_wall 58 | wall 1741
2020-10-11 23:37:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-11 23:37:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-11 23:37:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:37:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000515
2020-10-11 23:37:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005924
2020-10-11 23:37:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:37:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-11 23:37:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:37:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.107749
2020-10-11 23:37:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114296
2020-10-11 23:37:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:37:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:37:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004278
2020-10-11 23:37:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:37:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 23:37:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:37:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097068
2020-10-11 23:37:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101854
2020-10-11 23:37:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:37:16 | INFO | fairseq.trainer | begin training epoch 29
2020-10-11 23:37:22 | INFO | train_inner | epoch 029:     16 / 178 loss=6.487, nll_loss=5.193, ppl=36.59, wps=17406.7, ups=2.69, wpb=6480.5, bsz=243.8, num_updates=5000, lr=0.000178885, gnorm=1.226, clip=0, train_wall=32, wall=1747
2020-10-11 23:37:55 | INFO | train_inner | epoch 029:    116 / 178 loss=6.35, nll_loss=5.036, ppl=32.8, wps=19843.4, ups=3.03, wpb=6549.6, bsz=261.6, num_updates=5100, lr=0.000177123, gnorm=1.236, clip=0, train_wall=33, wall=1780
2020-10-11 23:38:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000672
2020-10-11 23:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028180
2020-10-11 23:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021367
2020-10-11 23:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050553
2020-10-11 23:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000599
2020-10-11 23:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027647
2020-10-11 23:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021106
2020-10-11 23:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049669
2020-10-11 23:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:38:17 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.64 | nll_loss 5.247 | ppl 37.98 | wps 44458 | wpb 2226.9 | bsz 84.8 | num_updates 5162 | best_loss 6.64
2020-10-11 23:38:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:38:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 29 @ 5162 updates, score 6.64) (writing took 1.6120936590014026 seconds)
2020-10-11 23:38:19 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-11 23:38:19 | INFO | train | epoch 029 | loss 6.373 | nll_loss 5.061 | ppl 33.38 | wps 18486.4 | ups 2.84 | wpb 6509.7 | bsz 250 | num_updates 5162 | lr 0.000176056 | gnorm 1.235 | clip 0 | train_wall 58 | wall 1804
2020-10-11 23:38:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-11 23:38:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-11 23:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:38:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000484
2020-10-11 23:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005545
2020-10-11 23:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 23:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098155
2020-10-11 23:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104207
2020-10-11 23:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004365
2020-10-11 23:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 23:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097701
2020-10-11 23:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102563
2020-10-11 23:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:38:19 | INFO | fairseq.trainer | begin training epoch 30
2020-10-11 23:38:32 | INFO | train_inner | epoch 030:     38 / 178 loss=6.346, nll_loss=5.029, ppl=32.65, wps=17673.8, ups=2.7, wpb=6536.2, bsz=238.4, num_updates=5200, lr=0.000175412, gnorm=1.21, clip=0, train_wall=33, wall=1817
2020-10-11 23:39:04 | INFO | train_inner | epoch 030:    138 / 178 loss=6.273, nll_loss=4.943, ppl=30.77, wps=19644.1, ups=3.04, wpb=6468.5, bsz=245.4, num_updates=5300, lr=0.000173749, gnorm=1.28, clip=0, train_wall=33, wall=1850
2020-10-11 23:39:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:39:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:39:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000669
2020-10-11 23:39:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:39:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027733
2020-10-11 23:39:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:39:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021286
2020-10-11 23:39:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050018
2020-10-11 23:39:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:39:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:39:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000559
2020-10-11 23:39:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:39:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027911
2020-10-11 23:39:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:39:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021329
2020-10-11 23:39:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050124
2020-10-11 23:39:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:39:20 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.607 | nll_loss 5.196 | ppl 36.67 | wps 44052.8 | wpb 2226.9 | bsz 84.8 | num_updates 5340 | best_loss 6.607
2020-10-11 23:39:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:39:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 30 @ 5340 updates, score 6.607) (writing took 1.5048395750054624 seconds)
2020-10-11 23:39:21 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-11 23:39:21 | INFO | train | epoch 030 | loss 6.254 | nll_loss 4.922 | ppl 30.31 | wps 18521.9 | ups 2.85 | wpb 6509.7 | bsz 250 | num_updates 5340 | lr 0.000173097 | gnorm 1.249 | clip 0 | train_wall 58 | wall 1867
2020-10-11 23:39:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-11 23:39:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-11 23:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:39:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000433
2020-10-11 23:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005776
2020-10-11 23:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-11 23:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100230
2020-10-11 23:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106508
2020-10-11 23:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004337
2020-10-11 23:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 23:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097976
2020-10-11 23:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102810
2020-10-11 23:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:39:22 | INFO | fairseq.trainer | begin training epoch 31
2020-10-11 23:39:41 | INFO | train_inner | epoch 031:     60 / 178 loss=6.174, nll_loss=4.83, ppl=28.44, wps=17455.7, ups=2.73, wpb=6401.3, bsz=239.3, num_updates=5400, lr=0.000172133, gnorm=1.241, clip=0, train_wall=32, wall=1887
2020-10-11 23:40:14 | INFO | train_inner | epoch 031:    160 / 178 loss=6.148, nll_loss=4.797, ppl=27.8, wps=19910.3, ups=3.02, wpb=6592.6, bsz=260.6, num_updates=5500, lr=0.000170561, gnorm=1.251, clip=0, train_wall=33, wall=1920
2020-10-11 23:40:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000740
2020-10-11 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027860
2020-10-11 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.020748
2020-10-11 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049686
2020-10-11 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000535
2020-10-11 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027956
2020-10-11 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021262
2020-10-11 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050064
2020-10-11 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:40:22 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.607 | nll_loss 5.189 | ppl 36.48 | wps 44487.8 | wpb 2226.9 | bsz 84.8 | num_updates 5518 | best_loss 6.607
2020-10-11 23:40:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:40:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 31 @ 5518 updates, score 6.607) (writing took 1.8046342980087502 seconds)
2020-10-11 23:40:24 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-11 23:40:24 | INFO | train | epoch 031 | loss 6.144 | nll_loss 4.793 | ppl 27.72 | wps 18448.6 | ups 2.83 | wpb 6509.7 | bsz 250 | num_updates 5518 | lr 0.000170282 | gnorm 1.258 | clip 0 | train_wall 58 | wall 1929
2020-10-11 23:40:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-11 23:40:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-11 23:40:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:40:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000509
2020-10-11 23:40:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005468
2020-10-11 23:40:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:40:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-11 23:40:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:40:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099316
2020-10-11 23:40:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105304
2020-10-11 23:40:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:40:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:40:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004343
2020-10-11 23:40:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:40:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-11 23:40:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:40:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098404
2020-10-11 23:40:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103291
2020-10-11 23:40:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:40:24 | INFO | fairseq.trainer | begin training epoch 32
2020-10-11 23:40:52 | INFO | train_inner | epoch 032:     82 / 178 loss=6.055, nll_loss=4.69, ppl=25.81, wps=17499.5, ups=2.68, wpb=6538.8, bsz=253.1, num_updates=5600, lr=0.000169031, gnorm=1.294, clip=0, train_wall=33, wall=1957
2020-10-11 23:41:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000740
2020-10-11 23:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028204
2020-10-11 23:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021411
2020-10-11 23:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050699
2020-10-11 23:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000559
2020-10-11 23:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027345
2020-10-11 23:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021316
2020-10-11 23:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049545
2020-10-11 23:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:41:25 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.476 | nll_loss 5.04 | ppl 32.91 | wps 44068.4 | wpb 2226.9 | bsz 84.8 | num_updates 5696 | best_loss 6.476
2020-10-11 23:41:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:41:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 32 @ 5696 updates, score 6.476) (writing took 1.4792978070036042 seconds)
2020-10-11 23:41:27 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-11 23:41:27 | INFO | train | epoch 032 | loss 6.049 | nll_loss 4.681 | ppl 25.66 | wps 18484.6 | ups 2.84 | wpb 6509.7 | bsz 250 | num_updates 5696 | lr 0.0001676 | gnorm 1.272 | clip 0 | train_wall 58 | wall 1992
2020-10-11 23:41:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-11 23:41:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-11 23:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:41:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000621
2020-10-11 23:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006215
2020-10-11 23:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-11 23:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102343
2020-10-11 23:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109157
2020-10-11 23:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004384
2020-10-11 23:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-11 23:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099231
2020-10-11 23:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104135
2020-10-11 23:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:41:27 | INFO | fairseq.trainer | begin training epoch 33
2020-10-11 23:41:28 | INFO | train_inner | epoch 033:      4 / 178 loss=6.049, nll_loss=4.68, ppl=25.63, wps=17667.8, ups=2.72, wpb=6491.1, bsz=252.4, num_updates=5700, lr=0.000167542, gnorm=1.254, clip=0, train_wall=32, wall=1994
2020-10-11 23:42:02 | INFO | train_inner | epoch 033:    104 / 178 loss=5.952, nll_loss=4.568, ppl=23.72, wps=19934.8, ups=3.02, wpb=6606.1, bsz=243.1, num_updates=5800, lr=0.000166091, gnorm=1.253, clip=0, train_wall=33, wall=2027
2020-10-11 23:42:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000736
2020-10-11 23:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028144
2020-10-11 23:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021349
2020-10-11 23:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050569
2020-10-11 23:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000553
2020-10-11 23:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027106
2020-10-11 23:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021338
2020-10-11 23:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049326
2020-10-11 23:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:42:28 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.437 | nll_loss 4.981 | ppl 31.59 | wps 44425.7 | wpb 2226.9 | bsz 84.8 | num_updates 5874 | best_loss 6.437
2020-10-11 23:42:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:42:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 33 @ 5874 updates, score 6.437) (writing took 1.4884176929917885 seconds)
2020-10-11 23:42:29 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-11 23:42:29 | INFO | train | epoch 033 | loss 5.942 | nll_loss 4.556 | ppl 23.53 | wps 18517.2 | ups 2.84 | wpb 6509.7 | bsz 250 | num_updates 5874 | lr 0.000165041 | gnorm 1.231 | clip 0 | train_wall 58 | wall 2055
2020-10-11 23:42:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-11 23:42:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-11 23:42:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:42:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000545
2020-10-11 23:42:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006160
2020-10-11 23:42:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:42:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000255
2020-10-11 23:42:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101205
2020-10-11 23:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107968
2020-10-11 23:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004247
2020-10-11 23:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-11 23:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097839
2020-10-11 23:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102584
2020-10-11 23:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:42:30 | INFO | fairseq.trainer | begin training epoch 34
2020-10-11 23:42:38 | INFO | train_inner | epoch 034:     26 / 178 loss=5.895, nll_loss=4.503, ppl=22.67, wps=17498.1, ups=2.72, wpb=6434.1, bsz=258.5, num_updates=5900, lr=0.000164677, gnorm=1.214, clip=0, train_wall=32, wall=2064
2020-10-11 23:43:11 | INFO | train_inner | epoch 034:    126 / 178 loss=5.843, nll_loss=4.441, ppl=21.72, wps=19956.9, ups=3.02, wpb=6611, bsz=256.2, num_updates=6000, lr=0.000163299, gnorm=1.265, clip=0, train_wall=33, wall=2097
2020-10-11 23:43:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000759
2020-10-11 23:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027812
2020-10-11 23:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021750
2020-10-11 23:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050667
2020-10-11 23:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000559
2020-10-11 23:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028097
2020-10-11 23:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:43:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021795
2020-10-11 23:43:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050780
2020-10-11 23:43:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:43:31 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.404 | nll_loss 4.935 | ppl 30.59 | wps 44003.9 | wpb 2226.9 | bsz 84.8 | num_updates 6052 | best_loss 6.404
2020-10-11 23:43:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:43:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 34 @ 6052 updates, score 6.404) (writing took 1.696933317987714 seconds)
2020-10-11 23:43:32 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-11 23:43:32 | INFO | train | epoch 034 | loss 5.848 | nll_loss 4.446 | ppl 21.8 | wps 18442.5 | ups 2.83 | wpb 6509.7 | bsz 250 | num_updates 6052 | lr 0.000162596 | gnorm 1.263 | clip 0 | train_wall 58 | wall 2118
2020-10-11 23:43:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-11 23:43:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-11 23:43:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:43:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000546
2020-10-11 23:43:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005551
2020-10-11 23:43:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:43:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-11 23:43:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:43:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101791
2020-10-11 23:43:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107866
2020-10-11 23:43:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:43:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:43:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004374
2020-10-11 23:43:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:43:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-11 23:43:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:43:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099918
2020-10-11 23:43:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104810
2020-10-11 23:43:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:43:32 | INFO | fairseq.trainer | begin training epoch 35
2020-10-11 23:43:48 | INFO | train_inner | epoch 035:     48 / 178 loss=5.821, nll_loss=4.415, ppl=21.34, wps=17198.6, ups=2.73, wpb=6298.1, bsz=231.4, num_updates=6100, lr=0.000161955, gnorm=1.245, clip=0, train_wall=32, wall=2133
2020-10-11 23:44:21 | INFO | train_inner | epoch 035:    148 / 178 loss=5.753, nll_loss=4.334, ppl=20.17, wps=19921, ups=3.01, wpb=6621, bsz=260.3, num_updates=6200, lr=0.000160644, gnorm=1.216, clip=0, train_wall=33, wall=2167
2020-10-11 23:44:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000668
2020-10-11 23:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028344
2020-10-11 23:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021129
2020-10-11 23:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050482
2020-10-11 23:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000573
2020-10-11 23:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027840
2020-10-11 23:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021246
2020-10-11 23:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049986
2020-10-11 23:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:44:33 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.358 | nll_loss 4.883 | ppl 29.5 | wps 44493.3 | wpb 2226.9 | bsz 84.8 | num_updates 6230 | best_loss 6.358
2020-10-11 23:44:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:44:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 35 @ 6230 updates, score 6.358) (writing took 1.5049782319983933 seconds)
2020-10-11 23:44:35 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-11 23:44:35 | INFO | train | epoch 035 | loss 5.754 | nll_loss 4.336 | ppl 20.2 | wps 18513.5 | ups 2.84 | wpb 6509.7 | bsz 250 | num_updates 6230 | lr 0.000160257 | gnorm 1.22 | clip 0 | train_wall 58 | wall 2180
2020-10-11 23:44:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-11 23:44:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-11 23:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:44:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000609
2020-10-11 23:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005674
2020-10-11 23:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-11 23:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097674
2020-10-11 23:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103907
2020-10-11 23:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004349
2020-10-11 23:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 23:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097652
2020-10-11 23:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102500
2020-10-11 23:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:44:35 | INFO | fairseq.trainer | begin training epoch 36
2020-10-11 23:44:58 | INFO | train_inner | epoch 036:     70 / 178 loss=5.69, nll_loss=4.261, ppl=19.18, wps=17643, ups=2.72, wpb=6489.8, bsz=247.9, num_updates=6300, lr=0.000159364, gnorm=1.239, clip=0, train_wall=32, wall=2203
2020-10-11 23:45:31 | INFO | train_inner | epoch 036:    170 / 178 loss=5.68, nll_loss=4.248, ppl=19.01, wps=19734.1, ups=3.02, wpb=6532, bsz=254.1, num_updates=6400, lr=0.000158114, gnorm=1.21, clip=0, train_wall=33, wall=2237
2020-10-11 23:45:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000671
2020-10-11 23:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027934
2020-10-11 23:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021144
2020-10-11 23:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050084
2020-10-11 23:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000560
2020-10-11 23:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028021
2020-10-11 23:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021098
2020-10-11 23:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050007
2020-10-11 23:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:45:36 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.326 | nll_loss 4.844 | ppl 28.73 | wps 43965.5 | wpb 2226.9 | bsz 84.8 | num_updates 6408 | best_loss 6.326
2020-10-11 23:45:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:45:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 36 @ 6408 updates, score 6.326) (writing took 1.3983632449962897 seconds)
2020-10-11 23:45:37 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-11 23:45:37 | INFO | train | epoch 036 | loss 5.67 | nll_loss 4.237 | ppl 18.86 | wps 18547.4 | ups 2.85 | wpb 6509.7 | bsz 250 | num_updates 6408 | lr 0.000158015 | gnorm 1.223 | clip 0 | train_wall 58 | wall 2243
2020-10-11 23:45:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-11 23:45:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-11 23:45:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:45:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000534
2020-10-11 23:45:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005491
2020-10-11 23:45:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:45:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 23:45:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:45:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097045
2020-10-11 23:45:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103038
2020-10-11 23:45:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:45:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:45:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004224
2020-10-11 23:45:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:45:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-11 23:45:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:45:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097917
2020-10-11 23:45:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102641
2020-10-11 23:45:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:45:38 | INFO | fairseq.trainer | begin training epoch 37
2020-10-11 23:46:08 | INFO | train_inner | epoch 037:     92 / 178 loss=5.589, nll_loss=4.143, ppl=17.66, wps=17756, ups=2.73, wpb=6515, bsz=243.9, num_updates=6500, lr=0.000156893, gnorm=1.235, clip=0, train_wall=32, wall=2273
2020-10-11 23:46:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000620
2020-10-11 23:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027244
2020-10-11 23:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021434
2020-10-11 23:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049628
2020-10-11 23:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000511
2020-10-11 23:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027626
2020-10-11 23:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021608
2020-10-11 23:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050067
2020-10-11 23:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:46:38 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.326 | nll_loss 4.839 | ppl 28.62 | wps 44387.4 | wpb 2226.9 | bsz 84.8 | num_updates 6586 | best_loss 6.326
2020-10-11 23:46:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:46:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 37 @ 6586 updates, score 6.326) (writing took 1.8058741959976032 seconds)
2020-10-11 23:46:40 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-11 23:46:40 | INFO | train | epoch 037 | loss 5.595 | nll_loss 4.149 | ppl 17.74 | wps 18419.3 | ups 2.83 | wpb 6509.7 | bsz 250 | num_updates 6586 | lr 0.000155865 | gnorm 1.238 | clip 0 | train_wall 58 | wall 2306
2020-10-11 23:46:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-11 23:46:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-11 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:46:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000573
2020-10-11 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005661
2020-10-11 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-11 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099501
2020-10-11 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105678
2020-10-11 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004264
2020-10-11 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-11 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098217
2020-10-11 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102993
2020-10-11 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:46:40 | INFO | fairseq.trainer | begin training epoch 38
2020-10-11 23:46:45 | INFO | train_inner | epoch 038:     14 / 178 loss=5.583, nll_loss=4.135, ppl=17.57, wps=17472.4, ups=2.68, wpb=6524.5, bsz=261, num_updates=6600, lr=0.0001557, gnorm=1.262, clip=0, train_wall=33, wall=2311
2020-10-11 23:47:18 | INFO | train_inner | epoch 038:    114 / 178 loss=5.499, nll_loss=4.038, ppl=16.43, wps=19496, ups=3.02, wpb=6456.2, bsz=251.1, num_updates=6700, lr=0.000154533, gnorm=1.214, clip=0, train_wall=33, wall=2344
2020-10-11 23:47:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:47:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:47:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000741
2020-10-11 23:47:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:47:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028017
2020-10-11 23:47:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:47:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021434
2020-10-11 23:47:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050533
2020-10-11 23:47:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:47:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:47:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000525
2020-10-11 23:47:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:47:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027938
2020-10-11 23:47:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:47:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021386
2020-10-11 23:47:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.050169
2020-10-11 23:47:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:47:41 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.275 | nll_loss 4.769 | ppl 27.27 | wps 44131.8 | wpb 2226.9 | bsz 84.8 | num_updates 6764 | best_loss 6.275
2020-10-11 23:47:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:47:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 38 @ 6764 updates, score 6.275) (writing took 1.9077830120077124 seconds)
2020-10-11 23:47:43 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-11 23:47:43 | INFO | train | epoch 038 | loss 5.512 | nll_loss 4.052 | ppl 16.59 | wps 18351.8 | ups 2.82 | wpb 6509.7 | bsz 250 | num_updates 6764 | lr 0.000153801 | gnorm 1.233 | clip 0 | train_wall 58 | wall 2369
2020-10-11 23:47:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-11 23:47:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-11 23:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:47:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000448
2020-10-11 23:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005436
2020-10-11 23:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-11 23:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098445
2020-10-11 23:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104394
2020-10-11 23:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004261
2020-10-11 23:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-11 23:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:47:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098935
2020-10-11 23:47:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103682
2020-10-11 23:47:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:47:44 | INFO | fairseq.trainer | begin training epoch 39
2020-10-11 23:47:56 | INFO | train_inner | epoch 039:     36 / 178 loss=5.51, nll_loss=4.048, ppl=16.54, wps=17833.8, ups=2.68, wpb=6644.9, bsz=247.8, num_updates=6800, lr=0.000153393, gnorm=1.231, clip=0, train_wall=33, wall=2381
2020-10-11 23:48:29 | INFO | train_inner | epoch 039:    136 / 178 loss=5.438, nll_loss=3.965, ppl=15.62, wps=19376, ups=3.03, wpb=6404.6, bsz=241.8, num_updates=6900, lr=0.000152277, gnorm=1.221, clip=0, train_wall=33, wall=2414
2020-10-11 23:48:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000743
2020-10-11 23:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027481
2020-10-11 23:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021336
2020-10-11 23:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049898
2020-10-11 23:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000534
2020-10-11 23:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027601
2020-10-11 23:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021283
2020-10-11 23:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049744
2020-10-11 23:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:48:44 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.269 | nll_loss 4.752 | ppl 26.95 | wps 44283.9 | wpb 2226.9 | bsz 84.8 | num_updates 6942 | best_loss 6.269
2020-10-11 23:48:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:48:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 39 @ 6942 updates, score 6.269) (writing took 1.4963841550052166 seconds)
2020-10-11 23:48:46 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-11 23:48:46 | INFO | train | epoch 039 | loss 5.444 | nll_loss 3.972 | ppl 15.69 | wps 18507.5 | ups 2.84 | wpb 6509.7 | bsz 250 | num_updates 6942 | lr 0.000151816 | gnorm 1.23 | clip 0 | train_wall 58 | wall 2431
2020-10-11 23:48:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-11 23:48:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-11 23:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:48:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000580
2020-10-11 23:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005989
2020-10-11 23:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-11 23:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099276
2020-10-11 23:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105895
2020-10-11 23:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004326
2020-10-11 23:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 23:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094326
2020-10-11 23:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099147
2020-10-11 23:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:48:46 | INFO | fairseq.trainer | begin training epoch 40
2020-10-11 23:49:05 | INFO | train_inner | epoch 040:     58 / 178 loss=5.398, nll_loss=3.918, ppl=15.12, wps=17767.6, ups=2.72, wpb=6536.2, bsz=253, num_updates=7000, lr=0.000151186, gnorm=1.231, clip=0, train_wall=32, wall=2451
2020-10-11 23:49:38 | INFO | train_inner | epoch 040:    158 / 178 loss=5.375, nll_loss=3.891, ppl=14.83, wps=19744.4, ups=3.04, wpb=6503.7, bsz=251.8, num_updates=7100, lr=0.000150117, gnorm=1.246, clip=0, train_wall=33, wall=2484
2020-10-11 23:49:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 23:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000728
2020-10-11 23:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027616
2020-10-11 23:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.020954
2020-10-11 23:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049633
2020-10-11 23:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000524
2020-10-11 23:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.027676
2020-10-11 23:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.021134
2020-10-11 23:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.049647
2020-10-11 23:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:49:47 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.24 | nll_loss 4.732 | ppl 26.57 | wps 44072.9 | wpb 2226.9 | bsz 84.8 | num_updates 7120 | best_loss 6.24
2020-10-11 23:49:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 23:49:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukr_sepspm8000/O2M/checkpoint_best.pt (epoch 40 @ 7120 updates, score 6.24) (writing took 1.48941124099656 seconds)
2020-10-11 23:49:49 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-11 23:49:49 | INFO | train | epoch 040 | loss 5.373 | nll_loss 3.888 | ppl 14.81 | wps 18509.6 | ups 2.84 | wpb 6509.7 | bsz 250 | num_updates 7120 | lr 0.000149906 | gnorm 1.239 | clip 0 | train_wall 58 | wall 2494
2020-10-11 23:49:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-11 23:49:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-11 23:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 23:49:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000469
2020-10-11 23:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005988
2020-10-11 23:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 23:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100143
2020-10-11 23:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106640
2020-10-11 23:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 23:49:49 | INFO | fairseq_cli.train | done training in 2493.9 seconds
