2020-10-12 20:17:04 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_azetur_sepspm8000/O2M/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok='tgt', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='eng-aze,eng-tur', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_azetur_sepspm8000/O2M/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 20:17:04 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 20:17:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['aze', 'eng', 'tur']
2020-10-12 20:17:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | [aze] dictionary: 19763 types
2020-10-12 20:17:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 19763 types
2020-10-12 20:17:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | [tur] dictionary: 19763 types
2020-10-12 20:17:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 20:17:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13308.0625Mb; avail=475372.6796875Mb
2020-10-12 20:17:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 20:17:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:eng-aze': 1, 'main:eng-tur': 1}
2020-10-12 20:17:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-aze src_langtok: 19760; tgt_langtok: None
2020-10-12 20:17:04 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azetur_sepspm8000/O2M/valid.eng-aze.eng
2020-10-12 20:17:04 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azetur_sepspm8000/O2M/valid.eng-aze.aze
2020-10-12 20:17:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azetur_sepspm8000/O2M/ valid eng-aze 671 examples
2020-10-12 20:17:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-tur src_langtok: 19762; tgt_langtok: None
2020-10-12 20:17:04 | INFO | fairseq.data.data_utils | loaded 4045 examples from: fairseq/data-bin/ted_azetur_sepspm8000/O2M/valid.eng-tur.eng
2020-10-12 20:17:04 | INFO | fairseq.data.data_utils | loaded 4045 examples from: fairseq/data-bin/ted_azetur_sepspm8000/O2M/valid.eng-tur.tur
2020-10-12 20:17:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azetur_sepspm8000/O2M/ valid eng-tur 4045 examples
2020-10-12 20:17:05 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19763, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19763, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19763, bias=False)
  )
)
2020-10-12 20:17:05 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 20:17:05 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 20:17:05 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 20:17:05 | INFO | fairseq_cli.train | num. model params: 41661952 (num. trained: 41661952)
2020-10-12 20:17:09 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 20:17:09 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 20:17:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 20:17:09 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 20:17:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 20:17:09 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 20:17:09 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 20:17:09 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_last.pt
2020-10-12 20:17:09 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 20:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 20:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17402.26953125Mb; avail=471258.4296875Mb
2020-10-12 20:17:09 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 20:17:09 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:eng-aze': 1, 'main:eng-tur': 1}
2020-10-12 20:17:09 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-aze src_langtok: 19760; tgt_langtok: None
2020-10-12 20:17:09 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azetur_sepspm8000/O2M/train.eng-aze.eng
2020-10-12 20:17:09 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azetur_sepspm8000/O2M/train.eng-aze.aze
2020-10-12 20:17:09 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azetur_sepspm8000/O2M/ train eng-aze 5946 examples
2020-10-12 20:17:09 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-tur src_langtok: 19762; tgt_langtok: None
2020-10-12 20:17:09 | INFO | fairseq.data.data_utils | loaded 19990 examples from: fairseq/data-bin/ted_azetur_sepspm8000/O2M/train.eng-tur.eng
2020-10-12 20:17:09 | INFO | fairseq.data.data_utils | loaded 19990 examples from: fairseq/data-bin/ted_azetur_sepspm8000/O2M/train.eng-tur.tur
2020-10-12 20:17:09 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azetur_sepspm8000/O2M/ train eng-tur 19990 examples
2020-10-12 20:17:09 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:eng-aze', 5946), ('main:eng-tur', 19990)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 20:17:09 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 20:17:09 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 25936
2020-10-12 20:17:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 25936; virtual dataset size 25936
2020-10-12 20:17:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 20:17:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:eng-aze': 5946, 'main:eng-tur': 19990}; raw total size: 25936
2020-10-12 20:17:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:eng-aze': 5946, 'main:eng-tur': 19990}; resampled total size: 25936
2020-10-12 20:17:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 20:17:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.004652
2020-10-12 20:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17402.70703125Mb; avail=471257.9921875Mb
2020-10-12 20:17:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000460
2020-10-12 20:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004708
2020-10-12 20:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17402.94921875Mb; avail=471257.75Mb
2020-10-12 20:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17402.94921875Mb; avail=471257.75Mb
2020-10-12 20:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091825
2020-10-12 20:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097461
2020-10-12 20:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17451.3984375Mb; avail=471209.5703125Mb
2020-10-12 20:17:09 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 20:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17472.14453125Mb; avail=471188.82421875Mb
2020-10-12 20:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003530
2020-10-12 20:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17472.14453125Mb; avail=471188.82421875Mb
2020-10-12 20:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 20:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17472.14453125Mb; avail=471188.82421875Mb
2020-10-12 20:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089466
2020-10-12 20:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093888
2020-10-12 20:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17510.06640625Mb; avail=471151.03515625Mb
2020-10-12 20:17:09 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 20:17:37 | INFO | train_inner | epoch 001:    100 / 104 loss=14.154, nll_loss=14.059, ppl=17066.3, wps=21282.1, ups=3.59, wpb=5928.3, bsz=251.4, num_updates=100, lr=5.0975e-06, gnorm=3.97, clip=0, train_wall=28, wall=29
2020-10-12 20:17:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:17:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18741.40625Mb; avail=469915.28125Mb
2020-10-12 20:17:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001623
2020-10-12 20:17:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18741.40625Mb; avail=469915.28125Mb
2020-10-12 20:17:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069107
2020-10-12 20:17:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18741.27734375Mb; avail=469915.40625Mb
2020-10-12 20:17:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052069
2020-10-12 20:17:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123654
2020-10-12 20:17:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18741.3984375Mb; avail=469915.28515625Mb
2020-10-12 20:17:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18741.3984375Mb; avail=469915.28515625Mb
2020-10-12 20:17:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001111
2020-10-12 20:17:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18741.3984375Mb; avail=469915.28515625Mb
2020-10-12 20:17:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069003
2020-10-12 20:17:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18741.3984375Mb; avail=469915.28515625Mb
2020-10-12 20:17:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048801
2020-10-12 20:17:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119707
2020-10-12 20:17:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18741.3984375Mb; avail=469915.28515625Mb
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2020-10-12 20:17:41 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.922 | nll_loss 12.664 | ppl 6490.11 | wps 57028.6 | wpb 2291.8 | bsz 96.2 | num_updates 104
2020-10-12 20:17:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:17:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 1 @ 104 updates, score 12.922) (writing took 1.526458456999535 seconds)
2020-10-12 20:17:42 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 20:17:42 | INFO | train | epoch 001 | loss 14.128 | nll_loss 14.03 | ppl 16730.1 | wps 18586.5 | ups 3.17 | wpb 5855.6 | bsz 249.4 | num_updates 104 | lr 5.2974e-06 | gnorm 3.972 | clip 0 | train_wall 29 | wall 34
2020-10-12 20:17:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 20:17:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 20:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16172.64453125Mb; avail=472480.9140625Mb
2020-10-12 20:17:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000769
2020-10-12 20:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005235
2020-10-12 20:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16175.671875Mb; avail=472477.88671875Mb
2020-10-12 20:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16176.27734375Mb; avail=472477.28125Mb
2020-10-12 20:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091038
2020-10-12 20:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097215
2020-10-12 20:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16204.01953125Mb; avail=472449.4296875Mb
2020-10-12 20:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16174.3359375Mb; avail=472486.0625Mb
2020-10-12 20:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003770
2020-10-12 20:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16092.5703125Mb; avail=472560.76953125Mb
2020-10-12 20:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-12 20:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16093.17578125Mb; avail=472560.1640625Mb
2020-10-12 20:17:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089611
2020-10-12 20:17:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094346
2020-10-12 20:17:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15903.0546875Mb; avail=472750.65625Mb
2020-10-12 20:17:43 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 20:18:09 | INFO | train_inner | epoch 002:     96 / 104 loss=12.916, nll_loss=12.674, ppl=6533.05, wps=18257.7, ups=3.13, wpb=5838.3, bsz=254.2, num_updates=200, lr=1.0095e-05, gnorm=1.694, clip=0, train_wall=27, wall=61
2020-10-12 20:18:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:18:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17943.93359375Mb; avail=470680.078125Mb
2020-10-12 20:18:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001526
2020-10-12 20:18:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17943.93359375Mb; avail=470680.078125Mb
2020-10-12 20:18:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076713
2020-10-12 20:18:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17943.6015625Mb; avail=470680.3203125Mb
2020-10-12 20:18:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051292
2020-10-12 20:18:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130360
2020-10-12 20:18:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17943.6015625Mb; avail=470680.3203125Mb
2020-10-12 20:18:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17943.6015625Mb; avail=470680.3203125Mb
2020-10-12 20:18:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001054
2020-10-12 20:18:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17943.6015625Mb; avail=470680.3203125Mb
2020-10-12 20:18:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073909
2020-10-12 20:18:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17943.703125Mb; avail=470680.44140625Mb
2020-10-12 20:18:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053979
2020-10-12 20:18:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129894
2020-10-12 20:18:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17943.89453125Mb; avail=470680.078125Mb
2020-10-12 20:18:14 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 12.334 | nll_loss 12 | ppl 4096.08 | wps 57710.3 | wpb 2291.8 | bsz 96.2 | num_updates 208 | best_loss 12.334
2020-10-12 20:18:14 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:18:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 2 @ 208 updates, score 12.334) (writing took 4.365576471999702 seconds)
2020-10-12 20:18:18 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 20:18:18 | INFO | train | epoch 002 | loss 12.892 | nll_loss 12.647 | ppl 6412.49 | wps 16938 | ups 2.89 | wpb 5855.6 | bsz 249.4 | num_updates 208 | lr 1.04948e-05 | gnorm 1.577 | clip 0 | train_wall 28 | wall 70
2020-10-12 20:18:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 20:18:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 20:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17983.83984375Mb; avail=470640.125Mb
2020-10-12 20:18:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000982
2020-10-12 20:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006015
2020-10-12 20:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17983.83984375Mb; avail=470640.125Mb
2020-10-12 20:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000277
2020-10-12 20:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17983.83984375Mb; avail=470640.125Mb
2020-10-12 20:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095570
2020-10-12 20:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102647
2020-10-12 20:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17984.48828125Mb; avail=470639.671875Mb
2020-10-12 20:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17984.58984375Mb; avail=470639.671875Mb
2020-10-12 20:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003618
2020-10-12 20:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17984.50390625Mb; avail=470639.671875Mb
2020-10-12 20:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 20:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17984.50390625Mb; avail=470639.671875Mb
2020-10-12 20:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089469
2020-10-12 20:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093991
2020-10-12 20:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17984.67578125Mb; avail=470639.30859375Mb
2020-10-12 20:18:19 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 20:18:44 | INFO | train_inner | epoch 003:     92 / 104 loss=12.36, nll_loss=12.055, ppl=4256.2, wps=16632, ups=2.85, wpb=5826.4, bsz=243.4, num_updates=300, lr=1.50925e-05, gnorm=1.241, clip=0, train_wall=27, wall=96
2020-10-12 20:18:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18523.30078125Mb; avail=470068.8671875Mb
2020-10-12 20:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001942
2020-10-12 20:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18523.30078125Mb; avail=470068.8671875Mb
2020-10-12 20:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078434
2020-10-12 20:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18523.06640625Mb; avail=470069.26171875Mb
2020-10-12 20:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055615
2020-10-12 20:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136938
2020-10-12 20:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18523.09765625Mb; avail=470069.23046875Mb
2020-10-12 20:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18523.12890625Mb; avail=470069.19921875Mb
2020-10-12 20:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001228
2020-10-12 20:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18523.12890625Mb; avail=470069.19921875Mb
2020-10-12 20:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077797
2020-10-12 20:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18523.13671875Mb; avail=470069.0625Mb
2020-10-12 20:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054996
2020-10-12 20:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.135018
2020-10-12 20:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18523.10546875Mb; avail=470069.18359375Mb
2020-10-12 20:18:50 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 11.697 | nll_loss 11.278 | ppl 2482.38 | wps 57910.2 | wpb 2291.8 | bsz 96.2 | num_updates 312 | best_loss 11.697
2020-10-12 20:18:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:19:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 3 @ 312 updates, score 11.697) (writing took 23.89448736600025 seconds)
2020-10-12 20:19:14 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 20:19:14 | INFO | train | epoch 003 | loss 12.299 | nll_loss 11.987 | ppl 4059.77 | wps 10915.1 | ups 1.86 | wpb 5855.6 | bsz 249.4 | num_updates 312 | lr 1.56922e-05 | gnorm 1.261 | clip 0 | train_wall 28 | wall 125
2020-10-12 20:19:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 20:19:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19015.55859375Mb; avail=469577.8984375Mb
2020-10-12 20:19:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000855
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005675
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19016.03515625Mb; avail=469576.9296875Mb
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000197
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19016.03515625Mb; avail=469576.9296875Mb
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090291
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096958
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18391.9765625Mb; avail=470202.9296875Mb
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18391.9921875Mb; avail=470206.11328125Mb
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003700
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18391.5Mb; avail=470205.8671875Mb
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000201
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18391.5Mb; avail=470205.8671875Mb
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089698
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094417
2020-10-12 20:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18392.04296875Mb; avail=470208.20703125Mb
2020-10-12 20:19:14 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 20:19:39 | INFO | train_inner | epoch 004:     88 / 104 loss=11.722, nll_loss=11.334, ppl=2581.91, wps=10660.8, ups=1.83, wpb=5837.8, bsz=249, num_updates=400, lr=2.009e-05, gnorm=1.662, clip=0, train_wall=27, wall=151
2020-10-12 20:19:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17854.68359375Mb; avail=470736.828125Mb
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001345
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.68359375Mb; avail=470736.828125Mb
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069868
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17859.36328125Mb; avail=470731.734375Mb
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049465
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121458
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17868.38671875Mb; avail=470723.87890625Mb
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17868.9921875Mb; avail=470723.2734375Mb
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001095
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17869.01953125Mb; avail=470723.15234375Mb
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069268
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17870.1171875Mb; avail=470721.7890625Mb
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050136
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121255
2020-10-12 20:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17870.25390625Mb; avail=470721.66796875Mb
2020-10-12 20:19:46 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.98 | nll_loss 10.445 | ppl 1394.27 | wps 58091.6 | wpb 2291.8 | bsz 96.2 | num_updates 416 | best_loss 10.98
2020-10-12 20:19:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:19:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 4 @ 416 updates, score 10.98) (writing took 10.957640761999755 seconds)
2020-10-12 20:19:57 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 20:19:57 | INFO | train | epoch 004 | loss 11.635 | nll_loss 11.234 | ppl 2409.15 | wps 14170 | ups 2.42 | wpb 5855.6 | bsz 249.4 | num_updates 416 | lr 2.08896e-05 | gnorm 1.649 | clip 0 | train_wall 29 | wall 168
2020-10-12 20:19:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 20:19:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 20:19:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19010.0Mb; avail=469582.8671875Mb
2020-10-12 20:19:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000976
2020-10-12 20:19:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005560
2020-10-12 20:19:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19010.0Mb; avail=469582.8671875Mb
2020-10-12 20:19:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:19:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19010.0Mb; avail=469582.8671875Mb
2020-10-12 20:19:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091163
2020-10-12 20:19:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097683
2020-10-12 20:19:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19091.04296875Mb; avail=469501.24609375Mb
2020-10-12 20:19:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19148.5625Mb; avail=469444.33203125Mb
2020-10-12 20:19:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003654
2020-10-12 20:19:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19153.40625Mb; avail=469439.48828125Mb
2020-10-12 20:19:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:19:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19154.01171875Mb; avail=469438.27734375Mb
2020-10-12 20:19:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091454
2020-10-12 20:19:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096051
2020-10-12 20:19:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19264.828125Mb; avail=469327.4609375Mb
2020-10-12 20:19:57 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 20:20:21 | INFO | train_inner | epoch 005:     84 / 104 loss=11.177, nll_loss=10.695, ppl=1658.02, wps=14047.8, ups=2.39, wpb=5882.1, bsz=243.1, num_updates=500, lr=2.50875e-05, gnorm=1.359, clip=0, train_wall=27, wall=192
2020-10-12 20:20:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:20:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17834.80078125Mb; avail=470757.10546875Mb
2020-10-12 20:20:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001653
2020-10-12 20:20:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.80078125Mb; avail=470757.10546875Mb
2020-10-12 20:20:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069964
2020-10-12 20:20:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.6484375Mb; avail=470756.984375Mb
2020-10-12 20:20:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049749
2020-10-12 20:20:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122145
2020-10-12 20:20:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.68359375Mb; avail=470757.10546875Mb
2020-10-12 20:20:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17834.74609375Mb; avail=470756.86328125Mb
2020-10-12 20:20:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001084
2020-10-12 20:20:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.74609375Mb; avail=470756.86328125Mb
2020-10-12 20:20:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070467
2020-10-12 20:20:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.75Mb; avail=470756.86328125Mb
2020-10-12 20:20:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049085
2020-10-12 20:20:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121561
2020-10-12 20:20:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.46875Mb; avail=470757.33984375Mb
2020-10-12 20:20:29 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.671 | nll_loss 10.04 | ppl 1052.58 | wps 58312.1 | wpb 2291.8 | bsz 96.2 | num_updates 520 | best_loss 10.671
2020-10-12 20:20:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:20:36 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 5 @ 520 updates, score 10.671) (writing took 7.423694878999413 seconds)
2020-10-12 20:20:36 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 20:20:36 | INFO | train | epoch 005 | loss 11.104 | nll_loss 10.608 | ppl 1560.47 | wps 15496.9 | ups 2.65 | wpb 5855.6 | bsz 249.4 | num_updates 520 | lr 2.6087e-05 | gnorm 1.352 | clip 0 | train_wall 28 | wall 208
2020-10-12 20:20:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 20:20:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17814.8125Mb; avail=470777.2109375Mb
2020-10-12 20:20:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000890
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005407
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17814.3203125Mb; avail=470777.82421875Mb
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000236
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17814.40234375Mb; avail=470777.82421875Mb
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093667
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100119
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17887.29296875Mb; avail=470704.9375Mb
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17944.12890625Mb; avail=470647.296875Mb
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003630
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17948.3671875Mb; avail=470643.6640625Mb
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17948.97265625Mb; avail=470643.05859375Mb
2020-10-12 20:20:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091257
2020-10-12 20:20:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095808
2020-10-12 20:20:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18061.609375Mb; avail=470530.3203125Mb
2020-10-12 20:20:37 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 20:20:59 | INFO | train_inner | epoch 006:     80 / 104 loss=10.875, nll_loss=10.325, ppl=1282.48, wps=15288.1, ups=2.64, wpb=5795.9, bsz=255.9, num_updates=600, lr=3.0085e-05, gnorm=1.181, clip=0, train_wall=27, wall=230
2020-10-12 20:21:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17819.03515625Mb; avail=470772.83203125Mb
2020-10-12 20:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001685
2020-10-12 20:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17819.03515625Mb; avail=470772.83203125Mb
2020-10-12 20:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069546
2020-10-12 20:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.796875Mb; avail=470773.0703125Mb
2020-10-12 20:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049078
2020-10-12 20:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121093
2020-10-12 20:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.8828125Mb; avail=470772.70703125Mb
2020-10-12 20:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17818.890625Mb; avail=470772.70703125Mb
2020-10-12 20:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001080
2020-10-12 20:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.890625Mb; avail=470772.70703125Mb
2020-10-12 20:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069279
2020-10-12 20:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.703125Mb; avail=470772.70703125Mb
2020-10-12 20:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048823
2020-10-12 20:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119942
2020-10-12 20:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17819.15234375Mb; avail=470772.83984375Mb
2020-10-12 20:21:08 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 10.443 | nll_loss 9.783 | ppl 880.81 | wps 57537.1 | wpb 2291.8 | bsz 96.2 | num_updates 624 | best_loss 10.443
2020-10-12 20:21:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:21:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 6 @ 624 updates, score 10.443) (writing took 10.76577112799987 seconds)
2020-10-12 20:21:19 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 20:21:19 | INFO | train | epoch 006 | loss 10.855 | nll_loss 10.297 | ppl 1258.29 | wps 14358.3 | ups 2.45 | wpb 5855.6 | bsz 249.4 | num_updates 624 | lr 3.12844e-05 | gnorm 1.174 | clip 0 | train_wall 28 | wall 250
2020-10-12 20:21:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 20:21:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 20:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18492.5390625Mb; avail=470100.00390625Mb
2020-10-12 20:21:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000802
2020-10-12 20:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005519
2020-10-12 20:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18492.5390625Mb; avail=470100.00390625Mb
2020-10-12 20:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000210
2020-10-12 20:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18492.5390625Mb; avail=470100.00390625Mb
2020-10-12 20:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092547
2020-10-12 20:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099071
2020-10-12 20:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18492.9921875Mb; avail=470099.72265625Mb
2020-10-12 20:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18492.99609375Mb; avail=470099.72265625Mb
2020-10-12 20:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003663
2020-10-12 20:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18492.99609375Mb; avail=470099.72265625Mb
2020-10-12 20:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 20:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18492.99609375Mb; avail=470099.6015625Mb
2020-10-12 20:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090915
2020-10-12 20:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095573
2020-10-12 20:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18493.19921875Mb; avail=470099.71484375Mb
2020-10-12 20:21:19 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 20:21:40 | INFO | train_inner | epoch 007:     76 / 104 loss=10.771, nll_loss=10.189, ppl=1167.4, wps=14263.6, ups=2.42, wpb=5905.9, bsz=252.1, num_updates=700, lr=3.50825e-05, gnorm=1.322, clip=0, train_wall=27, wall=272
2020-10-12 20:21:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18386.95703125Mb; avail=470205.140625Mb
2020-10-12 20:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001765
2020-10-12 20:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18387.05078125Mb; avail=470205.26171875Mb
2020-10-12 20:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070077
2020-10-12 20:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18387.171875Mb; avail=470205.140625Mb
2020-10-12 20:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051410
2020-10-12 20:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124213
2020-10-12 20:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18387.171875Mb; avail=470205.140625Mb
2020-10-12 20:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18387.1796875Mb; avail=470205.01953125Mb
2020-10-12 20:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001202
2020-10-12 20:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18387.1796875Mb; avail=470205.01953125Mb
2020-10-12 20:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070251
2020-10-12 20:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18387.51171875Mb; avail=470204.65625Mb
2020-10-12 20:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050653
2020-10-12 20:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122935
2020-10-12 20:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18387.48828125Mb; avail=470204.77734375Mb
2020-10-12 20:21:51 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 10.325 | nll_loss 9.649 | ppl 802.98 | wps 56997.6 | wpb 2291.8 | bsz 96.2 | num_updates 728 | best_loss 10.325
2020-10-12 20:21:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:21:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 7 @ 728 updates, score 10.325) (writing took 6.283434884999224 seconds)
2020-10-12 20:21:57 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 20:21:57 | INFO | train | epoch 007 | loss 10.738 | nll_loss 10.148 | ppl 1134.88 | wps 15997.1 | ups 2.73 | wpb 5855.6 | bsz 249.4 | num_updates 728 | lr 3.64818e-05 | gnorm 1.303 | clip 0 | train_wall 28 | wall 288
2020-10-12 20:21:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 20:21:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 20:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18360.625Mb; avail=470232.35546875Mb
2020-10-12 20:21:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000849
2020-10-12 20:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005468
2020-10-12 20:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18360.625Mb; avail=470232.35546875Mb
2020-10-12 20:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000209
2020-10-12 20:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18360.625Mb; avail=470232.35546875Mb
2020-10-12 20:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090932
2020-10-12 20:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097370
2020-10-12 20:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18360.94921875Mb; avail=470231.62890625Mb
2020-10-12 20:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18361.078125Mb; avail=470231.39453125Mb
2020-10-12 20:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003634
2020-10-12 20:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18361.03125Mb; avail=470231.63671875Mb
2020-10-12 20:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 20:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18361.03125Mb; avail=470231.63671875Mb
2020-10-12 20:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091128
2020-10-12 20:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095712
2020-10-12 20:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18361.01953125Mb; avail=470231.63671875Mb
2020-10-12 20:21:57 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 20:22:17 | INFO | train_inner | epoch 008:     72 / 104 loss=10.647, nll_loss=10.039, ppl=1051.87, wps=15565.6, ups=2.71, wpb=5747.6, bsz=245, num_updates=800, lr=4.008e-05, gnorm=1.164, clip=0, train_wall=27, wall=309
2020-10-12 20:22:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:22:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17813.84375Mb; avail=470780.734375Mb
2020-10-12 20:22:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001574
2020-10-12 20:22:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.0703125Mb; avail=470780.7421875Mb
2020-10-12 20:22:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070134
2020-10-12 20:22:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.45703125Mb; avail=470780.3203125Mb
2020-10-12 20:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051009
2020-10-12 20:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123544
2020-10-12 20:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.02734375Mb; avail=470779.98046875Mb
2020-10-12 20:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17813.4453125Mb; avail=470779.2578125Mb
2020-10-12 20:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001105
2020-10-12 20:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.31640625Mb; avail=470779.03125Mb
2020-10-12 20:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069517
2020-10-12 20:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.33203125Mb; avail=470778.27734375Mb
2020-10-12 20:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050081
2020-10-12 20:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121596
2020-10-12 20:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.265625Mb; avail=470778.734375Mb
2020-10-12 20:22:29 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.297 | nll_loss 9.62 | ppl 787.09 | wps 57583.3 | wpb 2291.8 | bsz 96.2 | num_updates 832 | best_loss 10.297
2020-10-12 20:22:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:22:36 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 8 @ 832 updates, score 10.297) (writing took 7.467647282000144 seconds)
2020-10-12 20:22:36 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 20:22:36 | INFO | train | epoch 008 | loss 10.627 | nll_loss 10.015 | ppl 1034.56 | wps 15465.5 | ups 2.64 | wpb 5855.6 | bsz 249.4 | num_updates 832 | lr 4.16792e-05 | gnorm 1.19 | clip 0 | train_wall 29 | wall 328
2020-10-12 20:22:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 20:22:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 20:22:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19111.01953125Mb; avail=469488.2578125Mb
2020-10-12 20:22:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000792
2020-10-12 20:22:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005462
2020-10-12 20:22:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19110.96484375Mb; avail=469488.203125Mb
2020-10-12 20:22:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000215
2020-10-12 20:22:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19110.96484375Mb; avail=469488.6953125Mb
2020-10-12 20:22:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091173
2020-10-12 20:22:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097700
2020-10-12 20:22:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19110.98828125Mb; avail=469487.57421875Mb
2020-10-12 20:22:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19111.44921875Mb; avail=469487.0625Mb
2020-10-12 20:22:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003879
2020-10-12 20:22:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19110.8046875Mb; avail=469486.98828125Mb
2020-10-12 20:22:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000190
2020-10-12 20:22:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19111.3828125Mb; avail=469486.875Mb
2020-10-12 20:22:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089502
2020-10-12 20:22:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094413
2020-10-12 20:22:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19111.27734375Mb; avail=469486.05859375Mb
2020-10-12 20:22:36 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 20:22:56 | INFO | train_inner | epoch 009:     68 / 104 loss=10.571, nll_loss=9.947, ppl=987.09, wps=15391.9, ups=2.62, wpb=5879.6, bsz=250.2, num_updates=900, lr=4.50775e-05, gnorm=1.327, clip=0, train_wall=27, wall=347
2020-10-12 20:23:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:23:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18376.5703125Mb; avail=470215.5703125Mb
2020-10-12 20:23:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001563
2020-10-12 20:23:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18376.5703125Mb; avail=470215.5703125Mb
2020-10-12 20:23:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070191
2020-10-12 20:23:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.80078125Mb; avail=470795.89453125Mb
2020-10-12 20:23:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049879
2020-10-12 20:23:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122545
2020-10-12 20:23:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.21875Mb; avail=470786.1640625Mb
2020-10-12 20:23:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17811.5Mb; avail=470787.390625Mb
2020-10-12 20:23:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001122
2020-10-12 20:23:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.9765625Mb; avail=470787.1640625Mb
2020-10-12 20:23:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070100
2020-10-12 20:23:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.50390625Mb; avail=470780.85546875Mb
2020-10-12 20:23:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049211
2020-10-12 20:23:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121221
2020-10-12 20:23:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.71484375Mb; avail=470780.015625Mb
2020-10-12 20:23:08 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 10.143 | nll_loss 9.422 | ppl 685.82 | wps 57009.6 | wpb 2291.8 | bsz 96.2 | num_updates 936 | best_loss 10.143
2020-10-12 20:23:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:23:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 9 @ 936 updates, score 10.143) (writing took 5.634223254999597 seconds)
2020-10-12 20:23:14 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 20:23:14 | INFO | train | epoch 009 | loss 10.538 | nll_loss 9.909 | ppl 961.68 | wps 16233.9 | ups 2.77 | wpb 5855.6 | bsz 249.4 | num_updates 936 | lr 4.68766e-05 | gnorm 1.266 | clip 0 | train_wall 28 | wall 365
2020-10-12 20:23:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 20:23:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 20:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18343.62890625Mb; avail=470255.41796875Mb
2020-10-12 20:23:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000898
2020-10-12 20:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005863
2020-10-12 20:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18343.62890625Mb; avail=470255.41796875Mb
2020-10-12 20:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000213
2020-10-12 20:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18343.62890625Mb; avail=470255.41796875Mb
2020-10-12 20:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090903
2020-10-12 20:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097839
2020-10-12 20:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18345.58984375Mb; avail=470253.3828125Mb
2020-10-12 20:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18345.1171875Mb; avail=470252.12109375Mb
2020-10-12 20:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003921
2020-10-12 20:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18345.5390625Mb; avail=470252.33203125Mb
2020-10-12 20:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000205
2020-10-12 20:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18345.41015625Mb; avail=470252.7109375Mb
2020-10-12 20:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090232
2020-10-12 20:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095252
2020-10-12 20:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18346.37109375Mb; avail=470250.39453125Mb
2020-10-12 20:23:14 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 20:23:32 | INFO | train_inner | epoch 010:     64 / 104 loss=10.448, nll_loss=9.806, ppl=895.05, wps=16107.8, ups=2.73, wpb=5891, bsz=250.9, num_updates=1000, lr=5.0075e-05, gnorm=1.166, clip=0, train_wall=27, wall=383
2020-10-12 20:23:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18420.03515625Mb; avail=470172.50390625Mb
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001688
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18420.03515625Mb; avail=470172.50390625Mb
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071976
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18422.62890625Mb; avail=470169.83203125Mb
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062088
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136694
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18434.73046875Mb; avail=470157.75390625Mb
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18436.546875Mb; avail=470155.9375Mb
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001219
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18436.546875Mb; avail=470155.9375Mb
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073327
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18438.8125Mb; avail=470153.515625Mb
2020-10-12 20:23:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051781
2020-10-12 20:23:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127189
2020-10-12 20:23:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18438.91015625Mb; avail=470154.0078125Mb
2020-10-12 20:23:46 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 10.046 | nll_loss 9.317 | ppl 637.65 | wps 56666.1 | wpb 2291.8 | bsz 96.2 | num_updates 1040 | best_loss 10.046
2020-10-12 20:23:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:24:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 10 @ 1040 updates, score 10.046) (writing took 16.676433938000628 seconds)
2020-10-12 20:24:02 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 20:24:02 | INFO | train | epoch 010 | loss 10.416 | nll_loss 9.768 | ppl 871.7 | wps 12539.8 | ups 2.14 | wpb 5855.6 | bsz 249.4 | num_updates 1040 | lr 5.2074e-05 | gnorm 1.243 | clip 0 | train_wall 28 | wall 414
2020-10-12 20:24:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 20:24:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 20:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18482.0546875Mb; avail=470111.03515625Mb
2020-10-12 20:24:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000978
2020-10-12 20:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006531
2020-10-12 20:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18481.5625Mb; avail=470111.52734375Mb
2020-10-12 20:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000221
2020-10-12 20:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18481.5625Mb; avail=470111.52734375Mb
2020-10-12 20:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102587
2020-10-12 20:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110331
2020-10-12 20:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18482.09375Mb; avail=470110.99609375Mb
2020-10-12 20:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18482.09375Mb; avail=470110.99609375Mb
2020-10-12 20:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004201
2020-10-12 20:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18482.09375Mb; avail=470110.99609375Mb
2020-10-12 20:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 20:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18482.09375Mb; avail=470110.99609375Mb
2020-10-12 20:24:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100680
2020-10-12 20:24:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105975
2020-10-12 20:24:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18482.22265625Mb; avail=470110.859375Mb
2020-10-12 20:24:03 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 20:24:20 | INFO | train_inner | epoch 011:     60 / 104 loss=10.385, nll_loss=9.73, ppl=849.25, wps=12527.6, ups=2.11, wpb=5938.9, bsz=247.7, num_updates=1100, lr=5.50725e-05, gnorm=1.262, clip=0, train_wall=27, wall=431
2020-10-12 20:24:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17868.75390625Mb; avail=470722.97265625Mb
2020-10-12 20:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001550
2020-10-12 20:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17868.75390625Mb; avail=470722.97265625Mb
2020-10-12 20:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073876
2020-10-12 20:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17868.6484375Mb; avail=470722.97265625Mb
2020-10-12 20:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049740
2020-10-12 20:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126077
2020-10-12 20:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17868.46484375Mb; avail=470723.21484375Mb
2020-10-12 20:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17868.5234375Mb; avail=470723.09375Mb
2020-10-12 20:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001160
2020-10-12 20:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17868.54296875Mb; avail=470722.97265625Mb
2020-10-12 20:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068778
2020-10-12 20:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17868.53125Mb; avail=470723.54296875Mb
2020-10-12 20:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051853
2020-10-12 20:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122545
2020-10-12 20:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17868.53125Mb; avail=470723.54296875Mb
2020-10-12 20:24:34 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 9.977 | nll_loss 9.246 | ppl 607.1 | wps 57692.4 | wpb 2291.8 | bsz 96.2 | num_updates 1144 | best_loss 9.977
2020-10-12 20:24:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:24:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 11 @ 1144 updates, score 9.977) (writing took 4.272939434999898 seconds)
2020-10-12 20:24:38 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 20:24:38 | INFO | train | epoch 011 | loss 10.329 | nll_loss 9.663 | ppl 810.82 | wps 16876.6 | ups 2.88 | wpb 5855.6 | bsz 249.4 | num_updates 1144 | lr 5.72714e-05 | gnorm 1.374 | clip 0 | train_wall 28 | wall 450
2020-10-12 20:24:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 20:24:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 20:24:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18422.69921875Mb; avail=470169.83203125Mb
2020-10-12 20:24:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000875
2020-10-12 20:24:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008708
2020-10-12 20:24:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18422.69921875Mb; avail=470169.83203125Mb
2020-10-12 20:24:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000295
2020-10-12 20:24:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18423.3671875Mb; avail=470169.34765625Mb
2020-10-12 20:24:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102880
2020-10-12 20:24:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112779
2020-10-12 20:24:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18423.484375Mb; avail=470169.30078125Mb
2020-10-12 20:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18423.390625Mb; avail=470169.50390625Mb
2020-10-12 20:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004017
2020-10-12 20:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18423.390625Mb; avail=470169.50390625Mb
2020-10-12 20:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000200
2020-10-12 20:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18423.390625Mb; avail=470169.50390625Mb
2020-10-12 20:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090786
2020-10-12 20:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095766
2020-10-12 20:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18423.26953125Mb; avail=470169.625Mb
2020-10-12 20:24:39 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 20:24:55 | INFO | train_inner | epoch 012:     56 / 104 loss=10.258, nll_loss=9.581, ppl=765.92, wps=16891.5, ups=2.85, wpb=5930, bsz=260.2, num_updates=1200, lr=6.007e-05, gnorm=1.337, clip=0, train_wall=27, wall=466
2020-10-12 20:25:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19213.44921875Mb; avail=469387.0625Mb
2020-10-12 20:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001570
2020-10-12 20:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19213.44921875Mb; avail=469387.0625Mb
2020-10-12 20:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070554
2020-10-12 20:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19213.8359375Mb; avail=469386.234375Mb
2020-10-12 20:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051783
2020-10-12 20:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124774
2020-10-12 20:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19212.96484375Mb; avail=469385.6484375Mb
2020-10-12 20:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19212.9765625Mb; avail=469385.4765625Mb
2020-10-12 20:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001175
2020-10-12 20:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19212.58984375Mb; avail=469385.7421875Mb
2020-10-12 20:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070180
2020-10-12 20:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19212.52734375Mb; avail=469384.82421875Mb
2020-10-12 20:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050179
2020-10-12 20:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122294
2020-10-12 20:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19213.59375Mb; avail=469383.62890625Mb
2020-10-12 20:25:10 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 9.854 | nll_loss 9.086 | ppl 543.46 | wps 57719.3 | wpb 2291.8 | bsz 96.2 | num_updates 1248 | best_loss 9.854
2020-10-12 20:25:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:25:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 12 @ 1248 updates, score 9.854) (writing took 13.726031120999323 seconds)
2020-10-12 20:25:24 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 20:25:24 | INFO | train | epoch 012 | loss 10.214 | nll_loss 9.531 | ppl 739.95 | wps 13333.2 | ups 2.28 | wpb 5855.6 | bsz 249.4 | num_updates 1248 | lr 6.24688e-05 | gnorm 1.131 | clip 0 | train_wall 28 | wall 495
2020-10-12 20:25:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 20:25:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17860.99609375Mb; avail=470731.3359375Mb
2020-10-12 20:25:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000915
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006277
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.50390625Mb; avail=470731.828125Mb
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000213
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.109375Mb; avail=470731.22265625Mb
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102594
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109956
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.7734375Mb; avail=470730.5703125Mb
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17862.015625Mb; avail=470730.25390625Mb
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004421
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.015625Mb; avail=470730.25390625Mb
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.015625Mb; avail=470730.25390625Mb
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100671
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106156
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.15625Mb; avail=470729.890625Mb
2020-10-12 20:25:24 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 20:25:39 | INFO | train_inner | epoch 013:     52 / 104 loss=10.163, nll_loss=9.471, ppl=709.81, wps=12997.8, ups=2.24, wpb=5803.1, bsz=243.3, num_updates=1300, lr=6.50675e-05, gnorm=1.197, clip=0, train_wall=27, wall=511
2020-10-12 20:25:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17821.5234375Mb; avail=470770.2265625Mb
2020-10-12 20:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001664
2020-10-12 20:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17821.5234375Mb; avail=470770.2265625Mb
2020-10-12 20:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070867
2020-10-12 20:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17821.40234375Mb; avail=470770.34765625Mb
2020-10-12 20:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049871
2020-10-12 20:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123223
2020-10-12 20:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17821.40234375Mb; avail=470770.34765625Mb
2020-10-12 20:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17821.40234375Mb; avail=470770.34765625Mb
2020-10-12 20:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001027
2020-10-12 20:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17821.40234375Mb; avail=470770.34765625Mb
2020-10-12 20:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071315
2020-10-12 20:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17821.66015625Mb; avail=470770.46875Mb
2020-10-12 20:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049780
2020-10-12 20:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123248
2020-10-12 20:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17821.76171875Mb; avail=470770.34765625Mb
2020-10-12 20:25:56 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 9.816 | nll_loss 9.029 | ppl 522.51 | wps 56923.4 | wpb 2291.8 | bsz 96.2 | num_updates 1352 | best_loss 9.816
2020-10-12 20:25:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:26:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 13 @ 1352 updates, score 9.816) (writing took 8.320365661000324 seconds)
2020-10-12 20:26:04 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 20:26:04 | INFO | train | epoch 013 | loss 10.117 | nll_loss 9.417 | ppl 683.63 | wps 15079.3 | ups 2.58 | wpb 5855.6 | bsz 249.4 | num_updates 1352 | lr 6.76662e-05 | gnorm 1.197 | clip 0 | train_wall 29 | wall 536
2020-10-12 20:26:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 20:26:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 20:26:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17818.1171875Mb; avail=470773.7578125Mb
2020-10-12 20:26:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000862
2020-10-12 20:26:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005711
2020-10-12 20:26:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.11328125Mb; avail=470773.7578125Mb
2020-10-12 20:26:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000227
2020-10-12 20:26:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.11328125Mb; avail=470773.7578125Mb
2020-10-12 20:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092952
2020-10-12 20:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099662
2020-10-12 20:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.45703125Mb; avail=470773.6328125Mb
2020-10-12 20:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17818.4140625Mb; avail=470773.75390625Mb
2020-10-12 20:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003705
2020-10-12 20:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.4140625Mb; avail=470773.75390625Mb
2020-10-12 20:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 20:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.4140625Mb; avail=470773.75390625Mb
2020-10-12 20:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089352
2020-10-12 20:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093989
2020-10-12 20:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.3125Mb; avail=470773.51171875Mb
2020-10-12 20:26:05 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 20:26:18 | INFO | train_inner | epoch 014:     48 / 104 loss=10.079, nll_loss=9.372, ppl=662.71, wps=14676.4, ups=2.57, wpb=5702.8, bsz=243.6, num_updates=1400, lr=7.0065e-05, gnorm=1.166, clip=0, train_wall=27, wall=549
2020-10-12 20:26:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18338.4609375Mb; avail=470254.13671875Mb
2020-10-12 20:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001793
2020-10-12 20:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18340.27734375Mb; avail=470252.3203125Mb
2020-10-12 20:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075566
2020-10-12 20:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18377.09765625Mb; avail=470220.17578125Mb
2020-10-12 20:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054683
2020-10-12 20:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.133166
2020-10-12 20:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18377.0703125Mb; avail=470222.453125Mb
2020-10-12 20:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18376.6953125Mb; avail=470222.6484375Mb
2020-10-12 20:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001435
2020-10-12 20:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18376.30859375Mb; avail=470222.1953125Mb
2020-10-12 20:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078049
2020-10-12 20:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18377.8515625Mb; avail=470220.53125Mb
2020-10-12 20:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.058145
2020-10-12 20:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138856
2020-10-12 20:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18377.65234375Mb; avail=470219.953125Mb
2020-10-12 20:26:36 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 9.666 | nll_loss 8.875 | ppl 469.37 | wps 56341.7 | wpb 2291.8 | bsz 96.2 | num_updates 1456 | best_loss 9.666
2020-10-12 20:26:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:26:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 14 @ 1456 updates, score 9.666) (writing took 7.9552506549998725 seconds)
2020-10-12 20:26:44 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 20:26:44 | INFO | train | epoch 014 | loss 10.024 | nll_loss 9.308 | ppl 633.69 | wps 15267.1 | ups 2.61 | wpb 5855.6 | bsz 249.4 | num_updates 1456 | lr 7.28636e-05 | gnorm 1.196 | clip 0 | train_wall 28 | wall 576
2020-10-12 20:26:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 20:26:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 20:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17797.9765625Mb; avail=470794.171875Mb
2020-10-12 20:26:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000917
2020-10-12 20:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005974
2020-10-12 20:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17797.9765625Mb; avail=470794.171875Mb
2020-10-12 20:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000216
2020-10-12 20:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17797.9765625Mb; avail=470794.171875Mb
2020-10-12 20:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093079
2020-10-12 20:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100028
2020-10-12 20:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17797.47265625Mb; avail=470794.78515625Mb
2020-10-12 20:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17798.08984375Mb; avail=470794.421875Mb
2020-10-12 20:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003708
2020-10-12 20:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.08984375Mb; avail=470794.421875Mb
2020-10-12 20:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.08984375Mb; avail=470794.421875Mb
2020-10-12 20:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091433
2020-10-12 20:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096073
2020-10-12 20:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17797.97265625Mb; avail=470794.54296875Mb
2020-10-12 20:26:45 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 20:26:57 | INFO | train_inner | epoch 015:     44 / 104 loss=9.986, nll_loss=9.264, ppl=614.64, wps=15139.2, ups=2.59, wpb=5855.5, bsz=237.6, num_updates=1500, lr=7.50625e-05, gnorm=1.209, clip=0, train_wall=27, wall=588
2020-10-12 20:27:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18407.0234375Mb; avail=470188.44921875Mb
2020-10-12 20:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001750
2020-10-12 20:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18406.75Mb; avail=470188.84765625Mb
2020-10-12 20:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070396
2020-10-12 20:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18400.48828125Mb; avail=470193.63671875Mb
2020-10-12 20:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050759
2020-10-12 20:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123919
2020-10-12 20:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18402.50390625Mb; avail=470191.34765625Mb
2020-10-12 20:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18401.7578125Mb; avail=470191.85546875Mb
2020-10-12 20:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001155
2020-10-12 20:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18402.10546875Mb; avail=470191.515625Mb
2020-10-12 20:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070117
2020-10-12 20:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18401.7265625Mb; avail=470190.890625Mb
2020-10-12 20:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049735
2020-10-12 20:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121920
2020-10-12 20:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18401.6328125Mb; avail=470190.9765625Mb
2020-10-12 20:27:16 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 9.597 | nll_loss 8.783 | ppl 440.46 | wps 57888.4 | wpb 2291.8 | bsz 96.2 | num_updates 1560 | best_loss 9.597
2020-10-12 20:27:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:27:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 15 @ 1560 updates, score 9.597) (writing took 7.251816437000343 seconds)
2020-10-12 20:27:23 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 20:27:23 | INFO | train | epoch 015 | loss 9.944 | nll_loss 9.215 | ppl 594.29 | wps 15606.2 | ups 2.67 | wpb 5855.6 | bsz 249.4 | num_updates 1560 | lr 7.8061e-05 | gnorm 1.325 | clip 0 | train_wall 28 | wall 615
2020-10-12 20:27:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 20:27:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 20:27:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17847.30859375Mb; avail=470744.86328125Mb
2020-10-12 20:27:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000948
2020-10-12 20:27:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006193
2020-10-12 20:27:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.4296875Mb; avail=470744.7421875Mb
2020-10-12 20:27:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000249
2020-10-12 20:27:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.4296875Mb; avail=470744.7421875Mb
2020-10-12 20:27:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095998
2020-10-12 20:27:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103358
2020-10-12 20:27:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.44140625Mb; avail=470745.10546875Mb
2020-10-12 20:27:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17847.46875Mb; avail=470745.10546875Mb
2020-10-12 20:27:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003704
2020-10-12 20:27:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.46875Mb; avail=470745.10546875Mb
2020-10-12 20:27:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-12 20:27:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.46875Mb; avail=470745.10546875Mb
2020-10-12 20:27:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089072
2020-10-12 20:27:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093737
2020-10-12 20:27:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.5390625Mb; avail=470745.23046875Mb
2020-10-12 20:27:24 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 20:27:35 | INFO | train_inner | epoch 016:     40 / 104 loss=9.891, nll_loss=9.154, ppl=569.72, wps=15506.1, ups=2.62, wpb=5921.3, bsz=266.8, num_updates=1600, lr=8.006e-05, gnorm=1.287, clip=0, train_wall=28, wall=626
2020-10-12 20:27:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18939.9921875Mb; avail=469653.203125Mb
2020-10-12 20:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001945
2020-10-12 20:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18939.60546875Mb; avail=469652.86328125Mb
2020-10-12 20:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082025
2020-10-12 20:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18939.453125Mb; avail=469652.890625Mb
2020-10-12 20:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060626
2020-10-12 20:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.146063
2020-10-12 20:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18939.54296875Mb; avail=469652.6015625Mb
2020-10-12 20:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18939.54296875Mb; avail=469652.6015625Mb
2020-10-12 20:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001453
2020-10-12 20:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18939.640625Mb; avail=469652.6015625Mb
2020-10-12 20:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079795
2020-10-12 20:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18939.6796875Mb; avail=469652.67578125Mb
2020-10-12 20:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061603
2020-10-12 20:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.144020
2020-10-12 20:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18939.6796875Mb; avail=469652.67578125Mb
2020-10-12 20:27:56 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 9.467 | nll_loss 8.643 | ppl 399.87 | wps 56507.3 | wpb 2291.8 | bsz 96.2 | num_updates 1664 | best_loss 9.467
2020-10-12 20:27:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:28:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 16 @ 1664 updates, score 9.467) (writing took 12.194493340000008 seconds)
2020-10-12 20:28:08 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 20:28:08 | INFO | train | epoch 016 | loss 9.829 | nll_loss 9.082 | ppl 541.94 | wps 13678.5 | ups 2.34 | wpb 5855.6 | bsz 249.4 | num_updates 1664 | lr 8.32584e-05 | gnorm 1.127 | clip 0 | train_wall 29 | wall 659
2020-10-12 20:28:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 20:28:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 20:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17849.484375Mb; avail=470741.17578125Mb
2020-10-12 20:28:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000979
2020-10-12 20:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006236
2020-10-12 20:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.36328125Mb; avail=470741.296875Mb
2020-10-12 20:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000209
2020-10-12 20:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.36328125Mb; avail=470741.296875Mb
2020-10-12 20:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096418
2020-10-12 20:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103782
2020-10-12 20:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.3125Mb; avail=470741.41796875Mb
2020-10-12 20:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17850.46875Mb; avail=470741.5390625Mb
2020-10-12 20:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003685
2020-10-12 20:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.484375Mb; avail=470741.41796875Mb
2020-10-12 20:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.484375Mb; avail=470741.41796875Mb
2020-10-12 20:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091768
2020-10-12 20:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096387
2020-10-12 20:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.484375Mb; avail=470741.41796875Mb
2020-10-12 20:28:08 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 20:28:18 | INFO | train_inner | epoch 017:     36 / 104 loss=9.786, nll_loss=9.032, ppl=523.55, wps=13392.7, ups=2.31, wpb=5792.3, bsz=239.4, num_updates=1700, lr=8.50575e-05, gnorm=1.184, clip=0, train_wall=28, wall=670
2020-10-12 20:28:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:28:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19615.3984375Mb; avail=468983.43359375Mb
2020-10-12 20:28:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001643
2020-10-12 20:28:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19615.01171875Mb; avail=468983.578125Mb
2020-10-12 20:28:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070401
2020-10-12 20:28:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19615.32421875Mb; avail=468982.6015625Mb
2020-10-12 20:28:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052063
2020-10-12 20:28:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125141
2020-10-12 20:28:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19614.984375Mb; avail=468982.33984375Mb
2020-10-12 20:28:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19614.50390625Mb; avail=468981.53125Mb
2020-10-12 20:28:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001184
2020-10-12 20:28:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19614.8515625Mb; avail=468981.796875Mb
2020-10-12 20:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068924
2020-10-12 20:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19614.97265625Mb; avail=468981.3515625Mb
2020-10-12 20:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050673
2020-10-12 20:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121799
2020-10-12 20:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19614.51171875Mb; avail=468980.5703125Mb
2020-10-12 20:28:40 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 9.368 | nll_loss 8.523 | ppl 367.84 | wps 56170.1 | wpb 2291.8 | bsz 96.2 | num_updates 1768 | best_loss 9.368
2020-10-12 20:28:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:29:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 17 @ 1768 updates, score 9.368) (writing took 20.386035159999665 seconds)
2020-10-12 20:29:00 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 20:29:00 | INFO | train | epoch 017 | loss 9.731 | nll_loss 8.969 | ppl 501.02 | wps 11662.9 | ups 1.99 | wpb 5855.6 | bsz 249.4 | num_updates 1768 | lr 8.84558e-05 | gnorm 1.214 | clip 0 | train_wall 28 | wall 711
2020-10-12 20:29:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 20:29:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 20:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18403.03125Mb; avail=470189.265625Mb
2020-10-12 20:29:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000737
2020-10-12 20:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005935
2020-10-12 20:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18403.03125Mb; avail=470189.265625Mb
2020-10-12 20:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000202
2020-10-12 20:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18403.03125Mb; avail=470189.265625Mb
2020-10-12 20:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102395
2020-10-12 20:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109434
2020-10-12 20:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18403.44140625Mb; avail=470188.90234375Mb
2020-10-12 20:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18403.5859375Mb; avail=470188.7578125Mb
2020-10-12 20:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004160
2020-10-12 20:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18403.5859375Mb; avail=470188.7578125Mb
2020-10-12 20:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-12 20:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18403.5859375Mb; avail=470188.7578125Mb
2020-10-12 20:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101148
2020-10-12 20:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106309
2020-10-12 20:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18403.65625Mb; avail=470188.6953125Mb
2020-10-12 20:29:00 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 20:29:09 | INFO | train_inner | epoch 018:     32 / 104 loss=9.724, nll_loss=8.96, ppl=498.07, wps=11507, ups=1.96, wpb=5876, bsz=239.9, num_updates=1800, lr=9.0055e-05, gnorm=1.255, clip=0, train_wall=27, wall=721
2020-10-12 20:29:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17850.49609375Mb; avail=470741.94140625Mb
2020-10-12 20:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001642
2020-10-12 20:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.49609375Mb; avail=470741.94140625Mb
2020-10-12 20:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070013
2020-10-12 20:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.7265625Mb; avail=470741.3359375Mb
2020-10-12 20:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050164
2020-10-12 20:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122716
2020-10-12 20:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.671875Mb; avail=470741.578125Mb
2020-10-12 20:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17850.671875Mb; avail=470741.578125Mb
2020-10-12 20:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001110
2020-10-12 20:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.671875Mb; avail=470741.578125Mb
2020-10-12 20:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069618
2020-10-12 20:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.734375Mb; avail=470741.69921875Mb
2020-10-12 20:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049508
2020-10-12 20:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121005
2020-10-12 20:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.56640625Mb; avail=470754.00390625Mb
2020-10-12 20:29:32 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 9.277 | nll_loss 8.428 | ppl 344.48 | wps 55826.6 | wpb 2291.8 | bsz 96.2 | num_updates 1872 | best_loss 9.277
2020-10-12 20:29:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:29:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 18 @ 1872 updates, score 9.277) (writing took 9.210174122999888 seconds)
2020-10-12 20:29:41 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 20:29:41 | INFO | train | epoch 018 | loss 9.624 | nll_loss 8.846 | ppl 460.04 | wps 14806.4 | ups 2.53 | wpb 5855.6 | bsz 249.4 | num_updates 1872 | lr 9.36532e-05 | gnorm 1.325 | clip 0 | train_wall 28 | wall 753
2020-10-12 20:29:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 20:29:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 20:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18590.1171875Mb; avail=470001.76953125Mb
2020-10-12 20:29:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000833
2020-10-12 20:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005814
2020-10-12 20:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18595.97265625Mb; avail=469995.80859375Mb
2020-10-12 20:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000263
2020-10-12 20:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18596.578125Mb; avail=469995.203125Mb
2020-10-12 20:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091021
2020-10-12 20:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097956
2020-10-12 20:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18694.7734375Mb; avail=469896.51953125Mb
2020-10-12 20:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18747.44921875Mb; avail=469844.44921875Mb
2020-10-12 20:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003808
2020-10-12 20:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18751.6875Mb; avail=469839.60546875Mb
2020-10-12 20:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000235
2020-10-12 20:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18752.29296875Mb; avail=469839.0Mb
2020-10-12 20:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089373
2020-10-12 20:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094182
2020-10-12 20:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18849.7734375Mb; avail=469742.125Mb
2020-10-12 20:29:41 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 20:29:50 | INFO | train_inner | epoch 019:     28 / 104 loss=9.563, nll_loss=8.776, ppl=438.37, wps=14747.3, ups=2.49, wpb=5925.2, bsz=262.9, num_updates=1900, lr=9.50525e-05, gnorm=1.257, clip=0, train_wall=27, wall=761
2020-10-12 20:30:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17896.26171875Mb; avail=470696.09375Mb
2020-10-12 20:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001627
2020-10-12 20:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17896.26171875Mb; avail=470696.09375Mb
2020-10-12 20:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070745
2020-10-12 20:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17895.45703125Mb; avail=470696.0390625Mb
2020-10-12 20:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050286
2020-10-12 20:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123497
2020-10-12 20:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17895.19140625Mb; avail=470696.43359375Mb
2020-10-12 20:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17895.1796875Mb; avail=470696.43359375Mb
2020-10-12 20:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001089
2020-10-12 20:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17895.1796875Mb; avail=470696.43359375Mb
2020-10-12 20:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069627
2020-10-12 20:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.03125Mb; avail=470740.6171875Mb
2020-10-12 20:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049932
2020-10-12 20:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121410
2020-10-12 20:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17845.234375Mb; avail=470746.09375Mb
2020-10-12 20:30:13 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 9.176 | nll_loss 8.3 | ppl 315.15 | wps 57523.1 | wpb 2291.8 | bsz 96.2 | num_updates 1976 | best_loss 9.176
2020-10-12 20:30:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:30:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 19 @ 1976 updates, score 9.176) (writing took 9.966700446000686 seconds)
2020-10-12 20:30:23 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 20:30:23 | INFO | train | epoch 019 | loss 9.505 | nll_loss 8.708 | ppl 418.11 | wps 14527 | ups 2.48 | wpb 5855.6 | bsz 249.4 | num_updates 1976 | lr 9.88506e-05 | gnorm 1.257 | clip 0 | train_wall 28 | wall 794
2020-10-12 20:30:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 20:30:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 20:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18429.2265625Mb; avail=470162.75Mb
2020-10-12 20:30:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000799
2020-10-12 20:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005912
2020-10-12 20:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18429.3203125Mb; avail=470162.85546875Mb
2020-10-12 20:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000250
2020-10-12 20:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18429.3203125Mb; avail=470162.85546875Mb
2020-10-12 20:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103370
2020-10-12 20:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110368
2020-10-12 20:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18429.3359375Mb; avail=470162.83984375Mb
2020-10-12 20:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18429.3359375Mb; avail=470162.83984375Mb
2020-10-12 20:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004331
2020-10-12 20:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18429.3359375Mb; avail=470162.83984375Mb
2020-10-12 20:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000250
2020-10-12 20:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18429.3359375Mb; avail=470162.83984375Mb
2020-10-12 20:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100931
2020-10-12 20:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106367
2020-10-12 20:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18429.55078125Mb; avail=470162.59765625Mb
2020-10-12 20:30:23 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 20:30:30 | INFO | train_inner | epoch 020:     24 / 104 loss=9.49, nll_loss=8.691, ppl=413.36, wps=14343.2, ups=2.45, wpb=5844.9, bsz=247.4, num_updates=2000, lr=0.00010005, gnorm=1.27, clip=0, train_wall=27, wall=802
2020-10-12 20:30:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18222.71484375Mb; avail=470373.08984375Mb
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001620
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18222.22265625Mb; avail=470373.58203125Mb
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069718
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18222.4921875Mb; avail=470376.6796875Mb
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050177
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122301
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18221.36328125Mb; avail=470377.796875Mb
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18221.890625Mb; avail=470377.30859375Mb
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001159
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18221.76171875Mb; avail=470377.08203125Mb
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069842
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18222.31640625Mb; avail=470375.75390625Mb
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049389
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121163
2020-10-12 20:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18221.60546875Mb; avail=470376.82421875Mb
2020-10-12 20:30:55 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 9.071 | nll_loss 8.174 | ppl 288.89 | wps 57014.1 | wpb 2291.8 | bsz 96.2 | num_updates 2080 | best_loss 9.071
2020-10-12 20:30:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:31:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 20 @ 2080 updates, score 9.071) (writing took 8.2779096330014 seconds)
2020-10-12 20:31:03 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 20:31:03 | INFO | train | epoch 020 | loss 9.388 | nll_loss 8.573 | ppl 380.76 | wps 15168.6 | ups 2.59 | wpb 5855.6 | bsz 249.4 | num_updates 2080 | lr 0.000104048 | gnorm 1.256 | clip 0 | train_wall 28 | wall 835
2020-10-12 20:31:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 20:31:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 20:31:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18459.96875Mb; avail=470132.27734375Mb
2020-10-12 20:31:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000857
2020-10-12 20:31:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006074
2020-10-12 20:31:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18459.96875Mb; avail=470132.27734375Mb
2020-10-12 20:31:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000251
2020-10-12 20:31:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18459.96875Mb; avail=470132.27734375Mb
2020-10-12 20:31:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105440
2020-10-12 20:31:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112945
2020-10-12 20:31:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18460.49609375Mb; avail=470131.75Mb
2020-10-12 20:31:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18460.00390625Mb; avail=470132.2421875Mb
2020-10-12 20:31:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004484
2020-10-12 20:31:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18460.00390625Mb; avail=470132.2421875Mb
2020-10-12 20:31:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000263
2020-10-12 20:31:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18460.00390625Mb; avail=470132.2421875Mb
2020-10-12 20:31:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100306
2020-10-12 20:31:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105920
2020-10-12 20:31:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18458.82421875Mb; avail=470133.2421875Mb
2020-10-12 20:31:04 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 20:31:09 | INFO | train_inner | epoch 021:     20 / 104 loss=9.363, nll_loss=8.544, ppl=373.27, wps=15023.5, ups=2.56, wpb=5870, bsz=257.4, num_updates=2100, lr=0.000105048, gnorm=1.28, clip=0, train_wall=27, wall=841
2020-10-12 20:31:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:31:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17810.32421875Mb; avail=470781.19140625Mb
2020-10-12 20:31:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001650
2020-10-12 20:31:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17810.32421875Mb; avail=470781.19140625Mb
2020-10-12 20:31:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068484
2020-10-12 20:31:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.93359375Mb; avail=470781.80078125Mb
2020-10-12 20:31:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048787
2020-10-12 20:31:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119712
2020-10-12 20:31:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.796875Mb; avail=470781.921875Mb
2020-10-12 20:31:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17809.796875Mb; avail=470781.921875Mb
2020-10-12 20:31:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001055
2020-10-12 20:31:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.796875Mb; avail=470781.921875Mb
2020-10-12 20:31:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068846
2020-10-12 20:31:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.9296875Mb; avail=470781.80078125Mb
2020-10-12 20:31:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049138
2020-10-12 20:31:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119779
2020-10-12 20:31:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.9296875Mb; avail=470781.80078125Mb
2020-10-12 20:31:35 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 8.988 | nll_loss 8.078 | ppl 270.31 | wps 58051.1 | wpb 2291.8 | bsz 96.2 | num_updates 2184 | best_loss 8.988
2020-10-12 20:31:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:31:39 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 21 @ 2184 updates, score 8.988) (writing took 4.24772025799939 seconds)
2020-10-12 20:31:39 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 20:31:39 | INFO | train | epoch 021 | loss 9.266 | nll_loss 8.431 | ppl 345.24 | wps 16838.7 | ups 2.88 | wpb 5855.6 | bsz 249.4 | num_updates 2184 | lr 0.000109245 | gnorm 1.249 | clip 0 | train_wall 28 | wall 871
2020-10-12 20:31:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 20:31:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17873.5703125Mb; avail=470717.79296875Mb
2020-10-12 20:31:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000901
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005230
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.5703125Mb; avail=470717.79296875Mb
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:31:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.5703125Mb; avail=470717.79296875Mb
2020-10-12 20:31:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089213
2020-10-12 20:31:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095383
2020-10-12 20:31:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.3671875Mb; avail=470716.75Mb
2020-10-12 20:31:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17873.59765625Mb; avail=470717.60546875Mb
2020-10-12 20:31:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003629
2020-10-12 20:31:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.578125Mb; avail=470717.7265625Mb
2020-10-12 20:31:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 20:31:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.578125Mb; avail=470717.7265625Mb
2020-10-12 20:31:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092210
2020-10-12 20:31:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097053
2020-10-12 20:31:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.96875Mb; avail=470717.484375Mb
2020-10-12 20:31:40 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 20:31:44 | INFO | train_inner | epoch 022:     16 / 104 loss=9.233, nll_loss=8.394, ppl=336.36, wps=16649.8, ups=2.85, wpb=5835.5, bsz=245.3, num_updates=2200, lr=0.000110045, gnorm=1.216, clip=0, train_wall=27, wall=876
2020-10-12 20:32:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:32:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17861.11328125Mb; avail=470730.0546875Mb
2020-10-12 20:32:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001599
2020-10-12 20:32:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.11328125Mb; avail=470730.0546875Mb
2020-10-12 20:32:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068607
2020-10-12 20:32:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.01953125Mb; avail=470730.0546875Mb
2020-10-12 20:32:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050112
2020-10-12 20:32:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121110
2020-10-12 20:32:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.140625Mb; avail=470730.0546875Mb
2020-10-12 20:32:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17861.01953125Mb; avail=470730.17578125Mb
2020-10-12 20:32:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001061
2020-10-12 20:32:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.01953125Mb; avail=470730.17578125Mb
2020-10-12 20:32:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068781
2020-10-12 20:32:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.88671875Mb; avail=470730.296875Mb
2020-10-12 20:32:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049824
2020-10-12 20:32:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120442
2020-10-12 20:32:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.85546875Mb; avail=470729.8125Mb
2020-10-12 20:32:11 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 8.932 | nll_loss 8.013 | ppl 258.35 | wps 56928.9 | wpb 2291.8 | bsz 96.2 | num_updates 2288 | best_loss 8.932
2020-10-12 20:32:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:32:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 22 @ 2288 updates, score 8.932) (writing took 4.311455983000997 seconds)
2020-10-12 20:32:16 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 20:32:16 | INFO | train | epoch 022 | loss 9.169 | nll_loss 8.318 | ppl 319.18 | wps 16760.7 | ups 2.86 | wpb 5855.6 | bsz 249.4 | num_updates 2288 | lr 0.000114443 | gnorm 1.302 | clip 0 | train_wall 29 | wall 907
2020-10-12 20:32:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 20:32:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 20:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17862.2421875Mb; avail=470729.19921875Mb
2020-10-12 20:32:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000919
2020-10-12 20:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006325
2020-10-12 20:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.92578125Mb; avail=470728.71484375Mb
2020-10-12 20:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000214
2020-10-12 20:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.92578125Mb; avail=470728.71484375Mb
2020-10-12 20:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096241
2020-10-12 20:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103639
2020-10-12 20:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.45703125Mb; avail=470729.20703125Mb
2020-10-12 20:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17862.9765625Mb; avail=470728.7109375Mb
2020-10-12 20:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003685
2020-10-12 20:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.9765625Mb; avail=470728.7109375Mb
2020-10-12 20:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 20:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.9765625Mb; avail=470728.7109375Mb
2020-10-12 20:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089592
2020-10-12 20:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094227
2020-10-12 20:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.8671875Mb; avail=470728.83203125Mb
2020-10-12 20:32:16 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 20:32:19 | INFO | train_inner | epoch 023:     12 / 104 loss=9.178, nll_loss=8.328, ppl=321.4, wps=16765.1, ups=2.85, wpb=5877.8, bsz=246, num_updates=2300, lr=0.000115043, gnorm=1.325, clip=0, train_wall=27, wall=911
2020-10-12 20:32:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17871.625Mb; avail=470719.93359375Mb
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001643
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.625Mb; avail=470719.93359375Mb
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.091932
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.453125Mb; avail=470720.296875Mb
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049543
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.143959
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.66015625Mb; avail=470720.296875Mb
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17871.66015625Mb; avail=470720.296875Mb
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001058
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.66015625Mb; avail=470720.296875Mb
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.117808
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.66015625Mb; avail=470720.296875Mb
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050089
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.169932
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.60546875Mb; avail=470719.66015625Mb
2020-10-12 20:32:48 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 8.846 | nll_loss 7.902 | ppl 239.22 | wps 58115.9 | wpb 2291.8 | bsz 96.2 | num_updates 2392 | best_loss 8.846
2020-10-12 20:32:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:32:52 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 23 @ 2392 updates, score 8.846) (writing took 4.233907354000621 seconds)
2020-10-12 20:32:52 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 20:32:52 | INFO | train | epoch 023 | loss 9.054 | nll_loss 8.185 | ppl 290.97 | wps 16770 | ups 2.86 | wpb 5855.6 | bsz 249.4 | num_updates 2392 | lr 0.00011964 | gnorm 1.183 | clip 0 | train_wall 29 | wall 943
2020-10-12 20:32:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 20:32:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 20:32:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17864.640625Mb; avail=470726.734375Mb
2020-10-12 20:32:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001116
2020-10-12 20:32:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006870
2020-10-12 20:32:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17864.640625Mb; avail=470726.734375Mb
2020-10-12 20:32:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000221
2020-10-12 20:32:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17864.640625Mb; avail=470726.734375Mb
2020-10-12 20:32:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095516
2020-10-12 20:32:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103516
2020-10-12 20:32:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.7109375Mb; avail=470727.35546875Mb
2020-10-12 20:32:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17863.796875Mb; avail=470727.35546875Mb
2020-10-12 20:32:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003685
2020-10-12 20:32:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.8828125Mb; avail=470727.4765625Mb
2020-10-12 20:32:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 20:32:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.8828125Mb; avail=470727.4765625Mb
2020-10-12 20:32:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092501
2020-10-12 20:32:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097267
2020-10-12 20:32:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.58984375Mb; avail=470727.57421875Mb
2020-10-12 20:32:52 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 20:32:55 | INFO | train_inner | epoch 024:      8 / 104 loss=9.043, nll_loss=8.171, ppl=288.29, wps=16493.8, ups=2.83, wpb=5819, bsz=249.6, num_updates=2400, lr=0.00012004, gnorm=1.177, clip=0, train_wall=28, wall=946
2020-10-12 20:33:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17895.00390625Mb; avail=470696.56640625Mb
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001835
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17895.00390625Mb; avail=470696.56640625Mb
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070671
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17895.09375Mb; avail=470696.56640625Mb
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051887
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125202
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17895.296875Mb; avail=470696.203125Mb
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17895.296875Mb; avail=470696.203125Mb
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001032
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17895.296875Mb; avail=470696.203125Mb
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069207
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17895.328125Mb; avail=470696.4453125Mb
2020-10-12 20:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052614
2020-10-12 20:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123909
2020-10-12 20:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17895.5078125Mb; avail=470695.9609375Mb
2020-10-12 20:33:25 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.782 | nll_loss 7.826 | ppl 226.92 | wps 57183.2 | wpb 2291.8 | bsz 96.2 | num_updates 2496 | best_loss 8.782
2020-10-12 20:33:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:33:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 24 @ 2496 updates, score 8.782) (writing took 8.705831069999476 seconds)
2020-10-12 20:33:33 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 20:33:33 | INFO | train | epoch 024 | loss 8.947 | nll_loss 8.06 | ppl 266.9 | wps 14781.8 | ups 2.52 | wpb 5855.6 | bsz 249.4 | num_updates 2496 | lr 0.000124838 | gnorm 1.205 | clip 0 | train_wall 29 | wall 985
2020-10-12 20:33:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 20:33:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 20:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18592.86328125Mb; avail=469998.84375Mb
2020-10-12 20:33:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001019
2020-10-12 20:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006615
2020-10-12 20:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18592.86328125Mb; avail=469998.84375Mb
2020-10-12 20:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000206
2020-10-12 20:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18592.86328125Mb; avail=469998.84375Mb
2020-10-12 20:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.107231
2020-10-12 20:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115233
2020-10-12 20:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18590.875Mb; avail=470000.91015625Mb
2020-10-12 20:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18590.44140625Mb; avail=470001.16015625Mb
2020-10-12 20:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004463
2020-10-12 20:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18590.44140625Mb; avail=470001.16015625Mb
2020-10-12 20:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000201
2020-10-12 20:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18590.44140625Mb; avail=470001.16015625Mb
2020-10-12 20:33:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105102
2020-10-12 20:33:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110700
2020-10-12 20:33:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18591.66796875Mb; avail=469999.88671875Mb
2020-10-12 20:33:34 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 20:33:35 | INFO | train_inner | epoch 025:      4 / 104 loss=8.937, nll_loss=8.049, ppl=264.8, wps=14650.5, ups=2.49, wpb=5874.8, bsz=253.6, num_updates=2500, lr=0.000125037, gnorm=1.204, clip=0, train_wall=28, wall=986
2020-10-12 20:34:03 | INFO | train_inner | epoch 025:    104 / 104 loss=8.852, nll_loss=7.95, ppl=247.26, wps=20708.7, ups=3.54, wpb=5845.3, bsz=248.2, num_updates=2600, lr=0.000130035, gnorm=1.326, clip=0, train_wall=27, wall=1014
2020-10-12 20:34:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:34:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17899.19921875Mb; avail=470692.18359375Mb
2020-10-12 20:34:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001679
2020-10-12 20:34:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17899.19921875Mb; avail=470692.18359375Mb
2020-10-12 20:34:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071218
2020-10-12 20:34:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17899.3203125Mb; avail=470692.0625Mb
2020-10-12 20:34:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049909
2020-10-12 20:34:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123595
2020-10-12 20:34:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17899.48828125Mb; avail=470691.69921875Mb
2020-10-12 20:34:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17899.58203125Mb; avail=470691.8203125Mb
2020-10-12 20:34:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001083
2020-10-12 20:34:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17899.58203125Mb; avail=470691.8203125Mb
2020-10-12 20:34:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069100
2020-10-12 20:34:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17900.17578125Mb; avail=470691.21484375Mb
2020-10-12 20:34:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050366
2020-10-12 20:34:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121330
2020-10-12 20:34:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17900.00390625Mb; avail=470691.3359375Mb
2020-10-12 20:34:05 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.736 | nll_loss 7.761 | ppl 216.93 | wps 57760.5 | wpb 2291.8 | bsz 96.2 | num_updates 2600 | best_loss 8.736
2020-10-12 20:34:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:34:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 25 @ 2600 updates, score 8.736) (writing took 4.278115365999838 seconds)
2020-10-12 20:34:10 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 20:34:10 | INFO | train | epoch 025 | loss 8.856 | nll_loss 7.954 | ppl 248.04 | wps 16759.7 | ups 2.86 | wpb 5855.6 | bsz 249.4 | num_updates 2600 | lr 0.000130035 | gnorm 1.319 | clip 0 | train_wall 29 | wall 1021
2020-10-12 20:34:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 20:34:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 20:34:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17920.4140625Mb; avail=470670.3203125Mb
2020-10-12 20:34:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000966
2020-10-12 20:34:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006406
2020-10-12 20:34:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17920.4140625Mb; avail=470670.3203125Mb
2020-10-12 20:34:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 20:34:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17920.4140625Mb; avail=470670.3203125Mb
2020-10-12 20:34:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097127
2020-10-12 20:34:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104676
2020-10-12 20:34:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17919.265625Mb; avail=470671.796875Mb
2020-10-12 20:34:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17918.52734375Mb; avail=470672.53125Mb
2020-10-12 20:34:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003812
2020-10-12 20:34:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17918.52734375Mb; avail=470672.53125Mb
2020-10-12 20:34:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000231
2020-10-12 20:34:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17918.49609375Mb; avail=470672.53125Mb
2020-10-12 20:34:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089641
2020-10-12 20:34:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094434
2020-10-12 20:34:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17918.20703125Mb; avail=470673.21875Mb
2020-10-12 20:34:10 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 20:34:38 | INFO | train_inner | epoch 026:    100 / 104 loss=8.774, nll_loss=7.858, ppl=232.07, wps=16932.1, ups=2.85, wpb=5934.8, bsz=245.7, num_updates=2700, lr=0.000135032, gnorm=1.313, clip=0, train_wall=27, wall=1049
2020-10-12 20:34:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17934.203125Mb; avail=470656.8046875Mb
2020-10-12 20:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001650
2020-10-12 20:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17934.203125Mb; avail=470656.8046875Mb
2020-10-12 20:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070316
2020-10-12 20:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17934.26953125Mb; avail=470656.92578125Mb
2020-10-12 20:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049994
2020-10-12 20:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122770
2020-10-12 20:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17934.0859375Mb; avail=470656.92578125Mb
2020-10-12 20:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17934.0859375Mb; avail=470656.92578125Mb
2020-10-12 20:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001038
2020-10-12 20:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17934.0859375Mb; avail=470656.92578125Mb
2020-10-12 20:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068617
2020-10-12 20:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17934.03515625Mb; avail=470656.92578125Mb
2020-10-12 20:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049637
2020-10-12 20:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120029
2020-10-12 20:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17933.93359375Mb; avail=470657.046875Mb
2020-10-12 20:34:41 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.757 | nll_loss 7.777 | ppl 219.31 | wps 56297.7 | wpb 2291.8 | bsz 96.2 | num_updates 2704 | best_loss 8.736
2020-10-12 20:34:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:34:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_last.pt (epoch 26 @ 2704 updates, score 8.757) (writing took 3.471482142000241 seconds)
2020-10-12 20:34:45 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 20:34:45 | INFO | train | epoch 026 | loss 8.757 | nll_loss 7.839 | ppl 229 | wps 17234.8 | ups 2.94 | wpb 5855.6 | bsz 249.4 | num_updates 2704 | lr 0.000135232 | gnorm 1.321 | clip 0 | train_wall 28 | wall 1056
2020-10-12 20:34:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 20:34:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 20:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17907.69921875Mb; avail=470683.7578125Mb
2020-10-12 20:34:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000979
2020-10-12 20:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006273
2020-10-12 20:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17907.69921875Mb; avail=470683.7578125Mb
2020-10-12 20:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000295
2020-10-12 20:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17907.69921875Mb; avail=470683.7578125Mb
2020-10-12 20:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095767
2020-10-12 20:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103204
2020-10-12 20:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17906.7734375Mb; avail=470684.2578125Mb
2020-10-12 20:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17906.890625Mb; avail=470684.2578125Mb
2020-10-12 20:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003692
2020-10-12 20:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17906.890625Mb; avail=470684.2578125Mb
2020-10-12 20:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 20:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17906.890625Mb; avail=470684.2578125Mb
2020-10-12 20:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090763
2020-10-12 20:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095416
2020-10-12 20:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17906.62890625Mb; avail=470684.62890625Mb
2020-10-12 20:34:45 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 20:35:13 | INFO | train_inner | epoch 027:     96 / 104 loss=8.632, nll_loss=7.695, ppl=207.2, wps=16662.5, ups=2.89, wpb=5767.2, bsz=255.4, num_updates=2800, lr=0.00014003, gnorm=1.312, clip=0, train_wall=28, wall=1084
2020-10-12 20:35:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17826.6796875Mb; avail=470746.9609375Mb
2020-10-12 20:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001766
2020-10-12 20:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.6796875Mb; avail=470746.9609375Mb
2020-10-12 20:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069636
2020-10-12 20:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.77734375Mb; avail=470746.9609375Mb
2020-10-12 20:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049556
2020-10-12 20:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121747
2020-10-12 20:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.90234375Mb; avail=470746.83984375Mb
2020-10-12 20:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17826.90234375Mb; avail=470746.83984375Mb
2020-10-12 20:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001150
2020-10-12 20:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.90234375Mb; avail=470746.83984375Mb
2020-10-12 20:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070148
2020-10-12 20:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.0859375Mb; avail=470746.234375Mb
2020-10-12 20:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050836
2020-10-12 20:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122927
2020-10-12 20:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.03515625Mb; avail=470746.4765625Mb
2020-10-12 20:35:17 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.625 | nll_loss 7.637 | ppl 199.07 | wps 58158 | wpb 2291.8 | bsz 96.2 | num_updates 2808 | best_loss 8.625
2020-10-12 20:35:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:35:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 27 @ 2808 updates, score 8.625) (writing took 4.234996505998424 seconds)
2020-10-12 20:35:21 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 20:35:21 | INFO | train | epoch 027 | loss 8.661 | nll_loss 7.728 | ppl 211.94 | wps 16692.2 | ups 2.85 | wpb 5855.6 | bsz 249.4 | num_updates 2808 | lr 0.00014043 | gnorm 1.288 | clip 0 | train_wall 29 | wall 1093
2020-10-12 20:35:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 20:35:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 20:35:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17863.8828125Mb; avail=470709.9375Mb
2020-10-12 20:35:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000990
2020-10-12 20:35:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005994
2020-10-12 20:35:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.8828125Mb; avail=470709.9375Mb
2020-10-12 20:35:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000243
2020-10-12 20:35:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.8828125Mb; avail=470709.9375Mb
2020-10-12 20:35:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095919
2020-10-12 20:35:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102963
2020-10-12 20:35:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.80859375Mb; avail=470710.1796875Mb
2020-10-12 20:35:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17863.4296875Mb; avail=470710.671875Mb
2020-10-12 20:35:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003735
2020-10-12 20:35:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.90234375Mb; avail=470710.30078125Mb
2020-10-12 20:35:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 20:35:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.90234375Mb; avail=470710.30078125Mb
2020-10-12 20:35:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093015
2020-10-12 20:35:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098070
2020-10-12 20:35:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.67578125Mb; avail=470710.421875Mb
2020-10-12 20:35:22 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 20:35:48 | INFO | train_inner | epoch 028:     92 / 104 loss=8.572, nll_loss=7.625, ppl=197.36, wps=16854.2, ups=2.86, wpb=5898.4, bsz=255.1, num_updates=2900, lr=0.000145028, gnorm=1.313, clip=0, train_wall=27, wall=1119
2020-10-12 20:35:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:35:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19141.9140625Mb; avail=469432.38671875Mb
2020-10-12 20:35:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001624
2020-10-12 20:35:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19141.9140625Mb; avail=469432.38671875Mb
2020-10-12 20:35:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072184
2020-10-12 20:35:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19142.078125Mb; avail=469432.14453125Mb
2020-10-12 20:35:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050790
2020-10-12 20:35:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125560
2020-10-12 20:35:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19142.02734375Mb; avail=469432.38671875Mb
2020-10-12 20:35:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19142.02734375Mb; avail=469432.38671875Mb
2020-10-12 20:35:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001223
2020-10-12 20:35:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19142.02734375Mb; avail=469432.38671875Mb
2020-10-12 20:35:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071151
2020-10-12 20:35:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19142.02734375Mb; avail=469432.38671875Mb
2020-10-12 20:35:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050964
2020-10-12 20:35:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124127
2020-10-12 20:35:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19142.17578125Mb; avail=469432.0234375Mb
2020-10-12 20:35:53 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.538 | nll_loss 7.523 | ppl 183.89 | wps 55427.2 | wpb 2291.8 | bsz 96.2 | num_updates 2912 | best_loss 8.538
2020-10-12 20:35:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:36:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 28 @ 2912 updates, score 8.538) (writing took 6.647666097000183 seconds)
2020-10-12 20:36:00 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 20:36:00 | INFO | train | epoch 028 | loss 8.552 | nll_loss 7.601 | ppl 194.19 | wps 15753.9 | ups 2.69 | wpb 5855.6 | bsz 249.4 | num_updates 2912 | lr 0.000145627 | gnorm 1.34 | clip 0 | train_wall 28 | wall 1131
2020-10-12 20:36:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 20:36:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 20:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17799.6640625Mb; avail=470774.5625Mb
2020-10-12 20:36:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001077
2020-10-12 20:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006263
2020-10-12 20:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17793.71875Mb; avail=470779.90234375Mb
2020-10-12 20:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000257
2020-10-12 20:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17794.32421875Mb; avail=470779.90234375Mb
2020-10-12 20:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095817
2020-10-12 20:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103253
2020-10-12 20:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17947.46875Mb; avail=470626.234375Mb
2020-10-12 20:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18036.09765625Mb; avail=470537.6015625Mb
2020-10-12 20:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003974
2020-10-12 20:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18044.57421875Mb; avail=470529.73046875Mb
2020-10-12 20:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18045.78515625Mb; avail=470528.51953125Mb
2020-10-12 20:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090136
2020-10-12 20:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095139
2020-10-12 20:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18214.1953125Mb; avail=470359.296875Mb
2020-10-12 20:36:00 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 20:36:25 | INFO | train_inner | epoch 029:     88 / 104 loss=8.443, nll_loss=7.476, ppl=178.05, wps=15546.4, ups=2.67, wpb=5822.1, bsz=248.1, num_updates=3000, lr=0.000150025, gnorm=1.338, clip=0, train_wall=27, wall=1157
2020-10-12 20:36:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16107.01171875Mb; avail=472483.24609375Mb
2020-10-12 20:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002997
2020-10-12 20:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16108.0546875Mb; avail=472481.84765625Mb
2020-10-12 20:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.101452
2020-10-12 20:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16113.90234375Mb; avail=472472.9375Mb
2020-10-12 20:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055831
2020-10-12 20:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.161702
2020-10-12 20:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16182.75390625Mb; avail=472403.09375Mb
2020-10-12 20:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16208.171875Mb; avail=472377.3828125Mb
2020-10-12 20:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002100
2020-10-12 20:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16211.8359375Mb; avail=472374.63671875Mb
2020-10-12 20:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.116968
2020-10-12 20:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16387.86328125Mb; avail=472197.52734375Mb
2020-10-12 20:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.078942
2020-10-12 20:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.199688
2020-10-12 20:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16509.4296875Mb; avail=472075.94921875Mb
2020-10-12 20:36:32 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.469 | nll_loss 7.432 | ppl 172.74 | wps 57806.1 | wpb 2291.8 | bsz 96.2 | num_updates 3016 | best_loss 8.469
2020-10-12 20:36:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:36:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 29 @ 3016 updates, score 8.469) (writing took 9.525966811999751 seconds)
2020-10-12 20:36:42 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 20:36:42 | INFO | train | epoch 029 | loss 8.439 | nll_loss 7.471 | ppl 177.46 | wps 14618.2 | ups 2.5 | wpb 5855.6 | bsz 249.4 | num_updates 3016 | lr 0.000150825 | gnorm 1.336 | clip 0 | train_wall 28 | wall 1173
2020-10-12 20:36:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 20:36:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17866.015625Mb; avail=470708.23828125Mb
2020-10-12 20:36:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000865
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006166
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17866.13671875Mb; avail=470708.1171875Mb
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000205
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17866.13671875Mb; avail=470708.1171875Mb
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102785
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110197
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.9765625Mb; avail=470708.24609375Mb
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17866.92578125Mb; avail=470707.609375Mb
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004365
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17866.92578125Mb; avail=470707.609375Mb
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000190
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17866.92578125Mb; avail=470707.609375Mb
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101318
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106811
2020-10-12 20:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17866.9453125Mb; avail=470707.39453125Mb
2020-10-12 20:36:42 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 20:37:06 | INFO | train_inner | epoch 030:     84 / 104 loss=8.347, nll_loss=7.365, ppl=164.88, wps=14389.2, ups=2.46, wpb=5842.4, bsz=242.5, num_updates=3100, lr=0.000155023, gnorm=1.348, clip=0, train_wall=27, wall=1197
2020-10-12 20:37:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15778.30859375Mb; avail=472810.33984375Mb
2020-10-12 20:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001637
2020-10-12 20:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15778.65625Mb; avail=472809.39453125Mb
2020-10-12 20:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071973
2020-10-12 20:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15811.58984375Mb; avail=472775.78515625Mb
2020-10-12 20:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048729
2020-10-12 20:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123326
2020-10-12 20:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15763.53515625Mb; avail=472823.703125Mb
2020-10-12 20:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15766.92578125Mb; avail=472819.50390625Mb
2020-10-12 20:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001200
2020-10-12 20:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15766.66796875Mb; avail=472820.375Mb
2020-10-12 20:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072406
2020-10-12 20:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15853.60546875Mb; avail=472732.3671875Mb
2020-10-12 20:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047909
2020-10-12 20:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122279
2020-10-12 20:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15856.58984375Mb; avail=472730.125Mb
2020-10-12 20:37:14 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.422 | nll_loss 7.38 | ppl 166.6 | wps 57302.3 | wpb 2291.8 | bsz 96.2 | num_updates 3120 | best_loss 8.422
2020-10-12 20:37:14 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:37:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 30 @ 3120 updates, score 8.422) (writing took 10.681590378999317 seconds)
2020-10-12 20:37:24 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 20:37:24 | INFO | train | epoch 030 | loss 8.326 | nll_loss 7.341 | ppl 162.15 | wps 14325.7 | ups 2.45 | wpb 5855.6 | bsz 249.4 | num_updates 3120 | lr 0.000156022 | gnorm 1.332 | clip 0 | train_wall 28 | wall 1216
2020-10-12 20:37:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 20:37:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 20:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17900.0078125Mb; avail=470680.53125Mb
2020-10-12 20:37:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000847
2020-10-12 20:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005666
2020-10-12 20:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17900.453125Mb; avail=470680.46875Mb
2020-10-12 20:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000199
2020-10-12 20:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17900.32421875Mb; avail=470680.35546875Mb
2020-10-12 20:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090792
2020-10-12 20:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097482
2020-10-12 20:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17900.10546875Mb; avail=470679.546875Mb
2020-10-12 20:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17899.59765625Mb; avail=470678.83203125Mb
2020-10-12 20:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003981
2020-10-12 20:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17900.29296875Mb; avail=470678.64453125Mb
2020-10-12 20:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000204
2020-10-12 20:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17900.1640625Mb; avail=470678.53125Mb
2020-10-12 20:37:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101649
2020-10-12 20:37:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106733
2020-10-12 20:37:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17899.86328125Mb; avail=470677.4765625Mb
2020-10-12 20:37:25 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 20:37:47 | INFO | train_inner | epoch 031:     80 / 104 loss=8.237, nll_loss=7.238, ppl=150.97, wps=14156.9, ups=2.41, wpb=5874.2, bsz=253.7, num_updates=3200, lr=0.00016002, gnorm=1.41, clip=0, train_wall=27, wall=1239
2020-10-12 20:37:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17901.59375Mb; avail=470671.5Mb
2020-10-12 20:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001569
2020-10-12 20:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17901.59375Mb; avail=470671.5Mb
2020-10-12 20:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069877
2020-10-12 20:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17901.3515625Mb; avail=470672.46875Mb
2020-10-12 20:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049984
2020-10-12 20:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122210
2020-10-12 20:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17901.3515625Mb; avail=470672.46875Mb
2020-10-12 20:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17901.37890625Mb; avail=470672.34765625Mb
2020-10-12 20:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001107
2020-10-12 20:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17901.37890625Mb; avail=470672.34765625Mb
2020-10-12 20:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070419
2020-10-12 20:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17916.00390625Mb; avail=470657.546875Mb
2020-10-12 20:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049694
2020-10-12 20:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121984
2020-10-12 20:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17916.0703125Mb; avail=470657.57421875Mb
2020-10-12 20:37:56 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.334 | nll_loss 7.283 | ppl 155.71 | wps 56492.3 | wpb 2291.8 | bsz 96.2 | num_updates 3224 | best_loss 8.334
2020-10-12 20:37:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:38:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 31 @ 3224 updates, score 8.334) (writing took 4.374959398999636 seconds)
2020-10-12 20:38:01 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 20:38:01 | INFO | train | epoch 031 | loss 8.217 | nll_loss 7.214 | ppl 148.5 | wps 16756.4 | ups 2.86 | wpb 5855.6 | bsz 249.4 | num_updates 3224 | lr 0.000161219 | gnorm 1.386 | clip 0 | train_wall 29 | wall 1252
2020-10-12 20:38:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 20:38:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 20:38:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17953.640625Mb; avail=470620.21484375Mb
2020-10-12 20:38:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001129
2020-10-12 20:38:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006203
2020-10-12 20:38:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17953.14453125Mb; avail=470620.70703125Mb
2020-10-12 20:38:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000188
2020-10-12 20:38:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17953.14453125Mb; avail=470620.70703125Mb
2020-10-12 20:38:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096169
2020-10-12 20:38:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103385
2020-10-12 20:38:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17953.515625Mb; avail=470620.70703125Mb
2020-10-12 20:38:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17953.75390625Mb; avail=470620.34375Mb
2020-10-12 20:38:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003608
2020-10-12 20:38:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17953.75390625Mb; avail=470620.34375Mb
2020-10-12 20:38:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 20:38:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17953.75390625Mb; avail=470620.34375Mb
2020-10-12 20:38:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091150
2020-10-12 20:38:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095659
2020-10-12 20:38:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17953.38671875Mb; avail=470620.8359375Mb
2020-10-12 20:38:01 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 20:38:22 | INFO | train_inner | epoch 032:     76 / 104 loss=8.132, nll_loss=7.116, ppl=138.67, wps=16773.8, ups=2.86, wpb=5858.6, bsz=247, num_updates=3300, lr=0.000165018, gnorm=1.368, clip=0, train_wall=27, wall=1274
2020-10-12 20:38:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17819.93359375Mb; avail=470734.3984375Mb
2020-10-12 20:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001608
2020-10-12 20:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17819.93359375Mb; avail=470734.3984375Mb
2020-10-12 20:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069028
2020-10-12 20:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.015625Mb; avail=470734.51953125Mb
2020-10-12 20:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049849
2020-10-12 20:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121261
2020-10-12 20:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.05078125Mb; avail=470734.27734375Mb
2020-10-12 20:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17820.05078125Mb; avail=470734.27734375Mb
2020-10-12 20:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001066
2020-10-12 20:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.05078125Mb; avail=470734.27734375Mb
2020-10-12 20:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067833
2020-10-12 20:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.3125Mb; avail=470734.03515625Mb
2020-10-12 20:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048692
2020-10-12 20:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118341
2020-10-12 20:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17819.953125Mb; avail=470734.27734375Mb
2020-10-12 20:38:32 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.266 | nll_loss 7.188 | ppl 145.79 | wps 57855.6 | wpb 2291.8 | bsz 96.2 | num_updates 3328 | best_loss 8.266
2020-10-12 20:38:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:38:36 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 32 @ 3328 updates, score 8.266) (writing took 4.247961102000772 seconds)
2020-10-12 20:38:36 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 20:38:36 | INFO | train | epoch 032 | loss 8.099 | nll_loss 7.078 | ppl 135.06 | wps 16999.1 | ups 2.9 | wpb 5855.6 | bsz 249.4 | num_updates 3328 | lr 0.000166417 | gnorm 1.388 | clip 0 | train_wall 28 | wall 1288
2020-10-12 20:38:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 20:38:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 20:38:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17820.7265625Mb; avail=470733.8984375Mb
2020-10-12 20:38:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000893
2020-10-12 20:38:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005894
2020-10-12 20:38:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.7265625Mb; avail=470733.8984375Mb
2020-10-12 20:38:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 20:38:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.7265625Mb; avail=470733.8984375Mb
2020-10-12 20:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093363
2020-10-12 20:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100300
2020-10-12 20:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.0234375Mb; avail=470734.51171875Mb
2020-10-12 20:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17820.14453125Mb; avail=470734.48046875Mb
2020-10-12 20:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003659
2020-10-12 20:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.14453125Mb; avail=470734.48046875Mb
2020-10-12 20:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000897
2020-10-12 20:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.14453125Mb; avail=470734.48046875Mb
2020-10-12 20:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090369
2020-10-12 20:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095673
2020-10-12 20:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.7109375Mb; avail=470719.828125Mb
2020-10-12 20:38:37 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 20:38:57 | INFO | train_inner | epoch 033:     72 / 104 loss=8.007, nll_loss=6.972, ppl=125.5, wps=17049.5, ups=2.87, wpb=5936.7, bsz=260.3, num_updates=3400, lr=0.000170015, gnorm=1.289, clip=0, train_wall=27, wall=1308
2020-10-12 20:39:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:39:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17816.23828125Mb; avail=470738.4375Mb
2020-10-12 20:39:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001676
2020-10-12 20:39:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.23828125Mb; avail=470738.4375Mb
2020-10-12 20:39:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075200
2020-10-12 20:39:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.30078125Mb; avail=470738.07421875Mb
2020-10-12 20:39:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051276
2020-10-12 20:39:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129118
2020-10-12 20:39:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.234375Mb; avail=470738.4375Mb
2020-10-12 20:39:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17816.234375Mb; avail=470738.4375Mb
2020-10-12 20:39:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001169
2020-10-12 20:39:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.234375Mb; avail=470738.4375Mb
2020-10-12 20:39:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069220
2020-10-12 20:39:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.28125Mb; avail=470738.1953125Mb
2020-10-12 20:39:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055783
2020-10-12 20:39:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127035
2020-10-12 20:39:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.609375Mb; avail=470738.07421875Mb
2020-10-12 20:39:08 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.236 | nll_loss 7.157 | ppl 142.74 | wps 57642.7 | wpb 2291.8 | bsz 96.2 | num_updates 3432 | best_loss 8.236
2020-10-12 20:39:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:39:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 33 @ 3432 updates, score 8.236) (writing took 4.873306765000962 seconds)
2020-10-12 20:39:13 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 20:39:13 | INFO | train | epoch 033 | loss 7.962 | nll_loss 6.919 | ppl 121.04 | wps 16645.1 | ups 2.84 | wpb 5855.6 | bsz 249.4 | num_updates 3432 | lr 0.000171614 | gnorm 1.311 | clip 0 | train_wall 28 | wall 1324
2020-10-12 20:39:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 20:39:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 20:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17808.6796875Mb; avail=470745.87109375Mb
2020-10-12 20:39:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001247
2020-10-12 20:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.009578
2020-10-12 20:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.5859375Mb; avail=470745.87109375Mb
2020-10-12 20:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000374
2020-10-12 20:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.5859375Mb; avail=470745.87109375Mb
2020-10-12 20:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.152027
2020-10-12 20:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.163366
2020-10-12 20:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.62109375Mb; avail=470745.9921875Mb
2020-10-12 20:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17808.6015625Mb; avail=470745.75Mb
2020-10-12 20:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003772
2020-10-12 20:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.6015625Mb; avail=470745.75Mb
2020-10-12 20:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 20:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.6015625Mb; avail=470745.75Mb
2020-10-12 20:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089598
2020-10-12 20:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094304
2020-10-12 20:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.7578125Mb; avail=470746.11328125Mb
2020-10-12 20:39:13 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 20:39:33 | INFO | train_inner | epoch 034:     68 / 104 loss=7.866, nll_loss=6.808, ppl=112.03, wps=16213.4, ups=2.8, wpb=5792.6, bsz=246.4, num_updates=3500, lr=0.000175013, gnorm=1.397, clip=0, train_wall=27, wall=1344
2020-10-12 20:39:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18416.3671875Mb; avail=470137.77734375Mb
2020-10-12 20:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001715
2020-10-12 20:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18416.3671875Mb; avail=470137.77734375Mb
2020-10-12 20:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069072
2020-10-12 20:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18416.25Mb; avail=470138.66015625Mb
2020-10-12 20:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048719
2020-10-12 20:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120314
2020-10-12 20:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18416.46484375Mb; avail=470138.5390625Mb
2020-10-12 20:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18416.484375Mb; avail=470138.41796875Mb
2020-10-12 20:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001088
2020-10-12 20:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18416.484375Mb; avail=470138.41796875Mb
2020-10-12 20:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069129
2020-10-12 20:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18416.7109375Mb; avail=470138.17578125Mb
2020-10-12 20:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048435
2020-10-12 20:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119419
2020-10-12 20:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18416.58984375Mb; avail=470138.296875Mb
2020-10-12 20:39:45 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.17 | nll_loss 7.073 | ppl 134.62 | wps 58440 | wpb 2291.8 | bsz 96.2 | num_updates 3536 | best_loss 8.17
2020-10-12 20:39:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:39:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 34 @ 3536 updates, score 8.17) (writing took 11.770754749999469 seconds)
2020-10-12 20:39:57 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 20:39:57 | INFO | train | epoch 034 | loss 7.847 | nll_loss 6.785 | ppl 110.27 | wps 13875.9 | ups 2.37 | wpb 5855.6 | bsz 249.4 | num_updates 3536 | lr 0.000176812 | gnorm 1.359 | clip 0 | train_wall 29 | wall 1368
2020-10-12 20:39:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 20:39:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17820.734375Mb; avail=470734.45703125Mb
2020-10-12 20:39:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000706
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005978
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.734375Mb; avail=470734.45703125Mb
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000210
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.734375Mb; avail=470734.45703125Mb
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101965
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108987
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.17578125Mb; avail=470752.796875Mb
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17795.55859375Mb; avail=470759.1171875Mb
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003718
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17795.55859375Mb; avail=470759.1171875Mb
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17795.55859375Mb; avail=470759.1171875Mb
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092904
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097808
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17796.046875Mb; avail=470758.6328125Mb
2020-10-12 20:39:57 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 20:40:15 | INFO | train_inner | epoch 035:     64 / 104 loss=7.765, nll_loss=6.689, ppl=103.19, wps=13710.1, ups=2.35, wpb=5828, bsz=240.4, num_updates=3600, lr=0.00018001, gnorm=1.331, clip=0, train_wall=27, wall=1387
2020-10-12 20:40:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:40:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18435.6484375Mb; avail=470118.55859375Mb
2020-10-12 20:40:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001779
2020-10-12 20:40:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18435.6484375Mb; avail=470118.55859375Mb
2020-10-12 20:40:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069487
2020-10-12 20:40:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18435.8515625Mb; avail=470118.55859375Mb
2020-10-12 20:40:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049078
2020-10-12 20:40:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121190
2020-10-12 20:40:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18435.87890625Mb; avail=470118.4375Mb
2020-10-12 20:40:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18435.87890625Mb; avail=470118.4375Mb
2020-10-12 20:40:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001070
2020-10-12 20:40:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18435.87890625Mb; avail=470118.4375Mb
2020-10-12 20:40:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069030
2020-10-12 20:40:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18435.828125Mb; avail=470118.55859375Mb
2020-10-12 20:40:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048757
2020-10-12 20:40:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119643
2020-10-12 20:40:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18435.7734375Mb; avail=470118.421875Mb
2020-10-12 20:40:29 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.088 | nll_loss 6.963 | ppl 124.72 | wps 58040.2 | wpb 2291.8 | bsz 96.2 | num_updates 3640 | best_loss 8.088
2020-10-12 20:40:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:40:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 35 @ 3640 updates, score 8.088) (writing took 11.105579352999484 seconds)
2020-10-12 20:40:40 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 20:40:40 | INFO | train | epoch 035 | loss 7.719 | nll_loss 6.636 | ppl 99.47 | wps 14174.2 | ups 2.42 | wpb 5855.6 | bsz 249.4 | num_updates 3640 | lr 0.000182009 | gnorm 1.342 | clip 0 | train_wall 28 | wall 1411
2020-10-12 20:40:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 20:40:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 20:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19111.80078125Mb; avail=469443.12890625Mb
2020-10-12 20:40:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001286
2020-10-12 20:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.009650
2020-10-12 20:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19111.9140625Mb; avail=469443.015625Mb
2020-10-12 20:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000464
2020-10-12 20:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19111.9140625Mb; avail=469443.015625Mb
2020-10-12 20:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104592
2020-10-12 20:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.116167
2020-10-12 20:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19111.78515625Mb; avail=469443.234375Mb
2020-10-12 20:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19111.7890625Mb; avail=469443.234375Mb
2020-10-12 20:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004146
2020-10-12 20:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19111.7890625Mb; avail=469443.234375Mb
2020-10-12 20:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 20:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19111.7890625Mb; avail=469443.234375Mb
2020-10-12 20:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093572
2020-10-12 20:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098773
2020-10-12 20:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19111.7890625Mb; avail=469443.234375Mb
2020-10-12 20:40:40 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 20:40:57 | INFO | train_inner | epoch 036:     60 / 104 loss=7.661, nll_loss=6.568, ppl=94.87, wps=13996, ups=2.39, wpb=5853.4, bsz=243.1, num_updates=3700, lr=0.000185008, gnorm=1.352, clip=0, train_wall=27, wall=1428
2020-10-12 20:41:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17801.3828125Mb; avail=470752.3984375Mb
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001657
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17801.3828125Mb; avail=470752.3984375Mb
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068642
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17801.58203125Mb; avail=470752.3984375Mb
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049241
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120321
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17801.60546875Mb; avail=470752.27734375Mb
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17801.60546875Mb; avail=470752.15625Mb
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001063
2020-10-12 20:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17801.6171875Mb; avail=470752.15625Mb
2020-10-12 20:41:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067726
2020-10-12 20:41:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17801.34375Mb; avail=470752.640625Mb
2020-10-12 20:41:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048918
2020-10-12 20:41:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118445
2020-10-12 20:41:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17801.23046875Mb; avail=470752.76171875Mb
2020-10-12 20:41:12 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.073 | nll_loss 6.945 | ppl 123.24 | wps 58076.4 | wpb 2291.8 | bsz 96.2 | num_updates 3744 | best_loss 8.073
2020-10-12 20:41:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:41:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 36 @ 3744 updates, score 8.073) (writing took 4.263990710000144 seconds)
2020-10-12 20:41:16 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 20:41:16 | INFO | train | epoch 036 | loss 7.598 | nll_loss 6.494 | ppl 90.16 | wps 16906.3 | ups 2.89 | wpb 5855.6 | bsz 249.4 | num_updates 3744 | lr 0.000187206 | gnorm 1.368 | clip 0 | train_wall 28 | wall 1447
2020-10-12 20:41:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 20:41:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 20:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17822.609375Mb; avail=470731.67578125Mb
2020-10-12 20:41:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001026
2020-10-12 20:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006305
2020-10-12 20:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.609375Mb; avail=470731.67578125Mb
2020-10-12 20:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.609375Mb; avail=470731.67578125Mb
2020-10-12 20:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095120
2020-10-12 20:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102522
2020-10-12 20:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.8046875Mb; avail=470731.796875Mb
2020-10-12 20:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17822.43359375Mb; avail=470732.0546875Mb
2020-10-12 20:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003670
2020-10-12 20:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.43359375Mb; avail=470732.0546875Mb
2020-10-12 20:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 20:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.43359375Mb; avail=470732.0546875Mb
2020-10-12 20:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089889
2020-10-12 20:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094455
2020-10-12 20:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.6484375Mb; avail=470731.8046875Mb
2020-10-12 20:41:16 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 20:41:32 | INFO | train_inner | epoch 037:     56 / 104 loss=7.528, nll_loss=6.413, ppl=85.22, wps=16898.9, ups=2.87, wpb=5897.8, bsz=256.2, num_updates=3800, lr=0.000190005, gnorm=1.385, clip=0, train_wall=27, wall=1463
2020-10-12 20:41:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19707.6015625Mb; avail=468847.640625Mb
2020-10-12 20:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001721
2020-10-12 20:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19707.56640625Mb; avail=468847.640625Mb
2020-10-12 20:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079622
2020-10-12 20:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19707.58984375Mb; avail=468847.76171875Mb
2020-10-12 20:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056967
2020-10-12 20:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139289
2020-10-12 20:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19707.6171875Mb; avail=468847.76953125Mb
2020-10-12 20:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19707.6171875Mb; avail=468847.76953125Mb
2020-10-12 20:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001254
2020-10-12 20:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19707.6171875Mb; avail=468847.76953125Mb
2020-10-12 20:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079684
2020-10-12 20:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19707.6171875Mb; avail=468847.76953125Mb
2020-10-12 20:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056214
2020-10-12 20:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137928
2020-10-12 20:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19707.62890625Mb; avail=468847.76953125Mb
2020-10-12 20:41:48 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8 | nll_loss 6.846 | ppl 115.01 | wps 56024.7 | wpb 2291.8 | bsz 96.2 | num_updates 3848 | best_loss 8
2020-10-12 20:41:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:42:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 37 @ 3848 updates, score 8.0) (writing took 13.661499350000668 seconds)
2020-10-12 20:42:01 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 20:42:01 | INFO | train | epoch 037 | loss 7.492 | nll_loss 6.371 | ppl 82.76 | wps 13382.9 | ups 2.29 | wpb 5855.6 | bsz 249.4 | num_updates 3848 | lr 0.000192404 | gnorm 1.468 | clip 0 | train_wall 28 | wall 1493
2020-10-12 20:42:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 20:42:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 20:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17929.03125Mb; avail=470628.4765625Mb
2020-10-12 20:42:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000702
2020-10-12 20:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005859
2020-10-12 20:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17929.33984375Mb; avail=470627.94921875Mb
2020-10-12 20:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17929.2109375Mb; avail=470627.8359375Mb
2020-10-12 20:42:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095542
2020-10-12 20:42:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102633
2020-10-12 20:42:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17928.66796875Mb; avail=470626.73828125Mb
2020-10-12 20:42:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17929.6875Mb; avail=470626.1875Mb
2020-10-12 20:42:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003675
2020-10-12 20:42:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17928.55078125Mb; avail=470626.8515625Mb
2020-10-12 20:42:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 20:42:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17929.15625Mb; avail=470626.73828125Mb
2020-10-12 20:42:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099040
2020-10-12 20:42:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103922
2020-10-12 20:42:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17928.41015625Mb; avail=470625.87109375Mb
2020-10-12 20:42:02 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 20:42:16 | INFO | train_inner | epoch 038:     52 / 104 loss=7.417, nll_loss=6.282, ppl=77.84, wps=13289.5, ups=2.25, wpb=5914.3, bsz=253.8, num_updates=3900, lr=0.000195003, gnorm=1.391, clip=0, train_wall=27, wall=1508
2020-10-12 20:42:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19116.68359375Mb; avail=469440.95703125Mb
2020-10-12 20:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001653
2020-10-12 20:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19117.03125Mb; avail=469440.6171875Mb
2020-10-12 20:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070399
2020-10-12 20:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19110.50390625Mb; avail=469448.26953125Mb
2020-10-12 20:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049002
2020-10-12 20:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122123
2020-10-12 20:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19110.3359375Mb; avail=469447.70703125Mb
2020-10-12 20:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19110.0Mb; avail=469447.59765625Mb
2020-10-12 20:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001170
2020-10-12 20:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19110.38671875Mb; avail=469447.4609375Mb
2020-10-12 20:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069181
2020-10-12 20:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19109.95703125Mb; avail=469446.31640625Mb
2020-10-12 20:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049091
2020-10-12 20:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120436
2020-10-12 20:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19109.61328125Mb; avail=469445.765625Mb
2020-10-12 20:42:33 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.95 | nll_loss 6.788 | ppl 110.52 | wps 57311.1 | wpb 2291.8 | bsz 96.2 | num_updates 3952 | best_loss 7.95
2020-10-12 20:42:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:42:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 38 @ 3952 updates, score 7.95) (writing took 9.348775071999626 seconds)
2020-10-12 20:42:43 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 20:42:43 | INFO | train | epoch 038 | loss 7.346 | nll_loss 6.2 | ppl 73.53 | wps 14798.7 | ups 2.53 | wpb 5855.6 | bsz 249.4 | num_updates 3952 | lr 0.000197601 | gnorm 1.321 | clip 0 | train_wall 28 | wall 1534
2020-10-12 20:42:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 20:42:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 20:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16241.14453125Mb; avail=472316.13671875Mb
2020-10-12 20:42:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001628
2020-10-12 20:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008761
2020-10-12 20:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16240.65234375Mb; avail=472316.13671875Mb
2020-10-12 20:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000202
2020-10-12 20:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16240.65234375Mb; avail=472316.13671875Mb
2020-10-12 20:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109454
2020-10-12 20:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.119570
2020-10-12 20:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16284.24609375Mb; avail=472272.84375Mb
2020-10-12 20:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16323.24609375Mb; avail=472233.73046875Mb
2020-10-12 20:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003996
2020-10-12 20:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16323.24609375Mb; avail=472233.73046875Mb
2020-10-12 20:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 20:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16323.24609375Mb; avail=472233.73046875Mb
2020-10-12 20:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089847
2020-10-12 20:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094859
2020-10-12 20:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16383.8984375Mb; avail=472173.2734375Mb
2020-10-12 20:42:43 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 20:42:56 | INFO | train_inner | epoch 039:     48 / 104 loss=7.272, nll_loss=6.115, ppl=69.29, wps=14455.6, ups=2.51, wpb=5760.6, bsz=246, num_updates=4000, lr=0.0002, gnorm=1.371, clip=0, train_wall=27, wall=1548
2020-10-12 20:43:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18496.71484375Mb; avail=470058.71484375Mb
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001636
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18496.71484375Mb; avail=470058.71484375Mb
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069724
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18496.95703125Mb; avail=470058.59375Mb
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048713
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120891
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18496.75390625Mb; avail=470058.71484375Mb
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18496.75390625Mb; avail=470058.71484375Mb
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001100
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18496.75390625Mb; avail=470058.71484375Mb
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069132
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18496.87109375Mb; avail=470058.59375Mb
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048569
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119592
2020-10-12 20:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18496.87109375Mb; avail=470058.59375Mb
2020-10-12 20:43:14 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.922 | nll_loss 6.74 | ppl 106.92 | wps 57892.2 | wpb 2291.8 | bsz 96.2 | num_updates 4056 | best_loss 7.922
2020-10-12 20:43:14 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:43:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 39 @ 4056 updates, score 7.922) (writing took 10.374942784999803 seconds)
2020-10-12 20:43:25 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 20:43:25 | INFO | train | epoch 039 | loss 7.227 | nll_loss 6.061 | ppl 66.76 | wps 14488.9 | ups 2.47 | wpb 5855.6 | bsz 249.4 | num_updates 4056 | lr 0.000198615 | gnorm 1.378 | clip 0 | train_wall 28 | wall 1576
2020-10-12 20:43:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 20:43:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 20:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17910.77734375Mb; avail=470644.1484375Mb
2020-10-12 20:43:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000806
2020-10-12 20:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006057
2020-10-12 20:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17911.3828125Mb; avail=470643.54296875Mb
2020-10-12 20:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17911.3828125Mb; avail=470643.54296875Mb
2020-10-12 20:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101702
2020-10-12 20:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108774
2020-10-12 20:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17911.4453125Mb; avail=470643.48046875Mb
2020-10-12 20:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17911.45703125Mb; avail=470643.20703125Mb
2020-10-12 20:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004221
2020-10-12 20:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17911.41015625Mb; avail=470643.44921875Mb
2020-10-12 20:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000232
2020-10-12 20:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17911.42578125Mb; avail=470643.43359375Mb
2020-10-12 20:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099856
2020-10-12 20:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105286
2020-10-12 20:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17911.41796875Mb; avail=470643.3125Mb
2020-10-12 20:43:25 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 20:43:37 | INFO | train_inner | epoch 040:     44 / 104 loss=7.201, nll_loss=6.029, ppl=65.28, wps=14448.6, ups=2.44, wpb=5924.5, bsz=247, num_updates=4100, lr=0.000197546, gnorm=1.356, clip=0, train_wall=27, wall=1589
2020-10-12 20:43:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16031.34375Mb; avail=472535.56640625Mb
2020-10-12 20:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001636
2020-10-12 20:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16031.34375Mb; avail=472535.56640625Mb
2020-10-12 20:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072311
2020-10-12 20:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16031.3515625Mb; avail=472535.56640625Mb
2020-10-12 20:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049545
2020-10-12 20:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124472
2020-10-12 20:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16031.4765625Mb; avail=472535.4453125Mb
2020-10-12 20:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16031.4765625Mb; avail=472535.4453125Mb
2020-10-12 20:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001162
2020-10-12 20:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16031.4765625Mb; avail=472535.4453125Mb
2020-10-12 20:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070356
2020-10-12 20:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15834.625Mb; avail=472735.5234375Mb
2020-10-12 20:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049701
2020-10-12 20:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122022
2020-10-12 20:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15815.921875Mb; avail=472754.96484375Mb
2020-10-12 20:43:56 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.867 | nll_loss 6.673 | ppl 102.04 | wps 56418.2 | wpb 2291.8 | bsz 96.2 | num_updates 4160 | best_loss 7.867
2020-10-12 20:43:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:44:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 40 @ 4160 updates, score 7.867) (writing took 4.445745335000538 seconds)
2020-10-12 20:44:01 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 20:44:01 | INFO | train | epoch 040 | loss 7.099 | nll_loss 5.91 | ppl 60.13 | wps 16771.3 | ups 2.86 | wpb 5855.6 | bsz 249.4 | num_updates 4160 | lr 0.000196116 | gnorm 1.35 | clip 0 | train_wall 28 | wall 1612
2020-10-12 20:44:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 20:44:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 20:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16616.04296875Mb; avail=471950.53125Mb
2020-10-12 20:44:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000695
2020-10-12 20:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006063
2020-10-12 20:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16622.78125Mb; avail=471943.87109375Mb
2020-10-12 20:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000196
2020-10-12 20:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16623.38671875Mb; avail=471943.265625Mb
2020-10-12 20:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104447
2020-10-12 20:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111624
2020-10-12 20:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16654.0625Mb; avail=471919.59375Mb
2020-10-12 20:44:01 | INFO | fairseq_cli.train | done training in 1612.3 seconds
