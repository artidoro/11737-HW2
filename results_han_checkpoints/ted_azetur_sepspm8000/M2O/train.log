2020-10-12 19:48:54 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_azetur_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='aze-eng,tur-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_azetur_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 19:48:54 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 19:48:54 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['aze', 'eng', 'tur']
2020-10-12 19:48:54 | INFO | fairseq.data.multilingual.multilingual_data_manager | [aze] dictionary: 19763 types
2020-10-12 19:48:54 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 19763 types
2020-10-12 19:48:54 | INFO | fairseq.data.multilingual.multilingual_data_manager | [tur] dictionary: 19763 types
2020-10-12 19:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 19:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16149.36328125Mb; avail=472601.2578125Mb
2020-10-12 19:48:54 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 19:48:54 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:aze-eng': 1, 'main:tur-eng': 1}
2020-10-12 19:48:54 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:54 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azetur_sepspm8000/M2O/valid.aze-eng.aze
2020-10-12 19:48:54 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azetur_sepspm8000/M2O/valid.aze-eng.eng
2020-10-12 19:48:54 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azetur_sepspm8000/M2O/ valid aze-eng 671 examples
2020-10-12 19:48:54 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:tur-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:54 | INFO | fairseq.data.data_utils | loaded 4045 examples from: fairseq/data-bin/ted_azetur_sepspm8000/M2O/valid.tur-eng.tur
2020-10-12 19:48:54 | INFO | fairseq.data.data_utils | loaded 4045 examples from: fairseq/data-bin/ted_azetur_sepspm8000/M2O/valid.tur-eng.eng
2020-10-12 19:48:54 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azetur_sepspm8000/M2O/ valid tur-eng 4045 examples
2020-10-12 19:48:55 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19763, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19763, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19763, bias=False)
  )
)
2020-10-12 19:48:55 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 19:48:55 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 19:48:55 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 19:48:55 | INFO | fairseq_cli.train | num. model params: 41661952 (num. trained: 41661952)
2020-10-12 19:48:59 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 19:48:59 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 19:48:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 19:48:59 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 19:48:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 19:48:59 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 19:48:59 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 19:48:59 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_last.pt
2020-10-12 19:48:59 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 19:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 19:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17770.5546875Mb; avail=470953.828125Mb
2020-10-12 19:48:59 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 19:48:59 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:aze-eng': 1, 'main:tur-eng': 1}
2020-10-12 19:48:59 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:59 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azetur_sepspm8000/M2O/train.aze-eng.aze
2020-10-12 19:48:59 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azetur_sepspm8000/M2O/train.aze-eng.eng
2020-10-12 19:48:59 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azetur_sepspm8000/M2O/ train aze-eng 5946 examples
2020-10-12 19:48:59 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:tur-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:59 | INFO | fairseq.data.data_utils | loaded 19990 examples from: fairseq/data-bin/ted_azetur_sepspm8000/M2O/train.tur-eng.tur
2020-10-12 19:48:59 | INFO | fairseq.data.data_utils | loaded 19990 examples from: fairseq/data-bin/ted_azetur_sepspm8000/M2O/train.tur-eng.eng
2020-10-12 19:48:59 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azetur_sepspm8000/M2O/ train tur-eng 19990 examples
2020-10-12 19:48:59 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:aze-eng', 5946), ('main:tur-eng', 19990)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 19:48:59 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 19:48:59 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 25936
2020-10-12 19:48:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 25936; virtual dataset size 25936
2020-10-12 19:48:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 19:48:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:aze-eng': 5946, 'main:tur-eng': 19990}; raw total size: 25936
2020-10-12 19:48:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:aze-eng': 5946, 'main:tur-eng': 19990}; resampled total size: 25936
2020-10-12 19:48:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 19:48:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.004377
2020-10-12 19:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17770.62890625Mb; avail=470953.75390625Mb
2020-10-12 19:48:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000573
2020-10-12 19:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005099
2020-10-12 19:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17771.26953125Mb; avail=470953.02734375Mb
2020-10-12 19:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000218
2020-10-12 19:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17771.26953125Mb; avail=470953.02734375Mb
2020-10-12 19:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101100
2020-10-12 19:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108696
2020-10-12 19:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17771.3671875Mb; avail=470953.04296875Mb
2020-10-12 19:48:59 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 19:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17771.7578125Mb; avail=470952.46484375Mb
2020-10-12 19:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003629
2020-10-12 19:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17771.63671875Mb; avail=470952.5859375Mb
2020-10-12 19:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000207
2020-10-12 19:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17771.63671875Mb; avail=470952.5859375Mb
2020-10-12 19:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098977
2020-10-12 19:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103559
2020-10-12 19:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17771.32421875Mb; avail=470952.58203125Mb
2020-10-12 19:48:59 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 19:49:28 | INFO | train_inner | epoch 001:    100 / 107 loss=13.974, nll_loss=13.86, ppl=14866.3, wps=22925, ups=3.56, wpb=6460.3, bsz=245.4, num_updates=100, lr=5.0975e-06, gnorm=4.782, clip=0, train_wall=28, wall=29
2020-10-12 19:49:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18404.47265625Mb; avail=470288.23046875Mb
2020-10-12 19:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002180
2020-10-12 19:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18405.42578125Mb; avail=470287.77734375Mb
2020-10-12 19:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081249
2020-10-12 19:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18404.5546875Mb; avail=470287.83203125Mb
2020-10-12 19:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062461
2020-10-12 19:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.147262
2020-10-12 19:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18405.09375Mb; avail=470286.48828125Mb
2020-10-12 19:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18404.55078125Mb; avail=470286.29296875Mb
2020-10-12 19:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001434
2020-10-12 19:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18404.8984375Mb; avail=470286.55859375Mb
2020-10-12 19:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078659
2020-10-12 19:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18404.890625Mb; avail=470285.46484375Mb
2020-10-12 19:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062652
2020-10-12 19:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.143868
2020-10-12 19:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18404.87890625Mb; avail=470285.4765625Mb
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2020-10-12 19:49:32 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.2 | nll_loss 11.871 | ppl 3744.46 | wps 62919.9 | wpb 2442.6 | bsz 92.5 | num_updates 107
2020-10-12 19:49:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:49:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 1 @ 107 updates, score 12.2) (writing took 1.491938795000351 seconds)
2020-10-12 19:49:34 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 19:49:34 | INFO | train | epoch 001 | loss 13.883 | nll_loss 13.759 | ppl 13860 | wps 20236.8 | ups 3.14 | wpb 6450.5 | bsz 242.4 | num_updates 107 | lr 5.44733e-06 | gnorm 4.651 | clip 0 | train_wall 30 | wall 35
2020-10-12 19:49:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 19:49:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 19:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18315.9921875Mb; avail=470358.4921875Mb
2020-10-12 19:49:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000849
2020-10-12 19:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005472
2020-10-12 19:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18315.9921875Mb; avail=470358.4921875Mb
2020-10-12 19:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000202
2020-10-12 19:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18315.9921875Mb; avail=470358.4921875Mb
2020-10-12 19:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099612
2020-10-12 19:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106062
2020-10-12 19:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18310.90234375Mb; avail=470363.47265625Mb
2020-10-12 19:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18311.0859375Mb; avail=470364.15625Mb
2020-10-12 19:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003782
2020-10-12 19:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18311.0859375Mb; avail=470364.15625Mb
2020-10-12 19:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-12 19:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18311.0859375Mb; avail=470364.15625Mb
2020-10-12 19:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097028
2020-10-12 19:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101804
2020-10-12 19:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18311.23046875Mb; avail=470363.7734375Mb
2020-10-12 19:49:34 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 19:50:00 | INFO | train_inner | epoch 002:     93 / 107 loss=12.113, nll_loss=11.783, ppl=3524.02, wps=19918.7, ups=3.1, wpb=6415.9, bsz=240.3, num_updates=200, lr=1.0095e-05, gnorm=2.1, clip=0, train_wall=27, wall=61
2020-10-12 19:50:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17846.3359375Mb; avail=470827.42578125Mb
2020-10-12 19:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002024
2020-10-12 19:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17846.29296875Mb; avail=470827.66796875Mb
2020-10-12 19:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079427
2020-10-12 19:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17846.265625Mb; avail=470827.66796875Mb
2020-10-12 19:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063755
2020-10-12 19:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.146230
2020-10-12 19:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17846.14453125Mb; avail=470827.7890625Mb
2020-10-12 19:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17846.4453125Mb; avail=470827.328125Mb
2020-10-12 19:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001331
2020-10-12 19:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17845.9140625Mb; avail=470827.8203125Mb
2020-10-12 19:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080667
2020-10-12 19:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17846.5859375Mb; avail=470827.546875Mb
2020-10-12 19:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062458
2020-10-12 19:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.145449
2020-10-12 19:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17846.57421875Mb; avail=470827.546875Mb
2020-10-12 19:50:06 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.359 | nll_loss 10.923 | ppl 1941.38 | wps 63659.6 | wpb 2442.6 | bsz 92.5 | num_updates 214 | best_loss 11.359
2020-10-12 19:50:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:50:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 2 @ 214 updates, score 11.359) (writing took 5.959185163999791 seconds)
2020-10-12 19:50:12 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 19:50:12 | INFO | train | epoch 002 | loss 12.024 | nll_loss 11.683 | ppl 3287.48 | wps 17887.5 | ups 2.77 | wpb 6450.5 | bsz 242.4 | num_updates 214 | lr 1.07947e-05 | gnorm 1.997 | clip 0 | train_wall 29 | wall 74
2020-10-12 19:50:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 19:50:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 19:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17818.484375Mb; avail=470855.3359375Mb
2020-10-12 19:50:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000781
2020-10-12 19:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005490
2020-10-12 19:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.51171875Mb; avail=470855.3359375Mb
2020-10-12 19:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-12 19:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.51171875Mb; avail=470855.3359375Mb
2020-10-12 19:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099564
2020-10-12 19:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106049
2020-10-12 19:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.1015625Mb; avail=470860.75Mb
2020-10-12 19:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17813.36328125Mb; avail=470860.265625Mb
2020-10-12 19:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003678
2020-10-12 19:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.36328125Mb; avail=470860.265625Mb
2020-10-12 19:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 19:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.36328125Mb; avail=470860.265625Mb
2020-10-12 19:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097950
2020-10-12 19:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102574
2020-10-12 19:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.34765625Mb; avail=470860.50390625Mb
2020-10-12 19:50:13 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 19:50:37 | INFO | train_inner | epoch 003:     86 / 107 loss=11.312, nll_loss=10.886, ppl=1893.02, wps=17730.4, ups=2.73, wpb=6501.8, bsz=244.4, num_updates=300, lr=1.50925e-05, gnorm=1.496, clip=0, train_wall=27, wall=98
2020-10-12 19:50:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18412.0078125Mb; avail=470262.5703125Mb
2020-10-12 19:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001785
2020-10-12 19:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18412.0078125Mb; avail=470262.5703125Mb
2020-10-12 19:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072070
2020-10-12 19:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18412.015625Mb; avail=470262.5703125Mb
2020-10-12 19:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056307
2020-10-12 19:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.131201
2020-10-12 19:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18412.0078125Mb; avail=470262.328125Mb
2020-10-12 19:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18412.0078125Mb; avail=470262.328125Mb
2020-10-12 19:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001330
2020-10-12 19:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18412.61328125Mb; avail=470261.72265625Mb
2020-10-12 19:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071266
2020-10-12 19:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18413.76953125Mb; avail=470260.75390625Mb
2020-10-12 19:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055404
2020-10-12 19:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128768
2020-10-12 19:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18413.9453125Mb; avail=470260.26953125Mb
2020-10-12 19:50:45 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.426 | nll_loss 9.867 | ppl 933.59 | wps 63258.2 | wpb 2442.6 | bsz 92.5 | num_updates 321 | best_loss 10.426
2020-10-12 19:50:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:50:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 3 @ 321 updates, score 10.426) (writing took 6.485610657000507 seconds)
2020-10-12 19:50:51 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 19:50:51 | INFO | train | epoch 003 | loss 11.157 | nll_loss 10.712 | ppl 1677.93 | wps 17660 | ups 2.74 | wpb 6450.5 | bsz 242.4 | num_updates 321 | lr 1.6142e-05 | gnorm 1.51 | clip 0 | train_wall 29 | wall 113
2020-10-12 19:50:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 19:50:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 19:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17841.69921875Mb; avail=470832.1796875Mb
2020-10-12 19:50:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001055
2020-10-12 19:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006348
2020-10-12 19:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.69921875Mb; avail=470832.1796875Mb
2020-10-12 19:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000208
2020-10-12 19:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.69921875Mb; avail=470832.1796875Mb
2020-10-12 19:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101750
2020-10-12 19:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109210
2020-10-12 19:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.7265625Mb; avail=470839.1953125Mb
2020-10-12 19:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17834.625Mb; avail=470839.10546875Mb
2020-10-12 19:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003611
2020-10-12 19:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.625Mb; avail=470839.10546875Mb
2020-10-12 19:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 19:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.70703125Mb; avail=470839.2265625Mb
2020-10-12 19:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097087
2020-10-12 19:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101663
2020-10-12 19:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.40234375Mb; avail=470839.34765625Mb
2020-10-12 19:50:52 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 19:51:14 | INFO | train_inner | epoch 004:     79 / 107 loss=10.325, nll_loss=9.762, ppl=868.09, wps=17314, ups=2.7, wpb=6404.3, bsz=242.4, num_updates=400, lr=2.009e-05, gnorm=1.643, clip=0, train_wall=27, wall=135
2020-10-12 19:51:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:51:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17802.984375Mb; avail=470870.36328125Mb
2020-10-12 19:51:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001661
2020-10-12 19:51:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.984375Mb; avail=470870.36328125Mb
2020-10-12 19:51:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070511
2020-10-12 19:51:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.6015625Mb; avail=470869.84765625Mb
2020-10-12 19:51:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053934
2020-10-12 19:51:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126936
2020-10-12 19:51:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.390625Mb; avail=470870.3046875Mb
2020-10-12 19:51:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17803.390625Mb; avail=470870.3046875Mb
2020-10-12 19:51:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001209
2020-10-12 19:51:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.99609375Mb; avail=470869.69921875Mb
2020-10-12 19:51:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069327
2020-10-12 19:51:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.35546875Mb; avail=470869.94140625Mb
2020-10-12 19:51:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054556
2020-10-12 19:51:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125868
2020-10-12 19:51:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.72265625Mb; avail=470869.546875Mb
2020-10-12 19:51:24 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.494 | nll_loss 8.736 | ppl 426.36 | wps 63473.5 | wpb 2442.6 | bsz 92.5 | num_updates 428 | best_loss 9.494
2020-10-12 19:51:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:51:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 4 @ 428 updates, score 9.494) (writing took 8.474056543999723 seconds)
2020-10-12 19:51:32 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 19:51:32 | INFO | train | epoch 004 | loss 10.111 | nll_loss 9.51 | ppl 729.16 | wps 16848.2 | ups 2.61 | wpb 6450.5 | bsz 242.4 | num_updates 428 | lr 2.14893e-05 | gnorm 1.631 | clip 0 | train_wall 29 | wall 154
2020-10-12 19:51:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 19:51:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 19:51:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17752.63671875Mb; avail=470920.34765625Mb
2020-10-12 19:51:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000849
2020-10-12 19:51:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005553
2020-10-12 19:51:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17753.2421875Mb; avail=470919.7421875Mb
2020-10-12 19:51:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-12 19:51:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17753.2421875Mb; avail=470919.7421875Mb
2020-10-12 19:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098113
2020-10-12 19:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104619
2020-10-12 19:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17747.8046875Mb; avail=470925.4140625Mb
2020-10-12 19:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17747.84375Mb; avail=470925.29296875Mb
2020-10-12 19:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003612
2020-10-12 19:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17747.84375Mb; avail=470925.29296875Mb
2020-10-12 19:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 19:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17747.84375Mb; avail=470925.29296875Mb
2020-10-12 19:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095914
2020-10-12 19:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100427
2020-10-12 19:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17747.80078125Mb; avail=470925.75390625Mb
2020-10-12 19:51:33 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 19:51:53 | INFO | train_inner | epoch 005:     72 / 107 loss=9.605, nll_loss=8.905, ppl=479.27, wps=16754.4, ups=2.57, wpb=6519.4, bsz=242, num_updates=500, lr=2.50875e-05, gnorm=1.467, clip=0, train_wall=27, wall=174
2020-10-12 19:52:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:52:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17769.5625Mb; avail=470904.08203125Mb
2020-10-12 19:52:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002301
2020-10-12 19:52:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17770.15234375Mb; avail=470903.4921875Mb
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080004
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17770.10546875Mb; avail=470903.734375Mb
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062270
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.145661
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17769.95703125Mb; avail=470903.85546875Mb
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17769.8671875Mb; avail=470903.9453125Mb
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002274
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17769.8671875Mb; avail=470903.9453125Mb
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078033
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17770.03125Mb; avail=470903.79296875Mb
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061843
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.143200
2020-10-12 19:52:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17769.9921875Mb; avail=470903.640625Mb
2020-10-12 19:52:05 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.126 | nll_loss 8.273 | ppl 309.26 | wps 62823.2 | wpb 2442.6 | bsz 92.5 | num_updates 535 | best_loss 9.126
2020-10-12 19:52:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:52:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 5 @ 535 updates, score 9.126) (writing took 6.084878695000043 seconds)
2020-10-12 19:52:11 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 19:52:11 | INFO | train | epoch 005 | loss 9.44 | nll_loss 8.705 | ppl 417.42 | wps 17921.8 | ups 2.78 | wpb 6450.5 | bsz 242.4 | num_updates 535 | lr 2.68366e-05 | gnorm 1.358 | clip 0 | train_wall 29 | wall 192
2020-10-12 19:52:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 19:52:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18786.21875Mb; avail=469887.87109375Mb
2020-10-12 19:52:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000868
2020-10-12 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005538
2020-10-12 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18792.38671875Mb; avail=469881.703125Mb
2020-10-12 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000205
2020-10-12 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18792.9921875Mb; avail=469881.09765625Mb
2020-10-12 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097317
2020-10-12 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103824
2020-10-12 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18859.609375Mb; avail=469818.35546875Mb
2020-10-12 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18859.625Mb; avail=469821.78515625Mb
2020-10-12 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003667
2020-10-12 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18859.625Mb; avail=469821.78515625Mb
2020-10-12 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18859.625Mb; avail=469821.78515625Mb
2020-10-12 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096222
2020-10-12 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100821
2020-10-12 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18861.8125Mb; avail=469818.0859375Mb
2020-10-12 19:52:11 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 19:52:29 | INFO | train_inner | epoch 006:     65 / 107 loss=9.222, nll_loss=8.431, ppl=345.15, wps=17443.5, ups=2.75, wpb=6343.4, bsz=234, num_updates=600, lr=3.0085e-05, gnorm=1.416, clip=0, train_wall=27, wall=210
2020-10-12 19:52:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18354.65234375Mb; avail=470318.2890625Mb
2020-10-12 19:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001892
2020-10-12 19:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18354.65234375Mb; avail=470318.2890625Mb
2020-10-12 19:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070724
2020-10-12 19:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18354.4921875Mb; avail=470318.0546875Mb
2020-10-12 19:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055255
2020-10-12 19:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128678
2020-10-12 19:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18354.47265625Mb; avail=470318.296875Mb
2020-10-12 19:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18354.50390625Mb; avail=470318.17578125Mb
2020-10-12 19:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001194
2020-10-12 19:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18354.50390625Mb; avail=470318.17578125Mb
2020-10-12 19:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069640
2020-10-12 19:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18354.6015625Mb; avail=470318.17578125Mb
2020-10-12 19:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054992
2020-10-12 19:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126577
2020-10-12 19:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18354.61328125Mb; avail=470318.17578125Mb
2020-10-12 19:52:43 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.913 | nll_loss 8.006 | ppl 257.04 | wps 63425.8 | wpb 2442.6 | bsz 92.5 | num_updates 642 | best_loss 8.913
2020-10-12 19:52:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:52:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 6 @ 642 updates, score 8.913) (writing took 4.841812853000192 seconds)
2020-10-12 19:52:48 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 19:52:48 | INFO | train | epoch 006 | loss 9.161 | nll_loss 8.35 | ppl 326.28 | wps 18704.9 | ups 2.9 | wpb 6450.5 | bsz 242.4 | num_updates 642 | lr 3.2184e-05 | gnorm 1.456 | clip 0 | train_wall 28 | wall 229
2020-10-12 19:52:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 19:52:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 19:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17983.015625Mb; avail=470689.94140625Mb
2020-10-12 19:52:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000839
2020-10-12 19:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005896
2020-10-12 19:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17983.015625Mb; avail=470689.94140625Mb
2020-10-12 19:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000278
2020-10-12 19:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17983.015625Mb; avail=470689.94140625Mb
2020-10-12 19:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100351
2020-10-12 19:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107316
2020-10-12 19:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17977.44140625Mb; avail=470695.63671875Mb
2020-10-12 19:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17977.4296875Mb; avail=470695.7578125Mb
2020-10-12 19:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003600
2020-10-12 19:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17977.53515625Mb; avail=470695.7578125Mb
2020-10-12 19:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 19:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17977.53515625Mb; avail=470695.7578125Mb
2020-10-12 19:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096177
2020-10-12 19:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100729
2020-10-12 19:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17977.515625Mb; avail=470695.63671875Mb
2020-10-12 19:52:48 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 19:53:04 | INFO | train_inner | epoch 007:     58 / 107 loss=9.068, nll_loss=8.231, ppl=300.42, wps=18343.9, ups=2.84, wpb=6470.1, bsz=260.7, num_updates=700, lr=3.50825e-05, gnorm=1.547, clip=0, train_wall=27, wall=246
2020-10-12 19:53:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:53:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18001.06640625Mb; avail=470672.09765625Mb
2020-10-12 19:53:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001912
2020-10-12 19:53:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18001.06640625Mb; avail=470672.09765625Mb
2020-10-12 19:53:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070458
2020-10-12 19:53:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18001.41015625Mb; avail=470671.49609375Mb
2020-10-12 19:53:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054259
2020-10-12 19:53:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127460
2020-10-12 19:53:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18001.41796875Mb; avail=470671.01171875Mb
2020-10-12 19:53:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18001.59765625Mb; avail=470671.25390625Mb
2020-10-12 19:53:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001214
2020-10-12 19:53:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18001.59765625Mb; avail=470671.25390625Mb
2020-10-12 19:53:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069953
2020-10-12 19:53:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18001.81640625Mb; avail=470670.65625Mb
2020-10-12 19:53:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054441
2020-10-12 19:53:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126370
2020-10-12 19:53:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18001.6953125Mb; avail=470671.01171875Mb
2020-10-12 19:53:20 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.779 | nll_loss 7.838 | ppl 228.86 | wps 63146.2 | wpb 2442.6 | bsz 92.5 | num_updates 749 | best_loss 8.779
2020-10-12 19:53:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:53:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 7 @ 749 updates, score 8.779) (writing took 7.994031696000093 seconds)
2020-10-12 19:53:28 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 19:53:28 | INFO | train | epoch 007 | loss 9.002 | nll_loss 8.151 | ppl 284.15 | wps 17079.6 | ups 2.65 | wpb 6450.5 | bsz 242.4 | num_updates 749 | lr 3.75313e-05 | gnorm 1.429 | clip 0 | train_wall 29 | wall 270
2020-10-12 19:53:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 19:53:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 19:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18459.61328125Mb; avail=470213.62109375Mb
2020-10-12 19:53:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000758
2020-10-12 19:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005896
2020-10-12 19:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18459.62890625Mb; avail=470213.5Mb
2020-10-12 19:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 19:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18459.62890625Mb; avail=470213.5Mb
2020-10-12 19:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109880
2020-10-12 19:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.116876
2020-10-12 19:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18453.84765625Mb; avail=470219.41015625Mb
2020-10-12 19:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18453.89453125Mb; avail=470219.36328125Mb
2020-10-12 19:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004098
2020-10-12 19:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18453.89453125Mb; avail=470219.36328125Mb
2020-10-12 19:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000230
2020-10-12 19:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18453.89453125Mb; avail=470219.36328125Mb
2020-10-12 19:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.108636
2020-10-12 19:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113795
2020-10-12 19:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18453.99609375Mb; avail=470219.1796875Mb
2020-10-12 19:53:28 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 19:53:43 | INFO | train_inner | epoch 008:     51 / 107 loss=8.946, nll_loss=8.082, ppl=270.99, wps=17035.3, ups=2.6, wpb=6543.3, bsz=227.7, num_updates=800, lr=4.008e-05, gnorm=1.386, clip=0, train_wall=27, wall=284
2020-10-12 19:53:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17802.76171875Mb; avail=470870.5703125Mb
2020-10-12 19:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001839
2020-10-12 19:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.76171875Mb; avail=470870.5703125Mb
2020-10-12 19:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078847
2020-10-12 19:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.23828125Mb; avail=470870.0859375Mb
2020-10-12 19:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059389
2020-10-12 19:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140912
2020-10-12 19:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.14453125Mb; avail=470870.20703125Mb
2020-10-12 19:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17803.13671875Mb; avail=470870.21484375Mb
2020-10-12 19:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001180
2020-10-12 19:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.13671875Mb; avail=470870.21484375Mb
2020-10-12 19:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079338
2020-10-12 19:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.30078125Mb; avail=470870.45703125Mb
2020-10-12 19:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.058264
2020-10-12 19:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139561
2020-10-12 19:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.30078125Mb; avail=470870.45703125Mb
2020-10-12 19:54:01 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.678 | nll_loss 7.718 | ppl 210.49 | wps 63116.8 | wpb 2442.6 | bsz 92.5 | num_updates 856 | best_loss 8.678
2020-10-12 19:54:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:54:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 8 @ 856 updates, score 8.678) (writing took 8.629918526999973 seconds)
2020-10-12 19:54:09 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 19:54:09 | INFO | train | epoch 008 | loss 8.855 | nll_loss 7.978 | ppl 252.14 | wps 16792.8 | ups 2.6 | wpb 6450.5 | bsz 242.4 | num_updates 856 | lr 4.28786e-05 | gnorm 1.549 | clip 0 | train_wall 29 | wall 311
2020-10-12 19:54:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 19:54:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 19:54:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18737.875Mb; avail=469934.93359375Mb
2020-10-12 19:54:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000929
2020-10-12 19:54:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005628
2020-10-12 19:54:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18745.22265625Mb; avail=469928.39453125Mb
2020-10-12 19:54:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000199
2020-10-12 19:54:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18745.828125Mb; avail=469927.7890625Mb
2020-10-12 19:54:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098742
2020-10-12 19:54:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105405
2020-10-12 19:54:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18855.25Mb; avail=469817.45703125Mb
2020-10-12 19:54:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18890.90625Mb; avail=469782.58203125Mb
2020-10-12 19:54:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003709
2020-10-12 19:54:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18895.14453125Mb; avail=469777.73828125Mb
2020-10-12 19:54:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 19:54:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18896.35546875Mb; avail=469777.1328125Mb
2020-10-12 19:54:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097772
2020-10-12 19:54:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102481
2020-10-12 19:54:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18972.4609375Mb; avail=469704.02734375Mb
2020-10-12 19:54:10 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 19:54:22 | INFO | train_inner | epoch 009:     44 / 107 loss=8.757, nll_loss=7.867, ppl=233.4, wps=16234.8, ups=2.56, wpb=6342.2, bsz=244.2, num_updates=900, lr=4.50775e-05, gnorm=1.475, clip=0, train_wall=27, wall=323
2020-10-12 19:54:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:54:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18484.82421875Mb; avail=470195.64453125Mb
2020-10-12 19:54:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001790
2020-10-12 19:54:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18485.04296875Mb; avail=470195.19140625Mb
2020-10-12 19:54:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070181
2020-10-12 19:54:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18484.6875Mb; avail=470194.80078125Mb
2020-10-12 19:54:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054247
2020-10-12 19:54:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127027
2020-10-12 19:54:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18485.0546875Mb; avail=470192.71875Mb
2020-10-12 19:54:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18485.7421875Mb; avail=470192.234375Mb
2020-10-12 19:54:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001198
2020-10-12 19:54:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18486.08984375Mb; avail=470191.89453125Mb
2020-10-12 19:54:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069854
2020-10-12 19:54:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18485.8828125Mb; avail=470190.4140625Mb
2020-10-12 19:54:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054202
2020-10-12 19:54:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126048
2020-10-12 19:54:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18486.66796875Mb; avail=470188.93359375Mb
2020-10-12 19:54:42 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.438 | nll_loss 7.458 | ppl 175.78 | wps 59513 | wpb 2442.6 | bsz 92.5 | num_updates 963 | best_loss 8.438
2020-10-12 19:54:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:54:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 9 @ 963 updates, score 8.438) (writing took 6.646571345999291 seconds)
2020-10-12 19:54:48 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 19:54:48 | INFO | train | epoch 009 | loss 8.689 | nll_loss 7.788 | ppl 221.03 | wps 17633.3 | ups 2.73 | wpb 6450.5 | bsz 242.4 | num_updates 963 | lr 4.82259e-05 | gnorm 1.43 | clip 0 | train_wall 29 | wall 350
2020-10-12 19:54:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 19:54:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17815.3671875Mb; avail=470857.07421875Mb
2020-10-12 19:54:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000941
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006066
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17815.51171875Mb; avail=470857.31640625Mb
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000262
2020-10-12 19:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17815.51171875Mb; avail=470857.31640625Mb
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099794
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107040
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.59375Mb; avail=470863.22265625Mb
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17809.58203125Mb; avail=470863.22265625Mb
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003690
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.58203125Mb; avail=470863.22265625Mb
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.58203125Mb; avail=470863.22265625Mb
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096623
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101242
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.30078125Mb; avail=470863.3671875Mb
2020-10-12 19:54:49 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 19:54:59 | INFO | train_inner | epoch 010:     37 / 107 loss=8.626, nll_loss=7.717, ppl=210.38, wps=17133.8, ups=2.67, wpb=6419.4, bsz=243.2, num_updates=1000, lr=5.0075e-05, gnorm=1.448, clip=0, train_wall=27, wall=361
2020-10-12 19:55:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:55:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17838.83203125Mb; avail=470841.93359375Mb
2020-10-12 19:55:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001810
2020-10-12 19:55:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.1796875Mb; avail=470841.59375Mb
2020-10-12 19:55:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069469
2020-10-12 19:55:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.42578125Mb; avail=470840.8984375Mb
2020-10-12 19:55:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.058205
2020-10-12 19:55:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130303
2020-10-12 19:55:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.2109375Mb; avail=470840.9921875Mb
2020-10-12 19:55:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17839.2578125Mb; avail=470840.1875Mb
2020-10-12 19:55:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001172
2020-10-12 19:55:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.0Mb; avail=470840.56640625Mb
2020-10-12 19:55:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069965
2020-10-12 19:55:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.66015625Mb; avail=470839.765625Mb
2020-10-12 19:55:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054993
2020-10-12 19:55:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126885
2020-10-12 19:55:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.640625Mb; avail=470838.8125Mb
2020-10-12 19:55:21 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.339 | nll_loss 7.315 | ppl 159.24 | wps 59504.5 | wpb 2442.6 | bsz 92.5 | num_updates 1070 | best_loss 8.339
2020-10-12 19:55:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:55:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 10 @ 1070 updates, score 8.339) (writing took 4.304843129000801 seconds)
2020-10-12 19:55:26 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 19:55:26 | INFO | train | epoch 010 | loss 8.53 | nll_loss 7.606 | ppl 194.77 | wps 18578.6 | ups 2.88 | wpb 6450.5 | bsz 242.4 | num_updates 1070 | lr 5.35733e-05 | gnorm 1.507 | clip 0 | train_wall 29 | wall 387
2020-10-12 19:55:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 19:55:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 19:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17823.58203125Mb; avail=470849.71484375Mb
2020-10-12 19:55:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000893
2020-10-12 19:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006070
2020-10-12 19:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.58203125Mb; avail=470849.71484375Mb
2020-10-12 19:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000204
2020-10-12 19:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.58203125Mb; avail=470849.71484375Mb
2020-10-12 19:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099272
2020-10-12 19:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106467
2020-10-12 19:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.0859375Mb; avail=470855.37890625Mb
2020-10-12 19:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17818.0859375Mb; avail=470855.37890625Mb
2020-10-12 19:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003681
2020-10-12 19:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.0859375Mb; avail=470855.37890625Mb
2020-10-12 19:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 19:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.0859375Mb; avail=470855.37890625Mb
2020-10-12 19:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096377
2020-10-12 19:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100976
2020-10-12 19:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.1484375Mb; avail=470854.89453125Mb
2020-10-12 19:55:26 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 19:55:34 | INFO | train_inner | epoch 011:     30 / 107 loss=8.484, nll_loss=7.553, ppl=187.76, wps=18465.2, ups=2.87, wpb=6441.5, bsz=227.7, num_updates=1100, lr=5.50725e-05, gnorm=1.513, clip=0, train_wall=27, wall=395
2020-10-12 19:55:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17869.359375Mb; avail=470804.18359375Mb
2020-10-12 19:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001721
2020-10-12 19:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17869.359375Mb; avail=470804.18359375Mb
2020-10-12 19:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070287
2020-10-12 19:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17869.359375Mb; avail=470804.18359375Mb
2020-10-12 19:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055096
2020-10-12 19:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127941
2020-10-12 19:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17869.67578125Mb; avail=470803.09375Mb
2020-10-12 19:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17869.93359375Mb; avail=470803.09375Mb
2020-10-12 19:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002020
2020-10-12 19:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17870.5390625Mb; avail=470802.48828125Mb
2020-10-12 19:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069960
2020-10-12 19:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17870.52734375Mb; avail=470802.7890625Mb
2020-10-12 19:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054477
2020-10-12 19:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127272
2020-10-12 19:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17870.625Mb; avail=470802.484375Mb
2020-10-12 19:55:58 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.186 | nll_loss 7.159 | ppl 142.87 | wps 60927.7 | wpb 2442.6 | bsz 92.5 | num_updates 1177 | best_loss 8.186
2020-10-12 19:55:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:56:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 11 @ 1177 updates, score 8.186) (writing took 4.23167548299989 seconds)
2020-10-12 19:56:02 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 19:56:02 | INFO | train | epoch 011 | loss 8.404 | nll_loss 7.462 | ppl 176.33 | wps 18784.8 | ups 2.91 | wpb 6450.5 | bsz 242.4 | num_updates 1177 | lr 5.89206e-05 | gnorm 1.541 | clip 0 | train_wall 29 | wall 424
2020-10-12 19:56:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 19:56:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 19:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17875.73828125Mb; avail=470796.921875Mb
2020-10-12 19:56:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000679
2020-10-12 19:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005899
2020-10-12 19:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17875.73828125Mb; avail=470796.921875Mb
2020-10-12 19:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000198
2020-10-12 19:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17875.73828125Mb; avail=470796.921875Mb
2020-10-12 19:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099636
2020-10-12 19:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106612
2020-10-12 19:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17869.734375Mb; avail=470803.1328125Mb
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17869.734375Mb; avail=470803.1328125Mb
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003680
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17869.734375Mb; avail=470803.1328125Mb
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17869.734375Mb; avail=470803.1328125Mb
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096595
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101258
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17869.75390625Mb; avail=470803.015625Mb
2020-10-12 19:56:03 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 19:56:09 | INFO | train_inner | epoch 012:     23 / 107 loss=8.368, nll_loss=7.421, ppl=171.42, wps=18581.7, ups=2.85, wpb=6512.6, bsz=262, num_updates=1200, lr=6.007e-05, gnorm=1.475, clip=0, train_wall=27, wall=431
2020-10-12 19:56:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18429.19921875Mb; avail=470243.8671875Mb
2020-10-12 19:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001686
2020-10-12 19:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18429.19921875Mb; avail=470243.8671875Mb
2020-10-12 19:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071020
2020-10-12 19:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18429.4296875Mb; avail=470243.625Mb
2020-10-12 19:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055881
2020-10-12 19:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129409
2020-10-12 19:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18429.4296875Mb; avail=470243.625Mb
2020-10-12 19:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18429.4296875Mb; avail=470243.625Mb
2020-10-12 19:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001220
2020-10-12 19:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18429.4296875Mb; avail=470243.625Mb
2020-10-12 19:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070953
2020-10-12 19:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18429.48828125Mb; avail=470243.140625Mb
2020-10-12 19:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055259
2020-10-12 19:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128228
2020-10-12 19:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18429.48046875Mb; avail=470243.26171875Mb
2020-10-12 19:56:35 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.05 | nll_loss 7.004 | ppl 128.37 | wps 62885.5 | wpb 2442.6 | bsz 92.5 | num_updates 1284 | best_loss 8.05
2020-10-12 19:56:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:56:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 12 @ 1284 updates, score 8.05) (writing took 4.831624946999909 seconds)
2020-10-12 19:56:40 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 19:56:40 | INFO | train | epoch 012 | loss 8.264 | nll_loss 7.303 | ppl 157.88 | wps 18537.8 | ups 2.87 | wpb 6450.5 | bsz 242.4 | num_updates 1284 | lr 6.42679e-05 | gnorm 1.421 | clip 0 | train_wall 29 | wall 461
2020-10-12 19:56:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 19:56:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 19:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18418.359375Mb; avail=470261.38671875Mb
2020-10-12 19:56:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000906
2020-10-12 19:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006249
2020-10-12 19:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18418.265625Mb; avail=470262.12890625Mb
2020-10-12 19:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000226
2020-10-12 19:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18416.16796875Mb; avail=470265.83984375Mb
2020-10-12 19:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098844
2020-10-12 19:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106180
2020-10-12 19:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18411.78125Mb; avail=470266.43359375Mb
2020-10-12 19:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18412.10546875Mb; avail=470265.83203125Mb
2020-10-12 19:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003672
2020-10-12 19:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18411.921875Mb; avail=470266.15625Mb
2020-10-12 19:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 19:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18411.79296875Mb; avail=470266.53515625Mb
2020-10-12 19:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097456
2020-10-12 19:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102052
2020-10-12 19:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18411.77734375Mb; avail=470264.83203125Mb
2020-10-12 19:56:40 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 19:56:44 | INFO | train_inner | epoch 013:     16 / 107 loss=8.252, nll_loss=7.288, ppl=156.31, wps=18512.3, ups=2.85, wpb=6505, bsz=239.8, num_updates=1300, lr=6.50675e-05, gnorm=1.49, clip=0, train_wall=27, wall=466
2020-10-12 19:57:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17834.42578125Mb; avail=470838.63671875Mb
2020-10-12 19:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001977
2020-10-12 19:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.03125Mb; avail=470838.03125Mb
2020-10-12 19:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071526
2020-10-12 19:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.3984375Mb; avail=470837.42578125Mb
2020-10-12 19:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055494
2020-10-12 19:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129856
2020-10-12 19:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.54296875Mb; avail=470837.3046875Mb
2020-10-12 19:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17835.54296875Mb; avail=470837.796875Mb
2020-10-12 19:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001198
2020-10-12 19:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.54296875Mb; avail=470837.796875Mb
2020-10-12 19:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070609
2020-10-12 19:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.69921875Mb; avail=470837.43359375Mb
2020-10-12 19:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055311
2020-10-12 19:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127922
2020-10-12 19:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.71484375Mb; avail=470837.765625Mb
2020-10-12 19:57:12 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.948 | nll_loss 6.894 | ppl 118.93 | wps 61876.9 | wpb 2442.6 | bsz 92.5 | num_updates 1391 | best_loss 7.948
2020-10-12 19:57:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:57:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 13 @ 1391 updates, score 7.948) (writing took 10.334943791999649 seconds)
2020-10-12 19:57:22 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 19:57:22 | INFO | train | epoch 013 | loss 8.152 | nll_loss 7.174 | ppl 144.42 | wps 16146.3 | ups 2.5 | wpb 6450.5 | bsz 242.4 | num_updates 1391 | lr 6.96152e-05 | gnorm 1.468 | clip 0 | train_wall 29 | wall 504
2020-10-12 19:57:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 19:57:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 19:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17804.140625Mb; avail=470872.453125Mb
2020-10-12 19:57:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000931
2020-10-12 19:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006386
2020-10-12 19:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17804.73828125Mb; avail=470872.2734375Mb
2020-10-12 19:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000209
2020-10-12 19:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.51171875Mb; avail=470872.40625Mb
2020-10-12 19:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100822
2020-10-12 19:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108362
2020-10-12 19:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17797.7890625Mb; avail=470877.6328125Mb
2020-10-12 19:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17797.89453125Mb; avail=470876.83203125Mb
2020-10-12 19:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003673
2020-10-12 19:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17797.36328125Mb; avail=470877.49609375Mb
2020-10-12 19:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000218
2020-10-12 19:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17797.234375Mb; avail=470877.3828125Mb
2020-10-12 19:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096889
2020-10-12 19:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101528
2020-10-12 19:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17796.5703125Mb; avail=470877.1875Mb
2020-10-12 19:57:23 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 19:57:25 | INFO | train_inner | epoch 014:      9 / 107 loss=8.132, nll_loss=7.152, ppl=142.21, wps=15622.3, ups=2.45, wpb=6373.3, bsz=240.9, num_updates=1400, lr=7.0065e-05, gnorm=1.435, clip=0, train_wall=27, wall=506
2020-10-12 19:57:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:57:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17862.7109375Mb; avail=470809.9375Mb
2020-10-12 19:57:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001946
2020-10-12 19:57:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.7109375Mb; avail=470809.9375Mb
2020-10-12 19:57:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072969
2020-10-12 19:57:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.94140625Mb; avail=470809.81640625Mb
2020-10-12 19:57:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056897
2020-10-12 19:57:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.132652
2020-10-12 19:57:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.96484375Mb; avail=470809.6953125Mb
2020-10-12 19:57:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17862.9296875Mb; avail=470810.30859375Mb
2020-10-12 19:57:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001777
2020-10-12 19:57:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.9296875Mb; avail=470810.30859375Mb
2020-10-12 19:57:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.091149
2020-10-12 19:57:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.8828125Mb; avail=470810.1875Mb
2020-10-12 19:57:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056652
2020-10-12 19:57:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.150588
2020-10-12 19:57:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.80078125Mb; avail=470810.1875Mb
2020-10-12 19:57:55 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.842 | nll_loss 6.761 | ppl 108.49 | wps 63202.6 | wpb 2442.6 | bsz 92.5 | num_updates 1498 | best_loss 7.842
2020-10-12 19:57:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:58:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 14 @ 1498 updates, score 7.842) (writing took 9.14718628799983 seconds)
2020-10-12 19:58:04 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 19:58:04 | INFO | train | epoch 014 | loss 8.049 | nll_loss 7.055 | ppl 132.98 | wps 16603.2 | ups 2.57 | wpb 6450.5 | bsz 242.4 | num_updates 1498 | lr 7.49626e-05 | gnorm 1.468 | clip 0 | train_wall 29 | wall 545
2020-10-12 19:58:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 19:58:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 19:58:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17780.5234375Mb; avail=470894.625Mb
2020-10-12 19:58:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000863
2020-10-12 19:58:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006428
2020-10-12 19:58:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17780.61328125Mb; avail=470894.07421875Mb
2020-10-12 19:58:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-12 19:58:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17780.484375Mb; avail=470893.9609375Mb
2020-10-12 19:58:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.111600
2020-10-12 19:58:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.119126
2020-10-12 19:58:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17774.48828125Mb; avail=470900.05078125Mb
2020-10-12 19:58:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17774.25390625Mb; avail=470899.38671875Mb
2020-10-12 19:58:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004231
2020-10-12 19:58:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17774.25390625Mb; avail=470899.38671875Mb
2020-10-12 19:58:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 19:58:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17774.25390625Mb; avail=470899.38671875Mb
2020-10-12 19:58:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.110914
2020-10-12 19:58:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.116271
2020-10-12 19:58:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17774.20703125Mb; avail=470899.85546875Mb
2020-10-12 19:58:04 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 19:58:05 | INFO | train_inner | epoch 015:      2 / 107 loss=8.059, nll_loss=7.066, ppl=134.03, wps=16293, ups=2.52, wpb=6475.3, bsz=241.5, num_updates=1500, lr=7.50625e-05, gnorm=1.479, clip=0, train_wall=27, wall=546
2020-10-12 19:58:33 | INFO | train_inner | epoch 015:    102 / 107 loss=7.951, nll_loss=6.943, ppl=123.01, wps=22911.3, ups=3.56, wpb=6442.3, bsz=240.9, num_updates=1600, lr=8.006e-05, gnorm=1.4, clip=0, train_wall=27, wall=574
2020-10-12 19:58:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:58:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18485.0234375Mb; avail=470188.375Mb
2020-10-12 19:58:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002141
2020-10-12 19:58:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18485.62890625Mb; avail=470187.76953125Mb
2020-10-12 19:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080941
2020-10-12 19:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18485.6328125Mb; avail=470187.6640625Mb
2020-10-12 19:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062509
2020-10-12 19:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.146475
2020-10-12 19:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18485.80078125Mb; avail=470187.54296875Mb
2020-10-12 19:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18485.80078125Mb; avail=470187.54296875Mb
2020-10-12 19:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001381
2020-10-12 19:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18485.80078125Mb; avail=470187.54296875Mb
2020-10-12 19:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081705
2020-10-12 19:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18485.796875Mb; avail=470187.1796875Mb
2020-10-12 19:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062801
2020-10-12 19:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.146721
2020-10-12 19:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18485.83203125Mb; avail=470187.54296875Mb
2020-10-12 19:58:37 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.754 | nll_loss 6.661 | ppl 101.17 | wps 61998.4 | wpb 2442.6 | bsz 92.5 | num_updates 1605 | best_loss 7.754
2020-10-12 19:58:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:58:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 15 @ 1605 updates, score 7.754) (writing took 8.137999882999793 seconds)
2020-10-12 19:58:45 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 19:58:45 | INFO | train | epoch 015 | loss 7.946 | nll_loss 6.937 | ppl 122.55 | wps 16782 | ups 2.6 | wpb 6450.5 | bsz 242.4 | num_updates 1605 | lr 8.03099e-05 | gnorm 1.387 | clip 0 | train_wall 29 | wall 586
2020-10-12 19:58:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 19:58:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 19:58:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18716.3515625Mb; avail=469957.28125Mb
2020-10-12 19:58:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000747
2020-10-12 19:58:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006275
2020-10-12 19:58:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18716.3515625Mb; avail=469957.28125Mb
2020-10-12 19:58:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000190
2020-10-12 19:58:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18716.3515625Mb; avail=469957.28125Mb
2020-10-12 19:58:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.111402
2020-10-12 19:58:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.118757
2020-10-12 19:58:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18710.11328125Mb; avail=469963.5234375Mb
2020-10-12 19:58:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18710.24609375Mb; avail=469963.28125Mb
2020-10-12 19:58:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004274
2020-10-12 19:58:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18710.26953125Mb; avail=469963.16015625Mb
2020-10-12 19:58:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-12 19:58:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18710.26953125Mb; avail=469963.16015625Mb
2020-10-12 19:58:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109315
2020-10-12 19:58:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114761
2020-10-12 19:58:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18709.265625Mb; avail=469963.984375Mb
2020-10-12 19:58:45 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 19:59:12 | INFO | train_inner | epoch 016:     95 / 107 loss=7.829, nll_loss=6.802, ppl=111.62, wps=16641.9, ups=2.55, wpb=6538.1, bsz=249.3, num_updates=1700, lr=8.50575e-05, gnorm=1.311, clip=0, train_wall=27, wall=614
2020-10-12 19:59:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:59:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17882.015625Mb; avail=470790.63671875Mb
2020-10-12 19:59:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001901
2020-10-12 19:59:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17882.015625Mb; avail=470790.63671875Mb
2020-10-12 19:59:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071056
2020-10-12 19:59:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17882.62109375Mb; avail=470790.39453125Mb
2020-10-12 19:59:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055704
2020-10-12 19:59:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129465
2020-10-12 19:59:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17882.65625Mb; avail=470790.15234375Mb
2020-10-12 19:59:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17882.671875Mb; avail=470790.03125Mb
2020-10-12 19:59:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001095
2020-10-12 19:59:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17882.671875Mb; avail=470790.03125Mb
2020-10-12 19:59:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070069
2020-10-12 19:59:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17882.55078125Mb; avail=470790.15234375Mb
2020-10-12 19:59:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054386
2020-10-12 19:59:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126301
2020-10-12 19:59:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17882.640625Mb; avail=470789.66796875Mb
2020-10-12 19:59:18 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.672 | nll_loss 6.574 | ppl 95.26 | wps 63680.5 | wpb 2442.6 | bsz 92.5 | num_updates 1712 | best_loss 7.672
2020-10-12 19:59:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:59:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 16 @ 1712 updates, score 7.672) (writing took 9.118033289999403 seconds)
2020-10-12 19:59:27 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 19:59:27 | INFO | train | epoch 016 | loss 7.835 | nll_loss 6.809 | ppl 112.16 | wps 16438.4 | ups 2.55 | wpb 6450.5 | bsz 242.4 | num_updates 1712 | lr 8.56572e-05 | gnorm 1.309 | clip 0 | train_wall 29 | wall 628
2020-10-12 19:59:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 19:59:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 19:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18945.44140625Mb; avail=469727.34765625Mb
2020-10-12 19:59:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000895
2020-10-12 19:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006558
2020-10-12 19:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18954.5234375Mb; avail=469718.265625Mb
2020-10-12 19:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000227
2020-10-12 19:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18955.734375Mb; avail=469717.66015625Mb
2020-10-12 19:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.110792
2020-10-12 19:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.118442
2020-10-12 19:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19090.30859375Mb; avail=469583.08984375Mb
2020-10-12 19:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19142.44140625Mb; avail=469530.78515625Mb
2020-10-12 19:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004304
2020-10-12 19:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19149.1015625Mb; avail=469524.73046875Mb
2020-10-12 19:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 19:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19149.70703125Mb; avail=469524.125Mb
2020-10-12 19:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.110176
2020-10-12 19:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115583
2020-10-12 19:59:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19282.93359375Mb; avail=469390.80078125Mb
2020-10-12 19:59:27 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 19:59:52 | INFO | train_inner | epoch 017:     88 / 107 loss=7.76, nll_loss=6.723, ppl=105.64, wps=15861.4, ups=2.51, wpb=6314.8, bsz=223.2, num_updates=1800, lr=9.0055e-05, gnorm=1.331, clip=0, train_wall=27, wall=653
2020-10-12 19:59:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:59:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17809.79296875Mb; avail=470863.28515625Mb
2020-10-12 19:59:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001942
2020-10-12 19:59:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17810.3984375Mb; avail=470862.6796875Mb
2020-10-12 19:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070947
2020-10-12 19:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17810.3984375Mb; avail=470862.6796875Mb
2020-10-12 19:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056267
2020-10-12 19:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129967
2020-10-12 19:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17810.58984375Mb; avail=470862.6796875Mb
2020-10-12 19:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17810.46875Mb; avail=470862.921875Mb
2020-10-12 19:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001058
2020-10-12 19:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17810.46875Mb; avail=470862.921875Mb
2020-10-12 19:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070583
2020-10-12 19:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17810.234375Mb; avail=470863.04296875Mb
2020-10-12 19:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055891
2020-10-12 19:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128323
2020-10-12 19:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17810.52734375Mb; avail=470862.921875Mb
2020-10-12 20:00:00 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.613 | nll_loss 6.502 | ppl 90.64 | wps 63617.5 | wpb 2442.6 | bsz 92.5 | num_updates 1819 | best_loss 7.613
2020-10-12 20:00:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:00:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 17 @ 1819 updates, score 7.613) (writing took 4.663588837000134 seconds)
2020-10-12 20:00:04 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 20:00:04 | INFO | train | epoch 017 | loss 7.747 | nll_loss 6.708 | ppl 104.57 | wps 18438.7 | ups 2.86 | wpb 6450.5 | bsz 242.4 | num_updates 1819 | lr 9.10045e-05 | gnorm 1.366 | clip 0 | train_wall 29 | wall 666
2020-10-12 20:00:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 20:00:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 20:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18350.671875Mb; avail=470322.66015625Mb
2020-10-12 20:00:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000699
2020-10-12 20:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005237
2020-10-12 20:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18350.671875Mb; avail=470322.66015625Mb
2020-10-12 20:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18350.671875Mb; avail=470322.66015625Mb
2020-10-12 20:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097350
2020-10-12 20:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103539
2020-10-12 20:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18344.81640625Mb; avail=470328.8671875Mb
2020-10-12 20:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18344.81640625Mb; avail=470328.8671875Mb
2020-10-12 20:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003728
2020-10-12 20:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18344.81640625Mb; avail=470328.8671875Mb
2020-10-12 20:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-12 20:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18344.81640625Mb; avail=470328.8671875Mb
2020-10-12 20:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097354
2020-10-12 20:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102045
2020-10-12 20:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18344.796875Mb; avail=470328.8671875Mb
2020-10-12 20:00:05 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 20:00:28 | INFO | train_inner | epoch 018:     81 / 107 loss=7.687, nll_loss=6.639, ppl=99.65, wps=18432.3, ups=2.82, wpb=6529.8, bsz=251.8, num_updates=1900, lr=9.50525e-05, gnorm=1.337, clip=0, train_wall=27, wall=689
2020-10-12 20:00:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17853.1015625Mb; avail=470819.69921875Mb
2020-10-12 20:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001761
2020-10-12 20:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.1015625Mb; avail=470819.69921875Mb
2020-10-12 20:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071595
2020-10-12 20:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.19140625Mb; avail=470819.45703125Mb
2020-10-12 20:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055557
2020-10-12 20:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129715
2020-10-12 20:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.02734375Mb; avail=470819.45703125Mb
2020-10-12 20:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17853.015625Mb; avail=470819.578125Mb
2020-10-12 20:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001229
2020-10-12 20:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.015625Mb; avail=470819.578125Mb
2020-10-12 20:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071121
2020-10-12 20:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.83984375Mb; avail=470819.94140625Mb
2020-10-12 20:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055462
2020-10-12 20:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128563
2020-10-12 20:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.83984375Mb; avail=470819.94140625Mb
2020-10-12 20:00:37 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.52 | nll_loss 6.409 | ppl 84.98 | wps 63604.1 | wpb 2442.6 | bsz 92.5 | num_updates 1926 | best_loss 7.52
2020-10-12 20:00:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:00:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 18 @ 1926 updates, score 7.52) (writing took 10.136885919000633 seconds)
2020-10-12 20:00:47 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 20:00:47 | INFO | train | epoch 018 | loss 7.66 | nll_loss 6.608 | ppl 97.57 | wps 16089.6 | ups 2.49 | wpb 6450.5 | bsz 242.4 | num_updates 1926 | lr 9.63519e-05 | gnorm 1.322 | clip 0 | train_wall 29 | wall 709
2020-10-12 20:00:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 20:00:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 20:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18433.390625Mb; avail=470247.39453125Mb
2020-10-12 20:00:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000825
2020-10-12 20:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006105
2020-10-12 20:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18432.96484375Mb; avail=470247.359375Mb
2020-10-12 20:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000263
2020-10-12 20:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18433.5703125Mb; avail=470247.24609375Mb
2020-10-12 20:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109358
2020-10-12 20:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.116657
2020-10-12 20:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18427.1875Mb; avail=470252.9140625Mb
2020-10-12 20:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18427.0078125Mb; avail=470252.5546875Mb
2020-10-12 20:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005398
2020-10-12 20:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18427.31640625Mb; avail=470252.02734375Mb
2020-10-12 20:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000284
2020-10-12 20:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18427.921875Mb; avail=470251.9140625Mb
2020-10-12 20:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109329
2020-10-12 20:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.116048
2020-10-12 20:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18427.9375Mb; avail=470250.625Mb
2020-10-12 20:00:48 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 20:01:08 | INFO | train_inner | epoch 019:     74 / 107 loss=7.577, nll_loss=6.513, ppl=91.36, wps=15751, ups=2.44, wpb=6455.3, bsz=248.2, num_updates=2000, lr=0.00010005, gnorm=1.345, clip=0, train_wall=27, wall=730
2020-10-12 20:01:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:01:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17863.38671875Mb; avail=470809.29296875Mb
2020-10-12 20:01:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001764
2020-10-12 20:01:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.38671875Mb; avail=470809.29296875Mb
2020-10-12 20:01:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072603
2020-10-12 20:01:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.14453125Mb; avail=470809.77734375Mb
2020-10-12 20:01:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056297
2020-10-12 20:01:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.131847
2020-10-12 20:01:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.16796875Mb; avail=470809.53515625Mb
2020-10-12 20:01:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17863.7734375Mb; avail=470808.9296875Mb
2020-10-12 20:01:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001424
2020-10-12 20:01:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.7734375Mb; avail=470808.9296875Mb
2020-10-12 20:01:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070442
2020-10-12 20:01:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.83203125Mb; avail=470808.6875Mb
2020-10-12 20:01:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056522
2020-10-12 20:01:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129223
2020-10-12 20:01:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17864.1328125Mb; avail=470808.4453125Mb
2020-10-12 20:01:20 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.458 | nll_loss 6.331 | ppl 80.49 | wps 63166.4 | wpb 2442.6 | bsz 92.5 | num_updates 2033 | best_loss 7.458
2020-10-12 20:01:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:01:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 19 @ 2033 updates, score 7.458) (writing took 4.245893897999849 seconds)
2020-10-12 20:01:24 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 20:01:24 | INFO | train | epoch 019 | loss 7.579 | nll_loss 6.516 | ppl 91.51 | wps 18694.4 | ups 2.9 | wpb 6450.5 | bsz 242.4 | num_updates 2033 | lr 0.000101699 | gnorm 1.332 | clip 0 | train_wall 29 | wall 746
2020-10-12 20:01:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 20:01:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17847.80859375Mb; avail=470825.875Mb
2020-10-12 20:01:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000869
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006257
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.31640625Mb; avail=470825.875Mb
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000242
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.31640625Mb; avail=470825.875Mb
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099510
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106846
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.4296875Mb; avail=470831.66015625Mb
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17841.6015625Mb; avail=470832.046875Mb
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003812
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.6015625Mb; avail=470832.046875Mb
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.6015625Mb; avail=470832.046875Mb
2020-10-12 20:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097981
2020-10-12 20:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102712
2020-10-12 20:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.6640625Mb; avail=470831.19921875Mb
2020-10-12 20:01:25 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 20:01:43 | INFO | train_inner | epoch 020:     67 / 107 loss=7.521, nll_loss=6.448, ppl=87.32, wps=18703.1, ups=2.87, wpb=6512.3, bsz=245.7, num_updates=2100, lr=0.000105048, gnorm=1.329, clip=0, train_wall=27, wall=765
2020-10-12 20:01:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:01:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17856.43359375Mb; avail=470815.80078125Mb
2020-10-12 20:01:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002030
2020-10-12 20:01:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17856.43359375Mb; avail=470815.80078125Mb
2020-10-12 20:01:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075272
2020-10-12 20:01:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.16796875Mb; avail=470814.94921875Mb
2020-10-12 20:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064340
2020-10-12 20:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142465
2020-10-12 20:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.1640625Mb; avail=470814.94921875Mb
2020-10-12 20:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17857.140625Mb; avail=470815.0703125Mb
2020-10-12 20:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001229
2020-10-12 20:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.05078125Mb; avail=470815.0703125Mb
2020-10-12 20:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.086118
2020-10-12 20:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.234375Mb; avail=470814.828125Mb
2020-10-12 20:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062094
2020-10-12 20:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.150233
2020-10-12 20:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.23046875Mb; avail=470815.0703125Mb
2020-10-12 20:01:57 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.402 | nll_loss 6.255 | ppl 76.39 | wps 63281.1 | wpb 2442.6 | bsz 92.5 | num_updates 2140 | best_loss 7.402
2020-10-12 20:01:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:02:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 20 @ 2140 updates, score 7.402) (writing took 15.704848704000142 seconds)
2020-10-12 20:02:12 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 20:02:12 | INFO | train | epoch 020 | loss 7.489 | nll_loss 6.412 | ppl 85.14 | wps 14318.8 | ups 2.22 | wpb 6450.5 | bsz 242.4 | num_updates 2140 | lr 0.000107047 | gnorm 1.313 | clip 0 | train_wall 29 | wall 794
2020-10-12 20:02:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 20:02:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 20:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17807.02734375Mb; avail=470866.00390625Mb
2020-10-12 20:02:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001093
2020-10-12 20:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006602
2020-10-12 20:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17807.02734375Mb; avail=470866.00390625Mb
2020-10-12 20:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000212
2020-10-12 20:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17807.02734375Mb; avail=470866.00390625Mb
2020-10-12 20:02:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.111191
2020-10-12 20:02:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.118930
2020-10-12 20:02:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.46875Mb; avail=470873.13671875Mb
2020-10-12 20:02:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17800.41015625Mb; avail=470873.0Mb
2020-10-12 20:02:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004275
2020-10-12 20:02:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.41015625Mb; avail=470872.984375Mb
2020-10-12 20:02:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000188
2020-10-12 20:02:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.42578125Mb; avail=470872.984375Mb
2020-10-12 20:02:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.110093
2020-10-12 20:02:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115452
2020-10-12 20:02:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.58203125Mb; avail=470872.6640625Mb
2020-10-12 20:02:13 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 20:02:30 | INFO | train_inner | epoch 021:     60 / 107 loss=7.506, nll_loss=6.431, ppl=86.3, wps=13961, ups=2.15, wpb=6478.7, bsz=234.2, num_updates=2200, lr=0.000110045, gnorm=1.267, clip=0, train_wall=27, wall=811
2020-10-12 20:02:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:02:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17836.93359375Mb; avail=470835.3671875Mb
2020-10-12 20:02:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002018
2020-10-12 20:02:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.5390625Mb; avail=470834.76171875Mb
2020-10-12 20:02:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069620
2020-10-12 20:02:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.203125Mb; avail=470835.49609375Mb
2020-10-12 20:02:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055963
2020-10-12 20:02:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128499
2020-10-12 20:02:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.2265625Mb; avail=470835.375Mb
2020-10-12 20:02:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17837.2265625Mb; avail=470835.375Mb
2020-10-12 20:02:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001260
2020-10-12 20:02:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.2265625Mb; avail=470835.375Mb
2020-10-12 20:02:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069396
2020-10-12 20:02:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.234375Mb; avail=470835.25390625Mb
2020-10-12 20:02:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054488
2020-10-12 20:02:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125903
2020-10-12 20:02:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.26171875Mb; avail=470835.1328125Mb
2020-10-12 20:02:45 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.378 | nll_loss 6.223 | ppl 74.72 | wps 63115.2 | wpb 2442.6 | bsz 92.5 | num_updates 2247 | best_loss 7.378
2020-10-12 20:02:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:02:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 21 @ 2247 updates, score 7.378) (writing took 6.501175618999696 seconds)
2020-10-12 20:02:51 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 20:02:51 | INFO | train | epoch 021 | loss 7.411 | nll_loss 6.322 | ppl 80.02 | wps 17709 | ups 2.75 | wpb 6450.5 | bsz 242.4 | num_updates 2247 | lr 0.000112394 | gnorm 1.265 | clip 0 | train_wall 29 | wall 833
2020-10-12 20:02:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 20:02:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 20:02:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17802.45703125Mb; avail=470871.3125Mb
2020-10-12 20:02:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000839
2020-10-12 20:02:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006119
2020-10-12 20:02:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.73046875Mb; avail=470871.3828125Mb
2020-10-12 20:02:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000206
2020-10-12 20:02:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.6015625Mb; avail=470871.26953125Mb
2020-10-12 20:02:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.110443
2020-10-12 20:02:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.117673
2020-10-12 20:02:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17796.765625Mb; avail=470876.546875Mb
2020-10-12 20:02:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17796.78125Mb; avail=470876.53125Mb
2020-10-12 20:02:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004320
2020-10-12 20:02:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17796.78125Mb; avail=470876.53125Mb
2020-10-12 20:02:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:02:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17796.78125Mb; avail=470876.53125Mb
2020-10-12 20:02:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.111163
2020-10-12 20:02:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.116539
2020-10-12 20:02:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17796.859375Mb; avail=470876.62109375Mb
2020-10-12 20:02:52 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 20:03:07 | INFO | train_inner | epoch 022:     53 / 107 loss=7.342, nll_loss=6.243, ppl=75.76, wps=17347.5, ups=2.71, wpb=6394.2, bsz=241.8, num_updates=2300, lr=0.000115043, gnorm=1.286, clip=0, train_wall=27, wall=848
2020-10-12 20:03:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17842.3203125Mb; avail=470831.04296875Mb
2020-10-12 20:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002001
2020-10-12 20:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.8515625Mb; avail=470830.51171875Mb
2020-10-12 20:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069883
2020-10-12 20:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.8515625Mb; avail=470830.51171875Mb
2020-10-12 20:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056162
2020-10-12 20:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128907
2020-10-12 20:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17843.66015625Mb; avail=470829.3828125Mb
2020-10-12 20:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17843.80078125Mb; avail=470829.50390625Mb
2020-10-12 20:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001237
2020-10-12 20:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17843.80078125Mb; avail=470829.50390625Mb
2020-10-12 20:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070030
2020-10-12 20:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17804.80859375Mb; avail=470868.5078125Mb
2020-10-12 20:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055790
2020-10-12 20:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127853
2020-10-12 20:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.82421875Mb; avail=470874.671875Mb
2020-10-12 20:03:24 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.336 | nll_loss 6.175 | ppl 72.28 | wps 61278.7 | wpb 2442.6 | bsz 92.5 | num_updates 2354 | best_loss 7.336
2020-10-12 20:03:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:03:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 22 @ 2354 updates, score 7.336) (writing took 7.06460411199987 seconds)
2020-10-12 20:03:31 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 20:03:31 | INFO | train | epoch 022 | loss 7.344 | nll_loss 6.244 | ppl 75.8 | wps 17326.6 | ups 2.69 | wpb 6450.5 | bsz 242.4 | num_updates 2354 | lr 0.000117741 | gnorm 1.345 | clip 0 | train_wall 29 | wall 873
2020-10-12 20:03:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 20:03:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 20:03:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17779.56640625Mb; avail=470893.89453125Mb
2020-10-12 20:03:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000861
2020-10-12 20:03:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005572
2020-10-12 20:03:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17779.56640625Mb; avail=470893.890625Mb
2020-10-12 20:03:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000276
2020-10-12 20:03:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17779.56640625Mb; avail=470893.890625Mb
2020-10-12 20:03:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099045
2020-10-12 20:03:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105668
2020-10-12 20:03:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17773.48828125Mb; avail=470899.97265625Mb
2020-10-12 20:03:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17773.515625Mb; avail=470899.8359375Mb
2020-10-12 20:03:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003807
2020-10-12 20:03:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17773.40234375Mb; avail=470899.8359375Mb
2020-10-12 20:03:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 20:03:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17773.40234375Mb; avail=470899.8359375Mb
2020-10-12 20:03:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097063
2020-10-12 20:03:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101823
2020-10-12 20:03:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17773.80859375Mb; avail=470899.8046875Mb
2020-10-12 20:03:32 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 20:03:44 | INFO | train_inner | epoch 023:     46 / 107 loss=7.3, nll_loss=6.195, ppl=73.25, wps=17050.6, ups=2.64, wpb=6454.1, bsz=246.6, num_updates=2400, lr=0.00012004, gnorm=1.326, clip=0, train_wall=27, wall=886
2020-10-12 20:04:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:04:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17861.546875Mb; avail=470811.09375Mb
2020-10-12 20:04:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002797
2020-10-12 20:04:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.546875Mb; avail=470811.09375Mb
2020-10-12 20:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.117032
2020-10-12 20:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.44921875Mb; avail=470810.62109375Mb
2020-10-12 20:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057474
2020-10-12 20:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.178459
2020-10-12 20:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.51171875Mb; avail=470810.58984375Mb
2020-10-12 20:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17861.578125Mb; avail=470810.7109375Mb
2020-10-12 20:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001332
2020-10-12 20:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.578125Mb; avail=470810.7109375Mb
2020-10-12 20:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072559
2020-10-12 20:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.56640625Mb; avail=470810.7109375Mb
2020-10-12 20:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055866
2020-10-12 20:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130553
2020-10-12 20:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.671875Mb; avail=470810.58984375Mb
2020-10-12 20:04:04 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.277 | nll_loss 6.105 | ppl 68.84 | wps 61023.5 | wpb 2442.6 | bsz 92.5 | num_updates 2461 | best_loss 7.277
2020-10-12 20:04:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:04:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 23 @ 2461 updates, score 7.277) (writing took 7.325446947000273 seconds)
2020-10-12 20:04:11 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 20:04:11 | INFO | train | epoch 023 | loss 7.257 | nll_loss 6.145 | ppl 70.76 | wps 17288.1 | ups 2.68 | wpb 6450.5 | bsz 242.4 | num_updates 2461 | lr 0.000123088 | gnorm 1.276 | clip 0 | train_wall 29 | wall 913
2020-10-12 20:04:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 20:04:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 20:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17803.05078125Mb; avail=470870.046875Mb
2020-10-12 20:04:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000990
2020-10-12 20:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006378
2020-10-12 20:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.05078125Mb; avail=470870.046875Mb
2020-10-12 20:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-12 20:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.05078125Mb; avail=470870.046875Mb
2020-10-12 20:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.112145
2020-10-12 20:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.119719
2020-10-12 20:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17797.45703125Mb; avail=470875.63671875Mb
2020-10-12 20:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17796.91015625Mb; avail=470875.9765625Mb
2020-10-12 20:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004246
2020-10-12 20:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17796.8515625Mb; avail=470876.21875Mb
2020-10-12 20:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000250
2020-10-12 20:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17796.8515625Mb; avail=470876.21875Mb
2020-10-12 20:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109702
2020-10-12 20:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115029
2020-10-12 20:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17796.76953125Mb; avail=470876.30859375Mb
2020-10-12 20:04:11 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 20:04:22 | INFO | train_inner | epoch 024:     39 / 107 loss=7.199, nll_loss=6.079, ppl=67.62, wps=16880.1, ups=2.63, wpb=6416.8, bsz=246.8, num_updates=2500, lr=0.000125037, gnorm=1.241, clip=0, train_wall=27, wall=924
2020-10-12 20:04:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18067.25Mb; avail=470606.04296875Mb
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001912
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18067.25Mb; avail=470606.04296875Mb
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071782
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18067.37109375Mb; avail=470605.921875Mb
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054653
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129160
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18067.41015625Mb; avail=470605.921875Mb
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18067.41015625Mb; avail=470605.921875Mb
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001207
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18067.41015625Mb; avail=470605.921875Mb
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070779
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18067.33203125Mb; avail=470605.80859375Mb
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055050
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127848
2020-10-12 20:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18067.26953125Mb; avail=470606.05078125Mb
2020-10-12 20:04:44 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.221 | nll_loss 6.035 | ppl 65.55 | wps 62980.7 | wpb 2442.6 | bsz 92.5 | num_updates 2568 | best_loss 7.221
2020-10-12 20:04:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:04:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 24 @ 2568 updates, score 7.221) (writing took 8.711839628000234 seconds)
2020-10-12 20:04:53 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 20:04:53 | INFO | train | epoch 024 | loss 7.184 | nll_loss 6.061 | ppl 66.77 | wps 16687.4 | ups 2.59 | wpb 6450.5 | bsz 242.4 | num_updates 2568 | lr 0.000128436 | gnorm 1.264 | clip 0 | train_wall 29 | wall 954
2020-10-12 20:04:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 20:04:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 20:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18095.37890625Mb; avail=470577.08984375Mb
2020-10-12 20:04:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000930
2020-10-12 20:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005433
2020-10-12 20:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18095.87890625Mb; avail=470576.58984375Mb
2020-10-12 20:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000224
2020-10-12 20:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18095.87890625Mb; avail=470576.58984375Mb
2020-10-12 20:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098219
2020-10-12 20:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104674
2020-10-12 20:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18090.140625Mb; avail=470582.25390625Mb
2020-10-12 20:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18090.140625Mb; avail=470582.3828125Mb
2020-10-12 20:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004419
2020-10-12 20:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18090.140625Mb; avail=470582.3828125Mb
2020-10-12 20:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000279
2020-10-12 20:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18090.140625Mb; avail=470582.3828125Mb
2020-10-12 20:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097607
2020-10-12 20:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103073
2020-10-12 20:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18090.109375Mb; avail=470582.3046875Mb
2020-10-12 20:04:53 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 20:05:02 | INFO | train_inner | epoch 025:     32 / 107 loss=7.205, nll_loss=6.083, ppl=67.79, wps=16658.3, ups=2.55, wpb=6537, bsz=230.7, num_updates=2600, lr=0.000130035, gnorm=1.324, clip=0, train_wall=27, wall=963
2020-10-12 20:05:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17857.55859375Mb; avail=470815.421875Mb
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001743
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.55859375Mb; avail=470815.421875Mb
2020-10-12 20:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070258
2020-10-12 20:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.1875Mb; avail=470814.6953125Mb
2020-10-12 20:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057598
2020-10-12 20:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130455
2020-10-12 20:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.171875Mb; avail=470813.96875Mb
2020-10-12 20:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17858.33984375Mb; avail=470814.08984375Mb
2020-10-12 20:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001192
2020-10-12 20:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.2890625Mb; avail=470814.33203125Mb
2020-10-12 20:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070306
2020-10-12 20:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.30078125Mb; avail=470814.33203125Mb
2020-10-12 20:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057343
2020-10-12 20:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129634
2020-10-12 20:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17875.53515625Mb; avail=470796.91796875Mb
2020-10-12 20:05:25 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.236 | nll_loss 6.066 | ppl 66.99 | wps 59815.2 | wpb 2442.6 | bsz 92.5 | num_updates 2675 | best_loss 7.221
2020-10-12 20:05:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:05:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_last.pt (epoch 25 @ 2675 updates, score 7.236) (writing took 2.2191727620001984 seconds)
2020-10-12 20:05:27 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 20:05:27 | INFO | train | epoch 025 | loss 7.105 | nll_loss 5.969 | ppl 62.64 | wps 19953 | ups 3.09 | wpb 6450.5 | bsz 242.4 | num_updates 2675 | lr 0.000133783 | gnorm 1.301 | clip 0 | train_wall 28 | wall 989
2020-10-12 20:05:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 20:05:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 20:05:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17851.04296875Mb; avail=470821.43359375Mb
2020-10-12 20:05:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000826
2020-10-12 20:05:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005555
2020-10-12 20:05:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.015625Mb; avail=470822.5390625Mb
2020-10-12 20:05:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-12 20:05:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.015625Mb; avail=470822.5390625Mb
2020-10-12 20:05:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097850
2020-10-12 20:05:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104391
2020-10-12 20:05:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17844.33203125Mb; avail=470828.4375Mb
2020-10-12 20:05:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17844.296875Mb; avail=470828.55859375Mb
2020-10-12 20:05:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003841
2020-10-12 20:05:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17844.296875Mb; avail=470828.55859375Mb
2020-10-12 20:05:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000214
2020-10-12 20:05:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17844.296875Mb; avail=470828.55859375Mb
2020-10-12 20:05:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096938
2020-10-12 20:05:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101737
2020-10-12 20:05:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17844.2890625Mb; avail=470828.55859375Mb
2020-10-12 20:05:27 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 20:05:34 | INFO | train_inner | epoch 026:     25 / 107 loss=7.074, nll_loss=5.934, ppl=61.15, wps=19376.2, ups=3.07, wpb=6312.4, bsz=242.4, num_updates=2700, lr=0.000135032, gnorm=1.264, clip=0, train_wall=27, wall=996
2020-10-12 20:05:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18415.37890625Mb; avail=470257.890625Mb
2020-10-12 20:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001754
2020-10-12 20:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18415.37890625Mb; avail=470257.890625Mb
2020-10-12 20:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070551
2020-10-12 20:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18415.15625Mb; avail=470258.01171875Mb
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055453
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128609
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18415.234375Mb; avail=470257.76953125Mb
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18415.7890625Mb; avail=470257.52734375Mb
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001197
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18415.7890625Mb; avail=470257.52734375Mb
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071018
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18405.54296875Mb; avail=470267.96875Mb
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055084
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128283
2020-10-12 20:05:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18458.33203125Mb; avail=470215.18359375Mb
2020-10-12 20:06:00 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.21 | nll_loss 6.011 | ppl 64.51 | wps 63178.1 | wpb 2442.6 | bsz 92.5 | num_updates 2782 | best_loss 7.21
2020-10-12 20:06:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:06:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 26 @ 2782 updates, score 7.21) (writing took 8.661328747999505 seconds)
2020-10-12 20:06:08 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 20:06:08 | INFO | train | epoch 026 | loss 7.022 | nll_loss 5.873 | ppl 58.61 | wps 16724.7 | ups 2.59 | wpb 6450.5 | bsz 242.4 | num_updates 2782 | lr 0.00013913 | gnorm 1.248 | clip 0 | train_wall 29 | wall 1030
2020-10-12 20:06:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 20:06:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 20:06:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17823.56640625Mb; avail=470849.34375Mb
2020-10-12 20:06:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000910
2020-10-12 20:06:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006154
2020-10-12 20:06:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.56640625Mb; avail=470849.34375Mb
2020-10-12 20:06:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000234
2020-10-12 20:06:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.56640625Mb; avail=470849.34375Mb
2020-10-12 20:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099035
2020-10-12 20:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106228
2020-10-12 20:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.2109375Mb; avail=470854.69140625Mb
2020-10-12 20:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17818.1171875Mb; avail=470854.8125Mb
2020-10-12 20:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003617
2020-10-12 20:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.1171875Mb; avail=470854.8125Mb
2020-10-12 20:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 20:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.1171875Mb; avail=470854.8125Mb
2020-10-12 20:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097374
2020-10-12 20:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101908
2020-10-12 20:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.4375Mb; avail=470854.69140625Mb
2020-10-12 20:06:09 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 20:06:14 | INFO | train_inner | epoch 027:     18 / 107 loss=7.005, nll_loss=5.853, ppl=57.82, wps=16336.5, ups=2.54, wpb=6436.6, bsz=238.9, num_updates=2800, lr=0.00014003, gnorm=1.343, clip=0, train_wall=27, wall=1035
2020-10-12 20:06:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:06:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19042.6953125Mb; avail=469630.44140625Mb
2020-10-12 20:06:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001735
2020-10-12 20:06:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19042.6953125Mb; avail=469630.44140625Mb
2020-10-12 20:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071023
2020-10-12 20:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19042.43359375Mb; avail=469630.83984375Mb
2020-10-12 20:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055619
2020-10-12 20:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129227
2020-10-12 20:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19042.6015625Mb; avail=469630.4765625Mb
2020-10-12 20:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19042.58203125Mb; avail=469630.71875Mb
2020-10-12 20:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001237
2020-10-12 20:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19042.58203125Mb; avail=469630.71875Mb
2020-10-12 20:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071824
2020-10-12 20:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19042.55078125Mb; avail=469630.71875Mb
2020-10-12 20:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055929
2020-10-12 20:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129783
2020-10-12 20:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19042.7265625Mb; avail=469630.71875Mb
2020-10-12 20:06:41 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.124 | nll_loss 5.914 | ppl 60.28 | wps 62588.3 | wpb 2442.6 | bsz 92.5 | num_updates 2889 | best_loss 7.124
2020-10-12 20:06:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:06:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 27 @ 2889 updates, score 7.124) (writing took 12.085883108999951 seconds)
2020-10-12 20:06:53 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 20:06:53 | INFO | train | epoch 027 | loss 6.959 | nll_loss 5.8 | ppl 55.72 | wps 15501.6 | ups 2.4 | wpb 6450.5 | bsz 242.4 | num_updates 2889 | lr 0.000144478 | gnorm 1.344 | clip 0 | train_wall 29 | wall 1074
2020-10-12 20:06:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 20:06:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 20:06:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17848.01953125Mb; avail=470824.42578125Mb
2020-10-12 20:06:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001129
2020-10-12 20:06:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006471
2020-10-12 20:06:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17848.625Mb; avail=470823.8203125Mb
2020-10-12 20:06:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000204
2020-10-12 20:06:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17848.625Mb; avail=470823.8203125Mb
2020-10-12 20:06:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.137509
2020-10-12 20:06:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.145143
2020-10-12 20:06:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.88671875Mb; avail=470829.15234375Mb
2020-10-12 20:06:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17842.953125Mb; avail=470829.2734375Mb
2020-10-12 20:06:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003809
2020-10-12 20:06:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17843.0Mb; avail=470829.15234375Mb
2020-10-12 20:06:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 20:06:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17843.0Mb; avail=470829.15234375Mb
2020-10-12 20:06:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096523
2020-10-12 20:06:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101254
2020-10-12 20:06:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17843.28515625Mb; avail=470829.2734375Mb
2020-10-12 20:06:53 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 20:06:57 | INFO | train_inner | epoch 028:     11 / 107 loss=6.929, nll_loss=5.766, ppl=54.42, wps=15250.2, ups=2.33, wpb=6535.5, bsz=254.2, num_updates=2900, lr=0.000145028, gnorm=1.293, clip=0, train_wall=27, wall=1078
2020-10-12 20:07:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17857.13671875Mb; avail=470815.984375Mb
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002105
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17860.1640625Mb; avail=470813.5625Mb
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080795
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17956.91015625Mb; avail=470716.7421875Mb
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062651
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.146577
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18030.69921875Mb; avail=470642.6328125Mb
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18061.9375Mb; avail=470610.9921875Mb
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001362
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18064.359375Mb; avail=470609.17578125Mb
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079859
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18172.1875Mb; avail=470500.75Mb
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063070
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.145091
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18246.609375Mb; avail=470426.73046875Mb
2020-10-12 20:07:26 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.076 | nll_loss 5.861 | ppl 58.12 | wps 60862 | wpb 2442.6 | bsz 92.5 | num_updates 2996 | best_loss 7.076
2020-10-12 20:07:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:07:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 28 @ 2996 updates, score 7.076) (writing took 7.134927473999596 seconds)
2020-10-12 20:07:33 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 20:07:33 | INFO | train | epoch 028 | loss 6.866 | nll_loss 5.694 | ppl 51.76 | wps 17084.3 | ups 2.65 | wpb 6450.5 | bsz 242.4 | num_updates 2996 | lr 0.000149825 | gnorm 1.253 | clip 0 | train_wall 29 | wall 1115
2020-10-12 20:07:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 20:07:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 20:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17860.26171875Mb; avail=470812.890625Mb
2020-10-12 20:07:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000879
2020-10-12 20:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005723
2020-10-12 20:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.84765625Mb; avail=470818.3046875Mb
2020-10-12 20:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000206
2020-10-12 20:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.84765625Mb; avail=470818.3046875Mb
2020-10-12 20:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097290
2020-10-12 20:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103960
2020-10-12 20:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.28125Mb; avail=470819.35546875Mb
2020-10-12 20:07:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17854.734375Mb; avail=470819.11328125Mb
2020-10-12 20:07:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003611
2020-10-12 20:07:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.734375Mb; avail=470819.11328125Mb
2020-10-12 20:07:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 20:07:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.734375Mb; avail=470819.11328125Mb
2020-10-12 20:07:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096609
2020-10-12 20:07:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101126
2020-10-12 20:07:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17854.828125Mb; avail=470819.046875Mb
2020-10-12 20:07:34 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 20:07:35 | INFO | train_inner | epoch 029:      4 / 107 loss=6.866, nll_loss=5.693, ppl=51.73, wps=16797, ups=2.61, wpb=6432.7, bsz=244.9, num_updates=3000, lr=0.000150025, gnorm=1.233, clip=0, train_wall=27, wall=1116
2020-10-12 20:08:02 | INFO | train_inner | epoch 029:    104 / 107 loss=6.801, nll_loss=5.617, ppl=49.06, wps=23229.3, ups=3.61, wpb=6426.3, bsz=238.1, num_updates=3100, lr=0.000155023, gnorm=1.347, clip=0, train_wall=27, wall=1144
2020-10-12 20:08:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18518.265625Mb; avail=470157.36328125Mb
2020-10-12 20:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002095
2020-10-12 20:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18518.35546875Mb; avail=470157.40234375Mb
2020-10-12 20:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071321
2020-10-12 20:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18518.515625Mb; avail=470156.0546875Mb
2020-10-12 20:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056538
2020-10-12 20:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130855
2020-10-12 20:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18513.98828125Mb; avail=470159.2734375Mb
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18502.7890625Mb; avail=470170.6171875Mb
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001370
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18502.40234375Mb; avail=470170.27734375Mb
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071079
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18497.14453125Mb; avail=470175.4765625Mb
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056582
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129898
2020-10-12 20:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18497.03125Mb; avail=470175.79296875Mb
2020-10-12 20:08:06 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.052 | nll_loss 5.826 | ppl 56.73 | wps 61866.6 | wpb 2442.6 | bsz 92.5 | num_updates 3103 | best_loss 7.052
2020-10-12 20:08:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:08:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 29 @ 3103 updates, score 7.052) (writing took 12.326306756999656 seconds)
2020-10-12 20:08:18 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 20:08:18 | INFO | train | epoch 029 | loss 6.799 | nll_loss 5.615 | ppl 49.02 | wps 15436.3 | ups 2.39 | wpb 6450.5 | bsz 242.4 | num_updates 3103 | lr 0.000155172 | gnorm 1.342 | clip 0 | train_wall 29 | wall 1159
2020-10-12 20:08:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 20:08:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18166.55078125Mb; avail=470506.2890625Mb
2020-10-12 20:08:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000976
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006437
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18160.5390625Mb; avail=470512.30078125Mb
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000247
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18160.5390625Mb; avail=470512.30078125Mb
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.110813
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.118447
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18160.2734375Mb; avail=470512.25390625Mb
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18160.328125Mb; avail=470512.48046875Mb
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004229
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18160.328125Mb; avail=470512.48046875Mb
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000228
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18160.328125Mb; avail=470512.48046875Mb
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.110488
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115982
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18159.9296875Mb; avail=470512.8515625Mb
2020-10-12 20:08:18 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 20:08:45 | INFO | train_inner | epoch 030:     97 / 107 loss=6.734, nll_loss=5.54, ppl=46.52, wps=15173.3, ups=2.35, wpb=6462.9, bsz=235.1, num_updates=3200, lr=0.00016002, gnorm=1.353, clip=0, train_wall=27, wall=1186
2020-10-12 20:08:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:08:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17824.328125Mb; avail=470848.11328125Mb
2020-10-12 20:08:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001908
2020-10-12 20:08:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.328125Mb; avail=470848.11328125Mb
2020-10-12 20:08:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070599
2020-10-12 20:08:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.37890625Mb; avail=470848.35546875Mb
2020-10-12 20:08:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056583
2020-10-12 20:08:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129906
2020-10-12 20:08:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.28515625Mb; avail=470848.4765625Mb
2020-10-12 20:08:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17824.28515625Mb; avail=470848.4765625Mb
2020-10-12 20:08:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001195
2020-10-12 20:08:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.28515625Mb; avail=470848.4765625Mb
2020-10-12 20:08:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071459
2020-10-12 20:08:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.28515625Mb; avail=470848.4765625Mb
2020-10-12 20:08:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056542
2020-10-12 20:08:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129968
2020-10-12 20:08:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.41015625Mb; avail=470848.234375Mb
2020-10-12 20:08:50 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.01 | nll_loss 5.78 | ppl 54.95 | wps 63218.7 | wpb 2442.6 | bsz 92.5 | num_updates 3210 | best_loss 7.01
2020-10-12 20:08:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:08:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 30 @ 3210 updates, score 7.01) (writing took 4.270577063000019 seconds)
2020-10-12 20:08:55 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 20:08:55 | INFO | train | epoch 030 | loss 6.726 | nll_loss 5.53 | ppl 46.22 | wps 18922.2 | ups 2.93 | wpb 6450.5 | bsz 242.4 | num_updates 3210 | lr 0.00016052 | gnorm 1.342 | clip 0 | train_wall 28 | wall 1196
2020-10-12 20:08:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 20:08:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 20:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18218.8828125Mb; avail=470453.2734375Mb
2020-10-12 20:08:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000869
2020-10-12 20:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006250
2020-10-12 20:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18226.75390625Mb; avail=470446.0078125Mb
2020-10-12 20:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000199
2020-10-12 20:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18227.359375Mb; avail=470445.40234375Mb
2020-10-12 20:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098589
2020-10-12 20:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105962
2020-10-12 20:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18338.87890625Mb; avail=470333.859375Mb
2020-10-12 20:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18363.4765625Mb; avail=470310.73828125Mb
2020-10-12 20:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003719
2020-10-12 20:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18362.984375Mb; avail=470310.4921875Mb
2020-10-12 20:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 20:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18363.4765625Mb; avail=470310.73828125Mb
2020-10-12 20:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097467
2020-10-12 20:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102194
2020-10-12 20:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18370.703125Mb; avail=470308.4765625Mb
2020-10-12 20:08:55 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 20:09:20 | INFO | train_inner | epoch 031:     90 / 107 loss=6.638, nll_loss=5.429, ppl=43.08, wps=18399.3, ups=2.85, wpb=6460.2, bsz=249.8, num_updates=3300, lr=0.000165018, gnorm=1.307, clip=0, train_wall=27, wall=1222
2020-10-12 20:09:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17822.00390625Mb; avail=470850.54296875Mb
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002017
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.00390625Mb; avail=470850.54296875Mb
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071128
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.85546875Mb; avail=470849.57421875Mb
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056697
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130889
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.140625Mb; avail=470849.33203125Mb
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17823.140625Mb; avail=470849.33203125Mb
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001396
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.140625Mb; avail=470849.33203125Mb
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069739
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.140625Mb; avail=470849.33203125Mb
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055608
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127500
2020-10-12 20:09:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.16015625Mb; avail=470849.2109375Mb
2020-10-12 20:09:27 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.989 | nll_loss 5.756 | ppl 54.04 | wps 62271.4 | wpb 2442.6 | bsz 92.5 | num_updates 3317 | best_loss 6.989
2020-10-12 20:09:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:09:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 31 @ 3317 updates, score 6.989) (writing took 4.280314116999762 seconds)
2020-10-12 20:09:32 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 20:09:32 | INFO | train | epoch 031 | loss 6.64 | nll_loss 5.43 | ppl 43.12 | wps 18635.3 | ups 2.89 | wpb 6450.5 | bsz 242.4 | num_updates 3317 | lr 0.000165867 | gnorm 1.303 | clip 0 | train_wall 29 | wall 1233
2020-10-12 20:09:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 20:09:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17833.4453125Mb; avail=470838.91015625Mb
2020-10-12 20:09:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000796
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005510
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17833.4453125Mb; avail=470838.91015625Mb
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000201
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17833.4453125Mb; avail=470838.91015625Mb
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097377
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103846
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.4921875Mb; avail=470844.86328125Mb
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17827.3828125Mb; avail=470844.7421875Mb
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003735
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.3828125Mb; avail=470844.7421875Mb
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.3828125Mb; avail=470844.7421875Mb
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097404
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102056
2020-10-12 20:09:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.765625Mb; avail=470848.1875Mb
2020-10-12 20:09:32 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 20:09:55 | INFO | train_inner | epoch 032:     83 / 107 loss=6.554, nll_loss=5.332, ppl=40.27, wps=18227.4, ups=2.84, wpb=6414.6, bsz=239.9, num_updates=3400, lr=0.000170015, gnorm=1.301, clip=0, train_wall=27, wall=1257
2020-10-12 20:10:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18466.94921875Mb; avail=470205.6796875Mb
2020-10-12 20:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002280
2020-10-12 20:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18467.5546875Mb; avail=470205.07421875Mb
2020-10-12 20:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078366
2020-10-12 20:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18467.6953125Mb; avail=470205.1953125Mb
2020-10-12 20:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063268
2020-10-12 20:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.144937
2020-10-12 20:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18147.04296875Mb; avail=470527.0234375Mb
2020-10-12 20:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18147.04296875Mb; avail=470529.73046875Mb
2020-10-12 20:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001481
2020-10-12 20:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18146.55078125Mb; avail=470529.73046875Mb
2020-10-12 20:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072878
2020-10-12 20:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18146.75390625Mb; avail=470532.6953125Mb
2020-10-12 20:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055136
2020-10-12 20:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130734
2020-10-12 20:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18146.21484375Mb; avail=470533.12890625Mb
2020-10-12 20:10:05 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.942 | nll_loss 5.692 | ppl 51.7 | wps 61596.2 | wpb 2442.6 | bsz 92.5 | num_updates 3424 | best_loss 6.942
2020-10-12 20:10:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:10:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 32 @ 3424 updates, score 6.942) (writing took 6.76967066600082 seconds)
2020-10-12 20:10:11 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 20:10:11 | INFO | train | epoch 032 | loss 6.552 | nll_loss 5.329 | ppl 40.19 | wps 17341.5 | ups 2.69 | wpb 6450.5 | bsz 242.4 | num_updates 3424 | lr 0.000171214 | gnorm 1.29 | clip 0 | train_wall 29 | wall 1273
2020-10-12 20:10:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 20:10:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 20:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18112.9140625Mb; avail=470562.52734375Mb
2020-10-12 20:10:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000728
2020-10-12 20:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005443
2020-10-12 20:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18112.78515625Mb; avail=470562.59765625Mb
2020-10-12 20:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000200
2020-10-12 20:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18113.390625Mb; avail=470562.484375Mb
2020-10-12 20:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096863
2020-10-12 20:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103259
2020-10-12 20:10:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18110.3984375Mb; avail=470563.87109375Mb
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18093.1015625Mb; avail=470580.75390625Mb
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003726
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18092.45703125Mb; avail=470580.56640625Mb
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18093.0625Mb; avail=470581.05859375Mb
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096656
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101317
2020-10-12 20:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18086.40625Mb; avail=470586.28515625Mb
2020-10-12 20:10:12 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 20:10:33 | INFO | train_inner | epoch 033:     76 / 107 loss=6.505, nll_loss=5.274, ppl=38.7, wps=17305.2, ups=2.64, wpb=6546.8, bsz=258.3, num_updates=3500, lr=0.000175013, gnorm=1.326, clip=0, train_wall=27, wall=1295
2020-10-12 20:10:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18661.390625Mb; avail=470010.62890625Mb
2020-10-12 20:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001992
2020-10-12 20:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18661.390625Mb; avail=470010.62890625Mb
2020-10-12 20:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071291
2020-10-12 20:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18660.75Mb; avail=470010.5078125Mb
2020-10-12 20:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055057
2020-10-12 20:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129215
2020-10-12 20:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18661.1015625Mb; avail=470010.265625Mb
2020-10-12 20:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18661.70703125Mb; avail=470009.66015625Mb
2020-10-12 20:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001282
2020-10-12 20:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18661.70703125Mb; avail=470009.66015625Mb
2020-10-12 20:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070590
2020-10-12 20:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18661.74609375Mb; avail=470009.5390625Mb
2020-10-12 20:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054012
2020-10-12 20:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126670
2020-10-12 20:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18661.8984375Mb; avail=470010.62890625Mb
2020-10-12 20:10:44 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.928 | nll_loss 5.68 | ppl 51.27 | wps 62328.1 | wpb 2442.6 | bsz 92.5 | num_updates 3531 | best_loss 6.928
2020-10-12 20:10:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:10:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 33 @ 3531 updates, score 6.928) (writing took 4.75979875900066 seconds)
2020-10-12 20:10:49 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 20:10:49 | INFO | train | epoch 033 | loss 6.483 | nll_loss 5.248 | ppl 37.99 | wps 18409.9 | ups 2.85 | wpb 6450.5 | bsz 242.4 | num_updates 3531 | lr 0.000176562 | gnorm 1.353 | clip 0 | train_wall 29 | wall 1310
2020-10-12 20:10:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 20:10:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 20:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17811.69921875Mb; avail=470860.34765625Mb
2020-10-12 20:10:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000850
2020-10-12 20:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005794
2020-10-12 20:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.69921875Mb; avail=470860.34765625Mb
2020-10-12 20:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000204
2020-10-12 20:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.69921875Mb; avail=470860.34765625Mb
2020-10-12 20:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098942
2020-10-12 20:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105743
2020-10-12 20:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.796875Mb; avail=470866.90625Mb
2020-10-12 20:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17805.796875Mb; avail=470866.90625Mb
2020-10-12 20:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003712
2020-10-12 20:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.765625Mb; avail=470866.90625Mb
2020-10-12 20:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.765625Mb; avail=470866.90625Mb
2020-10-12 20:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097104
2020-10-12 20:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101822
2020-10-12 20:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.66796875Mb; avail=470867.02734375Mb
2020-10-12 20:10:49 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 20:11:09 | INFO | train_inner | epoch 034:     69 / 107 loss=6.404, nll_loss=5.156, ppl=35.66, wps=18044.1, ups=2.83, wpb=6378.6, bsz=240.1, num_updates=3600, lr=0.00018001, gnorm=1.354, clip=0, train_wall=27, wall=1330
2020-10-12 20:11:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17893.26171875Mb; avail=470779.09765625Mb
2020-10-12 20:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001868
2020-10-12 20:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.26171875Mb; avail=470779.09765625Mb
2020-10-12 20:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078444
2020-10-12 20:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.12109375Mb; avail=470779.296875Mb
2020-10-12 20:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061676
2020-10-12 20:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142848
2020-10-12 20:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.8984375Mb; avail=470779.41796875Mb
2020-10-12 20:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17892.92578125Mb; avail=470779.296875Mb
2020-10-12 20:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001549
2020-10-12 20:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.92578125Mb; avail=470779.296875Mb
2020-10-12 20:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077674
2020-10-12 20:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.90625Mb; avail=470779.0546875Mb
2020-10-12 20:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.058550
2020-10-12 20:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138750
2020-10-12 20:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.07421875Mb; avail=470779.41796875Mb
2020-10-12 20:11:21 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.894 | nll_loss 5.634 | ppl 49.67 | wps 62462.4 | wpb 2442.6 | bsz 92.5 | num_updates 3638 | best_loss 6.894
2020-10-12 20:11:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:11:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 34 @ 3638 updates, score 6.894) (writing took 7.873993356000028 seconds)
2020-10-12 20:11:29 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 20:11:29 | INFO | train | epoch 034 | loss 6.404 | nll_loss 5.156 | ppl 35.67 | wps 17057 | ups 2.64 | wpb 6450.5 | bsz 242.4 | num_updates 3638 | lr 0.000181909 | gnorm 1.375 | clip 0 | train_wall 29 | wall 1351
2020-10-12 20:11:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 20:11:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 20:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17878.1796875Mb; avail=470794.29296875Mb
2020-10-12 20:11:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000789
2020-10-12 20:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006174
2020-10-12 20:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17878.1796875Mb; avail=470794.29296875Mb
2020-10-12 20:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000213
2020-10-12 20:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17878.1796875Mb; avail=470794.29296875Mb
2020-10-12 20:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099329
2020-10-12 20:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106595
2020-10-12 20:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17872.9609375Mb; avail=470799.59375Mb
2020-10-12 20:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17873.16796875Mb; avail=470799.47265625Mb
2020-10-12 20:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003623
2020-10-12 20:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.16796875Mb; avail=470799.47265625Mb
2020-10-12 20:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 20:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.16796875Mb; avail=470799.47265625Mb
2020-10-12 20:11:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096886
2020-10-12 20:11:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101430
2020-10-12 20:11:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17872.77734375Mb; avail=470799.96484375Mb
2020-10-12 20:11:30 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 20:11:47 | INFO | train_inner | epoch 035:     62 / 107 loss=6.356, nll_loss=5.101, ppl=34.31, wps=16730.7, ups=2.59, wpb=6468.2, bsz=235.4, num_updates=3700, lr=0.000185008, gnorm=1.293, clip=0, train_wall=27, wall=1369
2020-10-12 20:12:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17892.5390625Mb; avail=470780.31640625Mb
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002342
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.5390625Mb; avail=470780.31640625Mb
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070517
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.08203125Mb; avail=470779.46875Mb
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056051
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129747
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.6015625Mb; avail=470779.2265625Mb
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17893.6015625Mb; avail=470779.2265625Mb
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001204
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.6015625Mb; avail=470779.2265625Mb
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069908
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.5234375Mb; avail=470779.10546875Mb
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055334
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127288
2020-10-12 20:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17897.60546875Mb; avail=470775.22265625Mb
2020-10-12 20:12:02 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.847 | nll_loss 5.554 | ppl 46.99 | wps 62929 | wpb 2442.6 | bsz 92.5 | num_updates 3745 | best_loss 6.847
2020-10-12 20:12:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:12:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 35 @ 3745 updates, score 6.847) (writing took 6.529053889000352 seconds)
2020-10-12 20:12:09 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 20:12:09 | INFO | train | epoch 035 | loss 6.298 | nll_loss 5.035 | ppl 32.78 | wps 17387.6 | ups 2.7 | wpb 6450.5 | bsz 242.4 | num_updates 3745 | lr 0.000187256 | gnorm 1.246 | clip 0 | train_wall 29 | wall 1390
2020-10-12 20:12:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 20:12:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17852.76171875Mb; avail=470819.44921875Mb
2020-10-12 20:12:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000890
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005732
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.3671875Mb; avail=470818.84375Mb
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000242
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.3671875Mb; avail=470818.84375Mb
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099187
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105927
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.84375Mb; avail=470824.7421875Mb
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17847.828125Mb; avail=470824.5Mb
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003594
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.828125Mb; avail=470824.7421875Mb
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000190
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.7890625Mb; avail=470824.7421875Mb
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097641
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102197
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17848.05078125Mb; avail=470824.38671875Mb
2020-10-12 20:12:09 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 20:12:25 | INFO | train_inner | epoch 036:     55 / 107 loss=6.284, nll_loss=5.017, ppl=32.37, wps=17294.4, ups=2.65, wpb=6519.6, bsz=241, num_updates=3800, lr=0.000190005, gnorm=1.295, clip=0, train_wall=27, wall=1406
2020-10-12 20:12:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:12:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17913.01171875Mb; avail=470759.37109375Mb
2020-10-12 20:12:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002378
2020-10-12 20:12:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17913.01171875Mb; avail=470759.37109375Mb
2020-10-12 20:12:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072315
2020-10-12 20:12:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17913.14453125Mb; avail=470759.12890625Mb
2020-10-12 20:12:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056135
2020-10-12 20:12:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.131730
2020-10-12 20:12:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17913.1171875Mb; avail=470759.37109375Mb
2020-10-12 20:12:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17913.1171875Mb; avail=470759.37109375Mb
2020-10-12 20:12:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001266
2020-10-12 20:12:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17913.1171875Mb; avail=470759.37109375Mb
2020-10-12 20:12:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069614
2020-10-12 20:12:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17913.01171875Mb; avail=470759.25Mb
2020-10-12 20:12:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055770
2020-10-12 20:12:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127403
2020-10-12 20:12:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17912.7734375Mb; avail=470759.61328125Mb
2020-10-12 20:12:42 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.839 | nll_loss 5.526 | ppl 46.08 | wps 63030.1 | wpb 2442.6 | bsz 92.5 | num_updates 3852 | best_loss 6.839
2020-10-12 20:12:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:12:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 36 @ 3852 updates, score 6.839) (writing took 6.4748780019999685 seconds)
2020-10-12 20:12:48 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 20:12:48 | INFO | train | epoch 036 | loss 6.229 | nll_loss 4.952 | ppl 30.96 | wps 17596.5 | ups 2.73 | wpb 6450.5 | bsz 242.4 | num_updates 3852 | lr 0.000192604 | gnorm 1.341 | clip 0 | train_wall 29 | wall 1430
2020-10-12 20:12:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 20:12:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 20:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19033.890625Mb; avail=469639.18359375Mb
2020-10-12 20:12:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001144
2020-10-12 20:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006336
2020-10-12 20:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19028.484375Mb; avail=469644.4765625Mb
2020-10-12 20:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000224
2020-10-12 20:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19028.484375Mb; avail=469644.4765625Mb
2020-10-12 20:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109967
2020-10-12 20:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.117596
2020-10-12 20:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19027.3046875Mb; avail=469645.5390625Mb
2020-10-12 20:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19027.36328125Mb; avail=469645.78125Mb
2020-10-12 20:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004303
2020-10-12 20:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19027.36328125Mb; avail=469645.78125Mb
2020-10-12 20:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 20:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19027.36328125Mb; avail=469645.78125Mb
2020-10-12 20:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109593
2020-10-12 20:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114903
2020-10-12 20:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19027.359375Mb; avail=469645.66015625Mb
2020-10-12 20:12:49 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 20:13:02 | INFO | train_inner | epoch 037:     48 / 107 loss=6.167, nll_loss=4.88, ppl=29.45, wps=17323.2, ups=2.69, wpb=6434.6, bsz=242.1, num_updates=3900, lr=0.000195003, gnorm=1.294, clip=0, train_wall=27, wall=1443
2020-10-12 20:13:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17872.56640625Mb; avail=470799.47265625Mb
2020-10-12 20:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001977
2020-10-12 20:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.171875Mb; avail=470798.8671875Mb
2020-10-12 20:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073220
2020-10-12 20:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.171875Mb; avail=470798.8671875Mb
2020-10-12 20:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056311
2020-10-12 20:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.132349
2020-10-12 20:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.2265625Mb; avail=470798.625Mb
2020-10-12 20:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17873.203125Mb; avail=470798.8671875Mb
2020-10-12 20:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001216
2020-10-12 20:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.203125Mb; avail=470798.8671875Mb
2020-10-12 20:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071072
2020-10-12 20:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.6875Mb; avail=470798.50390625Mb
2020-10-12 20:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055494
2020-10-12 20:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128545
2020-10-12 20:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.6484375Mb; avail=470798.74609375Mb
2020-10-12 20:13:21 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.799 | nll_loss 5.491 | ppl 44.97 | wps 63112.9 | wpb 2442.6 | bsz 92.5 | num_updates 3959 | best_loss 6.799
2020-10-12 20:13:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:13:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 37 @ 3959 updates, score 6.799) (writing took 6.506606763000491 seconds)
2020-10-12 20:13:27 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 20:13:27 | INFO | train | epoch 037 | loss 6.13 | nll_loss 4.837 | ppl 28.58 | wps 17692.3 | ups 2.74 | wpb 6450.5 | bsz 242.4 | num_updates 3959 | lr 0.000197951 | gnorm 1.268 | clip 0 | train_wall 29 | wall 1469
2020-10-12 20:13:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 20:13:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18505.18359375Mb; avail=470168.37109375Mb
2020-10-12 20:13:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000953
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005703
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18504.5234375Mb; avail=470167.7265625Mb
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000205
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18505.0Mb; avail=470168.10546875Mb
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100367
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107191
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18500.03515625Mb; avail=470173.05078125Mb
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18499.69921875Mb; avail=470173.625Mb
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003883
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18499.69921875Mb; avail=470173.625Mb
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000201
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18499.69921875Mb; avail=470173.625Mb
2020-10-12 20:13:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098873
2020-10-12 20:13:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103805
2020-10-12 20:13:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18499.8203125Mb; avail=470173.50390625Mb
2020-10-12 20:13:28 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 20:13:39 | INFO | train_inner | epoch 038:     41 / 107 loss=6.11, nll_loss=4.814, ppl=28.13, wps=17301.6, ups=2.7, wpb=6419.8, bsz=241.8, num_updates=4000, lr=0.0002, gnorm=1.283, clip=0, train_wall=27, wall=1481
2020-10-12 20:13:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:13:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17908.84765625Mb; avail=470762.44140625Mb
2020-10-12 20:13:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001948
2020-10-12 20:13:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17911.875Mb; avail=470760.01953125Mb
2020-10-12 20:13:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069809
2020-10-12 20:13:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17999.6015625Mb; avail=470672.50390625Mb
2020-10-12 20:13:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056207
2020-10-12 20:13:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128777
2020-10-12 20:13:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18064.265625Mb; avail=470607.83984375Mb
2020-10-12 20:13:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18089.7890625Mb; avail=470581.8046875Mb
2020-10-12 20:13:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001196
2020-10-12 20:13:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18091.60546875Mb; avail=470579.98828125Mb
2020-10-12 20:13:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071635
2020-10-12 20:13:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18189.25Mb; avail=470482.74609375Mb
2020-10-12 20:13:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055260
2020-10-12 20:13:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128966
2020-10-12 20:13:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18256.73828125Mb; avail=470415.66015625Mb
2020-10-12 20:14:00 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.817 | nll_loss 5.511 | ppl 45.62 | wps 63400.6 | wpb 2442.6 | bsz 92.5 | num_updates 4066 | best_loss 6.799
2020-10-12 20:14:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:14:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_last.pt (epoch 38 @ 4066 updates, score 6.817) (writing took 5.790918041000623 seconds)
2020-10-12 20:14:06 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 20:14:06 | INFO | train | epoch 038 | loss 6.042 | nll_loss 4.734 | ppl 26.61 | wps 17917.6 | ups 2.78 | wpb 6450.5 | bsz 242.4 | num_updates 4066 | lr 0.00019837 | gnorm 1.303 | clip 0 | train_wall 29 | wall 1507
2020-10-12 20:14:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 20:14:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 20:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16632.078125Mb; avail=472052.53125Mb
2020-10-12 20:14:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000972
2020-10-12 20:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006390
2020-10-12 20:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16641.16015625Mb; avail=472043.44921875Mb
2020-10-12 20:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-12 20:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16641.765625Mb; avail=472042.23828125Mb
2020-10-12 20:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.115705
2020-10-12 20:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.123564
2020-10-12 20:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16813.984375Mb; avail=471870.21484375Mb
2020-10-12 20:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16825.4765625Mb; avail=471859.5234375Mb
2020-10-12 20:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004622
2020-10-12 20:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16827.8984375Mb; avail=471857.1015625Mb
2020-10-12 20:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000250
2020-10-12 20:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16828.50390625Mb; avail=471856.49609375Mb
2020-10-12 20:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109374
2020-10-12 20:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115253
2020-10-12 20:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16854.76953125Mb; avail=471830.234375Mb
2020-10-12 20:14:06 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 20:14:16 | INFO | train_inner | epoch 039:     34 / 107 loss=5.999, nll_loss=4.684, ppl=25.71, wps=17618.5, ups=2.74, wpb=6429.9, bsz=235.7, num_updates=4100, lr=0.000197546, gnorm=1.296, clip=0, train_wall=27, wall=1517
2020-10-12 20:14:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18529.6953125Mb; avail=470143.3671875Mb
2020-10-12 20:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002322
2020-10-12 20:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18530.30078125Mb; avail=470142.76171875Mb
2020-10-12 20:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080736
2020-10-12 20:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18530.30078125Mb; avail=470142.95703125Mb
2020-10-12 20:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062368
2020-10-12 20:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.146541
2020-10-12 20:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18530.3046875Mb; avail=470142.95703125Mb
2020-10-12 20:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18530.3046875Mb; avail=470142.95703125Mb
2020-10-12 20:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001441
2020-10-12 20:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18530.3046875Mb; avail=470142.95703125Mb
2020-10-12 20:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079047
2020-10-12 20:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18530.3203125Mb; avail=470142.94140625Mb
2020-10-12 20:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064217
2020-10-12 20:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.145538
2020-10-12 20:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18530.4375Mb; avail=470142.4453125Mb
2020-10-12 20:14:39 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.751 | nll_loss 5.424 | ppl 42.93 | wps 61347.9 | wpb 2442.6 | bsz 92.5 | num_updates 4173 | best_loss 6.751
2020-10-12 20:14:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:14:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 39 @ 4173 updates, score 6.751) (writing took 5.454024730000128 seconds)
2020-10-12 20:14:44 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 20:14:44 | INFO | train | epoch 039 | loss 5.946 | nll_loss 4.623 | ppl 24.64 | wps 18000.2 | ups 2.79 | wpb 6450.5 | bsz 242.4 | num_updates 4173 | lr 0.00019581 | gnorm 1.29 | clip 0 | train_wall 29 | wall 1545
2020-10-12 20:14:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 20:14:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 20:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17913.57421875Mb; avail=470759.39453125Mb
2020-10-12 20:14:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000720
2020-10-12 20:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005949
2020-10-12 20:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17913.57421875Mb; avail=470759.39453125Mb
2020-10-12 20:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000287
2020-10-12 20:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17913.57421875Mb; avail=470759.39453125Mb
2020-10-12 20:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098440
2020-10-12 20:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105439
2020-10-12 20:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17907.44921875Mb; avail=470765.30078125Mb
2020-10-12 20:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17907.40625Mb; avail=470765.54296875Mb
2020-10-12 20:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003709
2020-10-12 20:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17907.40625Mb; avail=470765.54296875Mb
2020-10-12 20:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 20:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17907.40625Mb; avail=470765.54296875Mb
2020-10-12 20:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096794
2020-10-12 20:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101418
2020-10-12 20:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17907.4921875Mb; avail=470765.6640625Mb
2020-10-12 20:14:44 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 20:14:52 | INFO | train_inner | epoch 040:     27 / 107 loss=5.928, nll_loss=4.601, ppl=24.27, wps=17922, ups=2.76, wpb=6491.8, bsz=243.8, num_updates=4200, lr=0.00019518, gnorm=1.306, clip=0, train_wall=27, wall=1553
2020-10-12 20:15:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:15:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19175.0859375Mb; avail=469498.01953125Mb
2020-10-12 20:15:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002021
2020-10-12 20:15:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19175.0859375Mb; avail=469498.01953125Mb
2020-10-12 20:15:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083368
2020-10-12 20:15:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19174.9296875Mb; avail=469497.9765625Mb
2020-10-12 20:15:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.097178
2020-10-12 20:15:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.183964
2020-10-12 20:15:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19174.98828125Mb; avail=469497.8984375Mb
2020-10-12 20:15:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19175.01953125Mb; avail=469497.8671875Mb
2020-10-12 20:15:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001975
2020-10-12 20:15:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19175.01953125Mb; avail=469497.8671875Mb
2020-10-12 20:15:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079458
2020-10-12 20:15:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19175.03515625Mb; avail=469497.8203125Mb
2020-10-12 20:15:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064733
2020-10-12 20:15:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.147171
2020-10-12 20:15:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19175.015625Mb; avail=469497.578125Mb
2020-10-12 20:15:17 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.749 | nll_loss 5.419 | ppl 42.77 | wps 63196.6 | wpb 2442.6 | bsz 92.5 | num_updates 4280 | best_loss 6.749
2020-10-12 20:15:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:15:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 40 @ 4280 updates, score 6.749) (writing took 8.40030102900073 seconds)
2020-10-12 20:15:25 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 20:15:25 | INFO | train | epoch 040 | loss 5.862 | nll_loss 4.524 | ppl 23.01 | wps 16874.1 | ups 2.62 | wpb 6450.5 | bsz 242.4 | num_updates 4280 | lr 0.000193347 | gnorm 1.339 | clip 0 | train_wall 29 | wall 1586
2020-10-12 20:15:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 20:15:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 20:15:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17887.55859375Mb; avail=470784.87109375Mb
2020-10-12 20:15:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000884
2020-10-12 20:15:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006042
2020-10-12 20:15:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17887.55859375Mb; avail=470784.87109375Mb
2020-10-12 20:15:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000241
2020-10-12 20:15:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17887.55859375Mb; avail=470784.87109375Mb
2020-10-12 20:15:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099109
2020-10-12 20:15:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106205
2020-10-12 20:15:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17881.80859375Mb; avail=470790.4140625Mb
2020-10-12 20:15:25 | INFO | fairseq_cli.train | done training in 1586.4 seconds
