2020-10-12 22:58:43 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:11571
2020-10-12 22:58:43 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:11571
2020-10-12 22:58:43 | INFO | fairseq.distributed_utils | initialized host ip-172-31-31-94 as rank 1
2020-10-12 22:58:43 | INFO | fairseq.distributed_utils | initialized host ip-172-31-31-94 as rank 0
2020-10-12 22:58:47 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_belukrLG_sepspm8000/O2M/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:11571', distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok='tgt', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='eng-bel,eng-ukr', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 22:58:47 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 22:58:47 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['bel', 'eng', 'ukr']
2020-10-12 22:58:47 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 22:58:47 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bel] dictionary: 21771 types
2020-10-12 22:58:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 21771 types
2020-10-12 22:58:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | [ukr] dictionary: 21771 types
2020-10-12 22:58:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 22:58:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20795.75Mb; avail=467693.265625Mb
2020-10-12 22:58:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 22:58:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:eng-bel': 1, 'main:eng-ukr': 1}
2020-10-12 22:58:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-bel src_langtok: 21768; tgt_langtok: None
2020-10-12 22:58:48 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belukrLG_sepspm8000/O2M/valid.eng-bel.eng
2020-10-12 22:58:48 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belukrLG_sepspm8000/O2M/valid.eng-bel.bel
2020-10-12 22:58:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belukrLG_sepspm8000/O2M/ valid eng-bel 248 examples
2020-10-12 22:58:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-ukr src_langtok: 21770; tgt_langtok: None
2020-10-12 22:58:48 | INFO | fairseq.data.data_utils | loaded 3060 examples from: fairseq/data-bin/ted_belukrLG_sepspm8000/O2M/valid.eng-ukr.eng
2020-10-12 22:58:48 | INFO | fairseq.data.data_utils | loaded 3060 examples from: fairseq/data-bin/ted_belukrLG_sepspm8000/O2M/valid.eng-ukr.ukr
2020-10-12 22:58:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belukrLG_sepspm8000/O2M/ valid eng-ukr 3060 examples
2020-10-12 22:58:48 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21771, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21771, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=21771, bias=False)
  )
)
2020-10-12 22:58:48 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 22:58:48 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 22:58:48 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 22:58:48 | INFO | fairseq_cli.train | num. model params: 42690048 (num. trained: 42690048)
2020-10-12 22:58:48 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 22:58:48 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 22:58:49 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************
2020-10-12 22:58:49 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 22:58:49 | INFO | fairseq.utils | rank   1: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 22:58:49 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************
2020-10-12 22:58:49 | INFO | fairseq_cli.train | training on 2 devices (GPUs/TPUs)
2020-10-12 22:58:49 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 22:58:49 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_last.pt
2020-10-12 22:58:49 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 22:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 22:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21034.01171875Mb; avail=467454.48046875Mb
2020-10-12 22:58:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 22:58:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:eng-bel': 1, 'main:eng-ukr': 1}
2020-10-12 22:58:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-bel src_langtok: 21768; tgt_langtok: None
2020-10-12 22:58:49 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belukrLG_sepspm8000/O2M/train.eng-bel.eng
2020-10-12 22:58:49 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belukrLG_sepspm8000/O2M/train.eng-bel.bel
2020-10-12 22:58:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belukrLG_sepspm8000/O2M/ train eng-bel 4509 examples
2020-10-12 22:58:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-ukr src_langtok: 21770; tgt_langtok: None
2020-10-12 22:58:49 | INFO | fairseq.data.data_utils | loaded 108463 examples from: fairseq/data-bin/ted_belukrLG_sepspm8000/O2M/train.eng-ukr.eng
2020-10-12 22:58:49 | INFO | fairseq.data.data_utils | loaded 108463 examples from: fairseq/data-bin/ted_belukrLG_sepspm8000/O2M/train.eng-ukr.ukr
2020-10-12 22:58:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belukrLG_sepspm8000/O2M/ train eng-ukr 108463 examples
2020-10-12 22:58:49 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 112972
2020-10-12 22:58:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:eng-bel', 4509), ('main:eng-ukr', 108463)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 22:58:49 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 22:58:49 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 112972
2020-10-12 22:58:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 112972; virtual dataset size 112972
2020-10-12 22:58:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 22:58:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:eng-bel': 4509, 'main:eng-ukr': 108463}; raw total size: 112972
2020-10-12 22:58:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:eng-bel': 4509, 'main:eng-ukr': 108463}; resampled total size: 112972
2020-10-12 22:58:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 22:58:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.012252
2020-10-12 22:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21036.1484375Mb; avail=467452.34375Mb
2020-10-12 22:58:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001846
2020-10-12 22:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.020025
2020-10-12 22:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21036.1484375Mb; avail=467452.34375Mb
2020-10-12 22:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000853
2020-10-12 22:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21036.1484375Mb; avail=467452.34375Mb
2020-10-12 22:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.399472
2020-10-12 22:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.421447
2020-10-12 22:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21040.1328125Mb; avail=467447.6796875Mb
2020-10-12 22:58:49 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 22:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21039.62890625Mb; avail=467449.02734375Mb
2020-10-12 22:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016290
2020-10-12 22:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21037.66015625Mb; avail=467450.99609375Mb
2020-10-12 22:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000820
2020-10-12 22:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21037.66015625Mb; avail=467450.99609375Mb
2020-10-12 22:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.399181
2020-10-12 22:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.417833
2020-10-12 22:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21040.234375Mb; avail=467448.6796875Mb
2020-10-12 22:58:49 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 22:59:24 | INFO | train_inner | epoch 001:    100 / 211 loss=14.207, nll_loss=14.101, ppl=17569.9, wps=44344.2, ups=3.07, wpb=14455.3, bsz=544.9, num_updates=100, lr=5.0975e-06, gnorm=3.752, clip=0, train_wall=33, wall=36
2020-10-12 22:59:56 | INFO | train_inner | epoch 001:    200 / 211 loss=12.808, nll_loss=12.538, ppl=5948.85, wps=44544.3, ups=3.15, wpb=14124.4, bsz=541.1, num_updates=200, lr=1.0095e-05, gnorm=1.433, clip=0, train_wall=31, wall=67
2020-10-12 22:59:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:59:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21222.71484375Mb; avail=467265.7421875Mb
2020-10-12 22:59:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001513
2020-10-12 22:59:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21222.71484375Mb; avail=467265.7421875Mb
2020-10-12 22:59:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047141
2020-10-12 22:59:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21222.71484375Mb; avail=467265.7421875Mb
2020-10-12 23:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033674
2020-10-12 23:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083106
2020-10-12 23:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21222.71484375Mb; avail=467265.7421875Mb
2020-10-12 23:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21222.71484375Mb; avail=467265.7421875Mb
2020-10-12 23:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000924
2020-10-12 23:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21222.71484375Mb; avail=467265.7421875Mb
2020-10-12 23:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047283
2020-10-12 23:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21222.71484375Mb; avail=467265.7421875Mb
2020-10-12 23:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033449
2020-10-12 23:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.082441
2020-10-12 23:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21222.24609375Mb; avail=467265.5Mb
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2020-10-12 23:00:02 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.14 | nll_loss 11.775 | ppl 3504.85 | wps 89288.4 | wpb 4554.7 | bsz 174.1 | num_updates 211
2020-10-12 23:00:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:00:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 1 @ 211 updates, score 12.14) (writing took 1.646193257998675 seconds)
2020-10-12 23:00:04 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 23:00:04 | INFO | train | epoch 001 | loss 13.466 | nll_loss 13.273 | ppl 9900.88 | wps 41800.5 | ups 2.94 | wpb 14249.3 | bsz 535.4 | num_updates 211 | lr 1.06447e-05 | gnorm 2.519 | clip 0 | train_wall 67 | wall 75
2020-10-12 23:00:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 23:00:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 23:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21295.95703125Mb; avail=467192.1796875Mb
2020-10-12 23:00:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003209
2020-10-12 23:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022907
2020-10-12 23:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21299.41015625Mb; avail=467188.546875Mb
2020-10-12 23:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000861
2020-10-12 23:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21299.41015625Mb; avail=467188.546875Mb
2020-10-12 23:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.403510
2020-10-12 23:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.428185
2020-10-12 23:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21323.1796875Mb; avail=467165.26953125Mb
2020-10-12 23:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21337.06640625Mb; avail=467151.3828125Mb
2020-10-12 23:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016489
2020-10-12 23:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21339.00390625Mb; avail=467149.4453125Mb
2020-10-12 23:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000870
2020-10-12 23:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21339.00390625Mb; avail=467149.4453125Mb
2020-10-12 23:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.404575
2020-10-12 23:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.422753
2020-10-12 23:00:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21339.95703125Mb; avail=467148.07421875Mb
2020-10-12 23:00:04 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 23:00:34 | INFO | train_inner | epoch 002:     89 / 211 loss=12.09, nll_loss=11.739, ppl=3417.94, wps=37301.9, ups=2.63, wpb=14187.2, bsz=505.8, num_updates=300, lr=1.50925e-05, gnorm=1.269, clip=0, train_wall=31, wall=105
2020-10-12 23:01:06 | INFO | train_inner | epoch 002:    189 / 211 loss=11.233, nll_loss=10.762, ppl=1735.99, wps=44365, ups=3.09, wpb=14335.6, bsz=553.5, num_updates=400, lr=2.009e-05, gnorm=1.324, clip=0, train_wall=32, wall=138
2020-10-12 23:01:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21278.14453125Mb; avail=467210.26171875Mb
2020-10-12 23:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001260
2020-10-12 23:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21278.14453125Mb; avail=467210.26171875Mb
2020-10-12 23:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048486
2020-10-12 23:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21278.14453125Mb; avail=467210.26171875Mb
2020-10-12 23:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034604
2020-10-12 23:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.085132
2020-10-12 23:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21278.05078125Mb; avail=467209.8984375Mb
2020-10-12 23:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21278.0390625Mb; avail=467210.01953125Mb
2020-10-12 23:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000872
2020-10-12 23:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21278.0390625Mb; avail=467210.01953125Mb
2020-10-12 23:01:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047582
2020-10-12 23:01:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21278.00390625Mb; avail=467210.26171875Mb
2020-10-12 23:01:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034119
2020-10-12 23:01:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083314
2020-10-12 23:01:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21278.0234375Mb; avail=467210.5546875Mb
2020-10-12 23:01:16 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.542 | nll_loss 9.923 | ppl 970.58 | wps 91761.2 | wpb 4554.7 | bsz 174.1 | num_updates 422 | best_loss 10.542
2020-10-12 23:01:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:01:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 2 @ 422 updates, score 10.542) (writing took 4.417173355999694 seconds)
2020-10-12 23:01:20 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 23:01:20 | INFO | train | epoch 002 | loss 11.537 | nll_loss 11.107 | ppl 2205.99 | wps 39207 | ups 2.75 | wpb 14249.3 | bsz 535.4 | num_updates 422 | lr 2.11895e-05 | gnorm 1.324 | clip 0 | train_wall 66 | wall 152
2020-10-12 23:01:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 23:01:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 23:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21347.13671875Mb; avail=467140.82421875Mb
2020-10-12 23:01:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002992
2020-10-12 23:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022157
2020-10-12 23:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21347.7421875Mb; avail=467140.21875Mb
2020-10-12 23:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000826
2020-10-12 23:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21347.7421875Mb; avail=467140.21875Mb
2020-10-12 23:01:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.408045
2020-10-12 23:01:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.431870
2020-10-12 23:01:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21349.109375Mb; avail=467138.88671875Mb
2020-10-12 23:01:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21348.6171875Mb; avail=467139.37890625Mb
2020-10-12 23:01:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015745
2020-10-12 23:01:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21348.6171875Mb; avail=467139.37890625Mb
2020-10-12 23:01:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000810
2020-10-12 23:01:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21348.6171875Mb; avail=467139.37890625Mb
2020-10-12 23:01:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.396949
2020-10-12 23:01:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.414334
2020-10-12 23:01:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21348.5078125Mb; avail=467139.5703125Mb
2020-10-12 23:01:21 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 23:01:47 | INFO | train_inner | epoch 003:     78 / 211 loss=10.678, nll_loss=10.098, ppl=1096.23, wps=34641.5, ups=2.43, wpb=14268.3, bsz=510, num_updates=500, lr=2.50875e-05, gnorm=1.054, clip=0, train_wall=31, wall=179
2020-10-12 23:02:19 | INFO | train_inner | epoch 003:    178 / 211 loss=10.348, nll_loss=9.689, ppl=825.49, wps=44865.5, ups=3.16, wpb=14204.7, bsz=549.4, num_updates=600, lr=3.0085e-05, gnorm=1.071, clip=0, train_wall=31, wall=211
2020-10-12 23:02:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21279.34375Mb; avail=467209.00390625Mb
2020-10-12 23:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001331
2020-10-12 23:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21279.34375Mb; avail=467209.00390625Mb
2020-10-12 23:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047491
2020-10-12 23:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21279.34375Mb; avail=467209.00390625Mb
2020-10-12 23:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034241
2020-10-12 23:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083838
2020-10-12 23:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21279.34375Mb; avail=467209.00390625Mb
2020-10-12 23:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21279.34375Mb; avail=467209.00390625Mb
2020-10-12 23:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000885
2020-10-12 23:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21279.34375Mb; avail=467209.00390625Mb
2020-10-12 23:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047576
2020-10-12 23:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21279.34375Mb; avail=467209.00390625Mb
2020-10-12 23:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033552
2020-10-12 23:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.082759
2020-10-12 23:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21279.34375Mb; avail=467209.00390625Mb
2020-10-12 23:02:32 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.006 | nll_loss 9.243 | ppl 605.73 | wps 87550.7 | wpb 4554.7 | bsz 174.1 | num_updates 633 | best_loss 10.006
2020-10-12 23:02:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:02:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 3 @ 633 updates, score 10.006) (writing took 4.424380710002879 seconds)
2020-10-12 23:02:37 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 23:02:37 | INFO | train | epoch 003 | loss 10.442 | nll_loss 9.805 | ppl 894.63 | wps 39377.4 | ups 2.76 | wpb 14249.3 | bsz 535.4 | num_updates 633 | lr 3.17342e-05 | gnorm 1.014 | clip 0 | train_wall 66 | wall 228
2020-10-12 23:02:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 23:02:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 23:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21356.4921875Mb; avail=467131.32421875Mb
2020-10-12 23:02:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002822
2020-10-12 23:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025223
2020-10-12 23:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21357.09765625Mb; avail=467130.71875Mb
2020-10-12 23:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000950
2020-10-12 23:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21357.09765625Mb; avail=467130.71875Mb
2020-10-12 23:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.408609
2020-10-12 23:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.435697
2020-10-12 23:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21359.1953125Mb; avail=467128.87109375Mb
2020-10-12 23:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21356.65234375Mb; avail=467131.453125Mb
2020-10-12 23:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015763
2020-10-12 23:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21356.65234375Mb; avail=467131.453125Mb
2020-10-12 23:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000872
2020-10-12 23:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21356.65234375Mb; avail=467131.453125Mb
2020-10-12 23:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.401640
2020-10-12 23:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.419139
2020-10-12 23:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21356.3359375Mb; avail=467131.57421875Mb
2020-10-12 23:02:37 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 23:03:00 | INFO | train_inner | epoch 004:     67 / 211 loss=10.23, nll_loss=9.534, ppl=741.18, wps=34403.9, ups=2.43, wpb=14183, bsz=545.2, num_updates=700, lr=3.50825e-05, gnorm=1.148, clip=0, train_wall=31, wall=252
2020-10-12 23:03:32 | INFO | train_inner | epoch 004:    167 / 211 loss=10.087, nll_loss=9.365, ppl=659.53, wps=45434, ups=3.16, wpb=14360.7, bsz=538.4, num_updates=800, lr=4.008e-05, gnorm=0.988, clip=0, train_wall=31, wall=283
2020-10-12 23:03:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21286.75Mb; avail=467201.93359375Mb
2020-10-12 23:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001362
2020-10-12 23:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21286.75Mb; avail=467201.93359375Mb
2020-10-12 23:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048121
2020-10-12 23:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21286.75Mb; avail=467201.93359375Mb
2020-10-12 23:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033654
2020-10-12 23:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083938
2020-10-12 23:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21286.75Mb; avail=467201.93359375Mb
2020-10-12 23:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21286.75Mb; avail=467201.93359375Mb
2020-10-12 23:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000943
2020-10-12 23:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21286.75Mb; avail=467201.93359375Mb
2020-10-12 23:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047817
2020-10-12 23:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21286.75Mb; avail=467201.93359375Mb
2020-10-12 23:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033907
2020-10-12 23:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083416
2020-10-12 23:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21286.75Mb; avail=467201.93359375Mb
2020-10-12 23:03:48 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.745 | nll_loss 8.95 | ppl 494.55 | wps 90419.6 | wpb 4554.7 | bsz 174.1 | num_updates 844 | best_loss 9.745
2020-10-12 23:03:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:03:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 4 @ 844 updates, score 9.745) (writing took 4.418457995998324 seconds)
2020-10-12 23:03:53 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 23:03:53 | INFO | train | epoch 004 | loss 10.116 | nll_loss 9.398 | ppl 674.71 | wps 39439.9 | ups 2.77 | wpb 14249.3 | bsz 535.4 | num_updates 844 | lr 4.22789e-05 | gnorm 1.047 | clip 0 | train_wall 66 | wall 304
2020-10-12 23:03:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 23:03:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 23:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21363.08984375Mb; avail=467124.953125Mb
2020-10-12 23:03:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002831
2020-10-12 23:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025248
2020-10-12 23:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21363.08984375Mb; avail=467124.953125Mb
2020-10-12 23:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000910
2020-10-12 23:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21363.08984375Mb; avail=467124.953125Mb
2020-10-12 23:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.403905
2020-10-12 23:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.430888
2020-10-12 23:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21365.98046875Mb; avail=467122.1484375Mb
2020-10-12 23:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21363.98046875Mb; avail=467124.72265625Mb
2020-10-12 23:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015817
2020-10-12 23:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21363.98046875Mb; avail=467124.72265625Mb
2020-10-12 23:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000813
2020-10-12 23:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21363.98046875Mb; avail=467124.72265625Mb
2020-10-12 23:03:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.401035
2020-10-12 23:03:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.418501
2020-10-12 23:03:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21090.80859375Mb; avail=467397.9140625Mb
2020-10-12 23:03:54 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 23:04:13 | INFO | train_inner | epoch 005:     56 / 211 loss=10.007, nll_loss=9.268, ppl=616.55, wps=34734.6, ups=2.45, wpb=14184.8, bsz=506.6, num_updates=900, lr=4.50775e-05, gnorm=1.068, clip=0, train_wall=31, wall=324
2020-10-12 23:04:45 | INFO | train_inner | epoch 005:    156 / 211 loss=9.893, nll_loss=9.137, ppl=563.13, wps=44624.2, ups=3.15, wpb=14182.5, bsz=557, num_updates=1000, lr=5.0075e-05, gnorm=1.097, clip=0, train_wall=31, wall=356
2020-10-12 23:05:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21306.046875Mb; avail=467182.19140625Mb
2020-10-12 23:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001412
2020-10-12 23:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21306.046875Mb; avail=467182.19140625Mb
2020-10-12 23:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048120
2020-10-12 23:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21306.04296875Mb; avail=467181.94921875Mb
2020-10-12 23:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033673
2020-10-12 23:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084028
2020-10-12 23:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21305.921875Mb; avail=467182.3203125Mb
2020-10-12 23:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21305.921875Mb; avail=467182.3203125Mb
2020-10-12 23:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000951
2020-10-12 23:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21305.921875Mb; avail=467182.3203125Mb
2020-10-12 23:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047241
2020-10-12 23:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21305.921875Mb; avail=467182.3203125Mb
2020-10-12 23:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033132
2020-10-12 23:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.082127
2020-10-12 23:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21305.921875Mb; avail=467182.3203125Mb
2020-10-12 23:05:05 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.521 | nll_loss 8.69 | ppl 412.92 | wps 87517.2 | wpb 4554.7 | bsz 174.1 | num_updates 1055 | best_loss 9.521
2020-10-12 23:05:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:05:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 5 @ 1055 updates, score 9.521) (writing took 4.422462938000535 seconds)
2020-10-12 23:05:09 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 23:05:09 | INFO | train | epoch 005 | loss 9.895 | nll_loss 9.138 | ppl 563.54 | wps 39442.2 | ups 2.77 | wpb 14249.3 | bsz 535.4 | num_updates 1055 | lr 5.28236e-05 | gnorm 1.108 | clip 0 | train_wall 66 | wall 381
2020-10-12 23:05:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 23:05:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 23:05:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21377.4921875Mb; avail=467110.84375Mb
2020-10-12 23:05:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002771
2020-10-12 23:05:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025033
2020-10-12 23:05:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21377.4921875Mb; avail=467110.84375Mb
2020-10-12 23:05:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000900
2020-10-12 23:05:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21377.4921875Mb; avail=467110.84375Mb
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.451613
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.478498
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21099.73828125Mb; avail=467388.765625Mb
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21097.76953125Mb; avail=467390.734375Mb
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015725
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21097.76953125Mb; avail=467390.734375Mb
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000804
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21097.76953125Mb; avail=467390.734375Mb
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.399366
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.416731
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21118.796875Mb; avail=467370.21484375Mb
2020-10-12 23:05:10 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 23:05:26 | INFO | train_inner | epoch 006:     45 / 211 loss=9.778, nll_loss=9.003, ppl=513.02, wps=34727.4, ups=2.43, wpb=14313.3, bsz=522, num_updates=1100, lr=5.50725e-05, gnorm=1.034, clip=0, train_wall=31, wall=397
2020-10-12 23:05:58 | INFO | train_inner | epoch 006:    145 / 211 loss=9.697, nll_loss=8.907, ppl=480.15, wps=44590.5, ups=3.15, wpb=14148.2, bsz=534.8, num_updates=1200, lr=6.007e-05, gnorm=1.157, clip=0, train_wall=31, wall=429
2020-10-12 23:06:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21310.14453125Mb; avail=467178.765625Mb
2020-10-12 23:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001343
2020-10-12 23:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21310.14453125Mb; avail=467178.765625Mb
2020-10-12 23:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.046975
2020-10-12 23:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21310.14453125Mb; avail=467178.765625Mb
2020-10-12 23:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033867
2020-10-12 23:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.082993
2020-10-12 23:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21310.14453125Mb; avail=467178.765625Mb
2020-10-12 23:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21310.0625Mb; avail=467178.765625Mb
2020-10-12 23:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000947
2020-10-12 23:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21310.0625Mb; avail=467178.765625Mb
2020-10-12 23:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048272
2020-10-12 23:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21309.5859375Mb; avail=467179.4921875Mb
2020-10-12 23:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033851
2020-10-12 23:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083809
2020-10-12 23:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21309.09375Mb; avail=467179.4921875Mb
2020-10-12 23:06:21 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.268 | nll_loss 8.388 | ppl 334.92 | wps 89551.3 | wpb 4554.7 | bsz 174.1 | num_updates 1266 | best_loss 9.268
2020-10-12 23:06:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:06:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 6 @ 1266 updates, score 9.268) (writing took 4.422366724997119 seconds)
2020-10-12 23:06:25 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 23:06:25 | INFO | train | epoch 006 | loss 9.689 | nll_loss 8.898 | ppl 477.16 | wps 39350.9 | ups 2.76 | wpb 14249.3 | bsz 535.4 | num_updates 1266 | lr 6.33684e-05 | gnorm 1.081 | clip 0 | train_wall 66 | wall 457
2020-10-12 23:06:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 23:06:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 23:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21101.953125Mb; avail=467387.328125Mb
2020-10-12 23:06:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003307
2020-10-12 23:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023772
2020-10-12 23:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21102.55859375Mb; avail=467386.72265625Mb
2020-10-12 23:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000897
2020-10-12 23:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21102.55859375Mb; avail=467386.72265625Mb
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.478278
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.503965
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21137.1640625Mb; avail=467351.8828125Mb
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21143.46875Mb; avail=467345.578125Mb
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.018591
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21150.37109375Mb; avail=467338.67578125Mb
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000804
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21150.37109375Mb; avail=467338.67578125Mb
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.399111
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.419380
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21269.91796875Mb; avail=467218.9375Mb
2020-10-12 23:06:26 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 23:06:39 | INFO | train_inner | epoch 007:     34 / 211 loss=9.612, nll_loss=8.809, ppl=448.38, wps=34636.1, ups=2.42, wpb=14296.1, bsz=545.3, num_updates=1300, lr=6.50675e-05, gnorm=1.022, clip=0, train_wall=31, wall=470
2020-10-12 23:07:11 | INFO | train_inner | epoch 007:    134 / 211 loss=9.463, nll_loss=8.638, ppl=398.5, wps=45395.7, ups=3.15, wpb=14420.8, bsz=564.6, num_updates=1400, lr=7.0065e-05, gnorm=1.134, clip=0, train_wall=31, wall=502
2020-10-12 23:07:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:07:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21316.3125Mb; avail=467172.18359375Mb
2020-10-12 23:07:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001305
2020-10-12 23:07:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21316.3125Mb; avail=467172.18359375Mb
2020-10-12 23:07:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048327
2020-10-12 23:07:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21316.34375Mb; avail=467172.0625Mb
2020-10-12 23:07:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034545
2020-10-12 23:07:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.085002
2020-10-12 23:07:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21316.30859375Mb; avail=467171.578125Mb
2020-10-12 23:07:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21316.42578125Mb; avail=467172.0625Mb
2020-10-12 23:07:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000926
2020-10-12 23:07:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21316.42578125Mb; avail=467172.0625Mb
2020-10-12 23:07:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047301
2020-10-12 23:07:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21316.42578125Mb; avail=467172.0625Mb
2020-10-12 23:07:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034332
2020-10-12 23:07:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083318
2020-10-12 23:07:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21316.42578125Mb; avail=467172.0625Mb
2020-10-12 23:07:38 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.999 | nll_loss 8.089 | ppl 272.22 | wps 87674.3 | wpb 4554.7 | bsz 174.1 | num_updates 1477 | best_loss 8.999
2020-10-12 23:07:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:07:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 7 @ 1477 updates, score 8.999) (writing took 4.433773974997166 seconds)
2020-10-12 23:07:42 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 23:07:42 | INFO | train | epoch 007 | loss 9.452 | nll_loss 8.626 | ppl 395.01 | wps 39296 | ups 2.76 | wpb 14249.3 | bsz 535.4 | num_updates 1477 | lr 7.39131e-05 | gnorm 1.128 | clip 0 | train_wall 66 | wall 533
2020-10-12 23:07:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 23:07:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 23:07:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21387.66015625Mb; avail=467100.33984375Mb
2020-10-12 23:07:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003751
2020-10-12 23:07:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025274
2020-10-12 23:07:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21389.4765625Mb; avail=467098.5234375Mb
2020-10-12 23:07:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000915
2020-10-12 23:07:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21389.4765625Mb; avail=467098.5234375Mb
2020-10-12 23:07:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.404835
2020-10-12 23:07:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.431835
2020-10-12 23:07:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21389.6953125Mb; avail=467098.0390625Mb
2020-10-12 23:07:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21389.6953125Mb; avail=467098.0390625Mb
2020-10-12 23:07:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015704
2020-10-12 23:07:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21389.6953125Mb; avail=467098.0390625Mb
2020-10-12 23:07:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000810
2020-10-12 23:07:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21389.6953125Mb; avail=467098.0390625Mb
2020-10-12 23:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.393657
2020-10-12 23:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.410938
2020-10-12 23:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21389.640625Mb; avail=467098.16015625Mb
2020-10-12 23:07:43 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 23:07:52 | INFO | train_inner | epoch 008:     23 / 211 loss=9.371, nll_loss=8.532, ppl=370.22, wps=34510.2, ups=2.44, wpb=14158.7, bsz=520.3, num_updates=1500, lr=7.50625e-05, gnorm=1.15, clip=0, train_wall=31, wall=543
2020-10-12 23:08:23 | INFO | train_inner | epoch 008:    123 / 211 loss=9.218, nll_loss=8.359, ppl=328.34, wps=45056, ups=3.15, wpb=14314.7, bsz=548.2, num_updates=1600, lr=8.006e-05, gnorm=1.024, clip=0, train_wall=31, wall=575
2020-10-12 23:08:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21329.34375Mb; avail=467159.2421875Mb
2020-10-12 23:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001419
2020-10-12 23:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21329.34375Mb; avail=467159.2421875Mb
2020-10-12 23:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048023
2020-10-12 23:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21329.34375Mb; avail=467159.2421875Mb
2020-10-12 23:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033831
2020-10-12 23:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084087
2020-10-12 23:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21329.34375Mb; avail=467159.2421875Mb
2020-10-12 23:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21329.34375Mb; avail=467159.2421875Mb
2020-10-12 23:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000941
2020-10-12 23:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21329.34375Mb; avail=467159.2421875Mb
2020-10-12 23:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047412
2020-10-12 23:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21329.2734375Mb; avail=467159.0Mb
2020-10-12 23:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034037
2020-10-12 23:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083151
2020-10-12 23:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21329.2109375Mb; avail=467159.36328125Mb
2020-10-12 23:08:54 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.767 | nll_loss 7.815 | ppl 225.15 | wps 90252.1 | wpb 4554.7 | bsz 174.1 | num_updates 1688 | best_loss 8.767
2020-10-12 23:08:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:08:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 8 @ 1688 updates, score 8.767) (writing took 4.440430100999947 seconds)
2020-10-12 23:08:58 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 23:08:58 | INFO | train | epoch 008 | loss 9.193 | nll_loss 8.329 | ppl 321.57 | wps 39307.8 | ups 2.76 | wpb 14249.3 | bsz 535.4 | num_updates 1688 | lr 8.44578e-05 | gnorm 1.064 | clip 0 | train_wall 66 | wall 610
2020-10-12 23:08:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 23:08:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 23:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21398.4140625Mb; avail=467089.95703125Mb
2020-10-12 23:08:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003640
2020-10-12 23:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025318
2020-10-12 23:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21399.625Mb; avail=467088.74609375Mb
2020-10-12 23:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001021
2020-10-12 23:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21399.625Mb; avail=467088.74609375Mb
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.410544
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.437842
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21118.3203125Mb; avail=467370.83203125Mb
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21118.3203125Mb; avail=467370.83203125Mb
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015830
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21118.19921875Mb; avail=467370.953125Mb
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000810
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21118.19921875Mb; avail=467370.953125Mb
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.400566
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.418049
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21150.73828125Mb; avail=467338.265625Mb
2020-10-12 23:08:59 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 23:09:05 | INFO | train_inner | epoch 009:     12 / 211 loss=9.125, nll_loss=8.251, ppl=304.6, wps=34092.1, ups=2.42, wpb=14104.1, bsz=513.6, num_updates=1700, lr=8.50575e-05, gnorm=1.117, clip=0, train_wall=31, wall=616
2020-10-12 23:09:36 | INFO | train_inner | epoch 009:    112 / 211 loss=9.012, nll_loss=8.12, ppl=278.29, wps=44692.4, ups=3.15, wpb=14175.1, bsz=524.2, num_updates=1800, lr=9.0055e-05, gnorm=1.095, clip=0, train_wall=31, wall=648
2020-10-12 23:10:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:10:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21418.37890625Mb; avail=467070.625Mb
2020-10-12 23:10:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001629
2020-10-12 23:10:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21418.984375Mb; avail=467070.01953125Mb
2020-10-12 23:10:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048024
2020-10-12 23:10:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21437.8828125Mb; avail=467051.1171875Mb
2020-10-12 23:10:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.037313
2020-10-12 23:10:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087942
2020-10-12 23:10:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21445.75390625Mb; avail=467043.24609375Mb
2020-10-12 23:10:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21452.4140625Mb; avail=467036.5859375Mb
2020-10-12 23:10:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001126
2020-10-12 23:10:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21453.01953125Mb; avail=467035.98046875Mb
2020-10-12 23:10:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049249
2020-10-12 23:10:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21465.32421875Mb; avail=467023.87109375Mb
2020-10-12 23:10:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036110
2020-10-12 23:10:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087434
2020-10-12 23:10:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21473.9140625Mb; avail=467015.28125Mb
2020-10-12 23:10:11 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.54 | nll_loss 7.551 | ppl 187.53 | wps 90332.9 | wpb 4554.7 | bsz 174.1 | num_updates 1899 | best_loss 8.54
2020-10-12 23:10:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:10:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 9 @ 1899 updates, score 8.54) (writing took 8.824386108997714 seconds)
2020-10-12 23:10:19 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 23:10:19 | INFO | train | epoch 009 | loss 8.963 | nll_loss 8.063 | ppl 267.47 | wps 37140.9 | ups 2.61 | wpb 14249.3 | bsz 535.4 | num_updates 1899 | lr 9.50025e-05 | gnorm 1.133 | clip 0 | train_wall 65 | wall 691
2020-10-12 23:10:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 23:10:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 23:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21635.7734375Mb; avail=466853.83203125Mb
2020-10-12 23:10:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006660
2020-10-12 23:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.040150
2020-10-12 23:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21642.0546875Mb; avail=466847.55078125Mb
2020-10-12 23:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001595
2020-10-12 23:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21642.66015625Mb; avail=466846.9453125Mb
2020-10-12 23:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.515262
2020-10-12 23:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.558782
2020-10-12 23:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21777.171875Mb; avail=466712.48828125Mb
2020-10-12 23:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21789.6640625Mb; avail=466699.7734375Mb
2020-10-12 23:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.017358
2020-10-12 23:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21794.75390625Mb; avail=466694.56640625Mb
2020-10-12 23:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000949
2020-10-12 23:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21794.75390625Mb; avail=466694.56640625Mb
2020-10-12 23:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.394840
2020-10-12 23:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.414041
2020-10-12 23:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21817.36328125Mb; avail=466671.4375Mb
2020-10-12 23:10:20 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 23:10:22 | INFO | train_inner | epoch 010:      1 / 211 loss=8.901, nll_loss=7.99, ppl=254.23, wps=31502.2, ups=2.2, wpb=14295.2, bsz=545, num_updates=1900, lr=9.50525e-05, gnorm=1.164, clip=0, train_wall=31, wall=693
2020-10-12 23:10:54 | INFO | train_inner | epoch 010:    101 / 211 loss=8.776, nll_loss=7.847, ppl=230.28, wps=43648.5, ups=3.12, wpb=14005.8, bsz=544.7, num_updates=2000, lr=0.00010005, gnorm=1.121, clip=0, train_wall=31, wall=725
2020-10-12 23:11:26 | INFO | train_inner | epoch 010:    201 / 211 loss=8.738, nll_loss=7.8, ppl=222.87, wps=45634.5, ups=3.13, wpb=14600.8, bsz=527.6, num_updates=2100, lr=0.000105048, gnorm=1.024, clip=0, train_wall=31, wall=757
2020-10-12 23:11:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21320.51953125Mb; avail=467168.24609375Mb
2020-10-12 23:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002025
2020-10-12 23:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21320.51953125Mb; avail=467168.24609375Mb
2020-10-12 23:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.053807
2020-10-12 23:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21320.53515625Mb; avail=467168.125Mb
2020-10-12 23:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033704
2020-10-12 23:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.090608
2020-10-12 23:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21320.7109375Mb; avail=467168.3671875Mb
2020-10-12 23:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21320.53515625Mb; avail=467168.25390625Mb
2020-10-12 23:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000892
2020-10-12 23:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21320.53515625Mb; avail=467168.25390625Mb
2020-10-12 23:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047405
2020-10-12 23:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21320.53515625Mb; avail=467168.25390625Mb
2020-10-12 23:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033570
2020-10-12 23:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.082623
2020-10-12 23:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21320.53515625Mb; avail=467168.25390625Mb
2020-10-12 23:11:32 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.331 | nll_loss 7.3 | ppl 157.57 | wps 91437.2 | wpb 4554.7 | bsz 174.1 | num_updates 2110 | best_loss 8.331
2020-10-12 23:11:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:11:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 10 @ 2110 updates, score 8.331) (writing took 5.210500876997685 seconds)
2020-10-12 23:11:37 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 23:11:37 | INFO | train | epoch 010 | loss 8.752 | nll_loss 7.818 | ppl 225.7 | wps 38839.5 | ups 2.73 | wpb 14249.3 | bsz 535.4 | num_updates 2110 | lr 0.000105547 | gnorm 1.08 | clip 0 | train_wall 66 | wall 768
2020-10-12 23:11:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 23:11:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 23:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21790.69921875Mb; avail=466698.62890625Mb
2020-10-12 23:11:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003232
2020-10-12 23:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022067
2020-10-12 23:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21789.93359375Mb; avail=466699.28125Mb
2020-10-12 23:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000833
2020-10-12 23:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21789.93359375Mb; avail=466699.28125Mb
2020-10-12 23:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.410291
2020-10-12 23:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.434039
2020-10-12 23:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21854.546875Mb; avail=466633.734375Mb
2020-10-12 23:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21854.50390625Mb; avail=466633.25Mb
2020-10-12 23:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016159
2020-10-12 23:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21854.66796875Mb; avail=466633.734375Mb
2020-10-12 23:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000862
2020-10-12 23:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21854.66796875Mb; avail=466633.734375Mb
2020-10-12 23:11:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.404106
2020-10-12 23:11:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.422225
2020-10-12 23:11:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21854.62890625Mb; avail=466633.76171875Mb
2020-10-12 23:11:38 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 23:12:08 | INFO | train_inner | epoch 011:     90 / 211 loss=8.62, nll_loss=7.664, ppl=202.87, wps=33838.3, ups=2.38, wpb=14234.7, bsz=535.5, num_updates=2200, lr=0.000110045, gnorm=1.101, clip=0, train_wall=31, wall=799
2020-10-12 23:12:40 | INFO | train_inner | epoch 011:    190 / 211 loss=8.52, nll_loss=7.549, ppl=187.3, wps=44454.6, ups=3.14, wpb=14179.8, bsz=534.2, num_updates=2300, lr=0.000115043, gnorm=1.119, clip=0, train_wall=31, wall=831
2020-10-12 23:12:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21323.703125Mb; avail=467164.5703125Mb
2020-10-12 23:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001339
2020-10-12 23:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21323.703125Mb; avail=467164.5703125Mb
2020-10-12 23:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048026
2020-10-12 23:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21323.8203125Mb; avail=467164.0859375Mb
2020-10-12 23:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035209
2020-10-12 23:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.085404
2020-10-12 23:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21323.79296875Mb; avail=467164.20703125Mb
2020-10-12 23:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21323.79296875Mb; avail=467164.20703125Mb
2020-10-12 23:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000990
2020-10-12 23:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21323.79296875Mb; avail=467164.20703125Mb
2020-10-12 23:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047279
2020-10-12 23:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21323.80859375Mb; avail=467164.0859375Mb
2020-10-12 23:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035007
2020-10-12 23:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084062
2020-10-12 23:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21323.80078125Mb; avail=467164.09375Mb
2020-10-12 23:12:49 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.119 | nll_loss 7.043 | ppl 131.85 | wps 86136.7 | wpb 4554.7 | bsz 174.1 | num_updates 2321 | best_loss 8.119
2020-10-12 23:12:49 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:12:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 11 @ 2321 updates, score 8.119) (writing took 5.3512534810033685 seconds)
2020-10-12 23:12:55 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 23:12:55 | INFO | train | epoch 011 | loss 8.553 | nll_loss 7.588 | ppl 192.34 | wps 38666.5 | ups 2.71 | wpb 14249.3 | bsz 535.4 | num_updates 2321 | lr 0.000116092 | gnorm 1.096 | clip 0 | train_wall 66 | wall 846
2020-10-12 23:12:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 23:12:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 23:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21852.5078125Mb; avail=466635.90234375Mb
2020-10-12 23:12:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003673
2020-10-12 23:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025094
2020-10-12 23:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21854.33203125Mb; avail=466634.078125Mb
2020-10-12 23:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001014
2020-10-12 23:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21854.33203125Mb; avail=466634.078125Mb
2020-10-12 23:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.454308
2020-10-12 23:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.481337
2020-10-12 23:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21855.83203125Mb; avail=466632.63671875Mb
2020-10-12 23:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21855.02734375Mb; avail=466633.37890625Mb
2020-10-12 23:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.018275
2020-10-12 23:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21855.02734375Mb; avail=466633.37890625Mb
2020-10-12 23:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000915
2020-10-12 23:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21855.02734375Mb; avail=466633.37890625Mb
2020-10-12 23:12:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.452426
2020-10-12 23:12:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.472577
2020-10-12 23:12:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21855.01171875Mb; avail=466633.84375Mb
2020-10-12 23:12:56 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 23:13:22 | INFO | train_inner | epoch 012:     79 / 211 loss=8.409, nll_loss=7.422, ppl=171.53, wps=33921.4, ups=2.37, wpb=14307.6, bsz=543.8, num_updates=2400, lr=0.00012004, gnorm=1.031, clip=0, train_wall=31, wall=874
2020-10-12 23:13:54 | INFO | train_inner | epoch 012:    179 / 211 loss=8.284, nll_loss=7.277, ppl=155.11, wps=44562.7, ups=3.14, wpb=14209.9, bsz=533.7, num_updates=2500, lr=0.000125037, gnorm=1.076, clip=0, train_wall=31, wall=905
2020-10-12 23:14:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21322.328125Mb; avail=467164.9140625Mb
2020-10-12 23:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001413
2020-10-12 23:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21322.328125Mb; avail=467164.9140625Mb
2020-10-12 23:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047988
2020-10-12 23:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21322.328125Mb; avail=467164.9140625Mb
2020-10-12 23:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035325
2020-10-12 23:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.085551
2020-10-12 23:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21322.328125Mb; avail=467164.9140625Mb
2020-10-12 23:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21322.328125Mb; avail=467164.9140625Mb
2020-10-12 23:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000944
2020-10-12 23:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21322.328125Mb; avail=467164.9140625Mb
2020-10-12 23:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047598
2020-10-12 23:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21322.1015625Mb; avail=467164.921875Mb
2020-10-12 23:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034854
2020-10-12 23:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084181
2020-10-12 23:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21322.12109375Mb; avail=467164.6796875Mb
2020-10-12 23:14:07 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.871 | nll_loss 6.759 | ppl 108.31 | wps 75287 | wpb 4554.7 | bsz 174.1 | num_updates 2532 | best_loss 7.871
2020-10-12 23:14:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:14:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 12 @ 2532 updates, score 7.871) (writing took 5.853463279003336 seconds)
2020-10-12 23:14:13 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 23:14:13 | INFO | train | epoch 012 | loss 8.325 | nll_loss 7.326 | ppl 160.4 | wps 38452.7 | ups 2.7 | wpb 14249.3 | bsz 535.4 | num_updates 2532 | lr 0.000126637 | gnorm 1.065 | clip 0 | train_wall 66 | wall 924
2020-10-12 23:14:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 23:14:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21853.2578125Mb; avail=466635.27734375Mb
2020-10-12 23:14:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003501
2020-10-12 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026168
2020-10-12 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21856.28515625Mb; avail=466632.25Mb
2020-10-12 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000897
2020-10-12 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21856.28515625Mb; avail=466632.25Mb
2020-10-12 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.512663
2020-10-12 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.540828
2020-10-12 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21859.9765625Mb; avail=466628.3984375Mb
2020-10-12 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21857.0234375Mb; avail=466631.3515625Mb
2020-10-12 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.018033
2020-10-12 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21857.0234375Mb; avail=466631.3515625Mb
2020-10-12 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000929
2020-10-12 23:14:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21857.0234375Mb; avail=466631.3515625Mb
2020-10-12 23:14:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.433138
2020-10-12 23:14:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.452968
2020-10-12 23:14:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21280.87890625Mb; avail=467207.1484375Mb
2020-10-12 23:14:14 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 23:14:37 | INFO | train_inner | epoch 013:     68 / 211 loss=8.204, nll_loss=7.186, ppl=145.64, wps=32969.1, ups=2.35, wpb=14022.1, bsz=497.6, num_updates=2600, lr=0.000130035, gnorm=1.074, clip=0, train_wall=31, wall=948
2020-10-12 23:15:08 | INFO | train_inner | epoch 013:    168 / 211 loss=8.064, nll_loss=7.025, ppl=130.24, wps=45178.7, ups=3.14, wpb=14365.3, bsz=555.8, num_updates=2700, lr=0.000135032, gnorm=1.036, clip=0, train_wall=31, wall=980
2020-10-12 23:15:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21315.296875Mb; avail=467172.9921875Mb
2020-10-12 23:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001420
2020-10-12 23:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21315.296875Mb; avail=467172.9921875Mb
2020-10-12 23:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047159
2020-10-12 23:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21315.19921875Mb; avail=467173.234375Mb
2020-10-12 23:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033563
2020-10-12 23:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.082938
2020-10-12 23:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21315.19921875Mb; avail=467173.234375Mb
2020-10-12 23:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21315.19921875Mb; avail=467173.234375Mb
2020-10-12 23:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000957
2020-10-12 23:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21315.19921875Mb; avail=467173.234375Mb
2020-10-12 23:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047812
2020-10-12 23:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21315.01953125Mb; avail=467173.0Mb
2020-10-12 23:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033567
2020-10-12 23:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083104
2020-10-12 23:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21314.86328125Mb; avail=467173.60546875Mb
2020-10-12 23:15:25 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.689 | nll_loss 6.543 | ppl 93.26 | wps 80514.5 | wpb 4554.7 | bsz 174.1 | num_updates 2743 | best_loss 7.689
2020-10-12 23:15:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:15:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 13 @ 2743 updates, score 7.689) (writing took 5.575130053002795 seconds)
2020-10-12 23:15:30 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 23:15:30 | INFO | train | epoch 013 | loss 8.092 | nll_loss 7.057 | ppl 133.17 | wps 38829.8 | ups 2.73 | wpb 14249.3 | bsz 535.4 | num_updates 2743 | lr 0.000137181 | gnorm 1.064 | clip 0 | train_wall 65 | wall 1002
2020-10-12 23:15:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 23:15:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 23:15:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21859.86328125Mb; avail=466628.5546875Mb
2020-10-12 23:15:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002818
2020-10-12 23:15:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025241
2020-10-12 23:15:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21859.40234375Mb; avail=466629.015625Mb
2020-10-12 23:15:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000939
2020-10-12 23:15:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21859.40234375Mb; avail=466629.015625Mb
2020-10-12 23:15:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.455686
2020-10-12 23:15:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.482893
2020-10-12 23:15:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21859.015625Mb; avail=466629.6171875Mb
2020-10-12 23:15:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21859.015625Mb; avail=466629.6171875Mb
2020-10-12 23:15:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.018049
2020-10-12 23:15:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21859.015625Mb; avail=466629.6171875Mb
2020-10-12 23:15:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000979
2020-10-12 23:15:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21859.015625Mb; avail=466629.6171875Mb
2020-10-12 23:15:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.454209
2020-10-12 23:15:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.474247
2020-10-12 23:15:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21859.06640625Mb; avail=466629.44921875Mb
2020-10-12 23:15:31 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 23:15:51 | INFO | train_inner | epoch 014:     57 / 211 loss=7.983, nll_loss=6.931, ppl=121.99, wps=33651.6, ups=2.36, wpb=14276.2, bsz=539.6, num_updates=2800, lr=0.00014003, gnorm=1.146, clip=0, train_wall=31, wall=1022
2020-10-12 23:16:22 | INFO | train_inner | epoch 014:    157 / 211 loss=7.848, nll_loss=6.775, ppl=109.55, wps=45423.4, ups=3.17, wpb=14317.7, bsz=541.3, num_updates=2900, lr=0.000145028, gnorm=1.125, clip=0, train_wall=31, wall=1054
2020-10-12 23:16:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21320.3203125Mb; avail=467167.58984375Mb
2020-10-12 23:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001380
2020-10-12 23:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21320.3203125Mb; avail=467167.58984375Mb
2020-10-12 23:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.046515
2020-10-12 23:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21320.25390625Mb; avail=467167.83203125Mb
2020-10-12 23:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033239
2020-10-12 23:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.081923
2020-10-12 23:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21320.25390625Mb; avail=467167.83203125Mb
2020-10-12 23:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21320.25390625Mb; avail=467167.83203125Mb
2020-10-12 23:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000898
2020-10-12 23:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21320.25390625Mb; avail=467167.83203125Mb
2020-10-12 23:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049458
2020-10-12 23:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21320.25Mb; avail=467167.8359375Mb
2020-10-12 23:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033641
2020-10-12 23:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084888
2020-10-12 23:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21320.19921875Mb; avail=467168.078125Mb
2020-10-12 23:16:42 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.406 | nll_loss 6.207 | ppl 73.86 | wps 90952.6 | wpb 4554.7 | bsz 174.1 | num_updates 2954 | best_loss 7.406
2020-10-12 23:16:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:16:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 14 @ 2954 updates, score 7.406) (writing took 4.896175039000809 seconds)
2020-10-12 23:16:47 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 23:16:47 | INFO | train | epoch 014 | loss 7.859 | nll_loss 6.788 | ppl 110.51 | wps 39321.1 | ups 2.76 | wpb 14249.3 | bsz 535.4 | num_updates 2954 | lr 0.000147726 | gnorm 1.114 | clip 0 | train_wall 65 | wall 1078
2020-10-12 23:16:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 23:16:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 23:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21750.71875Mb; avail=466738.52734375Mb
2020-10-12 23:16:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003156
2020-10-12 23:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024890
2020-10-12 23:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21750.71875Mb; avail=466738.52734375Mb
2020-10-12 23:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000911
2020-10-12 23:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21750.71875Mb; avail=466738.52734375Mb
2020-10-12 23:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.454981
2020-10-12 23:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.481708
2020-10-12 23:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21797.09765625Mb; avail=466692.15234375Mb
2020-10-12 23:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21798.11328125Mb; avail=466691.13671875Mb
2020-10-12 23:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.017922
2020-10-12 23:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21798.65625Mb; avail=466690.59375Mb
2020-10-12 23:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000920
2020-10-12 23:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21798.65625Mb; avail=466690.59375Mb
2020-10-12 23:16:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.452403
2020-10-12 23:16:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.472214
2020-10-12 23:16:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21862.3125Mb; avail=466625.58984375Mb
2020-10-12 23:16:48 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 23:17:03 | INFO | train_inner | epoch 015:     46 / 211 loss=7.769, nll_loss=6.684, ppl=102.84, wps=34302.2, ups=2.43, wpb=14125.4, bsz=503.8, num_updates=3000, lr=0.000150025, gnorm=1.072, clip=0, train_wall=31, wall=1095
2020-10-12 23:17:36 | INFO | train_inner | epoch 015:    146 / 211 loss=7.628, nll_loss=6.521, ppl=91.86, wps=45109.6, ups=3.11, wpb=14510.3, bsz=556.7, num_updates=3100, lr=0.000155023, gnorm=1.068, clip=0, train_wall=31, wall=1127
2020-10-12 23:17:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21328.44921875Mb; avail=467159.72265625Mb
2020-10-12 23:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001379
2020-10-12 23:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21328.44921875Mb; avail=467159.72265625Mb
2020-10-12 23:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047405
2020-10-12 23:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21328.44921875Mb; avail=467159.72265625Mb
2020-10-12 23:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034261
2020-10-12 23:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083837
2020-10-12 23:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21328.44921875Mb; avail=467159.72265625Mb
2020-10-12 23:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21328.3828125Mb; avail=467159.48046875Mb
2020-10-12 23:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000898
2020-10-12 23:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21328.3828125Mb; avail=467159.48046875Mb
2020-10-12 23:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047860
2020-10-12 23:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21328.44140625Mb; avail=467159.72265625Mb
2020-10-12 23:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033942
2020-10-12 23:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083491
2020-10-12 23:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21328.43359375Mb; avail=467159.73046875Mb
2020-10-12 23:17:59 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.169 | nll_loss 5.923 | ppl 60.67 | wps 90337.5 | wpb 4554.7 | bsz 174.1 | num_updates 3165 | best_loss 7.169
2020-10-12 23:17:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:18:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 15 @ 3165 updates, score 7.169) (writing took 5.246095394002623 seconds)
2020-10-12 23:18:04 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 23:18:04 | INFO | train | epoch 015 | loss 7.616 | nll_loss 6.508 | ppl 91.01 | wps 38721.4 | ups 2.72 | wpb 14249.3 | bsz 535.4 | num_updates 3165 | lr 0.000158271 | gnorm 1.09 | clip 0 | train_wall 66 | wall 1156
2020-10-12 23:18:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 23:18:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 23:18:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21812.09375Mb; avail=466677.265625Mb
2020-10-12 23:18:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003753
2020-10-12 23:18:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025534
2020-10-12 23:18:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21822.15234375Mb; avail=466667.20703125Mb
2020-10-12 23:18:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000915
2020-10-12 23:18:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21823.484375Mb; avail=466665.875Mb
2020-10-12 23:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.457855
2020-10-12 23:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.485552
2020-10-12 23:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21860.4375Mb; avail=466628.05859375Mb
2020-10-12 23:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21860.4375Mb; avail=466628.05859375Mb
2020-10-12 23:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.020131
2020-10-12 23:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21860.4375Mb; avail=466628.05859375Mb
2020-10-12 23:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000896
2020-10-12 23:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21860.4375Mb; avail=466628.05859375Mb
2020-10-12 23:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.458333
2020-10-12 23:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.480433
2020-10-12 23:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21860.234375Mb; avail=466628.1640625Mb
2020-10-12 23:18:05 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 23:18:18 | INFO | train_inner | epoch 016:     35 / 211 loss=7.482, nll_loss=6.354, ppl=81.8, wps=33479.4, ups=2.37, wpb=14110.5, bsz=561.5, num_updates=3200, lr=0.00016002, gnorm=1.12, clip=0, train_wall=31, wall=1169
2020-10-12 23:18:49 | INFO | train_inner | epoch 016:    135 / 211 loss=7.392, nll_loss=6.249, ppl=76.06, wps=44607.8, ups=3.15, wpb=14145.3, bsz=533.6, num_updates=3300, lr=0.000165018, gnorm=1.085, clip=0, train_wall=31, wall=1201
2020-10-12 23:19:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21322.546875Mb; avail=467166.32421875Mb
2020-10-12 23:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001387
2020-10-12 23:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21322.546875Mb; avail=467166.32421875Mb
2020-10-12 23:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048329
2020-10-12 23:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21322.546875Mb; avail=467166.32421875Mb
2020-10-12 23:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034050
2020-10-12 23:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084593
2020-10-12 23:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21322.546875Mb; avail=467166.32421875Mb
2020-10-12 23:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21322.546875Mb; avail=467166.32421875Mb
2020-10-12 23:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000928
2020-10-12 23:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21322.546875Mb; avail=467166.32421875Mb
2020-10-12 23:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047315
2020-10-12 23:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21323.23828125Mb; avail=467165.83984375Mb
2020-10-12 23:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034796
2020-10-12 23:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083834
2020-10-12 23:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21322.78125Mb; avail=467165.59765625Mb
2020-10-12 23:19:16 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.966 | nll_loss 5.685 | ppl 51.45 | wps 88749.1 | wpb 4554.7 | bsz 174.1 | num_updates 3376 | best_loss 6.966
2020-10-12 23:19:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:19:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 16 @ 3376 updates, score 6.966) (writing took 9.756723143000272 seconds)
2020-10-12 23:19:26 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 23:19:26 | INFO | train | epoch 016 | loss 7.382 | nll_loss 6.237 | ppl 75.41 | wps 36769.4 | ups 2.58 | wpb 14249.3 | bsz 535.4 | num_updates 3376 | lr 0.000168816 | gnorm 1.13 | clip 0 | train_wall 66 | wall 1238
2020-10-12 23:19:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 23:19:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17118.77734375Mb; avail=471394.6796875Mb
2020-10-12 23:19:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003631
2020-10-12 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025213
2020-10-12 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17118.28515625Mb; avail=471395.171875Mb
2020-10-12 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000974
2020-10-12 23:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17118.28515625Mb; avail=471395.15625Mb
2020-10-12 23:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.466275
2020-10-12 23:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.493583
2020-10-12 23:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16541.125Mb; avail=471972.125Mb
2020-10-12 23:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16540.93359375Mb; avail=471972.8671875Mb
2020-10-12 23:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.018435
2020-10-12 23:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16540.4140625Mb; avail=471972.73828125Mb
2020-10-12 23:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000930
2020-10-12 23:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16540.890625Mb; avail=471972.51171875Mb
2020-10-12 23:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.457545
2020-10-12 23:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.477993
2020-10-12 23:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16540.62890625Mb; avail=471972.94140625Mb
2020-10-12 23:19:27 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 23:19:36 | INFO | train_inner | epoch 017:     24 / 211 loss=7.337, nll_loss=6.184, ppl=72.7, wps=31101.9, ups=2.15, wpb=14473.2, bsz=514.6, num_updates=3400, lr=0.000170015, gnorm=1.2, clip=0, train_wall=31, wall=1247
2020-10-12 23:20:08 | INFO | train_inner | epoch 017:    124 / 211 loss=7.154, nll_loss=5.974, ppl=62.85, wps=44731.7, ups=3.14, wpb=14264, bsz=557.8, num_updates=3500, lr=0.000175013, gnorm=1.122, clip=0, train_wall=31, wall=1279
2020-10-12 23:20:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21161.453125Mb; avail=467326.984375Mb
2020-10-12 23:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001383
2020-10-12 23:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21161.453125Mb; avail=467326.984375Mb
2020-10-12 23:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047302
2020-10-12 23:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21161.453125Mb; avail=467326.984375Mb
2020-10-12 23:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034568
2020-10-12 23:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084053
2020-10-12 23:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21161.453125Mb; avail=467326.984375Mb
2020-10-12 23:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21161.453125Mb; avail=467326.984375Mb
2020-10-12 23:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000904
2020-10-12 23:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21161.453125Mb; avail=467326.984375Mb
2020-10-12 23:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047109
2020-10-12 23:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21161.24609375Mb; avail=467327.0546875Mb
2020-10-12 23:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034055
2020-10-12 23:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.082849
2020-10-12 23:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21161.28125Mb; avail=467327.41796875Mb
2020-10-12 23:20:38 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.759 | nll_loss 5.435 | ppl 43.25 | wps 86857.9 | wpb 4554.7 | bsz 174.1 | num_updates 3587 | best_loss 6.759
2020-10-12 23:20:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:20:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 17 @ 3587 updates, score 6.759) (writing took 5.293932630000199 seconds)
2020-10-12 23:20:44 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 23:20:44 | INFO | train | epoch 017 | loss 7.172 | nll_loss 5.994 | ppl 63.73 | wps 38807.4 | ups 2.72 | wpb 14249.3 | bsz 535.4 | num_updates 3587 | lr 0.00017936 | gnorm 1.171 | clip 0 | train_wall 66 | wall 1315
2020-10-12 23:20:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 23:20:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 23:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21628.8828125Mb; avail=466860.296875Mb
2020-10-12 23:20:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003773
2020-10-12 23:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025596
2020-10-12 23:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21632.40625Mb; avail=466856.7734375Mb
2020-10-12 23:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000919
2020-10-12 23:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21632.8984375Mb; avail=466856.28125Mb
2020-10-12 23:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.471512
2020-10-12 23:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.499036
2020-10-12 23:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21692.73046875Mb; avail=466795.890625Mb
2020-10-12 23:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21692.73046875Mb; avail=466795.890625Mb
2020-10-12 23:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.017939
2020-10-12 23:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21692.73046875Mb; avail=466795.890625Mb
2020-10-12 23:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000906
2020-10-12 23:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21692.73046875Mb; avail=466795.890625Mb
2020-10-12 23:20:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.462804
2020-10-12 23:20:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.483089
2020-10-12 23:20:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21692.41015625Mb; avail=466796.203125Mb
2020-10-12 23:20:45 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 23:20:50 | INFO | train_inner | epoch 018:     13 / 211 loss=7.134, nll_loss=5.949, ppl=61.76, wps=33578.4, ups=2.37, wpb=14162.2, bsz=524, num_updates=3600, lr=0.00018001, gnorm=1.187, clip=0, train_wall=31, wall=1322
2020-10-12 23:21:22 | INFO | train_inner | epoch 018:    113 / 211 loss=6.976, nll_loss=5.767, ppl=54.45, wps=44385.9, ups=3.11, wpb=14254.3, bsz=548.8, num_updates=3700, lr=0.000185008, gnorm=1.087, clip=0, train_wall=31, wall=1354
2020-10-12 23:21:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21161.7109375Mb; avail=467326.03515625Mb
2020-10-12 23:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001401
2020-10-12 23:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21161.7109375Mb; avail=467326.03515625Mb
2020-10-12 23:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048001
2020-10-12 23:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21161.83984375Mb; avail=467326.28515625Mb
2020-10-12 23:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034559
2020-10-12 23:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084802
2020-10-12 23:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21161.83984375Mb; avail=467326.28515625Mb
2020-10-12 23:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21161.83984375Mb; avail=467326.28515625Mb
2020-10-12 23:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000935
2020-10-12 23:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21161.83984375Mb; avail=467326.28515625Mb
2020-10-12 23:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047042
2020-10-12 23:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21161.83984375Mb; avail=467326.28125Mb
2020-10-12 23:21:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034638
2020-10-12 23:21:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083362
2020-10-12 23:21:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21161.83984375Mb; avail=467326.28125Mb
2020-10-12 23:21:56 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.649 | nll_loss 5.295 | ppl 39.27 | wps 80275.4 | wpb 4554.7 | bsz 174.1 | num_updates 3798 | best_loss 6.649
2020-10-12 23:21:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:22:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 18 @ 3798 updates, score 6.649) (writing took 4.76263138899958 seconds)
2020-10-12 23:22:01 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 23:22:01 | INFO | train | epoch 018 | loss 6.957 | nll_loss 5.744 | ppl 53.61 | wps 38989.4 | ups 2.74 | wpb 14249.3 | bsz 535.4 | num_updates 3798 | lr 0.000189905 | gnorm 1.102 | clip 0 | train_wall 66 | wall 1392
2020-10-12 23:22:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 23:22:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 23:22:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21527.22265625Mb; avail=466961.5390625Mb
2020-10-12 23:22:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004211
2020-10-12 23:22:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024252
2020-10-12 23:22:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21532.06640625Mb; avail=466956.6953125Mb
2020-10-12 23:22:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000913
2020-10-12 23:22:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21532.58203125Mb; avail=466956.1796875Mb
2020-10-12 23:22:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.511166
2020-10-12 23:22:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.537206
2020-10-12 23:22:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21615.46875Mb; avail=466873.59375Mb
2020-10-12 23:22:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21620.01953125Mb; avail=466869.25390625Mb
2020-10-12 23:22:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016091
2020-10-12 23:22:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21621.23046875Mb; avail=466868.04296875Mb
2020-10-12 23:22:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000813
2020-10-12 23:22:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21621.23046875Mb; avail=466868.04296875Mb
2020-10-12 23:22:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.401417
2020-10-12 23:22:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.419163
2020-10-12 23:22:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21685.62109375Mb; avail=466803.69140625Mb
2020-10-12 23:22:02 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 23:22:04 | INFO | train_inner | epoch 019:      2 / 211 loss=6.931, nll_loss=5.713, ppl=52.47, wps=34386.2, ups=2.42, wpb=14208.8, bsz=518.3, num_updates=3800, lr=0.000190005, gnorm=1.141, clip=0, train_wall=31, wall=1395
2020-10-12 23:22:36 | INFO | train_inner | epoch 019:    102 / 211 loss=6.796, nll_loss=5.558, ppl=47.13, wps=44228.7, ups=3.09, wpb=14297.3, bsz=532.8, num_updates=3900, lr=0.000195003, gnorm=1.073, clip=0, train_wall=32, wall=1427
2020-10-12 23:23:08 | INFO | train_inner | epoch 019:    202 / 211 loss=6.726, nll_loss=5.476, ppl=44.5, wps=45088.5, ups=3.15, wpb=14334.3, bsz=551.6, num_updates=4000, lr=0.0002, gnorm=1.064, clip=0, train_wall=31, wall=1459
2020-10-12 23:23:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21298.49609375Mb; avail=467190.01171875Mb
2020-10-12 23:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001398
2020-10-12 23:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21298.49609375Mb; avail=467190.01171875Mb
2020-10-12 23:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047797
2020-10-12 23:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21298.44921875Mb; avail=467189.76953125Mb
2020-10-12 23:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035935
2020-10-12 23:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086056
2020-10-12 23:23:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21298.36328125Mb; avail=467189.6484375Mb
2020-10-12 23:23:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21298.27734375Mb; avail=467190.375Mb
2020-10-12 23:23:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000898
2020-10-12 23:23:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21298.27734375Mb; avail=467190.375Mb
2020-10-12 23:23:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047451
2020-10-12 23:23:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21298.27734375Mb; avail=467190.375Mb
2020-10-12 23:23:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034002
2020-10-12 23:23:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083094
2020-10-12 23:23:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21298.27734375Mb; avail=467190.375Mb
2020-10-12 23:23:13 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.456 | nll_loss 5.078 | ppl 33.79 | wps 91686.7 | wpb 4554.7 | bsz 174.1 | num_updates 4009 | best_loss 6.456
2020-10-12 23:23:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:23:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 19 @ 4009 updates, score 6.456) (writing took 4.7873872150012176 seconds)
2020-10-12 23:23:18 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 23:23:18 | INFO | train | epoch 019 | loss 6.763 | nll_loss 5.519 | ppl 45.85 | wps 39033.6 | ups 2.74 | wpb 14249.3 | bsz 535.4 | num_updates 4009 | lr 0.000199775 | gnorm 1.074 | clip 0 | train_wall 66 | wall 1469
2020-10-12 23:23:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 23:23:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 23:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21662.27734375Mb; avail=466826.9609375Mb
2020-10-12 23:23:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003519
2020-10-12 23:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.038252
2020-10-12 23:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21668.31640625Mb; avail=466820.921875Mb
2020-10-12 23:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001868
2020-10-12 23:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21668.31640625Mb; avail=466820.921875Mb
2020-10-12 23:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.487745
2020-10-12 23:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.529397
2020-10-12 23:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21746.28515625Mb; avail=466742.96875Mb
2020-10-12 23:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21752.9453125Mb; avail=466736.30859375Mb
2020-10-12 23:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016008
2020-10-12 23:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21756.0859375Mb; avail=466733.16796875Mb
2020-10-12 23:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000838
2020-10-12 23:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21756.0859375Mb; avail=466733.16796875Mb
2020-10-12 23:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.399059
2020-10-12 23:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.416753
2020-10-12 23:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21778.73828125Mb; avail=466710.33984375Mb
2020-10-12 23:23:19 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 23:23:49 | INFO | train_inner | epoch 020:     91 / 211 loss=6.585, nll_loss=5.314, ppl=39.79, wps=33516.2, ups=2.41, wpb=13910.8, bsz=531.5, num_updates=4100, lr=0.000197546, gnorm=1.071, clip=0, train_wall=31, wall=1501
2020-10-12 23:24:21 | INFO | train_inner | epoch 020:    191 / 211 loss=6.571, nll_loss=5.295, ppl=39.27, wps=45143.4, ups=3.12, wpb=14456.7, bsz=542.5, num_updates=4200, lr=0.00019518, gnorm=1.11, clip=0, train_wall=31, wall=1533
2020-10-12 23:24:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21300.01171875Mb; avail=467188.390625Mb
2020-10-12 23:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001339
2020-10-12 23:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21300.01171875Mb; avail=467188.390625Mb
2020-10-12 23:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047809
2020-10-12 23:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21299.78125Mb; avail=467188.1484375Mb
2020-10-12 23:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034081
2020-10-12 23:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084001
2020-10-12 23:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21299.90234375Mb; avail=467188.1484375Mb
2020-10-12 23:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21299.90234375Mb; avail=467188.1484375Mb
2020-10-12 23:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000949
2020-10-12 23:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21299.90234375Mb; avail=467188.1484375Mb
2020-10-12 23:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047181
2020-10-12 23:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21299.90234375Mb; avail=467188.1484375Mb
2020-10-12 23:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033979
2020-10-12 23:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.082857
2020-10-12 23:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21299.90234375Mb; avail=467188.1484375Mb
2020-10-12 23:24:30 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.37 | nll_loss 4.955 | ppl 31.01 | wps 90360.8 | wpb 4554.7 | bsz 174.1 | num_updates 4220 | best_loss 6.37
2020-10-12 23:24:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:24:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 20 @ 4220 updates, score 6.37) (writing took 4.670135839001887 seconds)
2020-10-12 23:24:35 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 23:24:35 | INFO | train | epoch 020 | loss 6.574 | nll_loss 5.299 | ppl 39.38 | wps 39029.4 | ups 2.74 | wpb 14249.3 | bsz 535.4 | num_updates 4220 | lr 0.000194717 | gnorm 1.102 | clip 0 | train_wall 66 | wall 1546
2020-10-12 23:24:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 23:24:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 23:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21643.95703125Mb; avail=466845.27734375Mb
2020-10-12 23:24:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003018
2020-10-12 23:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024465
2020-10-12 23:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21646.87109375Mb; avail=466842.36328125Mb
2020-10-12 23:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000885
2020-10-12 23:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21647.4765625Mb; avail=466841.7578125Mb
2020-10-12 23:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.602502
2020-10-12 23:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.629093
2020-10-12 23:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21734.640625Mb; avail=466754.1953125Mb
2020-10-12 23:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21740.4296875Mb; avail=466748.40625Mb
2020-10-12 23:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.017347
2020-10-12 23:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21742.8515625Mb; avail=466745.37890625Mb
2020-10-12 23:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000999
2020-10-12 23:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21743.45703125Mb; avail=466745.37890625Mb
2020-10-12 23:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.484369
2020-10-12 23:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.503624
2020-10-12 23:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21777.03515625Mb; avail=466711.8125Mb
2020-10-12 23:24:36 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 23:25:03 | INFO | train_inner | epoch 021:     80 / 211 loss=6.474, nll_loss=5.184, ppl=36.35, wps=34248, ups=2.41, wpb=14211.4, bsz=506.3, num_updates=4300, lr=0.000192897, gnorm=1.104, clip=0, train_wall=31, wall=1574
2020-10-12 23:25:34 | INFO | train_inner | epoch 021:    180 / 211 loss=6.383, nll_loss=5.078, ppl=33.77, wps=45244.1, ups=3.15, wpb=14346.8, bsz=537, num_updates=4400, lr=0.000190693, gnorm=0.995, clip=0, train_wall=31, wall=1606
2020-10-12 23:25:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:25:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21297.10546875Mb; avail=467191.84375Mb
2020-10-12 23:25:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001397
2020-10-12 23:25:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21297.10546875Mb; avail=467191.84375Mb
2020-10-12 23:25:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049141
2020-10-12 23:25:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21297.08203125Mb; avail=467191.96484375Mb
2020-10-12 23:25:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034388
2020-10-12 23:25:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.085728
2020-10-12 23:25:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21297.08203125Mb; avail=467191.96484375Mb
2020-10-12 23:25:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21297.08203125Mb; avail=467191.96484375Mb
2020-10-12 23:25:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000902
2020-10-12 23:25:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21297.08203125Mb; avail=467191.96484375Mb
2020-10-12 23:25:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048871
2020-10-12 23:25:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21297.08203125Mb; avail=467191.96484375Mb
2020-10-12 23:25:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035104
2020-10-12 23:25:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.085666
2020-10-12 23:25:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21297.08203125Mb; avail=467191.96484375Mb
2020-10-12 23:25:47 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.179 | nll_loss 4.723 | ppl 26.4 | wps 89405.4 | wpb 4554.7 | bsz 174.1 | num_updates 4431 | best_loss 6.179
2020-10-12 23:25:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:25:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 21 @ 4431 updates, score 6.179) (writing took 4.5097504910008865 seconds)
2020-10-12 23:25:51 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 23:25:51 | INFO | train | epoch 021 | loss 6.401 | nll_loss 5.1 | ppl 34.29 | wps 39258.6 | ups 2.76 | wpb 14249.3 | bsz 535.4 | num_updates 4431 | lr 0.000190024 | gnorm 1.043 | clip 0 | train_wall 66 | wall 1623
2020-10-12 23:25:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 23:25:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 23:25:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21906.08203125Mb; avail=466581.8828125Mb
2020-10-12 23:25:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003519
2020-10-12 23:25:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024752
2020-10-12 23:25:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21909.1640625Mb; avail=466579.95703125Mb
2020-10-12 23:25:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001051
2020-10-12 23:25:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21909.51171875Mb; avail=466580.109375Mb
2020-10-12 23:25:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.448633
2020-10-12 23:25:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.475360
2020-10-12 23:25:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21908.89453125Mb; avail=466581.09375Mb
2020-10-12 23:25:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21913.65625Mb; avail=466576.55859375Mb
2020-10-12 23:25:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.019456
2020-10-12 23:25:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21913.11328125Mb; avail=466577.05859375Mb
2020-10-12 23:25:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001228
2020-10-12 23:25:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21913.11328125Mb; avail=466577.05859375Mb
2020-10-12 23:25:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.612673
2020-10-12 23:25:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.634529
2020-10-12 23:25:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22031.10546875Mb; avail=466458.46875Mb
2020-10-12 23:25:52 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 23:26:16 | INFO | train_inner | epoch 022:     69 / 211 loss=6.29, nll_loss=4.972, ppl=31.39, wps=34224, ups=2.43, wpb=14103.5, bsz=530.6, num_updates=4500, lr=0.000188562, gnorm=1.004, clip=0, train_wall=31, wall=1647
2020-10-12 23:26:48 | INFO | train_inner | epoch 022:    169 / 211 loss=6.243, nll_loss=4.915, ppl=30.18, wps=44580.2, ups=3.13, wpb=14253, bsz=529.2, num_updates=4600, lr=0.000186501, gnorm=0.979, clip=0, train_wall=31, wall=1679
2020-10-12 23:27:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21308.78515625Mb; avail=467179.7421875Mb
2020-10-12 23:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001417
2020-10-12 23:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21308.78515625Mb; avail=467179.7421875Mb
2020-10-12 23:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047612
2020-10-12 23:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21308.78515625Mb; avail=467179.7421875Mb
2020-10-12 23:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035059
2020-10-12 23:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084871
2020-10-12 23:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21308.69921875Mb; avail=467179.7421875Mb
2020-10-12 23:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21308.98828125Mb; avail=467179.7421875Mb
2020-10-12 23:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000942
2020-10-12 23:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21308.98828125Mb; avail=467179.7421875Mb
2020-10-12 23:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047664
2020-10-12 23:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21309.140625Mb; avail=467179.37890625Mb
2020-10-12 23:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034265
2020-10-12 23:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083678
2020-10-12 23:27:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21309.140625Mb; avail=467179.37890625Mb
2020-10-12 23:27:03 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.06 | nll_loss 4.592 | ppl 24.12 | wps 87072.7 | wpb 4554.7 | bsz 174.1 | num_updates 4642 | best_loss 6.06
2020-10-12 23:27:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:27:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 22 @ 4642 updates, score 6.06) (writing took 4.430389252000168 seconds)
2020-10-12 23:27:08 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 23:27:08 | INFO | train | epoch 022 | loss 6.238 | nll_loss 4.911 | ppl 30.08 | wps 39298.8 | ups 2.76 | wpb 14249.3 | bsz 535.4 | num_updates 4642 | lr 0.000185655 | gnorm 0.987 | clip 0 | train_wall 66 | wall 1699
2020-10-12 23:27:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 23:27:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 23:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21237.01953125Mb; avail=467251.9765625Mb
2020-10-12 23:27:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003710
2020-10-12 23:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024945
2020-10-12 23:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21244.17578125Mb; avail=467244.8203125Mb
2020-10-12 23:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000913
2020-10-12 23:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21245.38671875Mb; avail=467244.1015625Mb
2020-10-12 23:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.401035
2020-10-12 23:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.427783
2020-10-12 23:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21284.90625Mb; avail=467204.1015625Mb
2020-10-12 23:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21286.40625Mb; avail=467202.6015625Mb
2020-10-12 23:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015728
2020-10-12 23:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21286.40625Mb; avail=467202.6015625Mb
2020-10-12 23:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000844
2020-10-12 23:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21286.40625Mb; avail=467202.6015625Mb
2020-10-12 23:27:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.394428
2020-10-12 23:27:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.411840
2020-10-12 23:27:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21355.9453125Mb; avail=467132.15234375Mb
2020-10-12 23:27:09 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 23:27:28 | INFO | train_inner | epoch 023:     58 / 211 loss=6.148, nll_loss=4.806, ppl=27.98, wps=35380.2, ups=2.45, wpb=14431.6, bsz=564.8, num_updates=4700, lr=0.000184506, gnorm=0.977, clip=0, train_wall=31, wall=1720
2020-10-12 23:28:01 | INFO | train_inner | epoch 023:    158 / 211 loss=6.092, nll_loss=4.741, ppl=26.74, wps=44185.4, ups=3.11, wpb=14216.9, bsz=537.4, num_updates=4800, lr=0.000182574, gnorm=0.961, clip=0, train_wall=31, wall=1752
2020-10-12 23:28:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21317.765625Mb; avail=467170.609375Mb
2020-10-12 23:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001415
2020-10-12 23:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21317.765625Mb; avail=467170.609375Mb
2020-10-12 23:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047358
2020-10-12 23:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21317.765625Mb; avail=467170.609375Mb
2020-10-12 23:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034201
2020-10-12 23:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083786
2020-10-12 23:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21317.765625Mb; avail=467170.609375Mb
2020-10-12 23:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21317.765625Mb; avail=467170.609375Mb
2020-10-12 23:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000926
2020-10-12 23:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21317.765625Mb; avail=467170.609375Mb
2020-10-12 23:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047437
2020-10-12 23:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21317.69921875Mb; avail=467170.48828125Mb
2020-10-12 23:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034361
2020-10-12 23:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083517
2020-10-12 23:28:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21317.85546875Mb; avail=467170.73046875Mb
2020-10-12 23:28:20 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 5.951 | nll_loss 4.465 | ppl 22.09 | wps 89342.9 | wpb 4554.7 | bsz 174.1 | num_updates 4853 | best_loss 5.951
2020-10-12 23:28:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:28:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 23 @ 4853 updates, score 5.951) (writing took 4.444497783999395 seconds)
2020-10-12 23:28:24 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 23:28:24 | INFO | train | epoch 023 | loss 6.091 | nll_loss 4.741 | ppl 26.74 | wps 39373.5 | ups 2.76 | wpb 14249.3 | bsz 535.4 | num_updates 4853 | lr 0.000181574 | gnorm 0.961 | clip 0 | train_wall 66 | wall 1776
2020-10-12 23:28:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 23:28:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 23:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21367.11328125Mb; avail=467120.7890625Mb
2020-10-12 23:28:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003192
2020-10-12 23:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022196
2020-10-12 23:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21368.24609375Mb; avail=467119.4609375Mb
2020-10-12 23:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000833
2020-10-12 23:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21368.24609375Mb; avail=467119.4609375Mb
2020-10-12 23:28:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.397491
2020-10-12 23:28:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.421349
2020-10-12 23:28:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21368.41015625Mb; avail=467119.703125Mb
2020-10-12 23:28:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21368.5234375Mb; avail=467119.4609375Mb
2020-10-12 23:28:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015796
2020-10-12 23:28:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21368.49609375Mb; avail=467119.703125Mb
2020-10-12 23:28:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000835
2020-10-12 23:28:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21368.49609375Mb; avail=467119.703125Mb
2020-10-12 23:28:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.396438
2020-10-12 23:28:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.413903
2020-10-12 23:28:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21259.9296875Mb; avail=467229.13671875Mb
2020-10-12 23:28:25 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 23:28:42 | INFO | train_inner | epoch 024:     47 / 211 loss=6.012, nll_loss=4.65, ppl=25.11, wps=34822.6, ups=2.44, wpb=14279.4, bsz=554, num_updates=4900, lr=0.000180702, gnorm=0.964, clip=0, train_wall=31, wall=1793
2020-10-12 23:29:14 | INFO | train_inner | epoch 024:    147 / 211 loss=5.974, nll_loss=4.605, ppl=24.33, wps=44438.3, ups=3.12, wpb=14255.4, bsz=510.7, num_updates=5000, lr=0.000178885, gnorm=0.959, clip=0, train_wall=31, wall=1825
2020-10-12 23:29:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:29:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21510.07421875Mb; avail=466979.08203125Mb
2020-10-12 23:29:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001367
2020-10-12 23:29:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21510.07421875Mb; avail=466979.08203125Mb
2020-10-12 23:29:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047678
2020-10-12 23:29:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21515.625Mb; avail=466973.53125Mb
2020-10-12 23:29:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034524
2020-10-12 23:29:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084412
2020-10-12 23:29:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21557.328125Mb; avail=466931.828125Mb
2020-10-12 23:29:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21578.640625Mb; avail=466910.515625Mb
2020-10-12 23:29:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001046
2020-10-12 23:29:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21579.8515625Mb; avail=466909.3046875Mb
2020-10-12 23:29:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047314
2020-10-12 23:29:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21580.31640625Mb; avail=466908.20703125Mb
2020-10-12 23:29:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034275
2020-10-12 23:29:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083447
2020-10-12 23:29:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21579.671875Mb; avail=466908.5703125Mb
2020-10-12 23:29:37 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.841 | nll_loss 4.325 | ppl 20.05 | wps 89555.7 | wpb 4554.7 | bsz 174.1 | num_updates 5064 | best_loss 5.841
2020-10-12 23:29:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:29:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 24 @ 5064 updates, score 5.841) (writing took 9.99315380200278 seconds)
2020-10-12 23:29:47 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 23:29:47 | INFO | train | epoch 024 | loss 5.96 | nll_loss 4.59 | ppl 24.08 | wps 36522 | ups 2.56 | wpb 14249.3 | bsz 535.4 | num_updates 5064 | lr 0.000177751 | gnorm 0.934 | clip 0 | train_wall 66 | wall 1858
2020-10-12 23:29:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 23:29:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 23:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21644.62890625Mb; avail=466842.703125Mb
2020-10-12 23:29:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003765
2020-10-12 23:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025142
2020-10-12 23:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21645.51953125Mb; avail=466842.53515625Mb
2020-10-12 23:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000952
2020-10-12 23:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21645.26171875Mb; avail=466842.30859375Mb
2020-10-12 23:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.415871
2020-10-12 23:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.442997
2020-10-12 23:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21645.84765625Mb; avail=466843.05078125Mb
2020-10-12 23:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21645.66796875Mb; avail=466842.91015625Mb
2020-10-12 23:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016186
2020-10-12 23:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21645.31640625Mb; avail=466843.08203125Mb
2020-10-12 23:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000849
2020-10-12 23:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21645.79296875Mb; avail=466842.85546875Mb
2020-10-12 23:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.495052
2020-10-12 23:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.513149
2020-10-12 23:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21646.0234375Mb; avail=466842.484375Mb
2020-10-12 23:29:48 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 23:30:00 | INFO | train_inner | epoch 025:     36 / 211 loss=5.9, nll_loss=4.52, ppl=22.95, wps=30288.6, ups=2.14, wpb=14154, bsz=544.3, num_updates=5100, lr=0.000177123, gnorm=0.885, clip=0, train_wall=31, wall=1872
2020-10-12 23:30:32 | INFO | train_inner | epoch 025:    136 / 211 loss=5.883, nll_loss=4.499, ppl=22.61, wps=44402.6, ups=3.13, wpb=14206.6, bsz=490.6, num_updates=5200, lr=0.000175412, gnorm=0.928, clip=0, train_wall=31, wall=1904
2020-10-12 23:30:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21333.83203125Mb; avail=467154.5703125Mb
2020-10-12 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001410
2020-10-12 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21333.07421875Mb; avail=467155.328125Mb
2020-10-12 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048298
2020-10-12 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21321.7265625Mb; avail=467166.57421875Mb
2020-10-12 23:30:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034127
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084875
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21300.42578125Mb; avail=467187.37109375Mb
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21218.9140625Mb; avail=467270.1328125Mb
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001143
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21219.02734375Mb; avail=467269.52734375Mb
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048061
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21052.2109375Mb; avail=467436.34765625Mb
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034609
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084625
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21051.35546875Mb; avail=467437.66796875Mb
2020-10-12 23:30:59 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.784 | nll_loss 4.26 | ppl 19.17 | wps 89447.8 | wpb 4554.7 | bsz 174.1 | num_updates 5275 | best_loss 5.784
2020-10-12 23:30:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:31:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 25 @ 5275 updates, score 5.784) (writing took 7.071623558000283 seconds)
2020-10-12 23:31:06 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 23:31:06 | INFO | train | epoch 025 | loss 5.843 | nll_loss 4.454 | ppl 21.92 | wps 37821.7 | ups 2.65 | wpb 14249.3 | bsz 535.4 | num_updates 5275 | lr 0.00017416 | gnorm 0.921 | clip 0 | train_wall 66 | wall 1937
2020-10-12 23:31:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 23:31:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 23:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21288.88671875Mb; avail=467199.484375Mb
2020-10-12 23:31:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003227
2020-10-12 23:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022320
2020-10-12 23:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21290.703125Mb; avail=467197.66796875Mb
2020-10-12 23:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000892
2020-10-12 23:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21290.703125Mb; avail=467197.66796875Mb
2020-10-12 23:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.405436
2020-10-12 23:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.429630
2020-10-12 23:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21292.546875Mb; avail=467195.81640625Mb
2020-10-12 23:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21292.0546875Mb; avail=467196.30859375Mb
2020-10-12 23:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016497
2020-10-12 23:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21292.0546875Mb; avail=467196.30859375Mb
2020-10-12 23:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000888
2020-10-12 23:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21292.0546875Mb; avail=467196.30859375Mb
2020-10-12 23:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.400155
2020-10-12 23:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.418482
2020-10-12 23:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21291.48046875Mb; avail=467196.953125Mb
2020-10-12 23:31:07 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 23:31:16 | INFO | train_inner | epoch 026:     25 / 211 loss=5.792, nll_loss=4.396, ppl=21.05, wps=32640.7, ups=2.28, wpb=14340.5, bsz=563.6, num_updates=5300, lr=0.000173749, gnorm=0.938, clip=0, train_wall=31, wall=1948
2020-10-12 23:31:48 | INFO | train_inner | epoch 026:    125 / 211 loss=5.705, nll_loss=4.297, ppl=19.65, wps=44591.4, ups=3.12, wpb=14313.5, bsz=578.2, num_updates=5400, lr=0.000172133, gnorm=0.872, clip=0, train_wall=31, wall=1980
2020-10-12 23:32:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21332.94921875Mb; avail=467156.15234375Mb
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001364
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21332.94921875Mb; avail=467156.15234375Mb
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.052068
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21332.94921875Mb; avail=467156.15234375Mb
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.037480
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.091775
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21332.94921875Mb; avail=467156.15234375Mb
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21332.94921875Mb; avail=467156.15234375Mb
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000912
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21332.94921875Mb; avail=467156.15234375Mb
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047503
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21332.84765625Mb; avail=467156.2734375Mb
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035149
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084343
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21332.91796875Mb; avail=467156.39453125Mb
2020-10-12 23:32:18 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.69 | nll_loss 4.146 | ppl 17.7 | wps 90711.4 | wpb 4554.7 | bsz 174.1 | num_updates 5486 | best_loss 5.69
2020-10-12 23:32:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:32:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 26 @ 5486 updates, score 5.69) (writing took 7.044956485999137 seconds)
2020-10-12 23:32:25 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 23:32:25 | INFO | train | epoch 026 | loss 5.732 | nll_loss 4.327 | ppl 20.07 | wps 37981.6 | ups 2.67 | wpb 14249.3 | bsz 535.4 | num_updates 5486 | lr 0.000170778 | gnorm 0.878 | clip 0 | train_wall 66 | wall 2017
2020-10-12 23:32:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 23:32:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 23:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21644.77734375Mb; avail=466843.5625Mb
2020-10-12 23:32:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003417
2020-10-12 23:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025695
2020-10-12 23:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21647.11328125Mb; avail=466841.2265625Mb
2020-10-12 23:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000909
2020-10-12 23:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21647.11328125Mb; avail=466841.2265625Mb
2020-10-12 23:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.456569
2020-10-12 23:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.484082
2020-10-12 23:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21647.25Mb; avail=466841.17578125Mb
2020-10-12 23:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21647.265625Mb; avail=466841.16015625Mb
2020-10-12 23:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.018065
2020-10-12 23:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21647.28125Mb; avail=466841.14453125Mb
2020-10-12 23:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001007
2020-10-12 23:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21647.28125Mb; avail=466841.14453125Mb
2020-10-12 23:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.449350
2020-10-12 23:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.469381
2020-10-12 23:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21647.796875Mb; avail=466840.65234375Mb
2020-10-12 23:32:26 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 23:32:32 | INFO | train_inner | epoch 027:     14 / 211 loss=5.731, nll_loss=4.325, ppl=20.04, wps=32516.9, ups=2.29, wpb=14174.7, bsz=501.6, num_updates=5500, lr=0.000170561, gnorm=0.882, clip=0, train_wall=31, wall=2023
2020-10-12 23:33:04 | INFO | train_inner | epoch 027:    114 / 211 loss=5.63, nll_loss=4.21, ppl=18.51, wps=45126.1, ups=3.15, wpb=14333.2, bsz=551.1, num_updates=5600, lr=0.000169031, gnorm=0.877, clip=0, train_wall=31, wall=2055
2020-10-12 23:33:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21056.7578125Mb; avail=467432.296875Mb
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001413
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21056.7578125Mb; avail=467432.296875Mb
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048225
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21055.46484375Mb; avail=467433.84375Mb
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034529
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084966
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21055.46484375Mb; avail=467433.84375Mb
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21055.46484375Mb; avail=467433.84375Mb
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000898
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21055.46484375Mb; avail=467433.84375Mb
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047568
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21055.46484375Mb; avail=467433.84375Mb
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033771
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.082992
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21055.46484375Mb; avail=467433.84375Mb
2020-10-12 23:33:37 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.648 | nll_loss 4.104 | ppl 17.2 | wps 90313 | wpb 4554.7 | bsz 174.1 | num_updates 5697 | best_loss 5.648
2020-10-12 23:33:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:33:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 27 @ 5697 updates, score 5.648) (writing took 7.224831361996621 seconds)
2020-10-12 23:33:44 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 23:33:44 | INFO | train | epoch 027 | loss 5.636 | nll_loss 4.216 | ppl 18.59 | wps 37909.7 | ups 2.66 | wpb 14249.3 | bsz 535.4 | num_updates 5697 | lr 0.000167586 | gnorm 0.878 | clip 0 | train_wall 66 | wall 2096
2020-10-12 23:33:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 23:33:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 23:33:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21651.65234375Mb; avail=466836.69140625Mb
2020-10-12 23:33:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003531
2020-10-12 23:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026856
2020-10-12 23:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21651.16015625Mb; avail=466837.18359375Mb
2020-10-12 23:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000997
2020-10-12 23:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21651.16015625Mb; avail=466837.18359375Mb
2020-10-12 23:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.456384
2020-10-12 23:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.485306
2020-10-12 23:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21651.27734375Mb; avail=466837.28515625Mb
2020-10-12 23:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21651.2890625Mb; avail=466837.1640625Mb
2020-10-12 23:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.017726
2020-10-12 23:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21651.3046875Mb; avail=466837.1484375Mb
2020-10-12 23:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000923
2020-10-12 23:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21651.3046875Mb; avail=466837.1484375Mb
2020-10-12 23:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.455535
2020-10-12 23:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.475237
2020-10-12 23:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21651.6640625Mb; avail=466836.75Mb
2020-10-12 23:33:46 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 23:33:48 | INFO | train_inner | epoch 028:      3 / 211 loss=5.647, nll_loss=4.229, ppl=18.75, wps=32014.8, ups=2.27, wpb=14080.2, bsz=514.4, num_updates=5700, lr=0.000167542, gnorm=0.877, clip=0, train_wall=31, wall=2099
2020-10-12 23:34:19 | INFO | train_inner | epoch 028:    103 / 211 loss=5.549, nll_loss=4.117, ppl=17.35, wps=45268.8, ups=3.16, wpb=14335.6, bsz=543.3, num_updates=5800, lr=0.000166091, gnorm=0.849, clip=0, train_wall=31, wall=2131
2020-10-12 23:34:51 | INFO | train_inner | epoch 028:    203 / 211 loss=5.552, nll_loss=4.119, ppl=17.37, wps=44963.4, ups=3.15, wpb=14292.7, bsz=532.3, num_updates=5900, lr=0.000164677, gnorm=0.86, clip=0, train_wall=31, wall=2163
2020-10-12 23:34:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21327.453125Mb; avail=467160.9765625Mb
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001429
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21327.453125Mb; avail=467160.9765625Mb
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048494
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21327.39453125Mb; avail=467161.21875Mb
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035542
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086299
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21327.3125Mb; avail=467161.09765625Mb
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21327.3046875Mb; avail=467161.10546875Mb
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000923
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21327.3046875Mb; avail=467161.10546875Mb
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047211
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21327.3046875Mb; avail=467161.10546875Mb
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035363
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084283
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21327.77734375Mb; avail=467160.9609375Mb
2020-10-12 23:34:56 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.579 | nll_loss 4.019 | ppl 16.22 | wps 86938.9 | wpb 4554.7 | bsz 174.1 | num_updates 5908 | best_loss 5.579
2020-10-12 23:34:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:35:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 28 @ 5908 updates, score 5.579) (writing took 7.100802000997646 seconds)
2020-10-12 23:35:03 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 23:35:03 | INFO | train | epoch 028 | loss 5.55 | nll_loss 4.118 | ppl 17.36 | wps 38071.9 | ups 2.67 | wpb 14249.3 | bsz 535.4 | num_updates 5908 | lr 0.000164566 | gnorm 0.861 | clip 0 | train_wall 65 | wall 2175
2020-10-12 23:35:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 23:35:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 23:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21641.62890625Mb; avail=466846.33203125Mb
2020-10-12 23:35:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002602
2020-10-12 23:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022906
2020-10-12 23:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21641.62890625Mb; avail=466846.33203125Mb
2020-10-12 23:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000920
2020-10-12 23:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21641.62890625Mb; avail=466846.33203125Mb
2020-10-12 23:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.410227
2020-10-12 23:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.434916
2020-10-12 23:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21642.00390625Mb; avail=466846.4609375Mb
2020-10-12 23:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21642.00390625Mb; avail=466846.4609375Mb
2020-10-12 23:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016042
2020-10-12 23:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21642.00390625Mb; avail=466846.4609375Mb
2020-10-12 23:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000850
2020-10-12 23:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21642.00390625Mb; avail=466846.4609375Mb
2020-10-12 23:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.395310
2020-10-12 23:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.413072
2020-10-12 23:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21641.9609375Mb; avail=466846.53125Mb
2020-10-12 23:35:04 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 23:35:35 | INFO | train_inner | epoch 029:     92 / 211 loss=5.466, nll_loss=4.021, ppl=16.23, wps=32649.5, ups=2.29, wpb=14245.9, bsz=559.8, num_updates=6000, lr=0.000163299, gnorm=0.855, clip=0, train_wall=31, wall=2206
2020-10-12 23:36:07 | INFO | train_inner | epoch 029:    192 / 211 loss=5.477, nll_loss=4.033, ppl=16.37, wps=44964.9, ups=3.14, wpb=14327.1, bsz=525.2, num_updates=6100, lr=0.000161955, gnorm=0.856, clip=0, train_wall=31, wall=2238
2020-10-12 23:36:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21336.84375Mb; avail=467150.2265625Mb
2020-10-12 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001387
2020-10-12 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21336.84375Mb; avail=467150.2265625Mb
2020-10-12 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.046512
2020-10-12 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21336.84375Mb; avail=467150.2265625Mb
2020-10-12 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034952
2020-10-12 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083692
2020-10-12 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21336.01953125Mb; avail=467152.4140625Mb
2020-10-12 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21336.0859375Mb; avail=467152.171875Mb
2020-10-12 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000950
2020-10-12 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21336.109375Mb; avail=467152.05078125Mb
2020-10-12 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047941
2020-10-12 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21336.234375Mb; avail=467152.53515625Mb
2020-10-12 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034822
2020-10-12 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084498
2020-10-12 23:36:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21336.234375Mb; avail=467152.53515625Mb
2020-10-12 23:36:15 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.537 | nll_loss 3.964 | ppl 15.61 | wps 83337.8 | wpb 4554.7 | bsz 174.1 | num_updates 6119 | best_loss 5.537
2020-10-12 23:36:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:36:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 29 @ 6119 updates, score 5.537) (writing took 6.845683540999744 seconds)
2020-10-12 23:36:22 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 23:36:22 | INFO | train | epoch 029 | loss 5.472 | nll_loss 4.027 | ppl 16.3 | wps 38200.4 | ups 2.68 | wpb 14249.3 | bsz 535.4 | num_updates 6119 | lr 0.000161704 | gnorm 0.851 | clip 0 | train_wall 66 | wall 2254
2020-10-12 23:36:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 23:36:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 23:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21293.1953125Mb; avail=467194.92578125Mb
2020-10-12 23:36:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002805
2020-10-12 23:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025562
2020-10-12 23:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21293.1953125Mb; avail=467194.92578125Mb
2020-10-12 23:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000926
2020-10-12 23:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21293.1953125Mb; avail=467194.92578125Mb
2020-10-12 23:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.452310
2020-10-12 23:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.479776
2020-10-12 23:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21292.828125Mb; avail=467195.3046875Mb
2020-10-12 23:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21292.859375Mb; avail=467195.2734375Mb
2020-10-12 23:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.017739
2020-10-12 23:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21292.875Mb; avail=467195.2578125Mb
2020-10-12 23:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000984
2020-10-12 23:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21292.875Mb; avail=467195.2578125Mb
2020-10-12 23:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.443729
2020-10-12 23:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.463478
2020-10-12 23:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21293.3046875Mb; avail=467195.16796875Mb
2020-10-12 23:36:23 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 23:36:50 | INFO | train_inner | epoch 030:     81 / 211 loss=5.414, nll_loss=3.961, ppl=15.58, wps=32130.9, ups=2.3, wpb=13987.6, bsz=529.3, num_updates=6200, lr=0.000160644, gnorm=0.874, clip=0, train_wall=31, wall=2282
2020-10-12 23:37:23 | INFO | train_inner | epoch 030:    181 / 211 loss=5.395, nll_loss=3.938, ppl=15.33, wps=44404.1, ups=3.09, wpb=14350.7, bsz=527.8, num_updates=6300, lr=0.000159364, gnorm=0.825, clip=0, train_wall=32, wall=2314
2020-10-12 23:37:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21059.62890625Mb; avail=467429.28515625Mb
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001315
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21059.62890625Mb; avail=467429.28515625Mb
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047818
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21059.62890625Mb; avail=467429.28515625Mb
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035372
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.085294
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21059.62890625Mb; avail=467429.28515625Mb
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21059.62890625Mb; avail=467429.28515625Mb
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000906
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21059.62890625Mb; avail=467429.28515625Mb
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.050451
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21064.1328125Mb; avail=467424.78125Mb
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034933
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087164
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21077.34765625Mb; avail=467411.48046875Mb
2020-10-12 23:37:35 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.496 | nll_loss 3.917 | ppl 15.11 | wps 89022 | wpb 4554.7 | bsz 174.1 | num_updates 6330 | best_loss 5.496
2020-10-12 23:37:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:37:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 30 @ 6330 updates, score 5.496) (writing took 8.29650435000076 seconds)
2020-10-12 23:37:44 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 23:37:44 | INFO | train | epoch 030 | loss 5.394 | nll_loss 3.938 | ppl 15.33 | wps 36940.2 | ups 2.59 | wpb 14249.3 | bsz 535.4 | num_updates 6330 | lr 0.000158986 | gnorm 0.841 | clip 0 | train_wall 66 | wall 2335
2020-10-12 23:37:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 23:37:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 23:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21647.140625Mb; avail=466841.2265625Mb
2020-10-12 23:37:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003517
2020-10-12 23:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027187
2020-10-12 23:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21647.140625Mb; avail=466841.2265625Mb
2020-10-12 23:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000957
2020-10-12 23:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21647.140625Mb; avail=466841.2265625Mb
2020-10-12 23:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.460638
2020-10-12 23:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.490123
2020-10-12 23:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21647.1640625Mb; avail=466841.02734375Mb
2020-10-12 23:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21647.265625Mb; avail=466841.1171875Mb
2020-10-12 23:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.018384
2020-10-12 23:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21647.28125Mb; avail=466841.1015625Mb
2020-10-12 23:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000942
2020-10-12 23:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21647.28125Mb; avail=466841.1015625Mb
2020-10-12 23:37:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.428602
2020-10-12 23:37:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.448896
2020-10-12 23:37:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21290.9296875Mb; avail=467197.671875Mb
2020-10-12 23:37:45 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 23:38:08 | INFO | train_inner | epoch 031:     70 / 211 loss=5.328, nll_loss=3.863, ppl=14.55, wps=31017.6, ups=2.19, wpb=14151.9, bsz=524.5, num_updates=6400, lr=0.000158114, gnorm=0.826, clip=0, train_wall=31, wall=2360
2020-10-12 23:38:40 | INFO | train_inner | epoch 031:    170 / 211 loss=5.356, nll_loss=3.894, ppl=14.86, wps=45317, ups=3.14, wpb=14410.3, bsz=514.1, num_updates=6500, lr=0.000156893, gnorm=0.81, clip=0, train_wall=31, wall=2391
2020-10-12 23:38:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21505.078125Mb; avail=466984.49609375Mb
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001435
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21505.078125Mb; avail=466984.49609375Mb
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047201
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21516.33203125Mb; avail=466973.2421875Mb
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034358
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083819
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21526.01953125Mb; avail=466963.5546875Mb
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21530.86328125Mb; avail=466958.7109375Mb
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000931
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21532.07421875Mb; avail=466957.5Mb
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047468
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21544.296875Mb; avail=466945.27734375Mb
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034299
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083448
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21551.8203125Mb; avail=466937.75390625Mb
2020-10-12 23:38:56 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 5.46 | nll_loss 3.876 | ppl 14.69 | wps 87377.6 | wpb 4554.7 | bsz 174.1 | num_updates 6541 | best_loss 5.46
2020-10-12 23:38:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:39:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 31 @ 6541 updates, score 5.46) (writing took 8.540257426000608 seconds)
2020-10-12 23:39:04 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 23:39:04 | INFO | train | epoch 031 | loss 5.323 | nll_loss 3.856 | ppl 14.49 | wps 37297.5 | ups 2.62 | wpb 14249.3 | bsz 535.4 | num_updates 6541 | lr 0.0001564 | gnorm 0.816 | clip 0 | train_wall 66 | wall 2416
2020-10-12 23:39:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 23:39:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 23:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21334.02734375Mb; avail=467154.38671875Mb
2020-10-12 23:39:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008310
2020-10-12 23:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032545
2020-10-12 23:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21335.8984375Mb; avail=467152.515625Mb
2020-10-12 23:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000890
2020-10-12 23:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21335.77734375Mb; avail=467152.63671875Mb
2020-10-12 23:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.404749
2020-10-12 23:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.439222
2020-10-12 23:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21377.296875Mb; avail=467111.46875Mb
2020-10-12 23:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21378.27734375Mb; avail=467110.86328125Mb
2020-10-12 23:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.017320
2020-10-12 23:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21378.8828125Mb; avail=467110.2578125Mb
2020-10-12 23:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000940
2020-10-12 23:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21378.8828125Mb; avail=467110.2578125Mb
2020-10-12 23:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.397467
2020-10-12 23:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.416600
2020-10-12 23:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21415.2890625Mb; avail=467073.046875Mb
2020-10-12 23:39:05 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 23:39:25 | INFO | train_inner | epoch 032:     59 / 211 loss=5.269, nll_loss=3.795, ppl=13.88, wps=31447.4, ups=2.21, wpb=14210.4, bsz=556.3, num_updates=6600, lr=0.0001557, gnorm=0.855, clip=0, train_wall=31, wall=2437
2020-10-12 23:39:57 | INFO | train_inner | epoch 032:    159 / 211 loss=5.262, nll_loss=3.786, ppl=13.79, wps=45061.9, ups=3.14, wpb=14372, bsz=531, num_updates=6700, lr=0.000154533, gnorm=0.838, clip=0, train_wall=31, wall=2468
2020-10-12 23:40:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21314.05078125Mb; avail=467174.6015625Mb
2020-10-12 23:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001392
2020-10-12 23:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21314.05078125Mb; avail=467174.6015625Mb
2020-10-12 23:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048162
2020-10-12 23:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21292.41796875Mb; avail=467196.13671875Mb
2020-10-12 23:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034452
2020-10-12 23:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084801
2020-10-12 23:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21137.5234375Mb; avail=467351.12109375Mb
2020-10-12 23:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21065.84765625Mb; avail=467422.80078125Mb
2020-10-12 23:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000908
2020-10-12 23:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21067.05859375Mb; avail=467421.58984375Mb
2020-10-12 23:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047666
2020-10-12 23:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21105.671875Mb; avail=467382.9765625Mb
2020-10-12 23:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034251
2020-10-12 23:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083572
2020-10-12 23:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21140.1875Mb; avail=467348.05859375Mb
2020-10-12 23:40:16 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.429 | nll_loss 3.839 | ppl 14.31 | wps 84204.9 | wpb 4554.7 | bsz 174.1 | num_updates 6752 | best_loss 5.429
2020-10-12 23:40:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:40:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 32 @ 6752 updates, score 5.429) (writing took 6.64073625599849 seconds)
2020-10-12 23:40:23 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 23:40:23 | INFO | train | epoch 032 | loss 5.263 | nll_loss 3.788 | ppl 13.81 | wps 38235.6 | ups 2.68 | wpb 14249.3 | bsz 535.4 | num_updates 6752 | lr 0.000153937 | gnorm 0.856 | clip 0 | train_wall 66 | wall 2494
2020-10-12 23:40:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 23:40:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 23:40:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21411.86328125Mb; avail=467075.75Mb
2020-10-12 23:40:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002935
2020-10-12 23:40:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026943
2020-10-12 23:40:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21411.86328125Mb; avail=467075.75Mb
2020-10-12 23:40:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001513
2020-10-12 23:40:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21411.86328125Mb; avail=467075.75Mb
2020-10-12 23:40:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.454918
2020-10-12 23:40:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.485065
2020-10-12 23:40:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21413.046875Mb; avail=467075.16015625Mb
2020-10-12 23:40:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21412.92578125Mb; avail=467075.28125Mb
2020-10-12 23:40:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016906
2020-10-12 23:40:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21412.92578125Mb; avail=467075.28125Mb
2020-10-12 23:40:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000842
2020-10-12 23:40:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21412.92578125Mb; avail=467075.28125Mb
2020-10-12 23:40:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.392786
2020-10-12 23:40:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.411304
2020-10-12 23:40:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21412.57421875Mb; avail=467075.7734375Mb
2020-10-12 23:40:24 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 23:40:40 | INFO | train_inner | epoch 033:     48 / 211 loss=5.237, nll_loss=3.757, ppl=13.52, wps=32818.8, ups=2.3, wpb=14246.4, bsz=539.4, num_updates=6800, lr=0.000153393, gnorm=0.841, clip=0, train_wall=31, wall=2512
2020-10-12 23:41:12 | INFO | train_inner | epoch 033:    148 / 211 loss=5.186, nll_loss=3.699, ppl=12.99, wps=44943.5, ups=3.13, wpb=14339.4, bsz=564.7, num_updates=6900, lr=0.000152277, gnorm=0.811, clip=0, train_wall=31, wall=2544
2020-10-12 23:41:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:41:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21707.171875Mb; avail=466782.0234375Mb
2020-10-12 23:41:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001360
2020-10-12 23:41:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21707.77734375Mb; avail=466781.41796875Mb
2020-10-12 23:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047082
2020-10-12 23:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21715.76171875Mb; avail=466773.43359375Mb
2020-10-12 23:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035290
2020-10-12 23:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084532
2020-10-12 23:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21721.55078125Mb; avail=466767.44140625Mb
2020-10-12 23:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21723.06640625Mb; avail=466766.1171875Mb
2020-10-12 23:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000983
2020-10-12 23:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21723.671875Mb; avail=466765.51171875Mb
2020-10-12 23:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047182
2020-10-12 23:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21727.41796875Mb; avail=466761.16015625Mb
2020-10-12 23:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034880
2020-10-12 23:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083786
2020-10-12 23:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21731.65625Mb; avail=466757.52734375Mb
2020-10-12 23:41:35 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 5.386 | nll_loss 3.787 | ppl 13.8 | wps 92035.3 | wpb 4554.7 | bsz 174.1 | num_updates 6963 | best_loss 5.386
2020-10-12 23:41:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:41:39 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 33 @ 6963 updates, score 5.386) (writing took 4.561792798998795 seconds)
2020-10-12 23:41:39 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 23:41:39 | INFO | train | epoch 033 | loss 5.2 | nll_loss 3.715 | ppl 13.13 | wps 39231.6 | ups 2.75 | wpb 14249.3 | bsz 535.4 | num_updates 6963 | lr 0.000151587 | gnorm 0.818 | clip 0 | train_wall 66 | wall 2571
2020-10-12 23:41:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 23:41:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 23:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21417.83203125Mb; avail=467070.578125Mb
2020-10-12 23:41:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003456
2020-10-12 23:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026718
2020-10-12 23:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21418.4375Mb; avail=467069.97265625Mb
2020-10-12 23:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001079
2020-10-12 23:41:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21418.4375Mb; avail=467069.97265625Mb
2020-10-12 23:41:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.413583
2020-10-12 23:41:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.442374
2020-10-12 23:41:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21418.1484375Mb; avail=467070.66015625Mb
2020-10-12 23:41:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21417.703125Mb; avail=467069.8125Mb
2020-10-12 23:41:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015915
2020-10-12 23:41:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21417.4765625Mb; avail=467070.66015625Mb
2020-10-12 23:41:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000820
2020-10-12 23:41:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21417.4765625Mb; avail=467070.66015625Mb
2020-10-12 23:41:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.399055
2020-10-12 23:41:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.416575
2020-10-12 23:41:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21417.61328125Mb; avail=467070.41796875Mb
2020-10-12 23:41:40 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 23:41:53 | INFO | train_inner | epoch 034:     37 / 211 loss=5.192, nll_loss=3.706, ppl=13.05, wps=34348.3, ups=2.43, wpb=14132.5, bsz=511.8, num_updates=7000, lr=0.000151186, gnorm=0.808, clip=0, train_wall=31, wall=2585
2020-10-12 23:42:25 | INFO | train_inner | epoch 034:    137 / 211 loss=5.138, nll_loss=3.644, ppl=12.5, wps=44072.1, ups=3.13, wpb=14082.8, bsz=547.8, num_updates=7100, lr=0.000150117, gnorm=0.799, clip=0, train_wall=31, wall=2617
2020-10-12 23:42:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21237.12109375Mb; avail=467251.37890625Mb
2020-10-12 23:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001336
2020-10-12 23:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21236.86328125Mb; avail=467251.64453125Mb
2020-10-12 23:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047040
2020-10-12 23:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21236.62109375Mb; avail=467251.98046875Mb
2020-10-12 23:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034098
2020-10-12 23:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083334
2020-10-12 23:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21236.62109375Mb; avail=467251.98046875Mb
2020-10-12 23:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21236.62109375Mb; avail=467251.98046875Mb
2020-10-12 23:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001035
2020-10-12 23:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21236.62109375Mb; avail=467251.98046875Mb
2020-10-12 23:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047389
2020-10-12 23:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21236.73046875Mb; avail=467251.859375Mb
2020-10-12 23:42:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034002
2020-10-12 23:42:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083322
2020-10-12 23:42:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21236.73046875Mb; avail=467251.859375Mb
2020-10-12 23:42:52 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 5.364 | nll_loss 3.76 | ppl 13.55 | wps 90240.1 | wpb 4554.7 | bsz 174.1 | num_updates 7174 | best_loss 5.364
2020-10-12 23:42:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:42:56 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 34 @ 7174 updates, score 5.364) (writing took 4.504623803000868 seconds)
2020-10-12 23:42:56 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 23:42:56 | INFO | train | epoch 034 | loss 5.146 | nll_loss 3.654 | ppl 12.58 | wps 39137.6 | ups 2.75 | wpb 14249.3 | bsz 535.4 | num_updates 7174 | lr 0.000149341 | gnorm 0.808 | clip 0 | train_wall 66 | wall 2648
2020-10-12 23:42:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 23:42:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 23:42:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21426.45703125Mb; avail=467061.38671875Mb
2020-10-12 23:42:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004364
2020-10-12 23:42:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032816
2020-10-12 23:42:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21425.89453125Mb; avail=467062.0Mb
2020-10-12 23:42:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000910
2020-10-12 23:42:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21425.89453125Mb; avail=467062.0Mb
2020-10-12 23:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.446247
2020-10-12 23:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.480883
2020-10-12 23:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21426.15625Mb; avail=467061.15234375Mb
2020-10-12 23:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21426.15625Mb; avail=467061.15234375Mb
2020-10-12 23:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016165
2020-10-12 23:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21426.03515625Mb; avail=467061.2734375Mb
2020-10-12 23:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000866
2020-10-12 23:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21426.03515625Mb; avail=467061.2734375Mb
2020-10-12 23:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.398908
2020-10-12 23:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.416838
2020-10-12 23:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21426.5703125Mb; avail=467061.5Mb
2020-10-12 23:42:57 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 23:43:07 | INFO | train_inner | epoch 035:     26 / 211 loss=5.129, nll_loss=3.634, ppl=12.42, wps=34560.7, ups=2.42, wpb=14281.4, bsz=532.6, num_updates=7200, lr=0.000149071, gnorm=0.812, clip=0, train_wall=31, wall=2658
2020-10-12 23:43:39 | INFO | train_inner | epoch 035:    126 / 211 loss=5.094, nll_loss=3.594, ppl=12.07, wps=44747.4, ups=3.13, wpb=14275.3, bsz=544.6, num_updates=7300, lr=0.000148047, gnorm=0.812, clip=0, train_wall=31, wall=2690
2020-10-12 23:44:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21583.88671875Mb; avail=466904.94921875Mb
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001584
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21583.88671875Mb; avail=466904.94921875Mb
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.055100
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21583.765625Mb; avail=466905.0703125Mb
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.039738
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.097297
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21583.765625Mb; avail=466905.0703125Mb
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21583.765625Mb; avail=466905.0703125Mb
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000950
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21583.765625Mb; avail=466905.0703125Mb
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.055368
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21583.765625Mb; avail=466905.0703125Mb
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.039624
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.096757
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21583.765625Mb; avail=466905.0703125Mb
2020-10-12 23:44:08 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 5.324 | nll_loss 3.71 | ppl 13.09 | wps 87881.9 | wpb 4554.7 | bsz 174.1 | num_updates 7385 | best_loss 5.324
2020-10-12 23:44:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:44:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 35 @ 7385 updates, score 5.324) (writing took 4.443722659998457 seconds)
2020-10-12 23:44:13 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 23:44:13 | INFO | train | epoch 035 | loss 5.094 | nll_loss 3.593 | ppl 12.07 | wps 39273.2 | ups 2.76 | wpb 14249.3 | bsz 535.4 | num_updates 7385 | lr 0.000147192 | gnorm 0.803 | clip 0 | train_wall 66 | wall 2724
2020-10-12 23:44:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 23:44:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 23:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21421.61328125Mb; avail=467066.43359375Mb
2020-10-12 23:44:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003142
2020-10-12 23:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025733
2020-10-12 23:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21421.55859375Mb; avail=467066.0703125Mb
2020-10-12 23:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000952
2020-10-12 23:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21421.55859375Mb; avail=467066.0703125Mb
2020-10-12 23:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.403925
2020-10-12 23:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.431515
2020-10-12 23:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21421.75Mb; avail=467066.31640625Mb
2020-10-12 23:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21421.75Mb; avail=467066.31640625Mb
2020-10-12 23:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015781
2020-10-12 23:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21421.75Mb; avail=467066.31640625Mb
2020-10-12 23:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000816
2020-10-12 23:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21421.75Mb; avail=467066.31640625Mb
2020-10-12 23:44:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.395756
2020-10-12 23:44:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.413134
2020-10-12 23:44:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21421.1171875Mb; avail=467067.40625Mb
2020-10-12 23:44:14 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 23:44:20 | INFO | train_inner | epoch 036:     15 / 211 loss=5.09, nll_loss=3.588, ppl=12.02, wps=34478.3, ups=2.43, wpb=14203.2, bsz=526.3, num_updates=7400, lr=0.000147043, gnorm=0.795, clip=0, train_wall=31, wall=2731
2020-10-12 23:44:52 | INFO | train_inner | epoch 036:    115 / 211 loss=5.032, nll_loss=3.523, ppl=11.49, wps=44500.6, ups=3.12, wpb=14247.9, bsz=535.2, num_updates=7500, lr=0.000146059, gnorm=0.789, clip=0, train_wall=31, wall=2763
2020-10-12 23:45:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21224.390625Mb; avail=467264.015625Mb
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002392
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21224.40625Mb; avail=467263.89453125Mb
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047595
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21224.265625Mb; avail=467264.13671875Mb
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034224
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.085009
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21224.265625Mb; avail=467264.13671875Mb
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21224.265625Mb; avail=467264.13671875Mb
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000908
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21224.84375Mb; avail=467263.55859375Mb
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.052329
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21224.84375Mb; avail=467263.55859375Mb
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.037933
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.091977
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21224.84375Mb; avail=467263.55859375Mb
2020-10-12 23:45:25 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 5.299 | nll_loss 3.693 | ppl 12.93 | wps 87351.6 | wpb 4554.7 | bsz 174.1 | num_updates 7596 | best_loss 5.299
2020-10-12 23:45:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:45:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 36 @ 7596 updates, score 5.299) (writing took 4.446501059999719 seconds)
2020-10-12 23:45:30 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 23:45:30 | INFO | train | epoch 036 | loss 5.041 | nll_loss 3.532 | ppl 11.57 | wps 39128.4 | ups 2.75 | wpb 14249.3 | bsz 535.4 | num_updates 7596 | lr 0.000145133 | gnorm 0.787 | clip 0 | train_wall 66 | wall 2801
2020-10-12 23:45:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 23:45:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 23:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21414.8515625Mb; avail=467072.81640625Mb
2020-10-12 23:45:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002946
2020-10-12 23:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025454
2020-10-12 23:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21415.45703125Mb; avail=467072.2109375Mb
2020-10-12 23:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000966
2020-10-12 23:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21415.45703125Mb; avail=467072.2109375Mb
2020-10-12 23:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.401913
2020-10-12 23:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.429273
2020-10-12 23:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21415.30859375Mb; avail=467072.8203125Mb
2020-10-12 23:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21415.30859375Mb; avail=467072.8203125Mb
2020-10-12 23:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015700
2020-10-12 23:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21415.30859375Mb; avail=467072.8203125Mb
2020-10-12 23:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000817
2020-10-12 23:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21415.30859375Mb; avail=467072.8203125Mb
2020-10-12 23:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.400507
2020-10-12 23:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.417866
2020-10-12 23:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21414.91796875Mb; avail=467073.18359375Mb
2020-10-12 23:45:31 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 23:45:33 | INFO | train_inner | epoch 037:      4 / 211 loss=5.053, nll_loss=3.545, ppl=11.67, wps=34812.5, ups=2.43, wpb=14316.4, bsz=536.2, num_updates=7600, lr=0.000145095, gnorm=0.788, clip=0, train_wall=31, wall=2804
2020-10-12 23:46:05 | INFO | train_inner | epoch 037:    104 / 211 loss=4.985, nll_loss=3.469, ppl=11.07, wps=44740.8, ups=3.11, wpb=14399.4, bsz=534.1, num_updates=7700, lr=0.00014415, gnorm=0.803, clip=0, train_wall=31, wall=2837
2020-10-12 23:46:37 | INFO | train_inner | epoch 037:    204 / 211 loss=5.011, nll_loss=3.498, ppl=11.3, wps=44125.1, ups=3.13, wpb=14112.3, bsz=530, num_updates=7800, lr=0.000143223, gnorm=0.797, clip=0, train_wall=31, wall=2869
2020-10-12 23:46:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:46:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21257.58203125Mb; avail=467231.2890625Mb
2020-10-12 23:46:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001343
2020-10-12 23:46:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21257.58203125Mb; avail=467231.2890625Mb
2020-10-12 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048224
2020-10-12 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21262.42578125Mb; avail=467226.4453125Mb
2020-10-12 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034388
2020-10-12 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084752
2020-10-12 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21264.55078125Mb; avail=467224.3203125Mb
2020-10-12 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21265.15625Mb; avail=467223.71484375Mb
2020-10-12 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000990
2020-10-12 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21265.15625Mb; avail=467223.71484375Mb
2020-10-12 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047749
2020-10-12 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21276.0546875Mb; avail=467212.81640625Mb
2020-10-12 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034124
2020-10-12 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083609
2020-10-12 23:46:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21279.7109375Mb; avail=467208.94140625Mb
2020-10-12 23:46:42 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 5.295 | nll_loss 3.678 | ppl 12.8 | wps 92586.7 | wpb 4554.7 | bsz 174.1 | num_updates 7807 | best_loss 5.295
2020-10-12 23:46:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:46:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 37 @ 7807 updates, score 5.295) (writing took 4.468835227002273 seconds)
2020-10-12 23:46:46 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 23:46:46 | INFO | train | epoch 037 | loss 4.997 | nll_loss 3.483 | ppl 11.18 | wps 39205.2 | ups 2.75 | wpb 14249.3 | bsz 535.4 | num_updates 7807 | lr 0.000143159 | gnorm 0.802 | clip 0 | train_wall 66 | wall 2878
2020-10-12 23:46:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 23:46:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 23:46:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21414.79296875Mb; avail=467072.78515625Mb
2020-10-12 23:46:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003185
2020-10-12 23:46:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026317
2020-10-12 23:46:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21414.8671875Mb; avail=467073.02734375Mb
2020-10-12 23:46:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000959
2020-10-12 23:46:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21414.8671875Mb; avail=467073.02734375Mb
2020-10-12 23:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.404060
2020-10-12 23:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.432234
2020-10-12 23:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21415.390625Mb; avail=467072.05859375Mb
2020-10-12 23:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21415.390625Mb; avail=467072.05859375Mb
2020-10-12 23:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015849
2020-10-12 23:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21415.390625Mb; avail=467072.05859375Mb
2020-10-12 23:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000873
2020-10-12 23:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21415.390625Mb; avail=467072.05859375Mb
2020-10-12 23:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.396971
2020-10-12 23:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.414482
2020-10-12 23:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21415.8515625Mb; avail=467072.3046875Mb
2020-10-12 23:46:47 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 23:47:18 | INFO | train_inner | epoch 038:     93 / 211 loss=4.936, nll_loss=3.412, ppl=10.65, wps=34684.4, ups=2.45, wpb=14175, bsz=542.2, num_updates=7900, lr=0.000142314, gnorm=0.775, clip=0, train_wall=31, wall=2910
2020-10-12 23:47:50 | INFO | train_inner | epoch 038:    193 / 211 loss=4.967, nll_loss=3.446, ppl=10.9, wps=45169.2, ups=3.13, wpb=14449.2, bsz=534.8, num_updates=8000, lr=0.000141421, gnorm=0.808, clip=0, train_wall=31, wall=2942
2020-10-12 23:47:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:47:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13796.8359375Mb; avail=474730.00390625Mb
2020-10-12 23:47:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001276
2020-10-12 23:47:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13796.8359375Mb; avail=474729.3984375Mb
2020-10-12 23:47:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047473
2020-10-12 23:47:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13804.48046875Mb; avail=474721.75390625Mb
2020-10-12 23:47:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034284
2020-10-12 23:47:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084058
2020-10-12 23:47:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13819.640625Mb; avail=474707.1171875Mb
2020-10-12 23:47:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13826.0625Mb; avail=474700.08984375Mb
2020-10-12 23:47:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001089
2020-10-12 23:47:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13827.87890625Mb; avail=474698.87890625Mb
2020-10-12 23:47:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047890
2020-10-12 23:47:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13908.56640625Mb; avail=474618.1875Mb
2020-10-12 23:47:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034371
2020-10-12 23:47:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084231
2020-10-12 23:47:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13969.71875Mb; avail=474556.4296875Mb
2020-10-12 23:47:59 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 5.253 | nll_loss 3.63 | ppl 12.38 | wps 90098.3 | wpb 4554.7 | bsz 174.1 | num_updates 8018 | best_loss 5.253
2020-10-12 23:47:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:48:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 38 @ 8018 updates, score 5.253) (writing took 8.94657356800235 seconds)
2020-10-12 23:48:08 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 23:48:08 | INFO | train | epoch 038 | loss 4.952 | nll_loss 3.43 | ppl 10.78 | wps 36952.5 | ups 2.59 | wpb 14249.3 | bsz 535.4 | num_updates 8018 | lr 0.000141263 | gnorm 0.792 | clip 0 | train_wall 66 | wall 2959
2020-10-12 23:48:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 23:48:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 23:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16136.27734375Mb; avail=472377.16796875Mb
2020-10-12 23:48:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002439
2020-10-12 23:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022659
2020-10-12 23:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16135.75390625Mb; avail=472377.66015625Mb
2020-10-12 23:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000839
2020-10-12 23:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16135.75390625Mb; avail=472377.66015625Mb
2020-10-12 23:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.400652
2020-10-12 23:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.424938
2020-10-12 23:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16134.953125Mb; avail=472378.57421875Mb
2020-10-12 23:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16134.953125Mb; avail=472378.57421875Mb
2020-10-12 23:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015691
2020-10-12 23:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16134.953125Mb; avail=472378.57421875Mb
2020-10-12 23:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000803
2020-10-12 23:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16134.953125Mb; avail=472378.57421875Mb
2020-10-12 23:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.397886
2020-10-12 23:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.415262
2020-10-12 23:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16135.08984375Mb; avail=472378.46875Mb
2020-10-12 23:48:09 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 23:48:36 | INFO | train_inner | epoch 039:     82 / 211 loss=4.927, nll_loss=3.401, ppl=10.56, wps=30982.1, ups=2.18, wpb=14184.1, bsz=497.2, num_updates=8100, lr=0.000140546, gnorm=0.785, clip=0, train_wall=31, wall=2987
2020-10-12 23:49:07 | INFO | train_inner | epoch 039:    182 / 211 loss=4.898, nll_loss=3.369, ppl=10.33, wps=45021.7, ups=3.17, wpb=14207.9, bsz=558.4, num_updates=8200, lr=0.000139686, gnorm=0.777, clip=0, train_wall=31, wall=3019
2020-10-12 23:49:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:49:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16178.34375Mb; avail=472335.31640625Mb
2020-10-12 23:49:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001445
2020-10-12 23:49:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16178.34375Mb; avail=472335.31640625Mb
2020-10-12 23:49:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048603
2020-10-12 23:49:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16178.22265625Mb; avail=472335.4375Mb
2020-10-12 23:49:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034332
2020-10-12 23:49:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.085171
2020-10-12 23:49:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16178.0234375Mb; avail=472336.04296875Mb
2020-10-12 23:49:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16178.0234375Mb; avail=472336.04296875Mb
2020-10-12 23:49:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000909
2020-10-12 23:49:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16178.0234375Mb; avail=472336.04296875Mb
2020-10-12 23:49:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047850
2020-10-12 23:49:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16178.0234375Mb; avail=472336.04296875Mb
2020-10-12 23:49:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036023
2020-10-12 23:49:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.085532
2020-10-12 23:49:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16178.0234375Mb; avail=472336.04296875Mb
2020-10-12 23:49:19 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 5.243 | nll_loss 3.618 | ppl 12.28 | wps 92381.2 | wpb 4554.7 | bsz 174.1 | num_updates 8229 | best_loss 5.243
2020-10-12 23:49:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:49:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 39 @ 8229 updates, score 5.243) (writing took 4.467831101002957 seconds)
2020-10-12 23:49:23 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 23:49:23 | INFO | train | epoch 039 | loss 4.907 | nll_loss 3.378 | ppl 10.4 | wps 39709.9 | ups 2.79 | wpb 14249.3 | bsz 535.4 | num_updates 8229 | lr 0.00013944 | gnorm 0.78 | clip 0 | train_wall 65 | wall 3035
2020-10-12 23:49:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 23:49:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 23:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13727.30859375Mb; avail=474798.1640625Mb
2020-10-12 23:49:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002824
2020-10-12 23:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025508
2020-10-12 23:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13727.30859375Mb; avail=474798.1640625Mb
2020-10-12 23:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000924
2020-10-12 23:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13727.30859375Mb; avail=474798.1640625Mb
2020-10-12 23:49:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.399849
2020-10-12 23:49:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.427212
2020-10-12 23:49:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13727.1015625Mb; avail=474798.76953125Mb
2020-10-12 23:49:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13727.23046875Mb; avail=474798.30859375Mb
2020-10-12 23:49:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015608
2020-10-12 23:49:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13727.23046875Mb; avail=474798.30859375Mb
2020-10-12 23:49:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000832
2020-10-12 23:49:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13727.23046875Mb; avail=474798.30859375Mb
2020-10-12 23:49:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.396013
2020-10-12 23:49:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.413319
2020-10-12 23:49:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13727.484375Mb; avail=474797.82421875Mb
2020-10-12 23:49:24 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 23:49:48 | INFO | train_inner | epoch 040:     71 / 211 loss=4.864, nll_loss=3.328, ppl=10.04, wps=35160.5, ups=2.46, wpb=14309, bsz=552.6, num_updates=8300, lr=0.000138842, gnorm=0.767, clip=0, train_wall=31, wall=3060
2020-10-12 23:50:20 | INFO | train_inner | epoch 040:    171 / 211 loss=4.878, nll_loss=3.344, ppl=10.16, wps=44260.2, ups=3.11, wpb=14253.3, bsz=521.7, num_updates=8400, lr=0.000138013, gnorm=0.793, clip=0, train_wall=31, wall=3092
2020-10-12 23:50:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13654.3359375Mb; avail=474871.40234375Mb
2020-10-12 23:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001381
2020-10-12 23:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13654.3359375Mb; avail=474871.40234375Mb
2020-10-12 23:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047599
2020-10-12 23:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13654.11328125Mb; avail=474871.5234375Mb
2020-10-12 23:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034557
2020-10-12 23:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084332
2020-10-12 23:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13654.11328125Mb; avail=474871.5234375Mb
2020-10-12 23:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13654.11328125Mb; avail=474871.5234375Mb
2020-10-12 23:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000885
2020-10-12 23:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13654.11328125Mb; avail=474871.5234375Mb
2020-10-12 23:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047282
2020-10-12 23:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13654.11328125Mb; avail=474871.5234375Mb
2020-10-12 23:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034584
2020-10-12 23:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.083514
2020-10-12 23:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13654.11328125Mb; avail=474871.5234375Mb
2020-10-12 23:50:35 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 5.222 | nll_loss 3.596 | ppl 12.09 | wps 91387.5 | wpb 4554.7 | bsz 174.1 | num_updates 8440 | best_loss 5.222
2020-10-12 23:50:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:50:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/O2M/checkpoint_best.pt (epoch 40 @ 8440 updates, score 5.222) (writing took 4.464043577998382 seconds)
2020-10-12 23:50:40 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 23:50:40 | INFO | train | epoch 040 | loss 4.869 | nll_loss 3.335 | ppl 10.09 | wps 39359.8 | ups 2.76 | wpb 14249.3 | bsz 535.4 | num_updates 8440 | lr 0.000137686 | gnorm 0.778 | clip 0 | train_wall 66 | wall 3111
2020-10-12 23:50:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 23:50:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 23:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=11230.75390625Mb; avail=477307.109375Mb
2020-10-12 23:50:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003641
2020-10-12 23:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027388
2020-10-12 23:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=11230.75390625Mb; avail=477307.109375Mb
2020-10-12 23:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001071
2020-10-12 23:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=11230.75390625Mb; avail=477307.109375Mb
2020-10-12 23:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.400489
2020-10-12 23:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.429846
2020-10-12 23:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=11230.16796875Mb; avail=477307.85546875Mb
2020-10-12 23:50:40 | INFO | fairseq_cli.train | done training in 3111.3 seconds
