2020-10-12 22:06:00 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:16051
2020-10-12 22:06:00 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:16051
2020-10-12 22:06:00 | INFO | fairseq.distributed_utils | initialized host ip-172-31-31-94 as rank 1
2020-10-12 22:06:00 | INFO | fairseq.distributed_utils | initialized host ip-172-31-31-94 as rank 0
2020-10-12 22:06:04 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 22:06:04 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_belukrLG_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:16051', distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='bel-eng,ukr-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 22:06:04 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 22:06:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['bel', 'eng', 'ukr']
2020-10-12 22:06:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bel] dictionary: 21771 types
2020-10-12 22:06:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 21771 types
2020-10-12 22:06:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | [ukr] dictionary: 21771 types
2020-10-12 22:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 22:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12928.7890625Mb; avail=475620.5625Mb
2020-10-12 22:06:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 22:06:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:bel-eng': 1, 'main:ukr-eng': 1}
2020-10-12 22:06:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-12 22:06:04 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belukrLG_sepspm8000/M2O/valid.bel-eng.bel
2020-10-12 22:06:04 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belukrLG_sepspm8000/M2O/valid.bel-eng.eng
2020-10-12 22:06:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belukrLG_sepspm8000/M2O/ valid bel-eng 248 examples
2020-10-12 22:06:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:ukr-eng src_langtok: None; tgt_langtok: None
2020-10-12 22:06:04 | INFO | fairseq.data.data_utils | loaded 3060 examples from: fairseq/data-bin/ted_belukrLG_sepspm8000/M2O/valid.ukr-eng.ukr
2020-10-12 22:06:04 | INFO | fairseq.data.data_utils | loaded 3060 examples from: fairseq/data-bin/ted_belukrLG_sepspm8000/M2O/valid.ukr-eng.eng
2020-10-12 22:06:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belukrLG_sepspm8000/M2O/ valid ukr-eng 3060 examples
2020-10-12 22:06:05 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21771, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21771, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=21771, bias=False)
  )
)
2020-10-12 22:06:05 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 22:06:05 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 22:06:05 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 22:06:05 | INFO | fairseq_cli.train | num. model params: 42690048 (num. trained: 42690048)
2020-10-12 22:06:05 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 22:06:05 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 22:06:05 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************
2020-10-12 22:06:05 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 22:06:05 | INFO | fairseq.utils | rank   1: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 22:06:05 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************
2020-10-12 22:06:05 | INFO | fairseq_cli.train | training on 2 devices (GPUs/TPUs)
2020-10-12 22:06:05 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 22:06:05 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_last.pt
2020-10-12 22:06:05 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 22:06:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 22:06:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13177.80078125Mb; avail=475371.546875Mb
2020-10-12 22:06:05 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 22:06:05 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:bel-eng': 1, 'main:ukr-eng': 1}
2020-10-12 22:06:05 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-12 22:06:05 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belukrLG_sepspm8000/M2O/train.bel-eng.bel
2020-10-12 22:06:05 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belukrLG_sepspm8000/M2O/train.bel-eng.eng
2020-10-12 22:06:05 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belukrLG_sepspm8000/M2O/ train bel-eng 4509 examples
2020-10-12 22:06:05 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:ukr-eng src_langtok: None; tgt_langtok: None
2020-10-12 22:06:05 | INFO | fairseq.data.data_utils | loaded 108463 examples from: fairseq/data-bin/ted_belukrLG_sepspm8000/M2O/train.ukr-eng.ukr
2020-10-12 22:06:05 | INFO | fairseq.data.data_utils | loaded 108463 examples from: fairseq/data-bin/ted_belukrLG_sepspm8000/M2O/train.ukr-eng.eng
2020-10-12 22:06:05 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belukrLG_sepspm8000/M2O/ train ukr-eng 108463 examples
2020-10-12 22:06:05 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:bel-eng', 4509), ('main:ukr-eng', 108463)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 22:06:05 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 112972
2020-10-12 22:06:05 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 22:06:05 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 112972
2020-10-12 22:06:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 112972; virtual dataset size 112972
2020-10-12 22:06:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 22:06:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:bel-eng': 4509, 'main:ukr-eng': 108463}; raw total size: 112972
2020-10-12 22:06:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:bel-eng': 4509, 'main:ukr-eng': 108463}; resampled total size: 112972
2020-10-12 22:06:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 22:06:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.012280
2020-10-12 22:06:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13180.7734375Mb; avail=475369.06640625Mb
2020-10-12 22:06:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001935
2020-10-12 22:06:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.019864
2020-10-12 22:06:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13179.7890625Mb; avail=475369.55859375Mb
2020-10-12 22:06:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000871
2020-10-12 22:06:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13179.7890625Mb; avail=475369.55859375Mb
2020-10-12 22:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.461552
2020-10-12 22:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.483560
2020-10-12 22:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13183.671875Mb; avail=475365.7265625Mb
2020-10-12 22:06:06 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 22:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13183.03515625Mb; avail=475366.25Mb
2020-10-12 22:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016208
2020-10-12 22:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13183.03515625Mb; avail=475366.25Mb
2020-10-12 22:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000809
2020-10-12 22:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13183.03515625Mb; avail=475366.25Mb
2020-10-12 22:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.438579
2020-10-12 22:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.457094
2020-10-12 22:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13210.6640625Mb; avail=475338.6171875Mb
2020-10-12 22:06:06 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 22:06:40 | INFO | train_inner | epoch 001:    100 / 207 loss=14.093, nll_loss=13.977, ppl=16124.9, wps=44985, ups=3.08, wpb=14590.7, bsz=568.5, num_updates=100, lr=5.0975e-06, gnorm=4.191, clip=0, train_wall=32, wall=35
2020-10-12 22:07:13 | INFO | train_inner | epoch 001:    200 / 207 loss=12.322, nll_loss=12, ppl=4094.6, wps=44898, ups=3.09, wpb=14534, bsz=536.4, num_updates=200, lr=1.0095e-05, gnorm=1.752, clip=0, train_wall=31, wall=68
2020-10-12 22:07:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21233.15625Mb; avail=467254.5625Mb
2020-10-12 22:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001485
2020-10-12 22:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21233.1640625Mb; avail=467254.44140625Mb
2020-10-12 22:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049393
2020-10-12 22:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21233.13671875Mb; avail=467254.68359375Mb
2020-10-12 22:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036271
2020-10-12 22:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087963
2020-10-12 22:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21233.13671875Mb; avail=467254.68359375Mb
2020-10-12 22:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21233.13671875Mb; avail=467254.68359375Mb
2020-10-12 22:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000968
2020-10-12 22:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21233.13671875Mb; avail=467254.68359375Mb
2020-10-12 22:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049070
2020-10-12 22:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21233.109375Mb; avail=467254.68359375Mb
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2020-10-12 22:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035828
2020-10-12 22:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086626
2020-10-12 22:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21233.125Mb; avail=467254.92578125Mb
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2020-10-12 22:07:17 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.523 | nll_loss 11.091 | ppl 2180.9 | wps 96859.1 | wpb 4817.7 | bsz 183.8 | num_updates 207
2020-10-12 22:07:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:07:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 1 @ 207 updates, score 11.523) (writing took 1.6415486639998562 seconds)
2020-10-12 22:07:19 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 22:07:19 | INFO | train | epoch 001 | loss 13.169 | nll_loss 12.945 | ppl 7884.34 | wps 42282.6 | ups 2.91 | wpb 14533.2 | bsz 545.8 | num_updates 207 | lr 1.04448e-05 | gnorm 2.918 | clip 0 | train_wall 65 | wall 74
2020-10-12 22:07:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 22:07:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 22:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21136.72265625Mb; avail=467351.17578125Mb
2020-10-12 22:07:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004665
2020-10-12 22:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024648
2020-10-12 22:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21142.0390625Mb; avail=467345.7421875Mb
2020-10-12 22:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000902
2020-10-12 22:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21142.0546875Mb; avail=467345.62109375Mb
2020-10-12 22:07:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.438293
2020-10-12 22:07:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.464735
2020-10-12 22:07:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21177.3984375Mb; avail=467310.8515625Mb
2020-10-12 22:07:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21176.36328125Mb; avail=467311.48828125Mb
2020-10-12 22:07:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016112
2020-10-12 22:07:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21176.36328125Mb; avail=467311.48828125Mb
2020-10-12 22:07:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000799
2020-10-12 22:07:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21176.36328125Mb; avail=467311.48828125Mb
2020-10-12 22:07:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.415801
2020-10-12 22:07:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.433563
2020-10-12 22:07:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21176.9375Mb; avail=467311.18359375Mb
2020-10-12 22:07:20 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 22:07:52 | INFO | train_inner | epoch 002:     93 / 207 loss=11.448, nll_loss=11.022, ppl=2079.16, wps=37205.9, ups=2.57, wpb=14504.2, bsz=521.7, num_updates=300, lr=1.50925e-05, gnorm=1.433, clip=0, train_wall=32, wall=107
2020-10-12 22:08:25 | INFO | train_inner | epoch 002:    193 / 207 loss=10.331, nll_loss=9.746, ppl=858.45, wps=43699, ups=3.02, wpb=14488.8, bsz=551.9, num_updates=400, lr=2.009e-05, gnorm=1.641, clip=0, train_wall=32, wall=140
2020-10-12 22:08:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:08:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21406.91796875Mb; avail=467081.12109375Mb
2020-10-12 22:08:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001327
2020-10-12 22:08:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21406.91796875Mb; avail=467081.12109375Mb
2020-10-12 22:08:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.050083
2020-10-12 22:08:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21406.5546875Mb; avail=467081.37890625Mb
2020-10-12 22:08:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036349
2020-10-12 22:08:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.088583
2020-10-12 22:08:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21406.6875Mb; avail=467081.12109375Mb
2020-10-12 22:08:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21407.0625Mb; avail=467080.74609375Mb
2020-10-12 22:08:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000989
2020-10-12 22:08:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21407.17578125Mb; avail=467080.6328125Mb
2020-10-12 22:08:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049464
2020-10-12 22:08:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21407.28515625Mb; avail=467080.515625Mb
2020-10-12 22:08:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035644
2020-10-12 22:08:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086950
2020-10-12 22:08:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21407.23828125Mb; avail=467080.7578125Mb
2020-10-12 22:08:32 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.505 | nll_loss 8.733 | ppl 425.56 | wps 94613.5 | wpb 4817.7 | bsz 183.8 | num_updates 414 | best_loss 9.505
2020-10-12 22:08:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:08:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 2 @ 414 updates, score 9.505) (writing took 5.750247957001193 seconds)
2020-10-12 22:08:38 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 22:08:38 | INFO | train | epoch 002 | loss 10.79 | nll_loss 10.269 | ppl 1234.23 | wps 38210.5 | ups 2.63 | wpb 14533.2 | bsz 545.8 | num_updates 414 | lr 2.07896e-05 | gnorm 1.507 | clip 0 | train_wall 66 | wall 153
2020-10-12 22:08:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 22:08:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 22:08:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21296.078125Mb; avail=467191.5Mb
2020-10-12 22:08:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003672
2020-10-12 22:08:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025665
2020-10-12 22:08:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21295.60546875Mb; avail=467191.97265625Mb
2020-10-12 22:08:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000960
2020-10-12 22:08:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21295.60546875Mb; avail=467191.97265625Mb
2020-10-12 22:08:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.472019
2020-10-12 22:08:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.499640
2020-10-12 22:08:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21297.1484375Mb; avail=467190.46875Mb
2020-10-12 22:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21296.30859375Mb; avail=467191.30859375Mb
2020-10-12 22:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.018130
2020-10-12 22:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21296.33203125Mb; avail=467191.171875Mb
2020-10-12 22:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001029
2020-10-12 22:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21296.2265625Mb; avail=467191.171875Mb
2020-10-12 22:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.465077
2020-10-12 22:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.485236
2020-10-12 22:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21296.23828125Mb; avail=467191.6171875Mb
2020-10-12 22:08:39 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 22:09:08 | INFO | train_inner | epoch 003:     86 / 207 loss=9.636, nll_loss=8.92, ppl=484.31, wps=33600, ups=2.32, wpb=14464.5, bsz=526.4, num_updates=500, lr=2.50875e-05, gnorm=1.037, clip=0, train_wall=31, wall=183
2020-10-12 22:09:41 | INFO | train_inner | epoch 003:    186 / 207 loss=9.209, nll_loss=8.393, ppl=336.05, wps=44279.5, ups=3.04, wpb=14557.8, bsz=543, num_updates=600, lr=3.0085e-05, gnorm=1.261, clip=0, train_wall=32, wall=216
2020-10-12 22:09:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21391.76953125Mb; avail=467096.48828125Mb
2020-10-12 22:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001412
2020-10-12 22:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21391.76953125Mb; avail=467096.48828125Mb
2020-10-12 22:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.057383
2020-10-12 22:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21391.82421875Mb; avail=467096.8515625Mb
2020-10-12 22:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036978
2020-10-12 22:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.096583
2020-10-12 22:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21391.82421875Mb; avail=467096.8515625Mb
2020-10-12 22:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21391.82421875Mb; avail=467096.8515625Mb
2020-10-12 22:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000941
2020-10-12 22:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21391.82421875Mb; avail=467096.8515625Mb
2020-10-12 22:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049610
2020-10-12 22:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21391.8671875Mb; avail=467096.48828125Mb
2020-10-12 22:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035426
2020-10-12 22:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086724
2020-10-12 22:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21391.98828125Mb; avail=467096.8515625Mb
2020-10-12 22:09:50 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.902 | nll_loss 7.977 | ppl 252.01 | wps 95764.2 | wpb 4817.7 | bsz 183.8 | num_updates 621 | best_loss 8.902
2020-10-12 22:09:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:09:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 3 @ 621 updates, score 8.902) (writing took 7.042342143000496 seconds)
2020-10-12 22:09:57 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 22:09:57 | INFO | train | epoch 003 | loss 9.363 | nll_loss 8.582 | ppl 383.27 | wps 37809.5 | ups 2.6 | wpb 14533.2 | bsz 545.8 | num_updates 621 | lr 3.11345e-05 | gnorm 1.243 | clip 0 | train_wall 65 | wall 232
2020-10-12 22:09:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 22:09:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 22:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21272.65625Mb; avail=467215.18359375Mb
2020-10-12 22:09:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002758
2020-10-12 22:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025377
2020-10-12 22:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21271.1796875Mb; avail=467216.66015625Mb
2020-10-12 22:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000950
2020-10-12 22:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21271.1796875Mb; avail=467216.66015625Mb
2020-10-12 22:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.462626
2020-10-12 22:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.490228
2020-10-12 22:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21273.71875Mb; avail=467214.4921875Mb
2020-10-12 22:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21272.85546875Mb; avail=467215.2421875Mb
2020-10-12 22:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016839
2020-10-12 22:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21272.85546875Mb; avail=467215.2421875Mb
2020-10-12 22:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000834
2020-10-12 22:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21272.85546875Mb; avail=467215.2421875Mb
2020-10-12 22:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.417135
2020-10-12 22:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.435751
2020-10-12 22:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21273.0234375Mb; avail=467215.08984375Mb
2020-10-12 22:09:58 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 22:10:26 | INFO | train_inner | epoch 004:     79 / 207 loss=9.105, nll_loss=8.249, ppl=304.31, wps=32410.6, ups=2.24, wpb=14489.4, bsz=562, num_updates=700, lr=3.50825e-05, gnorm=1.229, clip=0, train_wall=32, wall=261
2020-10-12 22:10:59 | INFO | train_inner | epoch 004:    179 / 207 loss=8.905, nll_loss=8.015, ppl=258.65, wps=44082.3, ups=3.02, wpb=14616.3, bsz=551.4, num_updates=800, lr=4.008e-05, gnorm=1.258, clip=0, train_wall=32, wall=294
2020-10-12 22:11:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21411.8046875Mb; avail=467076.109375Mb
2020-10-12 22:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001388
2020-10-12 22:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21411.8046875Mb; avail=467076.109375Mb
2020-10-12 22:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.050059
2020-10-12 22:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21411.7421875Mb; avail=467075.8671875Mb
2020-10-12 22:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035960
2020-10-12 22:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.088226
2020-10-12 22:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21411.765625Mb; avail=467076.109375Mb
2020-10-12 22:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21411.8359375Mb; avail=467075.625Mb
2020-10-12 22:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000932
2020-10-12 22:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21411.8359375Mb; avail=467075.625Mb
2020-10-12 22:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049673
2020-10-12 22:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21411.99609375Mb; avail=467075.8671875Mb
2020-10-12 22:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034973
2020-10-12 22:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086318
2020-10-12 22:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21411.99609375Mb; avail=467075.8671875Mb
2020-10-12 22:11:11 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.536 | nll_loss 7.538 | ppl 185.9 | wps 96536.8 | wpb 4817.7 | bsz 183.8 | num_updates 828 | best_loss 8.536
2020-10-12 22:11:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:11:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 4 @ 828 updates, score 8.536) (writing took 4.423913911999989 seconds)
2020-10-12 22:11:15 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 22:11:15 | INFO | train | epoch 004 | loss 8.957 | nll_loss 8.076 | ppl 269.86 | wps 38774.4 | ups 2.67 | wpb 14533.2 | bsz 545.8 | num_updates 828 | lr 4.14793e-05 | gnorm 1.137 | clip 0 | train_wall 66 | wall 310
2020-10-12 22:11:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 22:11:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 22:11:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21312.09765625Mb; avail=467175.5390625Mb
2020-10-12 22:11:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002698
2020-10-12 22:11:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022634
2020-10-12 22:11:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21311.11328125Mb; avail=467176.5234375Mb
2020-10-12 22:11:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000836
2020-10-12 22:11:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21311.11328125Mb; avail=467176.5234375Mb
2020-10-12 22:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.421656
2020-10-12 22:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.446258
2020-10-12 22:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21314.08203125Mb; avail=467173.375Mb
2020-10-12 22:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21311.9609375Mb; avail=467175.3984375Mb
2020-10-12 22:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016892
2020-10-12 22:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21312.01171875Mb; avail=467175.03515625Mb
2020-10-12 22:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000846
2020-10-12 22:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21312.01171875Mb; avail=467175.03515625Mb
2020-10-12 22:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.417508
2020-10-12 22:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.436225
2020-10-12 22:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21311.23046875Mb; avail=467176.6328125Mb
2020-10-12 22:11:16 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 22:11:41 | INFO | train_inner | epoch 005:     72 / 207 loss=8.778, nll_loss=7.868, ppl=233.56, wps=34740.2, ups=2.37, wpb=14681.1, bsz=545, num_updates=900, lr=4.50775e-05, gnorm=1.18, clip=0, train_wall=32, wall=336
2020-10-12 22:12:14 | INFO | train_inner | epoch 005:    172 / 207 loss=8.533, nll_loss=7.588, ppl=192.44, wps=44226.7, ups=3.04, wpb=14539.6, bsz=549.2, num_updates=1000, lr=5.0075e-05, gnorm=1.305, clip=0, train_wall=32, wall=369
2020-10-12 22:12:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21423.203125Mb; avail=467064.18359375Mb
2020-10-12 22:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002134
2020-10-12 22:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21423.203125Mb; avail=467064.18359375Mb
2020-10-12 22:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049673
2020-10-12 22:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21423.203125Mb; avail=467064.18359375Mb
2020-10-12 22:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035857
2020-10-12 22:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.088458
2020-10-12 22:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21423.2578125Mb; avail=467064.0625Mb
2020-10-12 22:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21423.27734375Mb; avail=467063.578125Mb
2020-10-12 22:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000974
2020-10-12 22:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21423.29296875Mb; avail=467063.45703125Mb
2020-10-12 22:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049309
2020-10-12 22:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21422.9609375Mb; avail=467064.18359375Mb
2020-10-12 22:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035107
2020-10-12 22:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086150
2020-10-12 22:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21422.9609375Mb; avail=467064.18359375Mb
2020-10-12 22:12:28 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.138 | nll_loss 7.089 | ppl 136.15 | wps 92078.7 | wpb 4817.7 | bsz 183.8 | num_updates 1035 | best_loss 8.138
2020-10-12 22:12:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:12:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 5 @ 1035 updates, score 8.138) (writing took 4.431001995999395 seconds)
2020-10-12 22:12:32 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 22:12:32 | INFO | train | epoch 005 | loss 8.603 | nll_loss 7.668 | ppl 203.32 | wps 38943.8 | ups 2.68 | wpb 14533.2 | bsz 545.8 | num_updates 1035 | lr 5.18241e-05 | gnorm 1.236 | clip 0 | train_wall 66 | wall 387
2020-10-12 22:12:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 22:12:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 22:12:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21328.30859375Mb; avail=467159.2890625Mb
2020-10-12 22:12:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002772
2020-10-12 22:12:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022641
2020-10-12 22:12:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21328.76171875Mb; avail=467158.83984375Mb
2020-10-12 22:12:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000863
2020-10-12 22:12:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21328.76171875Mb; avail=467158.83984375Mb
2020-10-12 22:12:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.425734
2020-10-12 22:12:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.450073
2020-10-12 22:12:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21330.4375Mb; avail=467157.18359375Mb
2020-10-12 22:12:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21328.515625Mb; avail=467158.66796875Mb
2020-10-12 22:12:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015909
2020-10-12 22:12:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21328.515625Mb; avail=467158.66796875Mb
2020-10-12 22:12:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000802
2020-10-12 22:12:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21328.515625Mb; avail=467158.66796875Mb
2020-10-12 22:12:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.410346
2020-10-12 22:12:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.427914
2020-10-12 22:12:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21327.921875Mb; avail=467159.9453125Mb
2020-10-12 22:12:33 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 22:12:56 | INFO | train_inner | epoch 006:     65 / 207 loss=8.432, nll_loss=7.473, ppl=177.63, wps=33968.9, ups=2.38, wpb=14280.8, bsz=546.3, num_updates=1100, lr=5.50725e-05, gnorm=1.208, clip=0, train_wall=32, wall=411
2020-10-12 22:13:29 | INFO | train_inner | epoch 006:    165 / 207 loss=8.264, nll_loss=7.281, ppl=155.5, wps=44720.7, ups=3.06, wpb=14638.2, bsz=540.5, num_updates=1200, lr=6.007e-05, gnorm=1.194, clip=0, train_wall=32, wall=444
2020-10-12 22:13:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:13:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21447.93359375Mb; avail=467040.48046875Mb
2020-10-12 22:13:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001358
2020-10-12 22:13:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21447.93359375Mb; avail=467040.48046875Mb
2020-10-12 22:13:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048780
2020-10-12 22:13:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21447.66015625Mb; avail=467040.48046875Mb
2020-10-12 22:13:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.038403
2020-10-12 22:13:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.089508
2020-10-12 22:13:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21447.5390625Mb; avail=467040.6015625Mb
2020-10-12 22:13:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21447.5390625Mb; avail=467040.6015625Mb
2020-10-12 22:13:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001038
2020-10-12 22:13:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21447.5390625Mb; avail=467040.6015625Mb
2020-10-12 22:13:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.050204
2020-10-12 22:13:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21447.5390625Mb; avail=467040.6015625Mb
2020-10-12 22:13:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036199
2020-10-12 22:13:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.088398
2020-10-12 22:13:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21447.5390625Mb; avail=467040.6015625Mb
2020-10-12 22:13:45 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.88 | nll_loss 6.78 | ppl 109.88 | wps 93589.1 | wpb 4817.7 | bsz 183.8 | num_updates 1242 | best_loss 7.88
2020-10-12 22:13:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:13:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 6 @ 1242 updates, score 7.88) (writing took 4.447625806998985 seconds)
2020-10-12 22:13:50 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 22:13:50 | INFO | train | epoch 006 | loss 8.296 | nll_loss 7.317 | ppl 159.44 | wps 38868.2 | ups 2.67 | wpb 14533.2 | bsz 545.8 | num_updates 1242 | lr 6.2169e-05 | gnorm 1.216 | clip 0 | train_wall 66 | wall 465
2020-10-12 22:13:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 22:13:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 22:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21346.3671875Mb; avail=467141.65234375Mb
2020-10-12 22:13:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003403
2020-10-12 22:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024453
2020-10-12 22:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21346.328125Mb; avail=467141.69140625Mb
2020-10-12 22:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001136
2020-10-12 22:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21345.34375Mb; avail=467142.67578125Mb
2020-10-12 22:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.427308
2020-10-12 22:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.453796
2020-10-12 22:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21345.33984375Mb; avail=467142.58203125Mb
2020-10-12 22:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21345.44140625Mb; avail=467141.984375Mb
2020-10-12 22:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016530
2020-10-12 22:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21345.49609375Mb; avail=467141.984375Mb
2020-10-12 22:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000960
2020-10-12 22:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21345.45703125Mb; avail=467142.2265625Mb
2020-10-12 22:13:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.461743
2020-10-12 22:13:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.480621
2020-10-12 22:13:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21344.8828125Mb; avail=467143.21484375Mb
2020-10-12 22:13:51 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 22:14:11 | INFO | train_inner | epoch 007:     58 / 207 loss=8.109, nll_loss=7.103, ppl=137.49, wps=34149.1, ups=2.35, wpb=14523.4, bsz=571.7, num_updates=1300, lr=6.50675e-05, gnorm=1.193, clip=0, train_wall=32, wall=486
2020-10-12 22:14:44 | INFO | train_inner | epoch 007:    158 / 207 loss=8.078, nll_loss=7.066, ppl=133.98, wps=44846.3, ups=3.06, wpb=14664.2, bsz=541.9, num_updates=1400, lr=7.0065e-05, gnorm=1.157, clip=0, train_wall=32, wall=519
2020-10-12 22:15:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21450.49609375Mb; avail=467037.359375Mb
2020-10-12 22:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002456
2020-10-12 22:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21450.49609375Mb; avail=467037.359375Mb
2020-10-12 22:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.053678
2020-10-12 22:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21450.828125Mb; avail=467036.99609375Mb
2020-10-12 22:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035677
2020-10-12 22:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.092872
2020-10-12 22:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21450.80859375Mb; avail=467036.390625Mb
2020-10-12 22:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21450.8359375Mb; avail=467036.75390625Mb
2020-10-12 22:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000988
2020-10-12 22:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21450.8359375Mb; avail=467036.75390625Mb
2020-10-12 22:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049121
2020-10-12 22:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21450.8359375Mb; avail=467036.75390625Mb
2020-10-12 22:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035379
2020-10-12 22:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086252
2020-10-12 22:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21450.8359375Mb; avail=467036.75390625Mb
2020-10-12 22:15:02 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.614 | nll_loss 6.503 | ppl 90.72 | wps 94120.5 | wpb 4817.7 | bsz 183.8 | num_updates 1449 | best_loss 7.614
2020-10-12 22:15:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:15:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 7 @ 1449 updates, score 7.614) (writing took 4.564636250999683 seconds)
2020-10-12 22:15:07 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 22:15:07 | INFO | train | epoch 007 | loss 8.054 | nll_loss 7.039 | ppl 131.49 | wps 38936.4 | ups 2.68 | wpb 14533.2 | bsz 545.8 | num_updates 1449 | lr 7.25138e-05 | gnorm 1.194 | clip 0 | train_wall 65 | wall 542
2020-10-12 22:15:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 22:15:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 22:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21352.859375Mb; avail=467134.73828125Mb
2020-10-12 22:15:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003166
2020-10-12 22:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024434
2020-10-12 22:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21351.3828125Mb; avail=467136.21484375Mb
2020-10-12 22:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001028
2020-10-12 22:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21350.76953125Mb; avail=467136.828125Mb
2020-10-12 22:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.416465
2020-10-12 22:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.442735
2020-10-12 22:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21351.015625Mb; avail=467136.25Mb
2020-10-12 22:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21350.609375Mb; avail=467136.38671875Mb
2020-10-12 22:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015952
2020-10-12 22:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21350.609375Mb; avail=467136.38671875Mb
2020-10-12 22:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000847
2020-10-12 22:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21350.609375Mb; avail=467136.38671875Mb
2020-10-12 22:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.419555
2020-10-12 22:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.437574
2020-10-12 22:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21351.38671875Mb; avail=467136.625Mb
2020-10-12 22:15:08 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 22:15:26 | INFO | train_inner | epoch 008:     51 / 207 loss=7.978, nll_loss=6.951, ppl=123.74, wps=34542.3, ups=2.37, wpb=14564.8, bsz=541, num_updates=1500, lr=7.50625e-05, gnorm=1.182, clip=0, train_wall=32, wall=561
2020-10-12 22:15:59 | INFO | train_inner | epoch 008:    151 / 207 loss=7.833, nll_loss=6.786, ppl=110.34, wps=44275.8, ups=3.08, wpb=14364.2, bsz=536.5, num_updates=1600, lr=8.006e-05, gnorm=1.078, clip=0, train_wall=31, wall=593
2020-10-12 22:16:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21458.08984375Mb; avail=467029.6015625Mb
2020-10-12 22:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002228
2020-10-12 22:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21458.08984375Mb; avail=467029.6015625Mb
2020-10-12 22:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049745
2020-10-12 22:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21458.08984375Mb; avail=467029.6015625Mb
2020-10-12 22:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036520
2020-10-12 22:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.089342
2020-10-12 22:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21458.08984375Mb; avail=467029.6015625Mb
2020-10-12 22:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21458.609375Mb; avail=467029.23828125Mb
2020-10-12 22:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000943
2020-10-12 22:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21458.625Mb; avail=467029.23828125Mb
2020-10-12 22:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049793
2020-10-12 22:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21458.63671875Mb; avail=467029.23828125Mb
2020-10-12 22:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036245
2020-10-12 22:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087765
2020-10-12 22:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21458.74609375Mb; avail=467029.1171875Mb
2020-10-12 22:16:19 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.452 | nll_loss 6.311 | ppl 79.39 | wps 88716.3 | wpb 4817.7 | bsz 183.8 | num_updates 1656 | best_loss 7.452
2020-10-12 22:16:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:16:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 8 @ 1656 updates, score 7.452) (writing took 4.437544140000682 seconds)
2020-10-12 22:16:24 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 22:16:24 | INFO | train | epoch 008 | loss 7.845 | nll_loss 6.8 | ppl 111.43 | wps 39085.3 | ups 2.69 | wpb 14533.2 | bsz 545.8 | num_updates 1656 | lr 8.28586e-05 | gnorm 1.11 | clip 0 | train_wall 66 | wall 619
2020-10-12 22:16:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 22:16:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 22:16:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21357.9296875Mb; avail=467129.76953125Mb
2020-10-12 22:16:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004110
2020-10-12 22:16:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024071
2020-10-12 22:16:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21357.3984375Mb; avail=467130.30078125Mb
2020-10-12 22:16:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000835
2020-10-12 22:16:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21357.3984375Mb; avail=467130.30078125Mb
2020-10-12 22:16:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.417896
2020-10-12 22:16:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.443622
2020-10-12 22:16:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21355.99609375Mb; avail=467131.0390625Mb
2020-10-12 22:16:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21356.62890625Mb; avail=467130.43359375Mb
2020-10-12 22:16:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016639
2020-10-12 22:16:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21356.62890625Mb; avail=467130.43359375Mb
2020-10-12 22:16:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000817
2020-10-12 22:16:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21356.62890625Mb; avail=467130.43359375Mb
2020-10-12 22:16:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.414250
2020-10-12 22:16:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.432519
2020-10-12 22:16:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21356.35546875Mb; avail=467131.21484375Mb
2020-10-12 22:16:25 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 22:16:40 | INFO | train_inner | epoch 009:     44 / 207 loss=7.751, nll_loss=6.691, ppl=103.36, wps=34812.4, ups=2.39, wpb=14580.9, bsz=537, num_updates=1700, lr=8.50575e-05, gnorm=1.128, clip=0, train_wall=32, wall=635
2020-10-12 22:17:13 | INFO | train_inner | epoch 009:    144 / 207 loss=7.725, nll_loss=6.66, ppl=101.15, wps=43939.7, ups=3.03, wpb=14484, bsz=534.6, num_updates=1800, lr=9.0055e-05, gnorm=1.108, clip=0, train_wall=32, wall=668
2020-10-12 22:17:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:17:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21460.62890625Mb; avail=467027.26953125Mb
2020-10-12 22:17:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001368
2020-10-12 22:17:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21460.62890625Mb; avail=467027.26953125Mb
2020-10-12 22:17:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.050031
2020-10-12 22:17:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21460.62890625Mb; avail=467027.26953125Mb
2020-10-12 22:17:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035915
2020-10-12 22:17:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.088156
2020-10-12 22:17:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21460.62890625Mb; avail=467027.26953125Mb
2020-10-12 22:17:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21460.62890625Mb; avail=467027.26953125Mb
2020-10-12 22:17:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000892
2020-10-12 22:17:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21460.62890625Mb; avail=467027.26953125Mb
2020-10-12 22:17:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049469
2020-10-12 22:17:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21460.68359375Mb; avail=467027.02734375Mb
2020-10-12 22:17:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035721
2020-10-12 22:17:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086833
2020-10-12 22:17:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21460.625Mb; avail=467027.02734375Mb
2020-10-12 22:17:37 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.278 | nll_loss 6.097 | ppl 68.45 | wps 94375.9 | wpb 4817.7 | bsz 183.8 | num_updates 1863 | best_loss 7.278
2020-10-12 22:17:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:17:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 9 @ 1863 updates, score 7.278) (writing took 4.430526466001538 seconds)
2020-10-12 22:17:41 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 22:17:41 | INFO | train | epoch 009 | loss 7.679 | nll_loss 6.608 | ppl 97.55 | wps 38987.7 | ups 2.68 | wpb 14533.2 | bsz 545.8 | num_updates 1863 | lr 9.32034e-05 | gnorm 1.101 | clip 0 | train_wall 66 | wall 696
2020-10-12 22:17:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 22:17:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 22:17:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21362.0859375Mb; avail=467125.2578125Mb
2020-10-12 22:17:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003156
2020-10-12 22:17:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024215
2020-10-12 22:17:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21360.37890625Mb; avail=467126.921875Mb
2020-10-12 22:17:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000900
2020-10-12 22:17:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21360.37890625Mb; avail=467126.921875Mb
2020-10-12 22:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.418434
2020-10-12 22:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.444484
2020-10-12 22:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21360.6328125Mb; avail=467126.6015625Mb
2020-10-12 22:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21360.5859375Mb; avail=467127.69921875Mb
2020-10-12 22:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016573
2020-10-12 22:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21360.5859375Mb; avail=467127.69921875Mb
2020-10-12 22:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000880
2020-10-12 22:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21360.5859375Mb; avail=467127.69921875Mb
2020-10-12 22:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.413553
2020-10-12 22:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.431822
2020-10-12 22:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21360.0859375Mb; avail=467127.51171875Mb
2020-10-12 22:17:42 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 22:17:56 | INFO | train_inner | epoch 010:     37 / 207 loss=7.55, nll_loss=6.461, ppl=88.08, wps=34321.3, ups=2.37, wpb=14481.7, bsz=567, num_updates=1900, lr=9.50525e-05, gnorm=1.066, clip=0, train_wall=32, wall=710
2020-10-12 22:18:28 | INFO | train_inner | epoch 010:    137 / 207 loss=7.524, nll_loss=6.43, ppl=86.24, wps=44230.4, ups=3.07, wpb=14391.5, bsz=552.1, num_updates=2000, lr=0.00010005, gnorm=1.062, clip=0, train_wall=31, wall=743
2020-10-12 22:18:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21456.0546875Mb; avail=467031.9296875Mb
2020-10-12 22:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001469
2020-10-12 22:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21456.0546875Mb; avail=467031.9296875Mb
2020-10-12 22:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072255
2020-10-12 22:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21455.97265625Mb; avail=467031.8984375Mb
2020-10-12 22:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036567
2020-10-12 22:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111113
2020-10-12 22:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21455.97265625Mb; avail=467031.8984375Mb
2020-10-12 22:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21455.97265625Mb; avail=467031.8984375Mb
2020-10-12 22:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000958
2020-10-12 22:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21455.97265625Mb; avail=467031.8984375Mb
2020-10-12 22:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082686
2020-10-12 22:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21455.97265625Mb; avail=467031.8984375Mb
2020-10-12 22:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046930
2020-10-12 22:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.131739
2020-10-12 22:18:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21461.23046875Mb; avail=467026.44140625Mb
2020-10-12 22:18:53 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.183 | nll_loss 5.981 | ppl 63.14 | wps 96312.6 | wpb 4817.7 | bsz 183.8 | num_updates 2070 | best_loss 7.183
2020-10-12 22:18:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:18:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 10 @ 2070 updates, score 7.183) (writing took 4.444394190000821 seconds)
2020-10-12 22:18:58 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 22:18:58 | INFO | train | epoch 010 | loss 7.524 | nll_loss 6.43 | ppl 86.25 | wps 39122.9 | ups 2.69 | wpb 14533.2 | bsz 545.8 | num_updates 2070 | lr 0.000103548 | gnorm 1.066 | clip 0 | train_wall 65 | wall 773
2020-10-12 22:18:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 22:18:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 22:18:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21357.50390625Mb; avail=467130.04296875Mb
2020-10-12 22:18:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003057
2020-10-12 22:18:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023773
2020-10-12 22:18:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21356.515625Mb; avail=467131.02734375Mb
2020-10-12 22:18:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000892
2020-10-12 22:18:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21356.5703125Mb; avail=467130.78515625Mb
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.433887
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.459691
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21355.34765625Mb; avail=467132.97265625Mb
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21355.34765625Mb; avail=467132.97265625Mb
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016014
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21355.34765625Mb; avail=467132.97265625Mb
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000831
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21355.34765625Mb; avail=467132.97265625Mb
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.418184
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.435943
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21074.74609375Mb; avail=467414.23828125Mb
2020-10-12 22:18:59 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 22:19:10 | INFO | train_inner | epoch 011:     30 / 207 loss=7.493, nll_loss=6.393, ppl=84.04, wps=35000.8, ups=2.38, wpb=14703.7, bsz=539.5, num_updates=2100, lr=0.000105048, gnorm=1.094, clip=0, train_wall=32, wall=785
2020-10-12 22:19:43 | INFO | train_inner | epoch 011:    130 / 207 loss=7.411, nll_loss=6.299, ppl=78.76, wps=44347.2, ups=3.03, wpb=14645.4, bsz=551.3, num_updates=2200, lr=0.000110045, gnorm=1.009, clip=0, train_wall=32, wall=818
2020-10-12 22:20:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21463.140625Mb; avail=467024.5Mb
2020-10-12 22:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002127
2020-10-12 22:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21463.140625Mb; avail=467024.5Mb
2020-10-12 22:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049135
2020-10-12 22:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21463.140625Mb; avail=467024.5Mb
2020-10-12 22:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035876
2020-10-12 22:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087956
2020-10-12 22:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21463.01953125Mb; avail=467024.62109375Mb
2020-10-12 22:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21463.01953125Mb; avail=467024.62109375Mb
2020-10-12 22:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000980
2020-10-12 22:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21463.01953125Mb; avail=467024.62109375Mb
2020-10-12 22:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.054950
2020-10-12 22:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21462.88671875Mb; avail=467024.13671875Mb
2020-10-12 22:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036216
2020-10-12 22:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.093037
2020-10-12 22:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21462.921875Mb; avail=467024.74609375Mb
2020-10-12 22:20:11 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.048 | nll_loss 5.826 | ppl 56.71 | wps 96901.1 | wpb 4817.7 | bsz 183.8 | num_updates 2277 | best_loss 7.048
2020-10-12 22:20:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:20:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 11 @ 2277 updates, score 7.048) (writing took 4.42741143800049 seconds)
2020-10-12 22:20:15 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 22:20:15 | INFO | train | epoch 011 | loss 7.395 | nll_loss 6.28 | ppl 77.72 | wps 38937.5 | ups 2.68 | wpb 14533.2 | bsz 545.8 | num_updates 2277 | lr 0.000113893 | gnorm 1.067 | clip 0 | train_wall 66 | wall 850
2020-10-12 22:20:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 22:20:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21079.3515625Mb; avail=467409.484375Mb
2020-10-12 22:20:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004805
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.041268
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21078.859375Mb; avail=467409.9765625Mb
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001660
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21078.859375Mb; avail=467409.9765625Mb
2020-10-12 22:20:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.580133
2020-10-12 22:20:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.624514
2020-10-12 22:20:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21198.19921875Mb; avail=467290.953125Mb
2020-10-12 22:20:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21214.48046875Mb; avail=467274.49609375Mb
2020-10-12 22:20:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016737
2020-10-12 22:20:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21218.71875Mb; avail=467269.65234375Mb
2020-10-12 22:20:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000819
2020-10-12 22:20:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21219.32421875Mb; avail=467269.65234375Mb
2020-10-12 22:20:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.417993
2020-10-12 22:20:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.436836
2020-10-12 22:20:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21264.10546875Mb; avail=467225.140625Mb
2020-10-12 22:20:16 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 22:20:25 | INFO | train_inner | epoch 012:     23 / 207 loss=7.334, nll_loss=6.21, ppl=74.02, wps=34248.9, ups=2.38, wpb=14390.7, bsz=535.9, num_updates=2300, lr=0.000115043, gnorm=1.092, clip=0, train_wall=32, wall=860
2020-10-12 22:20:58 | INFO | train_inner | epoch 012:    123 / 207 loss=7.253, nll_loss=6.117, ppl=69.43, wps=44486.7, ups=3.06, wpb=14532.9, bsz=545.6, num_updates=2400, lr=0.00012004, gnorm=1.04, clip=0, train_wall=32, wall=893
2020-10-12 22:21:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21462.09765625Mb; avail=467025.6328125Mb
2020-10-12 22:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001537
2020-10-12 22:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21462.09765625Mb; avail=467025.6328125Mb
2020-10-12 22:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.051403
2020-10-12 22:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21462.0Mb; avail=467025.99609375Mb
2020-10-12 22:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035152
2020-10-12 22:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.088922
2020-10-12 22:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21462.0Mb; avail=467025.99609375Mb
2020-10-12 22:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21462.0Mb; avail=467025.99609375Mb
2020-10-12 22:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000948
2020-10-12 22:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21462.0Mb; avail=467025.99609375Mb
2020-10-12 22:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049100
2020-10-12 22:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21462.0Mb; avail=467025.99609375Mb
2020-10-12 22:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035403
2020-10-12 22:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086201
2020-10-12 22:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21462.0Mb; avail=467025.99609375Mb
2020-10-12 22:21:28 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.837 | nll_loss 5.581 | ppl 47.86 | wps 93069.5 | wpb 4817.7 | bsz 183.8 | num_updates 2484 | best_loss 6.837
2020-10-12 22:21:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:21:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 12 @ 2484 updates, score 6.837) (writing took 4.6887576219996845 seconds)
2020-10-12 22:21:33 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 22:21:33 | INFO | train | epoch 012 | loss 7.247 | nll_loss 6.111 | ppl 69.13 | wps 38771.7 | ups 2.67 | wpb 14533.2 | bsz 545.8 | num_updates 2484 | lr 0.000124238 | gnorm 1.018 | clip 0 | train_wall 65 | wall 928
2020-10-12 22:21:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 22:21:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 22:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21689.05078125Mb; avail=466799.390625Mb
2020-10-12 22:21:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002735
2020-10-12 22:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023034
2020-10-12 22:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21698.24609375Mb; avail=466790.1953125Mb
2020-10-12 22:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000809
2020-10-12 22:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21698.8515625Mb; avail=466789.58984375Mb
2020-10-12 22:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.421778
2020-10-12 22:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.446761
2020-10-12 22:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21754.29296875Mb; avail=466734.73828125Mb
2020-10-12 22:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21756.21484375Mb; avail=466732.81640625Mb
2020-10-12 22:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.017352
2020-10-12 22:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21756.9609375Mb; avail=466732.0703125Mb
2020-10-12 22:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000850
2020-10-12 22:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21756.9609375Mb; avail=466732.0703125Mb
2020-10-12 22:21:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.414194
2020-10-12 22:21:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.433288
2020-10-12 22:21:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21820.9765625Mb; avail=466667.2421875Mb
2020-10-12 22:21:34 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 22:21:40 | INFO | train_inner | epoch 013:     16 / 207 loss=7.237, nll_loss=6.099, ppl=68.53, wps=34577.8, ups=2.36, wpb=14636.6, bsz=544.6, num_updates=2500, lr=0.000125037, gnorm=0.995, clip=0, train_wall=32, wall=935
2020-10-12 22:22:13 | INFO | train_inner | epoch 013:    116 / 207 loss=7.095, nll_loss=5.937, ppl=61.24, wps=44692.8, ups=3.06, wpb=14616.4, bsz=543.8, num_updates=2600, lr=0.000130035, gnorm=1.018, clip=0, train_wall=32, wall=968
2020-10-12 22:22:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21207.31640625Mb; avail=467281.41796875Mb
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002125
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21207.31640625Mb; avail=467281.41796875Mb
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.115181
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21219.76171875Mb; avail=467269.234375Mb
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.083383
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.201975
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21230.53515625Mb; avail=467258.7265625Mb
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21234.91796875Mb; avail=467254.34375Mb
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001014
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21234.91796875Mb; avail=467254.40625Mb
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049227
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21254.765625Mb; avail=467234.74609375Mb
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036304
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087488
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21268.26953125Mb; avail=467221.2421875Mb
2020-10-12 22:22:45 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.682 | nll_loss 5.395 | ppl 42.08 | wps 76406.7 | wpb 4817.7 | bsz 183.8 | num_updates 2691 | best_loss 6.682
2020-10-12 22:22:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:22:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 13 @ 2691 updates, score 6.682) (writing took 13.115169180000521 seconds)
2020-10-12 22:22:59 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 22:22:59 | INFO | train | epoch 013 | loss 7.088 | nll_loss 5.928 | ppl 60.9 | wps 35094.5 | ups 2.41 | wpb 14533.2 | bsz 545.8 | num_updates 2691 | lr 0.000134583 | gnorm 1.015 | clip 0 | train_wall 65 | wall 1013
2020-10-12 22:22:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 22:22:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21252.75390625Mb; avail=467234.44921875Mb
2020-10-12 22:22:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003102
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023780
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21251.921875Mb; avail=467235.078125Mb
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000806
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21251.921875Mb; avail=467235.078125Mb
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.420085
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.445854
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21251.1015625Mb; avail=467236.69140625Mb
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21251.1015625Mb; avail=467236.69140625Mb
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.017413
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21251.1015625Mb; avail=467236.69140625Mb
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000879
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21251.1015625Mb; avail=467236.69140625Mb
2020-10-12 22:23:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.415805
2020-10-12 22:23:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.434952
2020-10-12 22:23:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21250.9921875Mb; avail=467236.8125Mb
2020-10-12 22:23:00 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 22:23:04 | INFO | train_inner | epoch 014:      9 / 207 loss=7.066, nll_loss=5.903, ppl=59.83, wps=28455.3, ups=1.97, wpb=14455.3, bsz=551.1, num_updates=2700, lr=0.000135032, gnorm=1.016, clip=0, train_wall=31, wall=1019
2020-10-12 22:23:36 | INFO | train_inner | epoch 014:    109 / 207 loss=6.973, nll_loss=5.796, ppl=55.58, wps=44383, ups=3.06, wpb=14486.2, bsz=530.9, num_updates=2800, lr=0.00014003, gnorm=1.05, clip=0, train_wall=31, wall=1051
2020-10-12 22:24:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21739.5859375Mb; avail=466749.73046875Mb
2020-10-12 22:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001481
2020-10-12 22:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21739.19921875Mb; avail=466749.8828125Mb
2020-10-12 22:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048998
2020-10-12 22:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21738.8203125Mb; avail=466749.87890625Mb
2020-10-12 22:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035719
2020-10-12 22:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087268
2020-10-12 22:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21739.5703125Mb; avail=466749.62890625Mb
2020-10-12 22:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21738.87109375Mb; avail=466749.58984375Mb
2020-10-12 22:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001187
2020-10-12 22:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21739.234375Mb; avail=466749.734375Mb
2020-10-12 22:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048896
2020-10-12 22:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21738.63671875Mb; avail=466749.1171875Mb
2020-10-12 22:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036286
2020-10-12 22:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087160
2020-10-12 22:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21738.5859375Mb; avail=466749.5234375Mb
2020-10-12 22:24:11 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.552 | nll_loss 5.24 | ppl 37.79 | wps 93475.5 | wpb 4817.7 | bsz 183.8 | num_updates 2898 | best_loss 6.552
2020-10-12 22:24:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:24:17 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 14 @ 2898 updates, score 6.552) (writing took 5.86286254299921 seconds)
2020-10-12 22:24:17 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 22:24:17 | INFO | train | epoch 014 | loss 6.929 | nll_loss 5.745 | ppl 53.65 | wps 38514.4 | ups 2.65 | wpb 14533.2 | bsz 545.8 | num_updates 2898 | lr 0.000144928 | gnorm 1.027 | clip 0 | train_wall 65 | wall 1092
2020-10-12 22:24:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 22:24:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 22:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21369.65625Mb; avail=467118.17578125Mb
2020-10-12 22:24:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003127
2020-10-12 22:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023618
2020-10-12 22:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21369.65625Mb; avail=467118.17578125Mb
2020-10-12 22:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000804
2020-10-12 22:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21369.65625Mb; avail=467118.17578125Mb
2020-10-12 22:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.418691
2020-10-12 22:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.444329
2020-10-12 22:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21367.328125Mb; avail=467120.4140625Mb
2020-10-12 22:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21367.1640625Mb; avail=467120.77734375Mb
2020-10-12 22:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015998
2020-10-12 22:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21367.16015625Mb; avail=467120.8984375Mb
2020-10-12 22:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000841
2020-10-12 22:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21367.16015625Mb; avail=467120.8984375Mb
2020-10-12 22:24:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.411762
2020-10-12 22:24:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.429376
2020-10-12 22:24:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21367.4140625Mb; avail=467120.91796875Mb
2020-10-12 22:24:18 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 22:24:20 | INFO | train_inner | epoch 015:      2 / 207 loss=6.872, nll_loss=5.68, ppl=51.26, wps=33687.4, ups=2.32, wpb=14539.8, bsz=556.5, num_updates=2900, lr=0.000145028, gnorm=1.002, clip=0, train_wall=31, wall=1094
2020-10-12 22:24:52 | INFO | train_inner | epoch 015:    102 / 207 loss=6.819, nll_loss=5.62, ppl=49.18, wps=44331.7, ups=3.06, wpb=14496.4, bsz=524, num_updates=3000, lr=0.000150025, gnorm=0.986, clip=0, train_wall=32, wall=1127
2020-10-12 22:25:25 | INFO | train_inner | epoch 015:    202 / 207 loss=6.711, nll_loss=5.494, ppl=45.08, wps=44435.3, ups=3.05, wpb=14571.5, bsz=568.8, num_updates=3100, lr=0.000155023, gnorm=1.006, clip=0, train_wall=32, wall=1160
2020-10-12 22:25:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21357.484375Mb; avail=467130.7734375Mb
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001432
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21357.484375Mb; avail=467130.7734375Mb
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049608
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21357.484375Mb; avail=467130.7734375Mb
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036472
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.088417
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21354.90234375Mb; avail=467133.35546875Mb
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21354.90234375Mb; avail=467133.35546875Mb
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001113
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21354.90234375Mb; avail=467133.35546875Mb
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048740
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21354.90234375Mb; avail=467133.35546875Mb
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036037
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086681
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21354.90234375Mb; avail=467133.35546875Mb
2020-10-12 22:25:29 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.358 | nll_loss 5.015 | ppl 32.33 | wps 97713.7 | wpb 4817.7 | bsz 183.8 | num_updates 3105 | best_loss 6.358
2020-10-12 22:25:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:25:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 15 @ 3105 updates, score 6.358) (writing took 4.425893106999865 seconds)
2020-10-12 22:25:34 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 22:25:34 | INFO | train | epoch 015 | loss 6.765 | nll_loss 5.557 | ppl 47.09 | wps 39132.1 | ups 2.69 | wpb 14533.2 | bsz 545.8 | num_updates 3105 | lr 0.000155272 | gnorm 0.995 | clip 0 | train_wall 65 | wall 1168
2020-10-12 22:25:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 22:25:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 22:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21372.4296875Mb; avail=467115.1640625Mb
2020-10-12 22:25:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003164
2020-10-12 22:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023828
2020-10-12 22:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21372.05078125Mb; avail=467115.54296875Mb
2020-10-12 22:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000819
2020-10-12 22:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21372.05078125Mb; avail=467115.54296875Mb
2020-10-12 22:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.422781
2020-10-12 22:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.448302
2020-10-12 22:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21371.703125Mb; avail=467116.18359375Mb
2020-10-12 22:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21371.76953125Mb; avail=467115.69921875Mb
2020-10-12 22:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016133
2020-10-12 22:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21371.9296875Mb; avail=467115.94140625Mb
2020-10-12 22:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000824
2020-10-12 22:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21371.9296875Mb; avail=467115.94140625Mb
2020-10-12 22:25:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.418171
2020-10-12 22:25:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.436219
2020-10-12 22:25:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21371.83203125Mb; avail=467116.06640625Mb
2020-10-12 22:25:35 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 22:26:07 | INFO | train_inner | epoch 016:     95 / 207 loss=6.597, nll_loss=5.365, ppl=41.21, wps=34981, ups=2.39, wpb=14618.9, bsz=561.8, num_updates=3200, lr=0.00016002, gnorm=1.1, clip=0, train_wall=32, wall=1202
2020-10-12 22:26:40 | INFO | train_inner | epoch 016:    195 / 207 loss=6.609, nll_loss=5.377, ppl=41.56, wps=44111.4, ups=3.05, wpb=14478.1, bsz=536.7, num_updates=3300, lr=0.000165018, gnorm=0.994, clip=0, train_wall=32, wall=1234
2020-10-12 22:26:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21478.46484375Mb; avail=467009.609375Mb
2020-10-12 22:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001396
2020-10-12 22:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21478.46484375Mb; avail=467009.609375Mb
2020-10-12 22:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.051775
2020-10-12 22:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21478.62890625Mb; avail=467009.3671875Mb
2020-10-12 22:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035924
2020-10-12 22:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.089897
2020-10-12 22:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21478.48046875Mb; avail=467009.609375Mb
2020-10-12 22:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21478.65234375Mb; avail=467009.8515625Mb
2020-10-12 22:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001006
2020-10-12 22:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21478.65234375Mb; avail=467009.8515625Mb
2020-10-12 22:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048961
2020-10-12 22:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21478.65234375Mb; avail=467009.8515625Mb
2020-10-12 22:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035650
2020-10-12 22:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086366
2020-10-12 22:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21478.5390625Mb; avail=467009.97265625Mb
2020-10-12 22:26:46 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.211 | nll_loss 4.828 | ppl 28.4 | wps 96628.4 | wpb 4817.7 | bsz 183.8 | num_updates 3312 | best_loss 6.211
2020-10-12 22:26:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:26:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 16 @ 3312 updates, score 6.211) (writing took 4.413013020999642 seconds)
2020-10-12 22:26:50 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 22:26:50 | INFO | train | epoch 016 | loss 6.603 | nll_loss 5.371 | ppl 41.37 | wps 39150 | ups 2.69 | wpb 14533.2 | bsz 545.8 | num_updates 3312 | lr 0.000165617 | gnorm 1.052 | clip 0 | train_wall 65 | wall 1245
2020-10-12 22:26:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 22:26:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 22:26:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21373.91015625Mb; avail=467113.87109375Mb
2020-10-12 22:26:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003243
2020-10-12 22:26:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023897
2020-10-12 22:26:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21372.234375Mb; avail=467115.546875Mb
2020-10-12 22:26:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000845
2020-10-12 22:26:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21372.234375Mb; avail=467115.546875Mb
2020-10-12 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.419612
2020-10-12 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.445297
2020-10-12 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21371.40234375Mb; avail=467116.64453125Mb
2020-10-12 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21372.0078125Mb; avail=467116.0390625Mb
2020-10-12 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016742
2020-10-12 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21372.07421875Mb; avail=467115.67578125Mb
2020-10-12 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000826
2020-10-12 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21372.07421875Mb; avail=467115.67578125Mb
2020-10-12 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.418284
2020-10-12 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.436711
2020-10-12 22:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21371.4609375Mb; avail=467116.28125Mb
2020-10-12 22:26:51 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 22:27:21 | INFO | train_inner | epoch 017:     88 / 207 loss=6.489, nll_loss=5.24, ppl=37.8, wps=34946.9, ups=2.39, wpb=14613.1, bsz=519.9, num_updates=3400, lr=0.000170015, gnorm=1.057, clip=0, train_wall=32, wall=1276
2020-10-12 22:27:54 | INFO | train_inner | epoch 017:    188 / 207 loss=6.404, nll_loss=5.141, ppl=35.29, wps=43946.4, ups=3.04, wpb=14466.3, bsz=576.4, num_updates=3500, lr=0.000175013, gnorm=1.046, clip=0, train_wall=32, wall=1309
2020-10-12 22:28:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21481.2578125Mb; avail=467006.1328125Mb
2020-10-12 22:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001413
2020-10-12 22:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21481.2578125Mb; avail=467006.1328125Mb
2020-10-12 22:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049831
2020-10-12 22:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21481.2578125Mb; avail=467006.1328125Mb
2020-10-12 22:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035943
2020-10-12 22:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087992
2020-10-12 22:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21481.2578125Mb; avail=467006.1328125Mb
2020-10-12 22:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21481.16015625Mb; avail=467006.1328125Mb
2020-10-12 22:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001018
2020-10-12 22:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21481.171875Mb; avail=467006.01171875Mb
2020-10-12 22:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048908
2020-10-12 22:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21481.2734375Mb; avail=467006.1328125Mb
2020-10-12 22:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035177
2020-10-12 22:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.085872
2020-10-12 22:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21481.2734375Mb; avail=467006.1328125Mb
2020-10-12 22:28:03 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.088 | nll_loss 4.675 | ppl 25.54 | wps 93681.1 | wpb 4817.7 | bsz 183.8 | num_updates 3519 | best_loss 6.088
2020-10-12 22:28:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:28:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 17 @ 3519 updates, score 6.088) (writing took 4.43087380399993 seconds)
2020-10-12 22:28:08 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 22:28:08 | INFO | train | epoch 017 | loss 6.438 | nll_loss 5.181 | ppl 36.29 | wps 39031.4 | ups 2.69 | wpb 14533.2 | bsz 545.8 | num_updates 3519 | lr 0.000175962 | gnorm 1.059 | clip 0 | train_wall 65 | wall 1322
2020-10-12 22:28:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 22:28:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 22:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21379.4765625Mb; avail=467107.859375Mb
2020-10-12 22:28:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002516
2020-10-12 22:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022653
2020-10-12 22:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21378.0Mb; avail=467109.3359375Mb
2020-10-12 22:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000818
2020-10-12 22:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21378.0Mb; avail=467109.3359375Mb
2020-10-12 22:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.472511
2020-10-12 22:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.496896
2020-10-12 22:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21380.5390625Mb; avail=467106.94140625Mb
2020-10-12 22:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21379.2265625Mb; avail=467108.640625Mb
2020-10-12 22:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.017065
2020-10-12 22:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21379.2265625Mb; avail=467108.640625Mb
2020-10-12 22:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000865
2020-10-12 22:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21379.2265625Mb; avail=467108.640625Mb
2020-10-12 22:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.414879
2020-10-12 22:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.433648
2020-10-12 22:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21379.4609375Mb; avail=467108.27734375Mb
2020-10-12 22:28:09 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 22:28:36 | INFO | train_inner | epoch 018:     81 / 207 loss=6.336, nll_loss=5.064, ppl=33.44, wps=34958.1, ups=2.39, wpb=14619.9, bsz=549.9, num_updates=3600, lr=0.00018001, gnorm=1.054, clip=0, train_wall=32, wall=1351
2020-10-12 22:29:09 | INFO | train_inner | epoch 018:    181 / 207 loss=6.248, nll_loss=4.961, ppl=31.15, wps=44526, ups=3.07, wpb=14482.9, bsz=544.3, num_updates=3700, lr=0.000185008, gnorm=0.998, clip=0, train_wall=31, wall=1384
2020-10-12 22:29:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:29:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21477.22265625Mb; avail=467010.671875Mb
2020-10-12 22:29:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002237
2020-10-12 22:29:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21477.22265625Mb; avail=467010.671875Mb
2020-10-12 22:29:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048427
2020-10-12 22:29:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21477.203125Mb; avail=467010.671875Mb
2020-10-12 22:29:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035282
2020-10-12 22:29:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086789
2020-10-12 22:29:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21477.44140625Mb; avail=467010.671875Mb
2020-10-12 22:29:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21477.44140625Mb; avail=467010.671875Mb
2020-10-12 22:29:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000992
2020-10-12 22:29:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21477.44140625Mb; avail=467010.671875Mb
2020-10-12 22:29:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048870
2020-10-12 22:29:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21477.44140625Mb; avail=467010.671875Mb
2020-10-12 22:29:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035594
2020-10-12 22:29:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086220
2020-10-12 22:29:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21477.44140625Mb; avail=467010.671875Mb
2020-10-12 22:29:20 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 5.987 | nll_loss 4.563 | ppl 23.65 | wps 94369 | wpb 4817.7 | bsz 183.8 | num_updates 3726 | best_loss 5.987
2020-10-12 22:29:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:29:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 18 @ 3726 updates, score 5.987) (writing took 4.442625019000843 seconds)
2020-10-12 22:29:24 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 22:29:24 | INFO | train | epoch 018 | loss 6.282 | nll_loss 5 | ppl 32.01 | wps 39254.1 | ups 2.7 | wpb 14533.2 | bsz 545.8 | num_updates 3726 | lr 0.000186307 | gnorm 1.036 | clip 0 | train_wall 65 | wall 1399
2020-10-12 22:29:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 22:29:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 22:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21377.3046875Mb; avail=467110.3515625Mb
2020-10-12 22:29:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002723
2020-10-12 22:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023130
2020-10-12 22:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.828125Mb; avail=467111.828125Mb
2020-10-12 22:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000808
2020-10-12 22:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.828125Mb; avail=467111.828125Mb
2020-10-12 22:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.419109
2020-10-12 22:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.443891
2020-10-12 22:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.43359375Mb; avail=467112.484375Mb
2020-10-12 22:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21375.43359375Mb; avail=467112.484375Mb
2020-10-12 22:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015787
2020-10-12 22:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.3125Mb; avail=467112.60546875Mb
2020-10-12 22:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000809
2020-10-12 22:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.3125Mb; avail=467112.60546875Mb
2020-10-12 22:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.416886
2020-10-12 22:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.434283
2020-10-12 22:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21374.61328125Mb; avail=467113.27734375Mb
2020-10-12 22:29:25 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 22:29:51 | INFO | train_inner | epoch 019:     74 / 207 loss=6.195, nll_loss=4.901, ppl=29.88, wps=34677.7, ups=2.39, wpb=14535.8, bsz=530.6, num_updates=3800, lr=0.000190005, gnorm=1.056, clip=0, train_wall=32, wall=1425
2020-10-12 22:30:23 | INFO | train_inner | epoch 019:    174 / 207 loss=6.104, nll_loss=4.796, ppl=27.77, wps=44652.6, ups=3.04, wpb=14668.1, bsz=560.1, num_updates=3900, lr=0.000195003, gnorm=1.011, clip=0, train_wall=32, wall=1458
2020-10-12 22:30:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:30:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21488.8828125Mb; avail=466999.0703125Mb
2020-10-12 22:30:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001430
2020-10-12 22:30:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21488.8828125Mb; avail=466999.0703125Mb
2020-10-12 22:30:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049592
2020-10-12 22:30:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21488.38671875Mb; avail=466999.44140625Mb
2020-10-12 22:30:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036810
2020-10-12 22:30:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.088674
2020-10-12 22:30:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21488.56640625Mb; avail=466999.5625Mb
2020-10-12 22:30:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21488.48828125Mb; avail=466999.67578125Mb
2020-10-12 22:30:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001025
2020-10-12 22:30:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21488.48828125Mb; avail=466999.67578125Mb
2020-10-12 22:30:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048968
2020-10-12 22:30:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21488.48828125Mb; avail=466999.67578125Mb
2020-10-12 22:30:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036407
2020-10-12 22:30:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087174
2020-10-12 22:30:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21488.48828125Mb; avail=466999.67578125Mb
2020-10-12 22:30:37 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 5.832 | nll_loss 4.373 | ppl 20.73 | wps 95612.3 | wpb 4817.7 | bsz 183.8 | num_updates 3933 | best_loss 5.832
2020-10-12 22:30:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:30:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 19 @ 3933 updates, score 5.832) (writing took 4.431355744998655 seconds)
2020-10-12 22:30:41 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 22:30:41 | INFO | train | epoch 019 | loss 6.126 | nll_loss 4.821 | ppl 28.28 | wps 39125.1 | ups 2.69 | wpb 14533.2 | bsz 545.8 | num_updates 3933 | lr 0.000196652 | gnorm 1.011 | clip 0 | train_wall 65 | wall 1476
2020-10-12 22:30:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 22:30:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 22:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21387.1875Mb; avail=467100.484375Mb
2020-10-12 22:30:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002850
2020-10-12 22:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023018
2020-10-12 22:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21385.65234375Mb; avail=467101.59765625Mb
2020-10-12 22:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000813
2020-10-12 22:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21385.65234375Mb; avail=467101.59765625Mb
2020-10-12 22:30:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.416919
2020-10-12 22:30:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.441656
2020-10-12 22:30:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.88671875Mb; avail=467103.0390625Mb
2020-10-12 22:30:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21384.88671875Mb; avail=467103.0390625Mb
2020-10-12 22:30:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015938
2020-10-12 22:30:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.88671875Mb; avail=467103.0390625Mb
2020-10-12 22:30:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000817
2020-10-12 22:30:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.88671875Mb; avail=467103.0390625Mb
2020-10-12 22:30:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.413563
2020-10-12 22:30:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.431131
2020-10-12 22:30:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.953125Mb; avail=467103.00390625Mb
2020-10-12 22:30:42 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 22:31:05 | INFO | train_inner | epoch 020:     67 / 207 loss=6.039, nll_loss=4.72, ppl=26.35, wps=34782.2, ups=2.4, wpb=14506, bsz=545.7, num_updates=4000, lr=0.0002, gnorm=0.994, clip=0, train_wall=31, wall=1500
2020-10-12 22:31:38 | INFO | train_inner | epoch 020:    167 / 207 loss=5.942, nll_loss=4.608, ppl=24.39, wps=44173, ups=3.04, wpb=14524.2, bsz=556.7, num_updates=4100, lr=0.000197546, gnorm=0.939, clip=0, train_wall=32, wall=1533
2020-10-12 22:31:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21481.65234375Mb; avail=467005.9921875Mb
2020-10-12 22:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001428
2020-10-12 22:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21481.65234375Mb; avail=467005.9921875Mb
2020-10-12 22:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048936
2020-10-12 22:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21481.65234375Mb; avail=467005.9921875Mb
2020-10-12 22:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035415
2020-10-12 22:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086594
2020-10-12 22:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21481.53125Mb; avail=467006.11328125Mb
2020-10-12 22:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21480.73828125Mb; avail=467005.9921875Mb
2020-10-12 22:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000976
2020-10-12 22:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21480.73828125Mb; avail=467005.9921875Mb
2020-10-12 22:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049832
2020-10-12 22:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21480.67578125Mb; avail=467006.4765625Mb
2020-10-12 22:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035996
2020-10-12 22:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087563
2020-10-12 22:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21480.40234375Mb; avail=467006.37109375Mb
2020-10-12 22:31:54 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 5.744 | nll_loss 4.266 | ppl 19.24 | wps 86463.8 | wpb 4817.7 | bsz 183.8 | num_updates 4140 | best_loss 5.744
2020-10-12 22:31:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:32:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 20 @ 4140 updates, score 5.744) (writing took 6.697920659998999 seconds)
2020-10-12 22:32:00 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 22:32:00 | INFO | train | epoch 020 | loss 5.962 | nll_loss 4.631 | ppl 24.78 | wps 37907.8 | ups 2.61 | wpb 14533.2 | bsz 545.8 | num_updates 4140 | lr 0.000196589 | gnorm 0.972 | clip 0 | train_wall 66 | wall 1555
2020-10-12 22:32:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 22:32:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 22:32:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21703.109375Mb; avail=466784.6796875Mb
2020-10-12 22:32:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003472
2020-10-12 22:32:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026588
2020-10-12 22:32:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21702.23828125Mb; avail=466785.55078125Mb
2020-10-12 22:32:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000863
2020-10-12 22:32:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21702.23828125Mb; avail=466785.55078125Mb
2020-10-12 22:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.481130
2020-10-12 22:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.509673
2020-10-12 22:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21701.859375Mb; avail=466786.16015625Mb
2020-10-12 22:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21701.9453125Mb; avail=466785.97265625Mb
2020-10-12 22:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.018254
2020-10-12 22:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21701.94921875Mb; avail=466785.8515625Mb
2020-10-12 22:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001070
2020-10-12 22:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21701.94921875Mb; avail=466785.8515625Mb
2020-10-12 22:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.468371
2020-10-12 22:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.488716
2020-10-12 22:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21702.11328125Mb; avail=466786.37890625Mb
2020-10-12 22:32:01 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 22:32:22 | INFO | train_inner | epoch 021:     60 / 207 loss=5.894, nll_loss=4.552, ppl=23.46, wps=32813.6, ups=2.26, wpb=14518.4, bsz=534.2, num_updates=4200, lr=0.00019518, gnorm=0.978, clip=0, train_wall=32, wall=1577
2020-10-12 22:32:55 | INFO | train_inner | epoch 021:    160 / 207 loss=5.818, nll_loss=4.464, ppl=22.07, wps=44155.5, ups=3.05, wpb=14500.5, bsz=538.7, num_updates=4300, lr=0.000192897, gnorm=0.97, clip=0, train_wall=32, wall=1610
2020-10-12 22:33:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:33:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21487.109375Mb; avail=467000.546875Mb
2020-10-12 22:33:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001258
2020-10-12 22:33:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21487.109375Mb; avail=467000.546875Mb
2020-10-12 22:33:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048604
2020-10-12 22:33:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21487.109375Mb; avail=467000.54296875Mb
2020-10-12 22:33:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036559
2020-10-12 22:33:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087234
2020-10-12 22:33:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21487.18359375Mb; avail=467000.05859375Mb
2020-10-12 22:33:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21487.16796875Mb; avail=467000.30078125Mb
2020-10-12 22:33:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001543
2020-10-12 22:33:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21487.1328125Mb; avail=467000.30078125Mb
2020-10-12 22:33:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.050498
2020-10-12 22:33:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21487.0390625Mb; avail=467000.1796875Mb
2020-10-12 22:33:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035305
2020-10-12 22:33:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.088335
2020-10-12 22:33:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21487.0390625Mb; avail=467000.1796875Mb
2020-10-12 22:33:13 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 5.582 | nll_loss 4.075 | ppl 16.86 | wps 93097.3 | wpb 4817.7 | bsz 183.8 | num_updates 4347 | best_loss 5.582
2020-10-12 22:33:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:33:17 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 21 @ 4347 updates, score 5.582) (writing took 4.457190234999871 seconds)
2020-10-12 22:33:17 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 22:33:17 | INFO | train | epoch 021 | loss 5.821 | nll_loss 4.467 | ppl 22.12 | wps 39071.1 | ups 2.69 | wpb 14533.2 | bsz 545.8 | num_updates 4347 | lr 0.000191851 | gnorm 0.957 | clip 0 | train_wall 65 | wall 1632
2020-10-12 22:33:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 22:33:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 22:33:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21382.11328125Mb; avail=467105.12890625Mb
2020-10-12 22:33:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003188
2020-10-12 22:33:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024182
2020-10-12 22:33:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21381.12890625Mb; avail=467106.11328125Mb
2020-10-12 22:33:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000830
2020-10-12 22:33:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21381.12890625Mb; avail=467106.11328125Mb
2020-10-12 22:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.424561
2020-10-12 22:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.450490
2020-10-12 22:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21380.609375Mb; avail=467107.2734375Mb
2020-10-12 22:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21380.69140625Mb; avail=467107.40234375Mb
2020-10-12 22:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016060
2020-10-12 22:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21380.68359375Mb; avail=467106.91796875Mb
2020-10-12 22:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000813
2020-10-12 22:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21380.68359375Mb; avail=467106.91796875Mb
2020-10-12 22:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.416609
2020-10-12 22:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.434323
2020-10-12 22:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21380.77734375Mb; avail=467107.2734375Mb
2020-10-12 22:33:18 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 22:33:37 | INFO | train_inner | epoch 022:     53 / 207 loss=5.741, nll_loss=4.376, ppl=20.77, wps=34473.2, ups=2.39, wpb=14402.4, bsz=544.4, num_updates=4400, lr=0.000190693, gnorm=0.929, clip=0, train_wall=32, wall=1652
2020-10-12 22:34:10 | INFO | train_inner | epoch 022:    153 / 207 loss=5.694, nll_loss=4.321, ppl=19.99, wps=43645.7, ups=3.04, wpb=14375.7, bsz=525.7, num_updates=4500, lr=0.000188562, gnorm=0.875, clip=0, train_wall=32, wall=1685
2020-10-12 22:34:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21483.67578125Mb; avail=467003.70703125Mb
2020-10-12 22:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001601
2020-10-12 22:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21483.67578125Mb; avail=467003.70703125Mb
2020-10-12 22:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049399
2020-10-12 22:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21483.67578125Mb; avail=467003.70703125Mb
2020-10-12 22:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036132
2020-10-12 22:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087971
2020-10-12 22:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21483.67578125Mb; avail=467003.21875Mb
2020-10-12 22:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21483.8125Mb; avail=467002.9765625Mb
2020-10-12 22:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000948
2020-10-12 22:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21483.8125Mb; avail=467002.9765625Mb
2020-10-12 22:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.050376
2020-10-12 22:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21483.85546875Mb; avail=467003.33984375Mb
2020-10-12 22:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035612
2020-10-12 22:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087695
2020-10-12 22:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21483.85546875Mb; avail=467003.33984375Mb
2020-10-12 22:34:30 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 5.505 | nll_loss 3.983 | ppl 15.82 | wps 93097.6 | wpb 4817.7 | bsz 183.8 | num_updates 4554 | best_loss 5.505
2020-10-12 22:34:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:34:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 22 @ 4554 updates, score 5.505) (writing took 4.43650561800132 seconds)
2020-10-12 22:34:35 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 22:34:35 | INFO | train | epoch 022 | loss 5.682 | nll_loss 4.307 | ppl 19.79 | wps 39000.3 | ups 2.68 | wpb 14533.2 | bsz 545.8 | num_updates 4554 | lr 0.000187441 | gnorm 0.885 | clip 0 | train_wall 66 | wall 1709
2020-10-12 22:34:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 22:34:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 22:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21384.92578125Mb; avail=467102.66015625Mb
2020-10-12 22:34:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003171
2020-10-12 22:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023914
2020-10-12 22:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21383.94140625Mb; avail=467103.64453125Mb
2020-10-12 22:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000814
2020-10-12 22:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21383.94140625Mb; avail=467103.64453125Mb
2020-10-12 22:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.416115
2020-10-12 22:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.441743
2020-10-12 22:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21382.8671875Mb; avail=467104.95703125Mb
2020-10-12 22:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21382.8671875Mb; avail=467104.95703125Mb
2020-10-12 22:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015811
2020-10-12 22:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21382.8671875Mb; avail=467104.95703125Mb
2020-10-12 22:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000816
2020-10-12 22:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21382.8671875Mb; avail=467104.95703125Mb
2020-10-12 22:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.414789
2020-10-12 22:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.432231
2020-10-12 22:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21382.9609375Mb; avail=467104.95703125Mb
2020-10-12 22:34:36 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 22:34:52 | INFO | train_inner | epoch 023:     46 / 207 loss=5.629, nll_loss=4.246, ppl=18.98, wps=35295.7, ups=2.39, wpb=14766.2, bsz=546.6, num_updates=4600, lr=0.000186501, gnorm=0.916, clip=0, train_wall=32, wall=1727
2020-10-12 22:35:25 | INFO | train_inner | epoch 023:    146 / 207 loss=5.549, nll_loss=4.154, ppl=17.8, wps=44306.3, ups=3.03, wpb=14626.9, bsz=557.4, num_updates=4700, lr=0.000184506, gnorm=0.898, clip=0, train_wall=32, wall=1760
2020-10-12 22:35:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:35:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21489.23046875Mb; avail=466998.16796875Mb
2020-10-12 22:35:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002249
2020-10-12 22:35:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21489.23046875Mb; avail=466998.16796875Mb
2020-10-12 22:35:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066012
2020-10-12 22:35:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21489.23046875Mb; avail=466998.16796875Mb
2020-10-12 22:35:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036024
2020-10-12 22:35:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105472
2020-10-12 22:35:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21489.23046875Mb; avail=466998.1640625Mb
2020-10-12 22:35:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21489.23046875Mb; avail=466998.1640625Mb
2020-10-12 22:35:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001583
2020-10-12 22:35:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21489.23046875Mb; avail=466998.1640625Mb
2020-10-12 22:35:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.106229
2020-10-12 22:35:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21490.80859375Mb; avail=466996.21875Mb
2020-10-12 22:35:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.038891
2020-10-12 22:35:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.148191
2020-10-12 22:35:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21494.99609375Mb; avail=466991.8515625Mb
2020-10-12 22:35:47 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 5.412 | nll_loss 3.869 | ppl 14.61 | wps 91197.7 | wpb 4817.7 | bsz 183.8 | num_updates 4761 | best_loss 5.412
2020-10-12 22:35:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:35:52 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 23 @ 4761 updates, score 5.412) (writing took 4.443849297998895 seconds)
2020-10-12 22:35:52 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 22:35:52 | INFO | train | epoch 023 | loss 5.56 | nll_loss 4.166 | ppl 17.95 | wps 38894.6 | ups 2.68 | wpb 14533.2 | bsz 545.8 | num_updates 4761 | lr 0.00018332 | gnorm 0.89 | clip 0 | train_wall 66 | wall 1787
2020-10-12 22:35:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 22:35:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 22:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21389.0703125Mb; avail=467098.19921875Mb
2020-10-12 22:35:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002887
2020-10-12 22:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023621
2020-10-12 22:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21387.56640625Mb; avail=467099.91796875Mb
2020-10-12 22:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000832
2020-10-12 22:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21387.53125Mb; avail=467100.16015625Mb
2020-10-12 22:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.417581
2020-10-12 22:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.442892
2020-10-12 22:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21386.24609375Mb; avail=467101.94140625Mb
2020-10-12 22:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21386.25390625Mb; avail=467101.93359375Mb
2020-10-12 22:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016443
2020-10-12 22:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21386.25390625Mb; avail=467101.93359375Mb
2020-10-12 22:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000833
2020-10-12 22:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21386.25390625Mb; avail=467101.93359375Mb
2020-10-12 22:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.412999
2020-10-12 22:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.431123
2020-10-12 22:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21385.54296875Mb; avail=467102.41015625Mb
2020-10-12 22:35:53 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 22:36:07 | INFO | train_inner | epoch 024:     39 / 207 loss=5.488, nll_loss=4.084, ppl=16.96, wps=34175.8, ups=2.37, wpb=14394.7, bsz=569, num_updates=4800, lr=0.000182574, gnorm=0.828, clip=0, train_wall=32, wall=1802
2020-10-12 22:36:40 | INFO | train_inner | epoch 024:    139 / 207 loss=5.478, nll_loss=4.07, ppl=16.79, wps=44429.9, ups=3.04, wpb=14634.7, bsz=520.7, num_updates=4900, lr=0.000180702, gnorm=0.866, clip=0, train_wall=32, wall=1835
2020-10-12 22:37:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21486.4921875Mb; avail=467001.34375Mb
2020-10-12 22:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001396
2020-10-12 22:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.4921875Mb; avail=467001.34375Mb
2020-10-12 22:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049884
2020-10-12 22:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.10546875Mb; avail=467001.109375Mb
2020-10-12 22:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035338
2020-10-12 22:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087408
2020-10-12 22:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.1875Mb; avail=467001.59375Mb
2020-10-12 22:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21486.30078125Mb; avail=467001.48046875Mb
2020-10-12 22:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000963
2020-10-12 22:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.30078125Mb; avail=467001.48046875Mb
2020-10-12 22:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049262
2020-10-12 22:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.30078125Mb; avail=467001.48046875Mb
2020-10-12 22:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034956
2020-10-12 22:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.085946
2020-10-12 22:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.1796875Mb; avail=467001.6015625Mb
2020-10-12 22:37:05 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.362 | nll_loss 3.812 | ppl 14.05 | wps 94630.3 | wpb 4817.7 | bsz 183.8 | num_updates 4968 | best_loss 5.362
2020-10-12 22:37:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:37:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 24 @ 4968 updates, score 5.362) (writing took 4.556078344001435 seconds)
2020-10-12 22:37:09 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 22:37:09 | INFO | train | epoch 024 | loss 5.448 | nll_loss 4.037 | ppl 16.42 | wps 38888.8 | ups 2.68 | wpb 14533.2 | bsz 545.8 | num_updates 4968 | lr 0.000179461 | gnorm 0.876 | clip 0 | train_wall 66 | wall 1864
2020-10-12 22:37:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 22:37:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 22:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21382.49609375Mb; avail=467104.9921875Mb
2020-10-12 22:37:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002677
2020-10-12 22:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023008
2020-10-12 22:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21382.515625Mb; avail=467104.75Mb
2020-10-12 22:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000823
2020-10-12 22:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21382.53125Mb; avail=467104.62890625Mb
2020-10-12 22:37:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.427045
2020-10-12 22:37:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.451691
2020-10-12 22:37:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21381.51171875Mb; avail=467105.91015625Mb
2020-10-12 22:37:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21381.51171875Mb; avail=467105.91015625Mb
2020-10-12 22:37:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015985
2020-10-12 22:37:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21381.51171875Mb; avail=467105.91015625Mb
2020-10-12 22:37:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000822
2020-10-12 22:37:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21381.51171875Mb; avail=467105.91015625Mb
2020-10-12 22:37:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.415667
2020-10-12 22:37:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.433306
2020-10-12 22:37:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21381.30859375Mb; avail=467106.5234375Mb
2020-10-12 22:37:10 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 22:37:22 | INFO | train_inner | epoch 025:     32 / 207 loss=5.402, nll_loss=3.984, ppl=15.82, wps=34286.5, ups=2.38, wpb=14416.4, bsz=555.2, num_updates=5000, lr=0.000178885, gnorm=0.9, clip=0, train_wall=32, wall=1877
2020-10-12 22:37:55 | INFO | train_inner | epoch 025:    132 / 207 loss=5.377, nll_loss=3.955, ppl=15.5, wps=43903.9, ups=3.04, wpb=14449, bsz=500.7, num_updates=5100, lr=0.000177123, gnorm=0.788, clip=0, train_wall=32, wall=1910
2020-10-12 22:38:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21486.6796875Mb; avail=467000.56640625Mb
2020-10-12 22:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002141
2020-10-12 22:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.6796875Mb; avail=467000.56640625Mb
2020-10-12 22:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.050666
2020-10-12 22:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.6796875Mb; avail=467000.56640625Mb
2020-10-12 22:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035307
2020-10-12 22:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.088976
2020-10-12 22:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.2265625Mb; avail=467000.6953125Mb
2020-10-12 22:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21486.3984375Mb; avail=467000.81640625Mb
2020-10-12 22:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001053
2020-10-12 22:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.3984375Mb; avail=467000.81640625Mb
2020-10-12 22:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.050540
2020-10-12 22:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.3984375Mb; avail=467000.81640625Mb
2020-10-12 22:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035304
2020-10-12 22:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087733
2020-10-12 22:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.9609375Mb; avail=467000.25Mb
2020-10-12 22:38:22 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.283 | nll_loss 3.712 | ppl 13.1 | wps 96989.6 | wpb 4817.7 | bsz 183.8 | num_updates 5175 | best_loss 5.283
2020-10-12 22:38:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:38:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 25 @ 5175 updates, score 5.283) (writing took 4.4926304669988895 seconds)
2020-10-12 22:38:26 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 22:38:26 | INFO | train | epoch 025 | loss 5.341 | nll_loss 3.914 | ppl 15.07 | wps 39081.3 | ups 2.69 | wpb 14533.2 | bsz 545.8 | num_updates 5175 | lr 0.000175835 | gnorm 0.807 | clip 0 | train_wall 65 | wall 1941
2020-10-12 22:38:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 22:38:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 22:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21384.5078125Mb; avail=467103.1953125Mb
2020-10-12 22:38:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003018
2020-10-12 22:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023542
2020-10-12 22:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21383.29296875Mb; avail=467104.41015625Mb
2020-10-12 22:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000813
2020-10-12 22:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21383.29296875Mb; avail=467104.41015625Mb
2020-10-12 22:38:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.425313
2020-10-12 22:38:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.450494
2020-10-12 22:38:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21383.68359375Mb; avail=467104.2265625Mb
2020-10-12 22:38:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21383.68359375Mb; avail=467104.2265625Mb
2020-10-12 22:38:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016086
2020-10-12 22:38:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21383.68359375Mb; avail=467104.2265625Mb
2020-10-12 22:38:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000810
2020-10-12 22:38:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21383.68359375Mb; avail=467104.2265625Mb
2020-10-12 22:38:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.424310
2020-10-12 22:38:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.442083
2020-10-12 22:38:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21383.91796875Mb; avail=467103.56640625Mb
2020-10-12 22:38:27 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 22:38:36 | INFO | train_inner | epoch 026:     25 / 207 loss=5.283, nll_loss=3.847, ppl=14.39, wps=35220.3, ups=2.4, wpb=14700.4, bsz=587.2, num_updates=5200, lr=0.000175412, gnorm=0.804, clip=0, train_wall=32, wall=1951
2020-10-12 22:39:09 | INFO | train_inner | epoch 026:    125 / 207 loss=5.239, nll_loss=3.797, ppl=13.9, wps=44292.2, ups=3.04, wpb=14592.5, bsz=564.7, num_updates=5300, lr=0.000173749, gnorm=0.81, clip=0, train_wall=32, wall=1984
2020-10-12 22:39:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21492.55859375Mb; avail=466995.2890625Mb
2020-10-12 22:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001475
2020-10-12 22:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21492.55859375Mb; avail=466995.2890625Mb
2020-10-12 22:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049380
2020-10-12 22:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21492.55859375Mb; avail=466995.2890625Mb
2020-10-12 22:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035186
2020-10-12 22:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086840
2020-10-12 22:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21492.55859375Mb; avail=466995.2890625Mb
2020-10-12 22:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21492.55859375Mb; avail=466995.2890625Mb
2020-10-12 22:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001028
2020-10-12 22:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21492.55859375Mb; avail=466995.2890625Mb
2020-10-12 22:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049037
2020-10-12 22:39:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21492.55859375Mb; avail=466995.2890625Mb
2020-10-12 22:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034993
2020-10-12 22:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.085805
2020-10-12 22:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21492.55859375Mb; avail=466995.2890625Mb
2020-10-12 22:39:39 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.244 | nll_loss 3.672 | ppl 12.75 | wps 92292 | wpb 4817.7 | bsz 183.8 | num_updates 5382 | best_loss 5.244
2020-10-12 22:39:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:39:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 26 @ 5382 updates, score 5.244) (writing took 4.4471379589995195 seconds)
2020-10-12 22:39:43 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 22:39:43 | INFO | train | epoch 026 | loss 5.256 | nll_loss 3.816 | ppl 14.08 | wps 38995.2 | ups 2.68 | wpb 14533.2 | bsz 545.8 | num_updates 5382 | lr 0.00017242 | gnorm 0.819 | clip 0 | train_wall 65 | wall 2018
2020-10-12 22:39:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 22:39:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 22:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21389.9140625Mb; avail=467097.58203125Mb
2020-10-12 22:39:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003218
2020-10-12 22:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023951
2020-10-12 22:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21388.390625Mb; avail=467098.81640625Mb
2020-10-12 22:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000836
2020-10-12 22:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21388.390625Mb; avail=467098.81640625Mb
2020-10-12 22:39:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.419229
2020-10-12 22:39:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.444834
2020-10-12 22:39:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21388.30078125Mb; avail=467099.328125Mb
2020-10-12 22:39:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21387.6171875Mb; avail=467100.2578125Mb
2020-10-12 22:39:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015941
2020-10-12 22:39:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21387.6171875Mb; avail=467100.2578125Mb
2020-10-12 22:39:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000806
2020-10-12 22:39:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21387.6171875Mb; avail=467100.2578125Mb
2020-10-12 22:39:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.414106
2020-10-12 22:39:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.431648
2020-10-12 22:39:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21387.5859375Mb; avail=467100.5Mb
2020-10-12 22:39:44 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 22:39:51 | INFO | train_inner | epoch 027:     18 / 207 loss=5.267, nll_loss=3.827, ppl=14.19, wps=34325.1, ups=2.39, wpb=14382, bsz=519, num_updates=5400, lr=0.000172133, gnorm=0.842, clip=0, train_wall=31, wall=2026
2020-10-12 22:40:24 | INFO | train_inner | epoch 027:    118 / 207 loss=5.168, nll_loss=3.715, ppl=13.13, wps=44289.8, ups=3.07, wpb=14436.6, bsz=548.2, num_updates=5500, lr=0.000170561, gnorm=0.805, clip=0, train_wall=32, wall=2059
2020-10-12 22:40:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21486.1953125Mb; avail=467000.29296875Mb
2020-10-12 22:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001473
2020-10-12 22:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.1953125Mb; avail=467000.29296875Mb
2020-10-12 22:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049903
2020-10-12 22:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.6171875Mb; avail=467001.2578125Mb
2020-10-12 22:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036606
2020-10-12 22:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.088802
2020-10-12 22:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.50390625Mb; avail=467000.89453125Mb
2020-10-12 22:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21486.51953125Mb; avail=467000.7734375Mb
2020-10-12 22:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001008
2020-10-12 22:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.51953125Mb; avail=467000.7734375Mb
2020-10-12 22:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048693
2020-10-12 22:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.640625Mb; avail=467001.2578125Mb
2020-10-12 22:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036145
2020-10-12 22:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086612
2020-10-12 22:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.640625Mb; avail=467001.2578125Mb
2020-10-12 22:40:55 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.223 | nll_loss 3.649 | ppl 12.54 | wps 95730 | wpb 4817.7 | bsz 183.8 | num_updates 5589 | best_loss 5.223
2020-10-12 22:40:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:41:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 27 @ 5589 updates, score 5.223) (writing took 4.421327657999427 seconds)
2020-10-12 22:41:00 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 22:41:00 | INFO | train | epoch 027 | loss 5.176 | nll_loss 3.723 | ppl 13.21 | wps 39331.4 | ups 2.71 | wpb 14533.2 | bsz 545.8 | num_updates 5589 | lr 0.000169197 | gnorm 0.807 | clip 0 | train_wall 65 | wall 2095
2020-10-12 22:41:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 22:41:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 22:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21389.0703125Mb; avail=467098.76953125Mb
2020-10-12 22:41:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002732
2020-10-12 22:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023523
2020-10-12 22:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21388.578125Mb; avail=467099.26171875Mb
2020-10-12 22:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000822
2020-10-12 22:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21388.578125Mb; avail=467099.26171875Mb
2020-10-12 22:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.418347
2020-10-12 22:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.443529
2020-10-12 22:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21387.46484375Mb; avail=467100.15234375Mb
2020-10-12 22:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21388.1015625Mb; avail=467099.91015625Mb
2020-10-12 22:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.017623
2020-10-12 22:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21388.1015625Mb; avail=467099.91015625Mb
2020-10-12 22:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000893
2020-10-12 22:41:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21388.1015625Mb; avail=467099.91015625Mb
2020-10-12 22:41:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.414862
2020-10-12 22:41:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.434454
2020-10-12 22:41:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21387.78125Mb; avail=467100.28125Mb
2020-10-12 22:41:01 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 22:41:06 | INFO | train_inner | epoch 028:     11 / 207 loss=5.171, nll_loss=3.717, ppl=13.15, wps=35030.8, ups=2.39, wpb=14637.7, bsz=545.4, num_updates=5600, lr=0.000169031, gnorm=0.803, clip=0, train_wall=32, wall=2101
2020-10-12 22:41:39 | INFO | train_inner | epoch 028:    111 / 207 loss=5.096, nll_loss=3.631, ppl=12.39, wps=44717.3, ups=3.04, wpb=14696.8, bsz=551.1, num_updates=5700, lr=0.000167542, gnorm=0.782, clip=0, train_wall=32, wall=2133
2020-10-12 22:42:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:42:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21486.77734375Mb; avail=467001.2734375Mb
2020-10-12 22:42:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002137
2020-10-12 22:42:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.77734375Mb; avail=467001.2734375Mb
2020-10-12 22:42:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048739
2020-10-12 22:42:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.77734375Mb; avail=467001.2734375Mb
2020-10-12 22:42:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035554
2020-10-12 22:42:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087233
2020-10-12 22:42:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.625Mb; avail=467000.90234375Mb
2020-10-12 22:42:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21486.8046875Mb; avail=467001.04296875Mb
2020-10-12 22:42:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001007
2020-10-12 22:42:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.8046875Mb; avail=467001.04296875Mb
2020-10-12 22:42:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048990
2020-10-12 22:42:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.7890625Mb; avail=467001.04296875Mb
2020-10-12 22:42:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035394
2020-10-12 22:42:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086154
2020-10-12 22:42:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21486.7890625Mb; avail=467001.04296875Mb
2020-10-12 22:42:12 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.162 | nll_loss 3.574 | ppl 11.91 | wps 92376.8 | wpb 4817.7 | bsz 183.8 | num_updates 5796 | best_loss 5.162
2020-10-12 22:42:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:42:17 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 28 @ 5796 updates, score 5.162) (writing took 4.425297629999477 seconds)
2020-10-12 22:42:17 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 22:42:17 | INFO | train | epoch 028 | loss 5.1 | nll_loss 3.636 | ppl 12.43 | wps 39127.2 | ups 2.69 | wpb 14533.2 | bsz 545.8 | num_updates 5796 | lr 0.000166148 | gnorm 0.786 | clip 0 | train_wall 66 | wall 2172
2020-10-12 22:42:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 22:42:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 22:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21384.0234375Mb; avail=467103.95703125Mb
2020-10-12 22:42:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002472
2020-10-12 22:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022831
2020-10-12 22:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21383.0390625Mb; avail=467104.94140625Mb
2020-10-12 22:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000860
2020-10-12 22:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21383.0390625Mb; avail=467104.94140625Mb
2020-10-12 22:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.417047
2020-10-12 22:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.441692
2020-10-12 22:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21382.0703125Mb; avail=467105.875Mb
2020-10-12 22:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21382.67578125Mb; avail=467105.26953125Mb
2020-10-12 22:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016178
2020-10-12 22:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21382.67578125Mb; avail=467105.26953125Mb
2020-10-12 22:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000882
2020-10-12 22:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21382.67578125Mb; avail=467105.26953125Mb
2020-10-12 22:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.418449
2020-10-12 22:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.436489
2020-10-12 22:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21382.33984375Mb; avail=467105.76171875Mb
2020-10-12 22:42:18 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 22:42:20 | INFO | train_inner | epoch 029:      4 / 207 loss=5.092, nll_loss=3.627, ppl=12.36, wps=34484.7, ups=2.4, wpb=14362.6, bsz=546.9, num_updates=5800, lr=0.000166091, gnorm=0.786, clip=0, train_wall=31, wall=2175
2020-10-12 22:42:53 | INFO | train_inner | epoch 029:    104 / 207 loss=5.015, nll_loss=3.539, ppl=11.62, wps=44834.1, ups=3.04, wpb=14730.4, bsz=562.7, num_updates=5900, lr=0.000164677, gnorm=0.759, clip=0, train_wall=32, wall=2208
2020-10-12 22:43:26 | INFO | train_inner | epoch 029:    204 / 207 loss=5.046, nll_loss=3.574, ppl=11.91, wps=43563.4, ups=3.03, wpb=14386, bsz=533.6, num_updates=6000, lr=0.000163299, gnorm=0.762, clip=0, train_wall=32, wall=2241
2020-10-12 22:43:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:43:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21492.4296875Mb; avail=466995.44140625Mb
2020-10-12 22:43:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002129
2020-10-12 22:43:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21492.4296875Mb; avail=466995.44140625Mb
2020-10-12 22:43:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048778
2020-10-12 22:43:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21492.4296875Mb; avail=466995.44140625Mb
2020-10-12 22:43:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034863
2020-10-12 22:43:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086596
2020-10-12 22:43:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21492.4296875Mb; avail=466995.44140625Mb
2020-10-12 22:43:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21492.3359375Mb; avail=466995.3203125Mb
2020-10-12 22:43:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000973
2020-10-12 22:43:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21492.3359375Mb; avail=466995.3203125Mb
2020-10-12 22:43:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049322
2020-10-12 22:43:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21492.2109375Mb; avail=466995.44140625Mb
2020-10-12 22:43:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035373
2020-10-12 22:43:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086427
2020-10-12 22:43:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21492.30859375Mb; avail=466995.5625Mb
2020-10-12 22:43:30 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.106 | nll_loss 3.504 | ppl 11.35 | wps 94151.8 | wpb 4817.7 | bsz 183.8 | num_updates 6003 | best_loss 5.106
2020-10-12 22:43:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:43:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 29 @ 6003 updates, score 5.106) (writing took 4.472909198000707 seconds)
2020-10-12 22:43:34 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 22:43:34 | INFO | train | epoch 029 | loss 5.029 | nll_loss 3.554 | ppl 11.75 | wps 38888.3 | ups 2.68 | wpb 14533.2 | bsz 545.8 | num_updates 6003 | lr 0.000163259 | gnorm 0.76 | clip 0 | train_wall 66 | wall 2249
2020-10-12 22:43:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 22:43:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 22:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21392.7890625Mb; avail=467094.375Mb
2020-10-12 22:43:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002543
2020-10-12 22:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022772
2020-10-12 22:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21391.4921875Mb; avail=467096.09375Mb
2020-10-12 22:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000809
2020-10-12 22:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21391.4921875Mb; avail=467096.09375Mb
2020-10-12 22:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.420522
2020-10-12 22:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.445193
2020-10-12 22:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21391.1640625Mb; avail=467096.41796875Mb
2020-10-12 22:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21391.1640625Mb; avail=467096.41796875Mb
2020-10-12 22:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016963
2020-10-12 22:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21391.1640625Mb; avail=467096.41796875Mb
2020-10-12 22:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000888
2020-10-12 22:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21391.1640625Mb; avail=467096.41796875Mb
2020-10-12 22:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.413761
2020-10-12 22:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.432490
2020-10-12 22:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21390.32421875Mb; avail=467097.5078125Mb
2020-10-12 22:43:35 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 22:44:08 | INFO | train_inner | epoch 030:     97 / 207 loss=4.955, nll_loss=3.47, ppl=11.08, wps=34141.6, ups=2.37, wpb=14407.9, bsz=563.3, num_updates=6100, lr=0.000161955, gnorm=0.773, clip=0, train_wall=32, wall=2283
2020-10-12 22:44:41 | INFO | train_inner | epoch 030:    197 / 207 loss=4.984, nll_loss=3.502, ppl=11.33, wps=45113.1, ups=3.08, wpb=14636.4, bsz=525.4, num_updates=6200, lr=0.000160644, gnorm=0.75, clip=0, train_wall=31, wall=2316
2020-10-12 22:44:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:44:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21833.39453125Mb; avail=466655.28125Mb
2020-10-12 22:44:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001410
2020-10-12 22:44:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21833.85546875Mb; avail=466655.3203125Mb
2020-10-12 22:44:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049036
2020-10-12 22:44:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21833.8671875Mb; avail=466654.87109375Mb
2020-10-12 22:44:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035976
2020-10-12 22:44:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087519
2020-10-12 22:44:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21833.4765625Mb; avail=466654.890625Mb
2020-10-12 22:44:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21833.7421875Mb; avail=466654.515625Mb
2020-10-12 22:44:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001109
2020-10-12 22:44:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21833.7421875Mb; avail=466654.515625Mb
2020-10-12 22:44:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.050008
2020-10-12 22:44:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21831.125Mb; avail=466657.83984375Mb
2020-10-12 22:44:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035671
2020-10-12 22:44:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087737
2020-10-12 22:44:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21831.109375Mb; avail=466657.85546875Mb
2020-10-12 22:44:47 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.076 | nll_loss 3.473 | ppl 11.1 | wps 91688.5 | wpb 4817.7 | bsz 183.8 | num_updates 6210 | best_loss 5.076
2020-10-12 22:44:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:44:52 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 30 @ 6210 updates, score 5.076) (writing took 5.463829366999562 seconds)
2020-10-12 22:44:52 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 22:44:52 | INFO | train | epoch 030 | loss 4.968 | nll_loss 3.484 | ppl 11.19 | wps 38530.3 | ups 2.65 | wpb 14533.2 | bsz 545.8 | num_updates 6210 | lr 0.000160514 | gnorm 0.76 | clip 0 | train_wall 65 | wall 2327
2020-10-12 22:44:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 22:44:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 22:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21388.34765625Mb; avail=467099.2734375Mb
2020-10-12 22:44:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002859
2020-10-12 22:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.029084
2020-10-12 22:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21386.37890625Mb; avail=467101.2421875Mb
2020-10-12 22:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001392
2020-10-12 22:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21386.37890625Mb; avail=467101.2421875Mb
2020-10-12 22:44:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.549539
2020-10-12 22:44:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.581636
2020-10-12 22:44:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21386.51953125Mb; avail=467101.06640625Mb
2020-10-12 22:44:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21386.375Mb; avail=467101.46484375Mb
2020-10-12 22:44:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.017006
2020-10-12 22:44:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21386.12109375Mb; avail=467101.22265625Mb
2020-10-12 22:44:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000871
2020-10-12 22:44:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21386.12109375Mb; avail=467101.22265625Mb
2020-10-12 22:44:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.479123
2020-10-12 22:44:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.497869
2020-10-12 22:44:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21386.6875Mb; avail=467101.22265625Mb
2020-10-12 22:44:53 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 22:45:24 | INFO | train_inner | epoch 031:     90 / 207 loss=4.923, nll_loss=3.432, ppl=10.8, wps=33647.6, ups=2.31, wpb=14553.7, bsz=529.2, num_updates=6300, lr=0.000159364, gnorm=0.768, clip=0, train_wall=32, wall=2359
2020-10-12 22:45:56 | INFO | train_inner | epoch 031:    190 / 207 loss=4.905, nll_loss=3.411, ppl=10.64, wps=44630.4, ups=3.09, wpb=14462.5, bsz=558.6, num_updates=6400, lr=0.000158114, gnorm=0.769, clip=0, train_wall=31, wall=2391
2020-10-12 22:46:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21491.546875Mb; avail=466996.21875Mb
2020-10-12 22:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002176
2020-10-12 22:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21491.42578125Mb; avail=466996.33984375Mb
2020-10-12 22:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049448
2020-10-12 22:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21491.42578125Mb; avail=466996.33984375Mb
2020-10-12 22:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035213
2020-10-12 22:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087638
2020-10-12 22:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21491.42578125Mb; avail=466996.33984375Mb
2020-10-12 22:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21491.42578125Mb; avail=466996.33984375Mb
2020-10-12 22:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000981
2020-10-12 22:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21491.42578125Mb; avail=466996.33984375Mb
2020-10-12 22:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.047737
2020-10-12 22:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21491.42578125Mb; avail=466996.33984375Mb
2020-10-12 22:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035289
2020-10-12 22:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084759
2020-10-12 22:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21491.34765625Mb; avail=466996.33984375Mb
2020-10-12 22:46:04 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 5.051 | nll_loss 3.451 | ppl 10.94 | wps 91985 | wpb 4817.7 | bsz 183.8 | num_updates 6417 | best_loss 5.051
2020-10-12 22:46:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:46:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 31 @ 6417 updates, score 5.051) (writing took 4.4460032989991305 seconds)
2020-10-12 22:46:09 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 22:46:09 | INFO | train | epoch 031 | loss 4.912 | nll_loss 3.42 | ppl 10.7 | wps 39189 | ups 2.7 | wpb 14533.2 | bsz 545.8 | num_updates 6417 | lr 0.000157904 | gnorm 0.771 | clip 0 | train_wall 65 | wall 2404
2020-10-12 22:46:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 22:46:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 22:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21392.7734375Mb; avail=467094.8125Mb
2020-10-12 22:46:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002786
2020-10-12 22:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023475
2020-10-12 22:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21392.7734375Mb; avail=467094.8125Mb
2020-10-12 22:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000890
2020-10-12 22:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21392.7734375Mb; avail=467094.8125Mb
2020-10-12 22:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.420455
2020-10-12 22:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.445670
2020-10-12 22:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21390.40625Mb; avail=467096.98046875Mb
2020-10-12 22:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21390.1484375Mb; avail=467097.34375Mb
2020-10-12 22:46:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.017899
2020-10-12 22:46:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21390.23828125Mb; avail=467097.34375Mb
2020-10-12 22:46:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000881
2020-10-12 22:46:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21390.23828125Mb; avail=467097.34375Mb
2020-10-12 22:46:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.413881
2020-10-12 22:46:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.433584
2020-10-12 22:46:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21390.3359375Mb; avail=467097.34375Mb
2020-10-12 22:46:10 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 22:46:39 | INFO | train_inner | epoch 032:     83 / 207 loss=4.875, nll_loss=3.377, ppl=10.39, wps=34181.4, ups=2.37, wpb=14425.8, bsz=530.4, num_updates=6500, lr=0.000156893, gnorm=0.771, clip=0, train_wall=32, wall=2433
2020-10-12 22:47:12 | INFO | train_inner | epoch 032:    183 / 207 loss=4.851, nll_loss=3.349, ppl=10.19, wps=44533.6, ups=3.02, wpb=14732, bsz=568.8, num_updates=6600, lr=0.0001557, gnorm=0.743, clip=0, train_wall=32, wall=2467
2020-10-12 22:47:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21494.0703125Mb; avail=466993.8046875Mb
2020-10-12 22:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002149
2020-10-12 22:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21494.0703125Mb; avail=466993.8046875Mb
2020-10-12 22:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049351
2020-10-12 22:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21494.0703125Mb; avail=466993.8046875Mb
2020-10-12 22:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035540
2020-10-12 22:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087866
2020-10-12 22:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21494.0703125Mb; avail=466993.8046875Mb
2020-10-12 22:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21494.0703125Mb; avail=466993.8046875Mb
2020-10-12 22:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000997
2020-10-12 22:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21494.0703125Mb; avail=466993.8046875Mb
2020-10-12 22:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048936
2020-10-12 22:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21493.85546875Mb; avail=466994.16796875Mb
2020-10-12 22:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035944
2020-10-12 22:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086643
2020-10-12 22:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21493.85546875Mb; avail=466994.16796875Mb
2020-10-12 22:47:22 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.025 | nll_loss 3.413 | ppl 10.65 | wps 95371 | wpb 4817.7 | bsz 183.8 | num_updates 6624 | best_loss 5.025
2020-10-12 22:47:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:47:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 32 @ 6624 updates, score 5.025) (writing took 4.425878255002317 seconds)
2020-10-12 22:47:27 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 22:47:27 | INFO | train | epoch 032 | loss 4.857 | nll_loss 3.356 | ppl 10.24 | wps 38745.3 | ups 2.67 | wpb 14533.2 | bsz 545.8 | num_updates 6624 | lr 0.000155417 | gnorm 0.76 | clip 0 | train_wall 66 | wall 2481
2020-10-12 22:47:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 22:47:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 22:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21369.1875Mb; avail=467118.0546875Mb
2020-10-12 22:47:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002770
2020-10-12 22:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023321
2020-10-12 22:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21368.65234375Mb; avail=467118.67578125Mb
2020-10-12 22:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000819
2020-10-12 22:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21368.65234375Mb; avail=467118.67578125Mb
2020-10-12 22:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.418850
2020-10-12 22:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.443829
2020-10-12 22:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21367.875Mb; avail=467120.37890625Mb
2020-10-12 22:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21120.46484375Mb; avail=467367.1875Mb
2020-10-12 22:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016102
2020-10-12 22:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21141.59375Mb; avail=467346.6640625Mb
2020-10-12 22:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000844
2020-10-12 22:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21143.41015625Mb; avail=467344.84765625Mb
2020-10-12 22:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.418424
2020-10-12 22:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.436330
2020-10-12 22:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21632.00390625Mb; avail=466855.58203125Mb
2020-10-12 22:47:28 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 22:47:54 | INFO | train_inner | epoch 033:     76 / 207 loss=4.805, nll_loss=3.297, ppl=9.83, wps=34296.1, ups=2.37, wpb=14455.4, bsz=524.9, num_updates=6700, lr=0.000154533, gnorm=0.745, clip=0, train_wall=32, wall=2509
2020-10-12 22:48:27 | INFO | train_inner | epoch 033:    176 / 207 loss=4.815, nll_loss=3.308, ppl=9.91, wps=44334.2, ups=3.06, wpb=14508.7, bsz=552.6, num_updates=6800, lr=0.000153393, gnorm=0.753, clip=0, train_wall=32, wall=2541
2020-10-12 22:48:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21493.81640625Mb; avail=466994.1015625Mb
2020-10-12 22:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001393
2020-10-12 22:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21493.81640625Mb; avail=466994.1015625Mb
2020-10-12 22:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.050245
2020-10-12 22:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21493.81640625Mb; avail=466994.1015625Mb
2020-10-12 22:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035329
2020-10-12 22:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087806
2020-10-12 22:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21493.81640625Mb; avail=466994.1015625Mb
2020-10-12 22:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21493.68359375Mb; avail=466994.2109375Mb
2020-10-12 22:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001048
2020-10-12 22:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21493.68359375Mb; avail=466994.2109375Mb
2020-10-12 22:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049880
2020-10-12 22:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21493.68359375Mb; avail=466994.2109375Mb
2020-10-12 22:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036115
2020-10-12 22:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087825
2020-10-12 22:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21493.50390625Mb; avail=466993.84765625Mb
2020-10-12 22:48:39 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 4.985 | nll_loss 3.377 | ppl 10.39 | wps 83836.7 | wpb 4817.7 | bsz 183.8 | num_updates 6831 | best_loss 4.985
2020-10-12 22:48:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:48:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 33 @ 6831 updates, score 4.985) (writing took 6.936609250999027 seconds)
2020-10-12 22:48:46 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 22:48:46 | INFO | train | epoch 033 | loss 4.805 | nll_loss 3.297 | ppl 9.83 | wps 37708.6 | ups 2.59 | wpb 14533.2 | bsz 545.8 | num_updates 6831 | lr 0.000153045 | gnorm 0.74 | clip 0 | train_wall 66 | wall 2561
2020-10-12 22:48:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 22:48:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 22:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21627.859375Mb; avail=466859.91796875Mb
2020-10-12 22:48:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003340
2020-10-12 22:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026364
2020-10-12 22:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21626.90625Mb; avail=466860.87109375Mb
2020-10-12 22:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000977
2020-10-12 22:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21626.90625Mb; avail=466860.87109375Mb
2020-10-12 22:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.471081
2020-10-12 22:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.499335
2020-10-12 22:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21627.3359375Mb; avail=466860.92578125Mb
2020-10-12 22:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21627.3359375Mb; avail=466860.92578125Mb
2020-10-12 22:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.017962
2020-10-12 22:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21627.3359375Mb; avail=466860.92578125Mb
2020-10-12 22:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000914
2020-10-12 22:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21627.3359375Mb; avail=466860.92578125Mb
2020-10-12 22:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.463334
2020-10-12 22:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.483113
2020-10-12 22:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21627.375Mb; avail=466860.90625Mb
2020-10-12 22:48:47 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 22:49:11 | INFO | train_inner | epoch 034:     69 / 207 loss=4.776, nll_loss=3.264, ppl=9.61, wps=32333.3, ups=2.24, wpb=14436, bsz=532.7, num_updates=6900, lr=0.000152277, gnorm=0.731, clip=0, train_wall=32, wall=2586
2020-10-12 22:49:44 | INFO | train_inner | epoch 034:    169 / 207 loss=4.765, nll_loss=3.251, ppl=9.52, wps=44336.4, ups=3.04, wpb=14602.3, bsz=566.9, num_updates=7000, lr=0.000151186, gnorm=0.78, clip=0, train_wall=32, wall=2619
2020-10-12 22:49:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21466.54296875Mb; avail=467021.57421875Mb
2020-10-12 22:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002378
2020-10-12 22:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21466.54296875Mb; avail=467021.57421875Mb
2020-10-12 22:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049707
2020-10-12 22:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21466.78125Mb; avail=467021.33203125Mb
2020-10-12 22:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036015
2020-10-12 22:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.088926
2020-10-12 22:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21466.64453125Mb; avail=467021.453125Mb
2020-10-12 22:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21466.7109375Mb; avail=467021.08984375Mb
2020-10-12 22:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001021
2020-10-12 22:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21466.7109375Mb; avail=467021.08984375Mb
2020-10-12 22:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048767
2020-10-12 22:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21466.8203125Mb; avail=467020.96875Mb
2020-10-12 22:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035224
2020-10-12 22:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.085841
2020-10-12 22:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21467.04296875Mb; avail=467020.72265625Mb
2020-10-12 22:49:59 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.962 | nll_loss 3.347 | ppl 10.17 | wps 85574 | wpb 4817.7 | bsz 183.8 | num_updates 7038 | best_loss 4.962
2020-10-12 22:49:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:50:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 34 @ 7038 updates, score 4.962) (writing took 7.493751134999911 seconds)
2020-10-12 22:50:07 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 22:50:07 | INFO | train | epoch 034 | loss 4.761 | nll_loss 3.246 | ppl 9.49 | wps 37477.1 | ups 2.58 | wpb 14533.2 | bsz 545.8 | num_updates 7038 | lr 0.000150777 | gnorm 0.753 | clip 0 | train_wall 66 | wall 2642
2020-10-12 22:50:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 22:50:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 22:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21353.4375Mb; avail=467134.32421875Mb
2020-10-12 22:50:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002500
2020-10-12 22:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022480
2020-10-12 22:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21355.25390625Mb; avail=467132.5078125Mb
2020-10-12 22:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000857
2020-10-12 22:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21355.25390625Mb; avail=467132.5078125Mb
2020-10-12 22:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.495110
2020-10-12 22:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.519425
2020-10-12 22:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21388.96875Mb; avail=467098.71484375Mb
2020-10-12 22:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21388.96875Mb; avail=467098.71484375Mb
2020-10-12 22:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.017047
2020-10-12 22:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21388.96875Mb; avail=467098.71484375Mb
2020-10-12 22:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000937
2020-10-12 22:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21388.96875Mb; avail=467098.71484375Mb
2020-10-12 22:50:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.412483
2020-10-12 22:50:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.431418
2020-10-12 22:50:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21389.37890625Mb; avail=467097.8671875Mb
2020-10-12 22:50:08 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 22:50:29 | INFO | train_inner | epoch 035:     62 / 207 loss=4.712, nll_loss=3.191, ppl=9.13, wps=32219.5, ups=2.24, wpb=14388.5, bsz=542.4, num_updates=7100, lr=0.000150117, gnorm=0.742, clip=0, train_wall=31, wall=2664
2020-10-12 22:51:01 | INFO | train_inner | epoch 035:    162 / 207 loss=4.719, nll_loss=3.198, ppl=9.18, wps=44839.2, ups=3.06, wpb=14653.4, bsz=543.9, num_updates=7200, lr=0.000149071, gnorm=0.722, clip=0, train_wall=32, wall=2696
2020-10-12 22:51:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:51:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21370.48828125Mb; avail=467117.57421875Mb
2020-10-12 22:51:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002252
2020-10-12 22:51:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21370.48828125Mb; avail=467117.57421875Mb
2020-10-12 22:51:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049279
2020-10-12 22:51:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21370.48828125Mb; avail=467117.57421875Mb
2020-10-12 22:51:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035740
2020-10-12 22:51:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.088118
2020-10-12 22:51:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21370.48828125Mb; avail=467117.57421875Mb
2020-10-12 22:51:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21370.48828125Mb; avail=467117.57421875Mb
2020-10-12 22:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001034
2020-10-12 22:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21370.48828125Mb; avail=467117.57421875Mb
2020-10-12 22:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049890
2020-10-12 22:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21370.52734375Mb; avail=467117.453125Mb
2020-10-12 22:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034852
2020-10-12 22:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086560
2020-10-12 22:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21370.625Mb; avail=467117.2109375Mb
2020-10-12 22:51:19 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.949 | nll_loss 3.327 | ppl 10.04 | wps 92336.5 | wpb 4817.7 | bsz 183.8 | num_updates 7245 | best_loss 4.949
2020-10-12 22:51:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:51:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 35 @ 7245 updates, score 4.949) (writing took 4.438913017998857 seconds)
2020-10-12 22:51:23 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 22:51:23 | INFO | train | epoch 035 | loss 4.713 | nll_loss 3.191 | ppl 9.13 | wps 39225.2 | ups 2.7 | wpb 14533.2 | bsz 545.8 | num_updates 7245 | lr 0.000148608 | gnorm 0.735 | clip 0 | train_wall 65 | wall 2718
2020-10-12 22:51:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 22:51:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 22:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21391.9921875Mb; avail=467095.62109375Mb
2020-10-12 22:51:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003006
2020-10-12 22:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023809
2020-10-12 22:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21391.0078125Mb; avail=467096.60546875Mb
2020-10-12 22:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000929
2020-10-12 22:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21391.12109375Mb; avail=467096.4921875Mb
2020-10-12 22:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.417720
2020-10-12 22:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.443522
2020-10-12 22:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21390.08203125Mb; avail=467097.1171875Mb
2020-10-12 22:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21390.234375Mb; avail=467097.734375Mb
2020-10-12 22:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016102
2020-10-12 22:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21390.45703125Mb; avail=467097.4921875Mb
2020-10-12 22:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000881
2020-10-12 22:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21390.4296875Mb; avail=467097.51953125Mb
2020-10-12 22:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.415326
2020-10-12 22:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.433196
2020-10-12 22:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21390.921875Mb; avail=467096.9140625Mb
2020-10-12 22:51:24 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 22:51:44 | INFO | train_inner | epoch 036:     55 / 207 loss=4.686, nll_loss=3.16, ppl=8.94, wps=34831.5, ups=2.38, wpb=14660, bsz=559.8, num_updates=7300, lr=0.000148047, gnorm=0.727, clip=0, train_wall=32, wall=2738
2020-10-12 22:52:16 | INFO | train_inner | epoch 036:    155 / 207 loss=4.676, nll_loss=3.149, ppl=8.87, wps=43590.4, ups=3.05, wpb=14310.2, bsz=546, num_updates=7400, lr=0.000147043, gnorm=0.746, clip=0, train_wall=32, wall=2771
2020-10-12 22:52:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:52:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21497.83203125Mb; avail=466989.94140625Mb
2020-10-12 22:52:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001439
2020-10-12 22:52:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21497.83203125Mb; avail=466989.94140625Mb
2020-10-12 22:52:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049384
2020-10-12 22:52:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21497.83203125Mb; avail=466989.94140625Mb
2020-10-12 22:52:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061283
2020-10-12 22:52:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112916
2020-10-12 22:52:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21497.69921875Mb; avail=466989.9375Mb
2020-10-12 22:52:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21497.703125Mb; avail=466990.0625Mb
2020-10-12 22:52:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000938
2020-10-12 22:52:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21497.703125Mb; avail=466990.0625Mb
2020-10-12 22:52:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048535
2020-10-12 22:52:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21497.703125Mb; avail=466990.0625Mb
2020-10-12 22:52:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.057560
2020-10-12 22:52:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107959
2020-10-12 22:52:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21499.40625Mb; avail=466988.359375Mb
2020-10-12 22:52:36 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 4.945 | nll_loss 3.323 | ppl 10.01 | wps 94579.3 | wpb 4817.7 | bsz 183.8 | num_updates 7452 | best_loss 4.945
2020-10-12 22:52:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:52:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 36 @ 7452 updates, score 4.945) (writing took 4.471064810000826 seconds)
2020-10-12 22:52:41 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 22:52:41 | INFO | train | epoch 036 | loss 4.671 | nll_loss 3.143 | ppl 8.83 | wps 38894 | ups 2.68 | wpb 14533.2 | bsz 545.8 | num_updates 7452 | lr 0.000146529 | gnorm 0.733 | clip 0 | train_wall 66 | wall 2796
2020-10-12 22:52:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 22:52:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 22:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21397.828125Mb; avail=467088.49609375Mb
2020-10-12 22:52:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003315
2020-10-12 22:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024523
2020-10-12 22:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21397.828125Mb; avail=467088.49609375Mb
2020-10-12 22:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000950
2020-10-12 22:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21396.84375Mb; avail=467089.48046875Mb
2020-10-12 22:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.431177
2020-10-12 22:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.457787
2020-10-12 22:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21397.08203125Mb; avail=467088.5703125Mb
2020-10-12 22:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21397.08203125Mb; avail=467088.5703125Mb
2020-10-12 22:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.017654
2020-10-12 22:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21397.1953125Mb; avail=467088.45703125Mb
2020-10-12 22:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000836
2020-10-12 22:52:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21397.1953125Mb; avail=467088.45703125Mb
2020-10-12 22:52:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.417176
2020-10-12 22:52:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.436614
2020-10-12 22:52:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21396.02734375Mb; avail=467091.7890625Mb
2020-10-12 22:52:42 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 22:52:58 | INFO | train_inner | epoch 037:     48 / 207 loss=4.657, nll_loss=3.126, ppl=8.73, wps=35122.4, ups=2.38, wpb=14779, bsz=549, num_updates=7500, lr=0.000146059, gnorm=0.731, clip=0, train_wall=32, wall=2813
2020-10-12 22:53:31 | INFO | train_inner | epoch 037:    148 / 207 loss=4.628, nll_loss=3.094, ppl=8.54, wps=44654.2, ups=3.06, wpb=14602.6, bsz=550.5, num_updates=7600, lr=0.000145095, gnorm=0.747, clip=0, train_wall=32, wall=2846
2020-10-12 22:53:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21843.1953125Mb; avail=466645.34375Mb
2020-10-12 22:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001480
2020-10-12 22:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21843.1953125Mb; avail=466645.34375Mb
2020-10-12 22:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049791
2020-10-12 22:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21843.1953125Mb; avail=466645.34375Mb
2020-10-12 22:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036257
2020-10-12 22:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.088419
2020-10-12 22:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21843.1953125Mb; avail=466645.34375Mb
2020-10-12 22:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21843.1953125Mb; avail=466645.34375Mb
2020-10-12 22:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000964
2020-10-12 22:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21843.1953125Mb; avail=466645.34375Mb
2020-10-12 22:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049710
2020-10-12 22:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21843.1953125Mb; avail=466645.34375Mb
2020-10-12 22:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036089
2020-10-12 22:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087546
2020-10-12 22:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21843.1953125Mb; avail=466645.34375Mb
2020-10-12 22:53:54 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 4.915 | nll_loss 3.286 | ppl 9.75 | wps 62922 | wpb 4817.7 | bsz 183.8 | num_updates 7659 | best_loss 4.915
2020-10-12 22:53:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:53:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 37 @ 7659 updates, score 4.915) (writing took 4.824959985999158 seconds)
2020-10-12 22:53:58 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 22:53:58 | INFO | train | epoch 037 | loss 4.632 | nll_loss 3.098 | ppl 8.56 | wps 38704.8 | ups 2.66 | wpb 14533.2 | bsz 545.8 | num_updates 7659 | lr 0.000144535 | gnorm 0.732 | clip 0 | train_wall 65 | wall 2873
2020-10-12 22:53:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 22:53:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 22:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21399.5234375Mb; avail=467088.55078125Mb
2020-10-12 22:53:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002618
2020-10-12 22:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022963
2020-10-12 22:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21398.5390625Mb; avail=467089.53515625Mb
2020-10-12 22:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000855
2020-10-12 22:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21398.5390625Mb; avail=467089.53515625Mb
2020-10-12 22:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.426333
2020-10-12 22:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.451002
2020-10-12 22:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21397.7265625Mb; avail=467090.1875Mb
2020-10-12 22:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21397.7890625Mb; avail=467089.9453125Mb
2020-10-12 22:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016226
2020-10-12 22:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21397.88671875Mb; avail=467089.8359375Mb
2020-10-12 22:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000810
2020-10-12 22:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21397.88671875Mb; avail=467089.8359375Mb
2020-10-12 22:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.416728
2020-10-12 22:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.434587
2020-10-12 22:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21398.15625Mb; avail=467089.234375Mb
2020-10-12 22:53:59 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 22:54:14 | INFO | train_inner | epoch 038:     41 / 207 loss=4.619, nll_loss=3.083, ppl=8.47, wps=33666.7, ups=2.33, wpb=14420.2, bsz=529.3, num_updates=7700, lr=0.00014415, gnorm=0.699, clip=0, train_wall=32, wall=2889
2020-10-12 22:54:47 | INFO | train_inner | epoch 038:    141 / 207 loss=4.582, nll_loss=3.041, ppl=8.23, wps=44254.7, ups=3.04, wpb=14539.2, bsz=566.6, num_updates=7800, lr=0.000143223, gnorm=0.735, clip=0, train_wall=32, wall=2922
2020-10-12 22:55:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16347.58984375Mb; avail=472165.11328125Mb
2020-10-12 22:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001440
2020-10-12 22:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16347.58984375Mb; avail=472165.11328125Mb
2020-10-12 22:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049283
2020-10-12 22:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16347.6640625Mb; avail=472165.234375Mb
2020-10-12 22:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035366
2020-10-12 22:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.086871
2020-10-12 22:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16347.65625Mb; avail=472165.2421875Mb
2020-10-12 22:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16347.62890625Mb; avail=472165.2421875Mb
2020-10-12 22:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000987
2020-10-12 22:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16347.62890625Mb; avail=472165.2421875Mb
2020-10-12 22:55:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048604
2020-10-12 22:55:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16347.62890625Mb; avail=472165.2421875Mb
2020-10-12 22:55:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034649
2020-10-12 22:55:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.084996
2020-10-12 22:55:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16347.62890625Mb; avail=472165.2421875Mb
2020-10-12 22:55:11 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 4.9 | nll_loss 3.272 | ppl 9.66 | wps 96667.8 | wpb 4817.7 | bsz 183.8 | num_updates 7866 | best_loss 4.9
2020-10-12 22:55:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:55:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 38 @ 7866 updates, score 4.9) (writing took 4.518145219000871 seconds)
2020-10-12 22:55:15 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 22:55:15 | INFO | train | epoch 038 | loss 4.595 | nll_loss 3.055 | ppl 8.31 | wps 39110.7 | ups 2.69 | wpb 14533.2 | bsz 545.8 | num_updates 7866 | lr 0.000142621 | gnorm 0.729 | clip 0 | train_wall 65 | wall 2950
2020-10-12 22:55:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 22:55:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 22:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16247.015625Mb; avail=472265.74609375Mb
2020-10-12 22:55:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002520
2020-10-12 22:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023065
2020-10-12 22:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16245.5390625Mb; avail=472267.22265625Mb
2020-10-12 22:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000833
2020-10-12 22:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16245.5390625Mb; avail=472267.22265625Mb
2020-10-12 22:55:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.415166
2020-10-12 22:55:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.439858
2020-10-12 22:55:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16244.6328125Mb; avail=472268.13671875Mb
2020-10-12 22:55:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16244.52734375Mb; avail=472268.13671875Mb
2020-10-12 22:55:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015730
2020-10-12 22:55:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16245.01953125Mb; avail=472267.8125Mb
2020-10-12 22:55:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000811
2020-10-12 22:55:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16244.8984375Mb; avail=472267.93359375Mb
2020-10-12 22:55:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.410439
2020-10-12 22:55:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.427749
2020-10-12 22:55:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16245.5234375Mb; avail=472267.58984375Mb
2020-10-12 22:55:16 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 22:55:28 | INFO | train_inner | epoch 039:     34 / 207 loss=4.606, nll_loss=3.068, ppl=8.39, wps=34725.4, ups=2.4, wpb=14445.5, bsz=496.1, num_updates=7900, lr=0.000142314, gnorm=0.747, clip=0, train_wall=31, wall=2963
2020-10-12 22:56:01 | INFO | train_inner | epoch 039:    134 / 207 loss=4.567, nll_loss=3.024, ppl=8.13, wps=44283.8, ups=3.05, wpb=14505.9, bsz=541.7, num_updates=8000, lr=0.000141421, gnorm=0.741, clip=0, train_wall=32, wall=2996
2020-10-12 22:56:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=14336.0703125Mb; avail=474179.26953125Mb
2020-10-12 22:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001525
2020-10-12 22:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14336.0703125Mb; avail=474179.26953125Mb
2020-10-12 22:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049273
2020-10-12 22:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14353.046875Mb; avail=474162.29296875Mb
2020-10-12 22:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054319
2020-10-12 22:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105926
2020-10-12 22:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14377.265625Mb; avail=474138.07421875Mb
2020-10-12 22:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=14387.671875Mb; avail=474127.66796875Mb
2020-10-12 22:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000958
2020-10-12 22:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14388.27734375Mb; avail=474127.0625Mb
2020-10-12 22:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.048573
2020-10-12 22:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14442.9765625Mb; avail=474072.20703125Mb
2020-10-12 22:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.035671
2020-10-12 22:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.085961
2020-10-12 22:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14450.6953125Mb; avail=474064.98046875Mb
2020-10-12 22:56:28 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 4.894 | nll_loss 3.267 | ppl 9.62 | wps 91097.2 | wpb 4817.7 | bsz 183.8 | num_updates 8073 | best_loss 4.894
2020-10-12 22:56:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:56:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 39 @ 8073 updates, score 4.894) (writing took 4.454533694999554 seconds)
2020-10-12 22:56:32 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 22:56:32 | INFO | train | epoch 039 | loss 4.56 | nll_loss 3.016 | ppl 8.09 | wps 39170.3 | ups 2.7 | wpb 14533.2 | bsz 545.8 | num_updates 8073 | lr 0.000140781 | gnorm 0.733 | clip 0 | train_wall 65 | wall 3027
2020-10-12 22:56:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 22:56:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 22:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20687.76953125Mb; avail=467801.171875Mb
2020-10-12 22:56:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003105
2020-10-12 22:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023742
2020-10-12 22:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20686.29296875Mb; avail=467802.6484375Mb
2020-10-12 22:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000851
2020-10-12 22:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20686.203125Mb; avail=467802.6484375Mb
2020-10-12 22:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.422306
2020-10-12 22:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.447911
2020-10-12 22:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20752.12890625Mb; avail=467736.59375Mb
2020-10-12 22:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20761.6953125Mb; avail=467727.203125Mb
2020-10-12 22:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.016467
2020-10-12 22:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20766.48828125Mb; avail=467722.41015625Mb
2020-10-12 22:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000879
2020-10-12 22:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20766.48828125Mb; avail=467722.41015625Mb
2020-10-12 22:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.564281
2020-10-12 22:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.582557
2020-10-12 22:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20872.9453125Mb; avail=467616.46484375Mb
2020-10-12 22:56:33 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 22:56:43 | INFO | train_inner | epoch 040:     27 / 207 loss=4.534, nll_loss=2.986, ppl=7.92, wps=34690.4, ups=2.39, wpb=14533.1, bsz=583.6, num_updates=8100, lr=0.000140546, gnorm=0.706, clip=0, train_wall=31, wall=3038
2020-10-12 22:57:15 | INFO | train_inner | epoch 040:    127 / 207 loss=4.515, nll_loss=2.964, ppl=7.8, wps=45186.3, ups=3.1, wpb=14599.5, bsz=558.7, num_updates=8200, lr=0.000139686, gnorm=0.738, clip=0, train_wall=31, wall=3070
2020-10-12 22:57:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:57:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21363.234375Mb; avail=467124.7265625Mb
2020-10-12 22:57:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001434
2020-10-12 22:57:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21363.234375Mb; avail=467124.7265625Mb
2020-10-12 22:57:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.050137
2020-10-12 22:57:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21363.3125Mb; avail=467124.48046875Mb
2020-10-12 22:57:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036714
2020-10-12 22:57:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.089116
2020-10-12 22:57:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21363.5234375Mb; avail=467124.359375Mb
2020-10-12 22:57:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21363.44140625Mb; avail=467124.359375Mb
2020-10-12 22:57:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001053
2020-10-12 22:57:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21363.44140625Mb; avail=467124.359375Mb
2020-10-12 22:57:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.049933
2020-10-12 22:57:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21363.59765625Mb; avail=467124.359375Mb
2020-10-12 22:57:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.036128
2020-10-12 22:57:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.087886
2020-10-12 22:57:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21363.5390625Mb; avail=467124.23828125Mb
2020-10-12 22:57:44 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 4.863 | nll_loss 3.224 | ppl 9.35 | wps 96654.1 | wpb 4817.7 | bsz 183.8 | num_updates 8280 | best_loss 4.863
2020-10-12 22:57:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:57:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belukrLG_sepspm8000/M2O/checkpoint_best.pt (epoch 40 @ 8280 updates, score 4.863) (writing took 4.523920903000544 seconds)
2020-10-12 22:57:49 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 22:57:49 | INFO | train | epoch 040 | loss 4.528 | nll_loss 2.979 | ppl 7.88 | wps 39277.4 | ups 2.7 | wpb 14533.2 | bsz 545.8 | num_updates 8280 | lr 0.00013901 | gnorm 0.746 | clip 0 | train_wall 65 | wall 3104
2020-10-12 22:57:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 22:57:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 22:57:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18887.87890625Mb; avail=469612.78125Mb
2020-10-12 22:57:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002725
2020-10-12 22:57:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022799
2020-10-12 22:57:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18887.5Mb; avail=469613.16015625Mb
2020-10-12 22:57:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000816
2020-10-12 22:57:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18887.5Mb; avail=469613.16015625Mb
2020-10-12 22:57:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.416215
2020-10-12 22:57:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.440664
2020-10-12 22:57:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18887.4296875Mb; avail=469613.1640625Mb
2020-10-12 22:57:49 | INFO | fairseq_cli.train | done training in 3103.5 seconds
