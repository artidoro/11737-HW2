2020-10-12 01:05:37 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_belpol_sepspm8000/O2M/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok='tgt', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='eng-bel,eng-pol', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_belpol_sepspm8000/O2M/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 01:05:37 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 01:05:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['bel', 'eng', 'pol']
2020-10-12 01:05:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bel] dictionary: 22227 types
2020-10-12 01:05:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 22227 types
2020-10-12 01:05:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | [pol] dictionary: 22227 types
2020-10-12 01:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 01:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:05:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 01:05:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:eng-bel': 1, 'main:eng-pol': 1}
2020-10-12 01:05:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-bel src_langtok: 22224; tgt_langtok: None
2020-10-12 01:05:37 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belpol_sepspm8000/O2M/valid.eng-bel.eng
2020-10-12 01:05:37 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belpol_sepspm8000/O2M/valid.eng-bel.bel
2020-10-12 01:05:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belpol_sepspm8000/O2M/ valid eng-bel 248 examples
2020-10-12 01:05:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-pol src_langtok: 22226; tgt_langtok: None
2020-10-12 01:05:37 | INFO | fairseq.data.data_utils | loaded 4108 examples from: fairseq/data-bin/ted_belpol_sepspm8000/O2M/valid.eng-pol.eng
2020-10-12 01:05:37 | INFO | fairseq.data.data_utils | loaded 4108 examples from: fairseq/data-bin/ted_belpol_sepspm8000/O2M/valid.eng-pol.pol
2020-10-12 01:05:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belpol_sepspm8000/O2M/ valid eng-pol 4108 examples
2020-10-12 01:05:37 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22227, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22227, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=22227, bias=False)
  )
)
2020-10-12 01:05:37 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 01:05:37 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 01:05:37 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 01:05:37 | INFO | fairseq_cli.train | num. model params: 42923520 (num. trained: 42923520)
2020-10-12 01:05:39 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 01:05:39 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 01:05:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 01:05:39 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.732 GB ; name = GeForce RTX 2080 Ti                     
2020-10-12 01:05:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 01:05:39 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 01:05:39 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 01:05:39 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_last.pt
2020-10-12 01:05:39 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 01:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 01:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:05:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 01:05:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:eng-bel': 1, 'main:eng-pol': 1}
2020-10-12 01:05:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-bel src_langtok: 22224; tgt_langtok: None
2020-10-12 01:05:39 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belpol_sepspm8000/O2M/train.eng-bel.eng
2020-10-12 01:05:39 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belpol_sepspm8000/O2M/train.eng-bel.bel
2020-10-12 01:05:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belpol_sepspm8000/O2M/ train eng-bel 4509 examples
2020-10-12 01:05:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-pol src_langtok: 22226; tgt_langtok: None
2020-10-12 01:05:39 | INFO | fairseq.data.data_utils | loaded 39985 examples from: fairseq/data-bin/ted_belpol_sepspm8000/O2M/train.eng-pol.eng
2020-10-12 01:05:39 | INFO | fairseq.data.data_utils | loaded 39985 examples from: fairseq/data-bin/ted_belpol_sepspm8000/O2M/train.eng-pol.pol
2020-10-12 01:05:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belpol_sepspm8000/O2M/ train eng-pol 39985 examples
2020-10-12 01:05:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:eng-bel', 4509), ('main:eng-pol', 39985)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 01:05:39 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 01:05:39 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 44494
2020-10-12 01:05:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 44494; virtual dataset size 44494
2020-10-12 01:05:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 01:05:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:eng-bel': 4509, 'main:eng-pol': 39985}; raw total size: 44494
2020-10-12 01:05:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:eng-bel': 4509, 'main:eng-pol': 39985}; resampled total size: 44494
2020-10-12 01:05:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 01:05:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.003054
2020-10-12 01:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:05:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000412
2020-10-12 01:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005243
2020-10-12 01:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-12 01:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094735
2020-10-12 01:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100566
2020-10-12 01:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:05:39 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 01:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004215
2020-10-12 01:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 01:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093768
2020-10-12 01:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098493
2020-10-12 01:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:05:39 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 01:06:11 | INFO | train_inner | epoch 001:    100 / 174 loss=14.221, nll_loss=14.113, ppl=17719.8, wps=19555.4, ups=3.17, wpb=6182.6, bsz=261.4, num_updates=100, lr=5.0975e-06, gnorm=3.929, clip=0, train_wall=31, wall=32
2020-10-12 01:06:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000788
2020-10-12 01:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037747
2020-10-12 01:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.028791
2020-10-12 01:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.067674
2020-10-12 01:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000651
2020-10-12 01:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037653
2020-10-12 01:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027365
2020-10-12 01:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065997
2020-10-12 01:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
/home/han/Documents/11737-hw/assign2/fairseq/fairseq/utils.py:340: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2020-10-12 01:06:38 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.555 | nll_loss 12.236 | ppl 4824.63 | wps 44829.2 | wpb 2306.9 | bsz 92.7 | num_updates 174
2020-10-12 01:06:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:06:39 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 1 @ 174 updates, score 12.555) (writing took 0.9573388999997405 seconds)
2020-10-12 01:06:39 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 01:06:39 | INFO | train | epoch 001 | loss 13.735 | nll_loss 13.57 | ppl 12161 | wps 18254.4 | ups 2.93 | wpb 6233 | bsz 255.7 | num_updates 174 | lr 8.79565e-06 | gnorm 2.996 | clip 0 | train_wall 55 | wall 60
2020-10-12 01:06:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 01:06:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 01:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:06:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000436
2020-10-12 01:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005326
2020-10-12 01:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-12 01:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095005
2020-10-12 01:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100828
2020-10-12 01:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004211
2020-10-12 01:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 01:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095532
2020-10-12 01:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100238
2020-10-12 01:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:06:39 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 01:06:48 | INFO | train_inner | epoch 002:     26 / 174 loss=12.984, nll_loss=12.731, ppl=6799.38, wps=17261.1, ups=2.73, wpb=6321.8, bsz=261.7, num_updates=200, lr=1.0095e-05, gnorm=1.712, clip=0, train_wall=32, wall=69
2020-10-12 01:07:21 | INFO | train_inner | epoch 002:    126 / 174 loss=12.384, nll_loss=12.065, ppl=4284.03, wps=19081.6, ups=3.04, wpb=6279.6, bsz=244.8, num_updates=300, lr=1.50925e-05, gnorm=1.232, clip=0, train_wall=32, wall=102
2020-10-12 01:07:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.006986
2020-10-12 01:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037775
2020-10-12 01:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027273
2020-10-12 01:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072394
2020-10-12 01:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000663
2020-10-12 01:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037527
2020-10-12 01:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027008
2020-10-12 01:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065527
2020-10-12 01:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:07:39 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.505 | nll_loss 11.038 | ppl 2102.14 | wps 44428.1 | wpb 2306.9 | bsz 92.7 | num_updates 348 | best_loss 11.505
2020-10-12 01:07:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:07:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 2 @ 348 updates, score 11.505) (writing took 1.5221282829879783 seconds)
2020-10-12 01:07:41 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 01:07:41 | INFO | train | epoch 002 | loss 12.29 | nll_loss 11.959 | ppl 3979.87 | wps 17579 | ups 2.82 | wpb 6233 | bsz 255.7 | num_updates 348 | lr 1.74913e-05 | gnorm 1.364 | clip 0 | train_wall 57 | wall 122
2020-10-12 01:07:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 01:07:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 01:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:07:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000438
2020-10-12 01:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005274
2020-10-12 01:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 01:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097067
2020-10-12 01:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102856
2020-10-12 01:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004335
2020-10-12 01:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 01:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097964
2020-10-12 01:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102811
2020-10-12 01:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:07:41 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 01:07:58 | INFO | train_inner | epoch 003:     52 / 174 loss=11.674, nll_loss=11.263, ppl=2457.91, wps=16602.2, ups=2.68, wpb=6202.2, bsz=257.4, num_updates=400, lr=2.009e-05, gnorm=1.466, clip=0, train_wall=32, wall=139
2020-10-12 01:08:31 | INFO | train_inner | epoch 003:    152 / 174 loss=11.099, nll_loss=10.585, ppl=1535.72, wps=18792.1, ups=3.02, wpb=6217.4, bsz=253.4, num_updates=500, lr=2.50875e-05, gnorm=1.508, clip=0, train_wall=33, wall=172
2020-10-12 01:08:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:08:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:08:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000771
2020-10-12 01:08:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.038649
2020-10-12 01:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027288
2020-10-12 01:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.067053
2020-10-12 01:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000661
2020-10-12 01:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.038147
2020-10-12 01:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027466
2020-10-12 01:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066597
2020-10-12 01:08:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:08:41 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.599 | nll_loss 9.956 | ppl 993.26 | wps 43811.9 | wpb 2306.9 | bsz 92.7 | num_updates 522 | best_loss 10.599
2020-10-12 01:08:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:08:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 3 @ 522 updates, score 10.599) (writing took 1.5128658589965198 seconds)
2020-10-12 01:08:43 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 01:08:43 | INFO | train | epoch 003 | loss 11.21 | nll_loss 10.715 | ppl 1681.28 | wps 17508.2 | ups 2.81 | wpb 6233 | bsz 255.7 | num_updates 522 | lr 2.6187e-05 | gnorm 1.445 | clip 0 | train_wall 57 | wall 184
2020-10-12 01:08:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 01:08:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 01:08:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:08:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000492
2020-10-12 01:08:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005358
2020-10-12 01:08:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:08:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 01:08:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:08:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096613
2020-10-12 01:08:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102472
2020-10-12 01:08:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:08:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:08:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004214
2020-10-12 01:08:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:08:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 01:08:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:08:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096027
2020-10-12 01:08:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100742
2020-10-12 01:08:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:08:43 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 01:09:09 | INFO | train_inner | epoch 004:     78 / 174 loss=10.846, nll_loss=10.267, ppl=1232.48, wps=16681.1, ups=2.66, wpb=6272.6, bsz=253.9, num_updates=600, lr=3.0085e-05, gnorm=1.174, clip=0, train_wall=33, wall=210
2020-10-12 01:09:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000772
2020-10-12 01:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037943
2020-10-12 01:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029097
2020-10-12 01:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.068155
2020-10-12 01:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000668
2020-10-12 01:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037998
2020-10-12 01:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027846
2020-10-12 01:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066839
2020-10-12 01:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:09:43 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.328 | nll_loss 9.631 | ppl 792.88 | wps 44183 | wpb 2306.9 | bsz 92.7 | num_updates 696 | best_loss 10.328
2020-10-12 01:09:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:09:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 4 @ 696 updates, score 10.328) (writing took 1.5112098200042965 seconds)
2020-10-12 01:09:45 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 01:09:45 | INFO | train | epoch 004 | loss 10.747 | nll_loss 10.142 | ppl 1130.22 | wps 17481.3 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 696 | lr 3.48826e-05 | gnorm 1.201 | clip 0 | train_wall 57 | wall 246
2020-10-12 01:09:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 01:09:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 01:09:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:09:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000472
2020-10-12 01:09:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005810
2020-10-12 01:09:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:09:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 01:09:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:09:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103569
2020-10-12 01:09:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109891
2020-10-12 01:09:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:09:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:09:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004275
2020-10-12 01:09:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:09:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 01:09:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:09:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096121
2020-10-12 01:09:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100899
2020-10-12 01:09:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:09:45 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 01:09:46 | INFO | train_inner | epoch 005:      4 / 174 loss=10.693, nll_loss=10.074, ppl=1077.63, wps=16504.4, ups=2.67, wpb=6192.3, bsz=254.8, num_updates=700, lr=3.50825e-05, gnorm=1.241, clip=0, train_wall=33, wall=247
2020-10-12 01:10:19 | INFO | train_inner | epoch 005:    104 / 174 loss=10.543, nll_loss=9.891, ppl=949.49, wps=18717.2, ups=3.02, wpb=6201.6, bsz=256.6, num_updates=800, lr=4.008e-05, gnorm=1.236, clip=0, train_wall=33, wall=280
2020-10-12 01:10:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:10:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:10:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000701
2020-10-12 01:10:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:10:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037847
2020-10-12 01:10:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:10:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027765
2020-10-12 01:10:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066651
2020-10-12 01:10:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:10:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:10:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000698
2020-10-12 01:10:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:10:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.038456
2020-10-12 01:10:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:10:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027560
2020-10-12 01:10:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.067038
2020-10-12 01:10:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:10:45 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.104 | nll_loss 9.356 | ppl 655.07 | wps 44176.2 | wpb 2306.9 | bsz 92.7 | num_updates 870 | best_loss 10.104
2020-10-12 01:10:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:10:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 5 @ 870 updates, score 10.104) (writing took 1.5006945279892534 seconds)
2020-10-12 01:10:47 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 01:10:47 | INFO | train | epoch 005 | loss 10.479 | nll_loss 9.817 | ppl 902.21 | wps 17458.7 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 870 | lr 4.35783e-05 | gnorm 1.273 | clip 0 | train_wall 57 | wall 308
2020-10-12 01:10:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 01:10:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 01:10:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:10:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000546
2020-10-12 01:10:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005952
2020-10-12 01:10:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:10:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 01:10:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:10:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101109
2020-10-12 01:10:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107577
2020-10-12 01:10:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:10:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:10:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004249
2020-10-12 01:10:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:10:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-12 01:10:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:10:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096912
2020-10-12 01:10:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101656
2020-10-12 01:10:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:10:47 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 01:10:57 | INFO | train_inner | epoch 006:     30 / 174 loss=10.371, nll_loss=9.69, ppl=825.86, wps=16726.2, ups=2.66, wpb=6299, bsz=252.7, num_updates=900, lr=4.50775e-05, gnorm=1.197, clip=0, train_wall=33, wall=318
2020-10-12 01:11:30 | INFO | train_inner | epoch 006:    130 / 174 loss=10.285, nll_loss=9.588, ppl=769.78, wps=19018, ups=3.02, wpb=6305.4, bsz=253.8, num_updates=1000, lr=5.0075e-05, gnorm=1.206, clip=0, train_wall=33, wall=351
2020-10-12 01:11:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000841
2020-10-12 01:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037063
2020-10-12 01:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027149
2020-10-12 01:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065402
2020-10-12 01:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000659
2020-10-12 01:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037685
2020-10-12 01:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.026971
2020-10-12 01:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065636
2020-10-12 01:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:11:47 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.934 | nll_loss 9.161 | ppl 572.35 | wps 44161.1 | wpb 2306.9 | bsz 92.7 | num_updates 1044 | best_loss 9.934
2020-10-12 01:11:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:11:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 6 @ 1044 updates, score 9.934) (writing took 1.4937281169986818 seconds)
2020-10-12 01:11:49 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 01:11:49 | INFO | train | epoch 006 | loss 10.266 | nll_loss 9.566 | ppl 758.08 | wps 17463.8 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 1044 | lr 5.22739e-05 | gnorm 1.192 | clip 0 | train_wall 57 | wall 370
2020-10-12 01:11:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 01:11:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 01:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:11:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000545
2020-10-12 01:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005925
2020-10-12 01:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 01:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102269
2020-10-12 01:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108798
2020-10-12 01:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004324
2020-10-12 01:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 01:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096105
2020-10-12 01:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100933
2020-10-12 01:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:11:49 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 01:12:08 | INFO | train_inner | epoch 007:     56 / 174 loss=10.152, nll_loss=9.435, ppl=691.99, wps=16122.6, ups=2.67, wpb=6033.3, bsz=268.7, num_updates=1100, lr=5.50725e-05, gnorm=1.226, clip=0, train_wall=33, wall=389
2020-10-12 01:12:41 | INFO | train_inner | epoch 007:    156 / 174 loss=10.108, nll_loss=9.381, ppl=666.87, wps=18882.4, ups=3.02, wpb=6246.6, bsz=250, num_updates=1200, lr=6.007e-05, gnorm=1.264, clip=0, train_wall=33, wall=422
2020-10-12 01:12:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000838
2020-10-12 01:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037432
2020-10-12 01:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027838
2020-10-12 01:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066457
2020-10-12 01:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000630
2020-10-12 01:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037203
2020-10-12 01:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027436
2020-10-12 01:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065591
2020-10-12 01:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:12:50 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.81 | nll_loss 9.021 | ppl 519.56 | wps 43894.1 | wpb 2306.9 | bsz 92.7 | num_updates 1218 | best_loss 9.81
2020-10-12 01:12:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:12:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 7 @ 1218 updates, score 9.81) (writing took 1.4948133900034009 seconds)
2020-10-12 01:12:51 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 01:12:51 | INFO | train | epoch 007 | loss 10.125 | nll_loss 9.401 | ppl 676.28 | wps 17462.9 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 1218 | lr 6.09696e-05 | gnorm 1.209 | clip 0 | train_wall 57 | wall 432
2020-10-12 01:12:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 01:12:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 01:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:12:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000477
2020-10-12 01:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005775
2020-10-12 01:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 01:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099658
2020-10-12 01:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105998
2020-10-12 01:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004349
2020-10-12 01:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 01:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096101
2020-10-12 01:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100972
2020-10-12 01:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:12:51 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 01:13:19 | INFO | train_inner | epoch 008:     82 / 174 loss=10.071, nll_loss=9.336, ppl=646.29, wps=17007.7, ups=2.64, wpb=6437.5, bsz=260.2, num_updates=1300, lr=6.50675e-05, gnorm=1.17, clip=0, train_wall=33, wall=460
2020-10-12 01:13:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:13:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:13:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000826
2020-10-12 01:13:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:13:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.038219
2020-10-12 01:13:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:13:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027871
2020-10-12 01:13:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.067266
2020-10-12 01:13:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:13:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:13:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000640
2020-10-12 01:13:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:13:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037444
2020-10-12 01:13:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:13:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027740
2020-10-12 01:13:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066189
2020-10-12 01:13:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:13:52 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.702 | nll_loss 8.889 | ppl 474.11 | wps 43897.9 | wpb 2306.9 | bsz 92.7 | num_updates 1392 | best_loss 9.702
2020-10-12 01:13:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:13:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 8 @ 1392 updates, score 9.702) (writing took 1.5088189289963339 seconds)
2020-10-12 01:13:53 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 01:13:53 | INFO | train | epoch 008 | loss 9.999 | nll_loss 9.253 | ppl 610.09 | wps 17448.6 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 1392 | lr 6.96652e-05 | gnorm 1.161 | clip 0 | train_wall 57 | wall 494
2020-10-12 01:13:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 01:13:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 01:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:13:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000460
2020-10-12 01:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005810
2020-10-12 01:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 01:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098147
2020-10-12 01:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104555
2020-10-12 01:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004268
2020-10-12 01:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 01:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094506
2020-10-12 01:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099286
2020-10-12 01:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:13:53 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 01:13:56 | INFO | train_inner | epoch 009:      8 / 174 loss=9.951, nll_loss=9.197, ppl=586.91, wps=16264.4, ups=2.67, wpb=6082.4, bsz=249, num_updates=1400, lr=7.0065e-05, gnorm=1.121, clip=0, train_wall=33, wall=497
2020-10-12 01:14:29 | INFO | train_inner | epoch 009:    108 / 174 loss=9.873, nll_loss=9.107, ppl=551.46, wps=18706, ups=3.01, wpb=6214.5, bsz=262.4, num_updates=1500, lr=7.50625e-05, gnorm=1.139, clip=0, train_wall=33, wall=530
2020-10-12 01:14:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000755
2020-10-12 01:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.038067
2020-10-12 01:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027221
2020-10-12 01:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066378
2020-10-12 01:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000661
2020-10-12 01:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037995
2020-10-12 01:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.026789
2020-10-12 01:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065760
2020-10-12 01:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:14:54 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 9.52 | nll_loss 8.672 | ppl 407.97 | wps 44156.9 | wpb 2306.9 | bsz 92.7 | num_updates 1566 | best_loss 9.52
2020-10-12 01:14:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:14:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 9 @ 1566 updates, score 9.52) (writing took 1.4939177259948337 seconds)
2020-10-12 01:14:55 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 01:14:55 | INFO | train | epoch 009 | loss 9.866 | nll_loss 9.097 | ppl 547.75 | wps 17467.1 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 1566 | lr 7.83609e-05 | gnorm 1.182 | clip 0 | train_wall 57 | wall 556
2020-10-12 01:14:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 01:14:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 01:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:14:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000591
2020-10-12 01:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005937
2020-10-12 01:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000242
2020-10-12 01:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099407
2020-10-12 01:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105923
2020-10-12 01:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004231
2020-10-12 01:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000164
2020-10-12 01:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:14:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094056
2020-10-12 01:14:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098780
2020-10-12 01:14:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:14:56 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 01:15:07 | INFO | train_inner | epoch 010:     34 / 174 loss=9.8, nll_loss=9.021, ppl=519.43, wps=16732.2, ups=2.67, wpb=6264.3, bsz=246.2, num_updates=1600, lr=8.006e-05, gnorm=1.281, clip=0, train_wall=33, wall=568
2020-10-12 01:15:40 | INFO | train_inner | epoch 010:    134 / 174 loss=9.685, nll_loss=8.89, ppl=474.36, wps=18517.7, ups=3.02, wpb=6128.3, bsz=268.8, num_updates=1700, lr=8.50575e-05, gnorm=1.216, clip=0, train_wall=33, wall=601
2020-10-12 01:15:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000758
2020-10-12 01:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037819
2020-10-12 01:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027358
2020-10-12 01:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066272
2020-10-12 01:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000670
2020-10-12 01:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037100
2020-10-12 01:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027206
2020-10-12 01:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065292
2020-10-12 01:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:15:56 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 9.334 | nll_loss 8.469 | ppl 354.38 | wps 44027.3 | wpb 2306.9 | bsz 92.7 | num_updates 1740 | best_loss 9.334
2020-10-12 01:15:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:15:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 10 @ 1740 updates, score 9.334) (writing took 1.497916266991524 seconds)
2020-10-12 01:15:57 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 01:15:57 | INFO | train | epoch 010 | loss 9.71 | nll_loss 8.919 | ppl 483.92 | wps 17481.7 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 1740 | lr 8.70565e-05 | gnorm 1.227 | clip 0 | train_wall 57 | wall 618
2020-10-12 01:15:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 01:15:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 01:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:15:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000559
2020-10-12 01:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005796
2020-10-12 01:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 01:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100817
2020-10-12 01:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107111
2020-10-12 01:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004316
2020-10-12 01:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 01:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:15:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096682
2020-10-12 01:15:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101506
2020-10-12 01:15:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:15:58 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 01:16:18 | INFO | train_inner | epoch 011:     60 / 174 loss=9.659, nll_loss=8.86, ppl=464.68, wps=16870.7, ups=2.65, wpb=6367.2, bsz=260.9, num_updates=1800, lr=9.0055e-05, gnorm=1.245, clip=0, train_wall=33, wall=638
2020-10-12 01:16:51 | INFO | train_inner | epoch 011:    160 / 174 loss=9.527, nll_loss=8.708, ppl=418.21, wps=18755.2, ups=3.02, wpb=6202.6, bsz=244.2, num_updates=1900, lr=9.50525e-05, gnorm=1.134, clip=0, train_wall=33, wall=672
2020-10-12 01:16:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000767
2020-10-12 01:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037805
2020-10-12 01:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027422
2020-10-12 01:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066336
2020-10-12 01:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000664
2020-10-12 01:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036628
2020-10-12 01:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027438
2020-10-12 01:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065049
2020-10-12 01:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:16:58 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 9.177 | nll_loss 8.278 | ppl 310.33 | wps 44307.8 | wpb 2306.9 | bsz 92.7 | num_updates 1914 | best_loss 9.177
2020-10-12 01:16:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:16:59 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 11 @ 1914 updates, score 9.177) (writing took 1.5107438910054043 seconds)
2020-10-12 01:16:59 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 01:16:59 | INFO | train | epoch 011 | loss 9.534 | nll_loss 8.717 | ppl 420.68 | wps 17469 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 1914 | lr 9.57522e-05 | gnorm 1.182 | clip 0 | train_wall 57 | wall 680
2020-10-12 01:16:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 01:16:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 01:16:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:16:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000560
2020-10-12 01:16:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005777
2020-10-12 01:16:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:16:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 01:16:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099108
2020-10-12 01:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105407
2020-10-12 01:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004333
2020-10-12 01:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 01:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097251
2020-10-12 01:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102086
2020-10-12 01:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:17:00 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 01:17:28 | INFO | train_inner | epoch 012:     86 / 174 loss=9.409, nll_loss=8.573, ppl=380.72, wps=16605.6, ups=2.66, wpb=6232.7, bsz=255.8, num_updates=2000, lr=0.00010005, gnorm=1.237, clip=0, train_wall=33, wall=709
2020-10-12 01:17:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000826
2020-10-12 01:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.038134
2020-10-12 01:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027291
2020-10-12 01:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066596
2020-10-12 01:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000611
2020-10-12 01:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036353
2020-10-12 01:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.026973
2020-10-12 01:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064251
2020-10-12 01:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:18:00 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 9.094 | nll_loss 8.181 | ppl 290.22 | wps 44292.5 | wpb 2306.9 | bsz 92.7 | num_updates 2088 | best_loss 9.094
2020-10-12 01:18:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:18:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 12 @ 2088 updates, score 9.094) (writing took 1.4974897509964649 seconds)
2020-10-12 01:18:01 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 01:18:01 | INFO | train | epoch 012 | loss 9.382 | nll_loss 8.541 | ppl 372.42 | wps 17478.9 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 2088 | lr 0.000104448 | gnorm 1.209 | clip 0 | train_wall 57 | wall 742
2020-10-12 01:18:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 01:18:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 01:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:18:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000543
2020-10-12 01:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005864
2020-10-12 01:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000302
2020-10-12 01:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100243
2020-10-12 01:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106749
2020-10-12 01:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004222
2020-10-12 01:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 01:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094815
2020-10-12 01:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099522
2020-10-12 01:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:18:02 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 01:18:06 | INFO | train_inner | epoch 013:     12 / 174 loss=9.318, nll_loss=8.468, ppl=354.02, wps=16318.6, ups=2.68, wpb=6094.4, bsz=248.5, num_updates=2100, lr=0.000105048, gnorm=1.176, clip=0, train_wall=33, wall=746
2020-10-12 01:18:39 | INFO | train_inner | epoch 013:    112 / 174 loss=9.27, nll_loss=8.41, ppl=340.22, wps=19108.8, ups=3.01, wpb=6341.1, bsz=248.9, num_updates=2200, lr=0.000110045, gnorm=1.19, clip=0, train_wall=33, wall=780
2020-10-12 01:18:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000836
2020-10-12 01:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037928
2020-10-12 01:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027222
2020-10-12 01:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066328
2020-10-12 01:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000662
2020-10-12 01:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037567
2020-10-12 01:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027109
2020-10-12 01:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065655
2020-10-12 01:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:19:02 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.95 | nll_loss 8.017 | ppl 259.05 | wps 43998.1 | wpb 2306.9 | bsz 92.7 | num_updates 2262 | best_loss 8.95
2020-10-12 01:19:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:19:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 13 @ 2262 updates, score 8.95) (writing took 1.5020282649929868 seconds)
2020-10-12 01:19:03 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 01:19:03 | INFO | train | epoch 013 | loss 9.238 | nll_loss 8.374 | ppl 331.69 | wps 17490.7 | ups 2.81 | wpb 6233 | bsz 255.7 | num_updates 2262 | lr 0.000113143 | gnorm 1.204 | clip 0 | train_wall 57 | wall 804
2020-10-12 01:19:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 01:19:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 01:19:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:19:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000482
2020-10-12 01:19:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005645
2020-10-12 01:19:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:19:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 01:19:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:19:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093574
2020-10-12 01:19:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099753
2020-10-12 01:19:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:19:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:19:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004270
2020-10-12 01:19:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:19:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000199
2020-10-12 01:19:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:19:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092417
2020-10-12 01:19:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097212
2020-10-12 01:19:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:19:04 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 01:19:16 | INFO | train_inner | epoch 014:     38 / 174 loss=9.159, nll_loss=8.283, ppl=311.45, wps=16420.5, ups=2.68, wpb=6126.5, bsz=265, num_updates=2300, lr=0.000115043, gnorm=1.201, clip=0, train_wall=32, wall=817
2020-10-12 01:19:50 | INFO | train_inner | epoch 014:    138 / 174 loss=9.125, nll_loss=8.242, ppl=302.68, wps=18958.5, ups=2.97, wpb=6391.6, bsz=267.9, num_updates=2400, lr=0.00012004, gnorm=1.209, clip=0, train_wall=33, wall=851
2020-10-12 01:20:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:20:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:20:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000831
2020-10-12 01:20:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.038287
2020-10-12 01:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027116
2020-10-12 01:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066582
2020-10-12 01:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000674
2020-10-12 01:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037586
2020-10-12 01:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027512
2020-10-12 01:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066099
2020-10-12 01:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:20:04 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.787 | nll_loss 7.82 | ppl 225.95 | wps 43944 | wpb 2306.9 | bsz 92.7 | num_updates 2436 | best_loss 8.787
2020-10-12 01:20:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:20:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 14 @ 2436 updates, score 8.787) (writing took 1.5110579629981657 seconds)
2020-10-12 01:20:06 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 01:20:06 | INFO | train | epoch 014 | loss 9.106 | nll_loss 8.22 | ppl 298.17 | wps 17435.8 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 2436 | lr 0.000121839 | gnorm 1.181 | clip 0 | train_wall 57 | wall 867
2020-10-12 01:20:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 01:20:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 01:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:20:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000524
2020-10-12 01:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005970
2020-10-12 01:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000246
2020-10-12 01:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098870
2020-10-12 01:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105432
2020-10-12 01:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004294
2020-10-12 01:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-12 01:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094971
2020-10-12 01:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099761
2020-10-12 01:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:20:06 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 01:20:27 | INFO | train_inner | epoch 015:     64 / 174 loss=9.024, nll_loss=8.125, ppl=279.13, wps=16623.3, ups=2.66, wpb=6250.9, bsz=242.4, num_updates=2500, lr=0.000125037, gnorm=1.087, clip=0, train_wall=33, wall=888
2020-10-12 01:21:00 | INFO | train_inner | epoch 015:    164 / 174 loss=8.946, nll_loss=8.034, ppl=262.07, wps=18710.7, ups=3.03, wpb=6165.6, bsz=259, num_updates=2600, lr=0.000130035, gnorm=1.291, clip=0, train_wall=33, wall=921
2020-10-12 01:21:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000823
2020-10-12 01:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037580
2020-10-12 01:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027308
2020-10-12 01:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066056
2020-10-12 01:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000665
2020-10-12 01:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036929
2020-10-12 01:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027405
2020-10-12 01:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065322
2020-10-12 01:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:21:06 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.742 | nll_loss 7.772 | ppl 218.59 | wps 44132.8 | wpb 2306.9 | bsz 92.7 | num_updates 2610 | best_loss 8.742
2020-10-12 01:21:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:21:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 15 @ 2610 updates, score 8.742) (writing took 1.496688554994762 seconds)
2020-10-12 01:21:08 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 01:21:08 | INFO | train | epoch 015 | loss 8.967 | nll_loss 8.059 | ppl 266.74 | wps 17474.2 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 2610 | lr 0.000130535 | gnorm 1.22 | clip 0 | train_wall 57 | wall 929
2020-10-12 01:21:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 01:21:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 01:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:21:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000565
2020-10-12 01:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005959
2020-10-12 01:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-12 01:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099730
2020-10-12 01:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106265
2020-10-12 01:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004252
2020-10-12 01:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 01:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094637
2020-10-12 01:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099377
2020-10-12 01:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:21:08 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 01:21:38 | INFO | train_inner | epoch 016:     90 / 174 loss=8.838, nll_loss=7.91, ppl=240.5, wps=16556.4, ups=2.67, wpb=6207.8, bsz=256.3, num_updates=2700, lr=0.000135032, gnorm=1.151, clip=0, train_wall=33, wall=959
2020-10-12 01:22:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000818
2020-10-12 01:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037738
2020-10-12 01:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027477
2020-10-12 01:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066380
2020-10-12 01:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000685
2020-10-12 01:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037642
2020-10-12 01:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027114
2020-10-12 01:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065766
2020-10-12 01:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:22:08 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.565 | nll_loss 7.564 | ppl 189.27 | wps 44027.2 | wpb 2306.9 | bsz 92.7 | num_updates 2784 | best_loss 8.565
2020-10-12 01:22:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:22:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 16 @ 2784 updates, score 8.565) (writing took 1.506475779999164 seconds)
2020-10-12 01:22:10 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 01:22:10 | INFO | train | epoch 016 | loss 8.836 | nll_loss 7.907 | ppl 240.02 | wps 17464.9 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 2784 | lr 0.00013923 | gnorm 1.162 | clip 0 | train_wall 57 | wall 991
2020-10-12 01:22:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 01:22:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 01:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:22:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000465
2020-10-12 01:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005846
2020-10-12 01:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000246
2020-10-12 01:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100986
2020-10-12 01:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107480
2020-10-12 01:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004268
2020-10-12 01:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 01:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097572
2020-10-12 01:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102338
2020-10-12 01:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:22:10 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 01:22:15 | INFO | train_inner | epoch 017:     16 / 174 loss=8.812, nll_loss=7.879, ppl=235.37, wps=16665.1, ups=2.65, wpb=6277.1, bsz=262.6, num_updates=2800, lr=0.00014003, gnorm=1.181, clip=0, train_wall=33, wall=996
2020-10-12 01:22:49 | INFO | train_inner | epoch 017:    116 / 174 loss=8.695, nll_loss=7.745, ppl=214.49, wps=18710.9, ups=3.01, wpb=6216.8, bsz=257.4, num_updates=2900, lr=0.000145028, gnorm=1.155, clip=0, train_wall=33, wall=1030
2020-10-12 01:23:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000771
2020-10-12 01:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037943
2020-10-12 01:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027394
2020-10-12 01:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066448
2020-10-12 01:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000704
2020-10-12 01:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037713
2020-10-12 01:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027504
2020-10-12 01:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066243
2020-10-12 01:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:23:10 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 8.483 | nll_loss 7.469 | ppl 177.16 | wps 44256.4 | wpb 2306.9 | bsz 92.7 | num_updates 2958 | best_loss 8.483
2020-10-12 01:23:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:23:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 17 @ 2958 updates, score 8.483) (writing took 1.4902029929944547 seconds)
2020-10-12 01:23:12 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 01:23:12 | INFO | train | epoch 017 | loss 8.688 | nll_loss 7.737 | ppl 213.34 | wps 17451.7 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 2958 | lr 0.000147926 | gnorm 1.142 | clip 0 | train_wall 57 | wall 1053
2020-10-12 01:23:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 01:23:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 01:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:23:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000441
2020-10-12 01:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005797
2020-10-12 01:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-12 01:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100625
2020-10-12 01:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106932
2020-10-12 01:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004236
2020-10-12 01:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 01:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095914
2020-10-12 01:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100641
2020-10-12 01:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:23:12 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 01:23:26 | INFO | train_inner | epoch 018:     42 / 174 loss=8.67, nll_loss=7.714, ppl=209.98, wps=16774.3, ups=2.65, wpb=6323.6, bsz=247.2, num_updates=3000, lr=0.000150025, gnorm=1.229, clip=0, train_wall=33, wall=1067
2020-10-12 01:23:59 | INFO | train_inner | epoch 018:    142 / 174 loss=8.534, nll_loss=7.558, ppl=188.48, wps=18561.5, ups=3.03, wpb=6123.8, bsz=258.3, num_updates=3100, lr=0.000155023, gnorm=1.222, clip=0, train_wall=33, wall=1100
2020-10-12 01:24:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000770
2020-10-12 01:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037851
2020-10-12 01:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027577
2020-10-12 01:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066544
2020-10-12 01:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000681
2020-10-12 01:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037458
2020-10-12 01:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027441
2020-10-12 01:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065910
2020-10-12 01:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:24:13 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 8.361 | nll_loss 7.32 | ppl 159.77 | wps 43865.3 | wpb 2306.9 | bsz 92.7 | num_updates 3132 | best_loss 8.361
2020-10-12 01:24:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:24:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 18 @ 3132 updates, score 8.361) (writing took 1.4919516450027004 seconds)
2020-10-12 01:24:14 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 01:24:14 | INFO | train | epoch 018 | loss 8.554 | nll_loss 7.581 | ppl 191.49 | wps 17450.8 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 3132 | lr 0.000156622 | gnorm 1.252 | clip 0 | train_wall 57 | wall 1115
2020-10-12 01:24:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 01:24:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 01:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:24:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000510
2020-10-12 01:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006133
2020-10-12 01:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000258
2020-10-12 01:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100246
2020-10-12 01:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107005
2020-10-12 01:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004362
2020-10-12 01:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 01:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098169
2020-10-12 01:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103052
2020-10-12 01:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:24:14 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 01:24:37 | INFO | train_inner | epoch 019:     68 / 174 loss=8.421, nll_loss=7.429, ppl=172.34, wps=16548.8, ups=2.66, wpb=6229.3, bsz=260.2, num_updates=3200, lr=0.00016002, gnorm=1.163, clip=0, train_wall=33, wall=1138
2020-10-12 01:25:10 | INFO | train_inner | epoch 019:    168 / 174 loss=8.386, nll_loss=7.386, ppl=167.3, wps=18787.7, ups=3.02, wpb=6223.7, bsz=249.7, num_updates=3300, lr=0.000165018, gnorm=1.246, clip=0, train_wall=33, wall=1171
2020-10-12 01:25:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000697
2020-10-12 01:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037147
2020-10-12 01:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027066
2020-10-12 01:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065250
2020-10-12 01:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000611
2020-10-12 01:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037367
2020-10-12 01:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.026987
2020-10-12 01:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065280
2020-10-12 01:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:25:15 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 8.171 | nll_loss 7.095 | ppl 136.7 | wps 43788.4 | wpb 2306.9 | bsz 92.7 | num_updates 3306 | best_loss 8.171
2020-10-12 01:25:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:25:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 19 @ 3306 updates, score 8.171) (writing took 1.420699637994403 seconds)
2020-10-12 01:25:16 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 01:25:16 | INFO | train | epoch 019 | loss 8.385 | nll_loss 7.386 | ppl 167.31 | wps 17461.5 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 3306 | lr 0.000165317 | gnorm 1.195 | clip 0 | train_wall 57 | wall 1177
2020-10-12 01:25:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 01:25:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 01:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:25:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000559
2020-10-12 01:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005957
2020-10-12 01:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000196
2020-10-12 01:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098311
2020-10-12 01:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104845
2020-10-12 01:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004219
2020-10-12 01:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-12 01:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096474
2020-10-12 01:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101194
2020-10-12 01:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:25:16 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 01:25:48 | INFO | train_inner | epoch 020:     94 / 174 loss=8.29, nll_loss=7.277, ppl=155.09, wps=16815.5, ups=2.67, wpb=6302.7, bsz=233.6, num_updates=3400, lr=0.000170015, gnorm=1.228, clip=0, train_wall=33, wall=1209
2020-10-12 01:26:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000834
2020-10-12 01:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.038054
2020-10-12 01:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027058
2020-10-12 01:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066292
2020-10-12 01:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000676
2020-10-12 01:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036507
2020-10-12 01:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027467
2020-10-12 01:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064974
2020-10-12 01:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:26:17 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 8.058 | nll_loss 6.959 | ppl 124.38 | wps 44018.1 | wpb 2306.9 | bsz 92.7 | num_updates 3480 | best_loss 8.058
2020-10-12 01:26:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:26:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 20 @ 3480 updates, score 8.058) (writing took 1.527150084002642 seconds)
2020-10-12 01:26:18 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 01:26:18 | INFO | train | epoch 020 | loss 8.235 | nll_loss 7.213 | ppl 148.35 | wps 17458.5 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 3480 | lr 0.000174013 | gnorm 1.275 | clip 0 | train_wall 57 | wall 1239
2020-10-12 01:26:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 01:26:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 01:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:26:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000475
2020-10-12 01:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005368
2020-10-12 01:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 01:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098788
2020-10-12 01:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104683
2020-10-12 01:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004260
2020-10-12 01:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 01:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:26:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094749
2020-10-12 01:26:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099521
2020-10-12 01:26:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:26:19 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 01:26:25 | INFO | train_inner | epoch 021:     20 / 174 loss=8.158, nll_loss=7.125, ppl=139.6, wps=16503.6, ups=2.67, wpb=6187.3, bsz=279.5, num_updates=3500, lr=0.000175013, gnorm=1.302, clip=0, train_wall=33, wall=1246
2020-10-12 01:26:58 | INFO | train_inner | epoch 021:    120 / 174 loss=8.098, nll_loss=7.055, ppl=132.95, wps=18917.2, ups=3.02, wpb=6257.6, bsz=240.9, num_updates=3600, lr=0.00018001, gnorm=1.256, clip=0, train_wall=33, wall=1279
2020-10-12 01:27:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000959
2020-10-12 01:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037721
2020-10-12 01:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027510
2020-10-12 01:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066542
2020-10-12 01:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000663
2020-10-12 01:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036429
2020-10-12 01:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027573
2020-10-12 01:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064989
2020-10-12 01:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:27:19 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.907 | nll_loss 6.768 | ppl 108.99 | wps 44058.7 | wpb 2306.9 | bsz 92.7 | num_updates 3654 | best_loss 7.907
2020-10-12 01:27:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:27:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 21 @ 3654 updates, score 7.907) (writing took 1.5045227160007926 seconds)
2020-10-12 01:27:20 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 01:27:20 | INFO | train | epoch 021 | loss 8.067 | nll_loss 7.019 | ppl 129.74 | wps 17493.2 | ups 2.81 | wpb 6233 | bsz 255.7 | num_updates 3654 | lr 0.000182709 | gnorm 1.273 | clip 0 | train_wall 57 | wall 1301
2020-10-12 01:27:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 01:27:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 01:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:27:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000532
2020-10-12 01:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005823
2020-10-12 01:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 01:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100743
2020-10-12 01:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107154
2020-10-12 01:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004272
2020-10-12 01:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 01:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096559
2020-10-12 01:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101343
2020-10-12 01:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:27:21 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 01:27:36 | INFO | train_inner | epoch 022:     46 / 174 loss=7.968, nll_loss=6.904, ppl=119.8, wps=16344.3, ups=2.66, wpb=6143.2, bsz=265, num_updates=3700, lr=0.000185008, gnorm=1.263, clip=0, train_wall=33, wall=1317
2020-10-12 01:28:09 | INFO | train_inner | epoch 022:    146 / 174 loss=7.87, nll_loss=6.792, ppl=110.8, wps=18866.1, ups=3.02, wpb=6254, bsz=260.6, num_updates=3800, lr=0.000190005, gnorm=1.252, clip=0, train_wall=33, wall=1350
2020-10-12 01:28:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:28:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:28:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000863
2020-10-12 01:28:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:28:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037152
2020-10-12 01:28:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:28:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027332
2020-10-12 01:28:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065698
2020-10-12 01:28:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:28:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:28:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000663
2020-10-12 01:28:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:28:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037608
2020-10-12 01:28:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:28:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027458
2020-10-12 01:28:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066047
2020-10-12 01:28:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:28:21 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.761 | nll_loss 6.601 | ppl 97.05 | wps 44174.5 | wpb 2306.9 | bsz 92.7 | num_updates 3828 | best_loss 7.761
2020-10-12 01:28:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:28:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 22 @ 3828 updates, score 7.761) (writing took 1.508582310998463 seconds)
2020-10-12 01:28:22 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 01:28:22 | INFO | train | epoch 022 | loss 7.881 | nll_loss 6.805 | ppl 111.78 | wps 17473.8 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 3828 | lr 0.000191404 | gnorm 1.236 | clip 0 | train_wall 57 | wall 1363
2020-10-12 01:28:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 01:28:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 01:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:28:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000470
2020-10-12 01:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006051
2020-10-12 01:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 01:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100638
2020-10-12 01:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107281
2020-10-12 01:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004341
2020-10-12 01:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-12 01:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095658
2020-10-12 01:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100495
2020-10-12 01:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:28:23 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 01:28:47 | INFO | train_inner | epoch 023:     72 / 174 loss=7.755, nll_loss=6.659, ppl=101.05, wps=16932.9, ups=2.65, wpb=6397.6, bsz=260, num_updates=3900, lr=0.000195003, gnorm=1.256, clip=0, train_wall=33, wall=1388
2020-10-12 01:29:20 | INFO | train_inner | epoch 023:    172 / 174 loss=7.703, nll_loss=6.595, ppl=96.7, wps=18538.8, ups=3.03, wpb=6121.6, bsz=250.1, num_updates=4000, lr=0.0002, gnorm=1.251, clip=0, train_wall=33, wall=1421
2020-10-12 01:29:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:29:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:29:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000775
2020-10-12 01:29:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:29:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037463
2020-10-12 01:29:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:29:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.026801
2020-10-12 01:29:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065373
2020-10-12 01:29:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:29:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:29:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000667
2020-10-12 01:29:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:29:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037148
2020-10-12 01:29:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:29:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027125
2020-10-12 01:29:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065259
2020-10-12 01:29:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:29:23 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.665 | nll_loss 6.484 | ppl 89.51 | wps 43674.9 | wpb 2306.9 | bsz 92.7 | num_updates 4002 | best_loss 7.665
2020-10-12 01:29:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:29:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 23 @ 4002 updates, score 7.665) (writing took 1.49428158000228 seconds)
2020-10-12 01:29:25 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 01:29:25 | INFO | train | epoch 023 | loss 7.706 | nll_loss 6.601 | ppl 97.08 | wps 17455.2 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 4002 | lr 0.00019995 | gnorm 1.259 | clip 0 | train_wall 57 | wall 1425
2020-10-12 01:29:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 01:29:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 01:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:29:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000509
2020-10-12 01:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005851
2020-10-12 01:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000240
2020-10-12 01:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098606
2020-10-12 01:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105038
2020-10-12 01:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004240
2020-10-12 01:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 01:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094790
2020-10-12 01:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099522
2020-10-12 01:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:29:25 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 01:29:57 | INFO | train_inner | epoch 024:     98 / 174 loss=7.57, nll_loss=6.444, ppl=87.07, wps=16507.4, ups=2.69, wpb=6143, bsz=236.7, num_updates=4100, lr=0.000197546, gnorm=1.272, clip=0, train_wall=32, wall=1458
2020-10-12 01:30:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:30:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:30:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000765
2020-10-12 01:30:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:30:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.038358
2020-10-12 01:30:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027622
2020-10-12 01:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.067079
2020-10-12 01:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000671
2020-10-12 01:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037524
2020-10-12 01:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027169
2020-10-12 01:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065683
2020-10-12 01:30:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:30:25 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.514 | nll_loss 6.3 | ppl 78.77 | wps 44126.2 | wpb 2306.9 | bsz 92.7 | num_updates 4176 | best_loss 7.514
2020-10-12 01:30:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:30:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 24 @ 4176 updates, score 7.514) (writing took 1.4890003220061772 seconds)
2020-10-12 01:30:27 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 01:30:27 | INFO | train | epoch 024 | loss 7.521 | nll_loss 6.386 | ppl 83.65 | wps 17479.8 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 4176 | lr 0.00019574 | gnorm 1.26 | clip 0 | train_wall 57 | wall 1488
2020-10-12 01:30:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 01:30:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 01:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:30:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000559
2020-10-12 01:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005709
2020-10-12 01:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 01:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097656
2020-10-12 01:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103905
2020-10-12 01:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004378
2020-10-12 01:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 01:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097629
2020-10-12 01:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102510
2020-10-12 01:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:30:27 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 01:30:35 | INFO | train_inner | epoch 025:     24 / 174 loss=7.421, nll_loss=6.272, ppl=77.26, wps=16522, ups=2.65, wpb=6235.8, bsz=278.8, num_updates=4200, lr=0.00019518, gnorm=1.262, clip=0, train_wall=33, wall=1496
2020-10-12 01:31:08 | INFO | train_inner | epoch 025:    124 / 174 loss=7.379, nll_loss=6.22, ppl=74.57, wps=18875.9, ups=2.99, wpb=6306.2, bsz=256.5, num_updates=4300, lr=0.000192897, gnorm=1.288, clip=0, train_wall=33, wall=1529
2020-10-12 01:31:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:31:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:31:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000774
2020-10-12 01:31:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:31:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037940
2020-10-12 01:31:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:31:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.028080
2020-10-12 01:31:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.067143
2020-10-12 01:31:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:31:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:31:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000674
2020-10-12 01:31:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:31:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037809
2020-10-12 01:31:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:31:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027532
2020-10-12 01:31:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066341
2020-10-12 01:31:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:31:27 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.438 | nll_loss 6.187 | ppl 72.84 | wps 44224.3 | wpb 2306.9 | bsz 92.7 | num_updates 4350 | best_loss 7.438
2020-10-12 01:31:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:31:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 25 @ 4350 updates, score 7.438) (writing took 1.503499496000586 seconds)
2020-10-12 01:31:29 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 01:31:29 | INFO | train | epoch 025 | loss 7.355 | nll_loss 6.193 | ppl 73.18 | wps 17439.6 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 4350 | lr 0.000191785 | gnorm 1.275 | clip 0 | train_wall 57 | wall 1550
2020-10-12 01:31:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 01:31:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 01:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:31:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000539
2020-10-12 01:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006084
2020-10-12 01:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000258
2020-10-12 01:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102174
2020-10-12 01:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108865
2020-10-12 01:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004261
2020-10-12 01:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 01:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096170
2020-10-12 01:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100951
2020-10-12 01:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:31:29 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 01:31:46 | INFO | train_inner | epoch 026:     50 / 174 loss=7.263, nll_loss=6.086, ppl=67.92, wps=16450.1, ups=2.67, wpb=6162, bsz=248.1, num_updates=4400, lr=0.000190693, gnorm=1.275, clip=0, train_wall=33, wall=1566
2020-10-12 01:32:19 | INFO | train_inner | epoch 026:    150 / 174 loss=7.219, nll_loss=6.032, ppl=65.44, wps=19161.9, ups=3, wpb=6386.5, bsz=260.1, num_updates=4500, lr=0.000188562, gnorm=1.278, clip=0, train_wall=33, wall=1600
2020-10-12 01:32:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000700
2020-10-12 01:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037860
2020-10-12 01:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027285
2020-10-12 01:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066176
2020-10-12 01:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000609
2020-10-12 01:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037876
2020-10-12 01:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027513
2020-10-12 01:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066314
2020-10-12 01:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:32:29 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.333 | nll_loss 6.077 | ppl 67.49 | wps 44086.7 | wpb 2306.9 | bsz 92.7 | num_updates 4524 | best_loss 7.333
2020-10-12 01:32:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:32:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 26 @ 4524 updates, score 7.333) (writing took 1.505987943004584 seconds)
2020-10-12 01:32:31 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 01:32:31 | INFO | train | epoch 026 | loss 7.204 | nll_loss 6.016 | ppl 64.72 | wps 17459.9 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 4524 | lr 0.000188061 | gnorm 1.292 | clip 0 | train_wall 57 | wall 1612
2020-10-12 01:32:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 01:32:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 01:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:32:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000528
2020-10-12 01:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005915
2020-10-12 01:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000242
2020-10-12 01:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096544
2020-10-12 01:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103039
2020-10-12 01:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004314
2020-10-12 01:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 01:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094914
2020-10-12 01:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099729
2020-10-12 01:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:32:31 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 01:32:56 | INFO | train_inner | epoch 027:     76 / 174 loss=7.084, nll_loss=5.878, ppl=58.8, wps=16663.1, ups=2.67, wpb=6245.9, bsz=252.6, num_updates=4600, lr=0.000186501, gnorm=1.28, clip=0, train_wall=33, wall=1637
2020-10-12 01:33:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000846
2020-10-12 01:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.038092
2020-10-12 01:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027440
2020-10-12 01:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066729
2020-10-12 01:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000626
2020-10-12 01:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037916
2020-10-12 01:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027608
2020-10-12 01:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066469
2020-10-12 01:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:33:32 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.249 | nll_loss 5.962 | ppl 62.33 | wps 44088.8 | wpb 2306.9 | bsz 92.7 | num_updates 4698 | best_loss 7.249
2020-10-12 01:33:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:33:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 27 @ 4698 updates, score 7.249) (writing took 1.5001579629897606 seconds)
2020-10-12 01:33:33 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 01:33:33 | INFO | train | epoch 027 | loss 7.047 | nll_loss 5.834 | ppl 57.06 | wps 17455.4 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 4698 | lr 0.000184546 | gnorm 1.256 | clip 0 | train_wall 57 | wall 1674
2020-10-12 01:33:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 01:33:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 01:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:33:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000623
2020-10-12 01:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006038
2020-10-12 01:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000248
2020-10-12 01:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101347
2020-10-12 01:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107969
2020-10-12 01:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004361
2020-10-12 01:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 01:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095945
2020-10-12 01:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100816
2020-10-12 01:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:33:33 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 01:33:34 | INFO | train_inner | epoch 028:      2 / 174 loss=7.038, nll_loss=5.822, ppl=56.59, wps=16363.3, ups=2.66, wpb=6158.1, bsz=256, num_updates=4700, lr=0.000184506, gnorm=1.238, clip=0, train_wall=33, wall=1675
2020-10-12 01:34:07 | INFO | train_inner | epoch 028:    102 / 174 loss=6.919, nll_loss=5.685, ppl=51.45, wps=19153.5, ups=2.99, wpb=6399.3, bsz=257.3, num_updates=4800, lr=0.000182574, gnorm=1.253, clip=0, train_wall=33, wall=1708
2020-10-12 01:34:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000835
2020-10-12 01:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.038180
2020-10-12 01:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027334
2020-10-12 01:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066700
2020-10-12 01:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000631
2020-10-12 01:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037251
2020-10-12 01:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027220
2020-10-12 01:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065424
2020-10-12 01:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:34:34 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.126 | nll_loss 5.804 | ppl 55.87 | wps 44005.9 | wpb 2306.9 | bsz 92.7 | num_updates 4872 | best_loss 7.126
2020-10-12 01:34:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:34:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 28 @ 4872 updates, score 7.126) (writing took 1.491641045009601 seconds)
2020-10-12 01:34:35 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 01:34:35 | INFO | train | epoch 028 | loss 6.912 | nll_loss 5.676 | ppl 51.12 | wps 17438.3 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 4872 | lr 0.00018122 | gnorm 1.286 | clip 0 | train_wall 57 | wall 1736
2020-10-12 01:34:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 01:34:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 01:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:34:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000543
2020-10-12 01:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005980
2020-10-12 01:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 01:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101120
2020-10-12 01:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107684
2020-10-12 01:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004260
2020-10-12 01:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 01:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097031
2020-10-12 01:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101799
2020-10-12 01:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:34:35 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 01:34:45 | INFO | train_inner | epoch 029:     28 / 174 loss=6.853, nll_loss=5.607, ppl=48.75, wps=16140.9, ups=2.67, wpb=6047.6, bsz=261.4, num_updates=4900, lr=0.000180702, gnorm=1.307, clip=0, train_wall=33, wall=1746
2020-10-12 01:35:18 | INFO | train_inner | epoch 029:    128 / 174 loss=6.775, nll_loss=5.516, ppl=45.75, wps=18628.8, ups=3.03, wpb=6156.6, bsz=253.3, num_updates=5000, lr=0.000178885, gnorm=1.295, clip=0, train_wall=33, wall=1779
2020-10-12 01:35:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:35:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:35:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000827
2020-10-12 01:35:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:35:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037768
2020-10-12 01:35:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:35:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027024
2020-10-12 01:35:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065960
2020-10-12 01:35:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:35:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:35:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000616
2020-10-12 01:35:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:35:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037869
2020-10-12 01:35:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:35:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.026958
2020-10-12 01:35:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065758
2020-10-12 01:35:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:35:36 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.118 | nll_loss 5.791 | ppl 55.38 | wps 44127.3 | wpb 2306.9 | bsz 92.7 | num_updates 5046 | best_loss 7.118
2020-10-12 01:35:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:35:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 29 @ 5046 updates, score 7.118) (writing took 1.4902314280043356 seconds)
2020-10-12 01:35:37 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 01:35:37 | INFO | train | epoch 029 | loss 6.775 | nll_loss 5.515 | ppl 45.74 | wps 17450.3 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 5046 | lr 0.000178068 | gnorm 1.28 | clip 0 | train_wall 57 | wall 1798
2020-10-12 01:35:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 01:35:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 01:35:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:35:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000543
2020-10-12 01:35:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005788
2020-10-12 01:35:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:35:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 01:35:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:35:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100220
2020-10-12 01:35:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106519
2020-10-12 01:35:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:35:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004257
2020-10-12 01:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 01:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094273
2020-10-12 01:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099027
2020-10-12 01:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:35:38 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 01:35:55 | INFO | train_inner | epoch 030:     54 / 174 loss=6.703, nll_loss=5.432, ppl=43.16, wps=16482, ups=2.67, wpb=6171.5, bsz=245.7, num_updates=5100, lr=0.000177123, gnorm=1.289, clip=0, train_wall=33, wall=1816
2020-10-12 01:36:29 | INFO | train_inner | epoch 030:    154 / 174 loss=6.668, nll_loss=5.389, ppl=41.91, wps=19005.6, ups=3.02, wpb=6300.6, bsz=254.1, num_updates=5200, lr=0.000175412, gnorm=1.249, clip=0, train_wall=33, wall=1849
2020-10-12 01:36:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000784
2020-10-12 01:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037705
2020-10-12 01:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027607
2020-10-12 01:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066431
2020-10-12 01:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000610
2020-10-12 01:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.038250
2020-10-12 01:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027417
2020-10-12 01:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066592
2020-10-12 01:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:36:38 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.028 | nll_loss 5.684 | ppl 51.4 | wps 44133.4 | wpb 2306.9 | bsz 92.7 | num_updates 5220 | best_loss 7.028
2020-10-12 01:36:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:36:39 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 30 @ 5220 updates, score 7.028) (writing took 1.4912142740067793 seconds)
2020-10-12 01:36:39 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 01:36:39 | INFO | train | epoch 030 | loss 6.653 | nll_loss 5.372 | ppl 41.42 | wps 17479.4 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 5220 | lr 0.000175075 | gnorm 1.269 | clip 0 | train_wall 57 | wall 1860
2020-10-12 01:36:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 01:36:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 01:36:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:36:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000458
2020-10-12 01:36:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005669
2020-10-12 01:36:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:36:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 01:36:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:36:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099998
2020-10-12 01:36:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106217
2020-10-12 01:36:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:36:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:36:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004385
2020-10-12 01:36:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:36:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-12 01:36:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:36:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097297
2020-10-12 01:36:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102195
2020-10-12 01:36:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:36:40 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 01:37:06 | INFO | train_inner | epoch 031:     80 / 174 loss=6.584, nll_loss=5.292, ppl=39.18, wps=16523.1, ups=2.68, wpb=6174.6, bsz=236.9, num_updates=5300, lr=0.000173749, gnorm=1.307, clip=0, train_wall=33, wall=1887
2020-10-12 01:37:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:37:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:37:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000789
2020-10-12 01:37:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:37:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.038022
2020-10-12 01:37:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:37:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027243
2020-10-12 01:37:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066406
2020-10-12 01:37:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:37:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:37:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000689
2020-10-12 01:37:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037053
2020-10-12 01:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027253
2020-10-12 01:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065319
2020-10-12 01:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:37:40 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.008 | nll_loss 5.649 | ppl 50.17 | wps 44138.6 | wpb 2306.9 | bsz 92.7 | num_updates 5394 | best_loss 7.008
2020-10-12 01:37:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:37:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 31 @ 5394 updates, score 7.008) (writing took 1.5004089700087206 seconds)
2020-10-12 01:37:42 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 01:37:42 | INFO | train | epoch 031 | loss 6.54 | nll_loss 5.24 | ppl 37.8 | wps 17455.7 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 5394 | lr 0.000172228 | gnorm 1.285 | clip 0 | train_wall 57 | wall 1922
2020-10-12 01:37:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 01:37:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 01:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:37:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000467
2020-10-12 01:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005856
2020-10-12 01:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 01:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099263
2020-10-12 01:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105631
2020-10-12 01:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004224
2020-10-12 01:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 01:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094163
2020-10-12 01:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098894
2020-10-12 01:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:37:42 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 01:37:44 | INFO | train_inner | epoch 032:      6 / 174 loss=6.527, nll_loss=5.224, ppl=37.38, wps=16847.1, ups=2.64, wpb=6388.1, bsz=279.8, num_updates=5400, lr=0.000172133, gnorm=1.26, clip=0, train_wall=33, wall=1925
2020-10-12 01:38:17 | INFO | train_inner | epoch 032:    106 / 174 loss=6.422, nll_loss=5.103, ppl=34.38, wps=18952.1, ups=3, wpb=6310, bsz=256.8, num_updates=5500, lr=0.000170561, gnorm=1.284, clip=0, train_wall=33, wall=1958
2020-10-12 01:38:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:38:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:38:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000774
2020-10-12 01:38:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037085
2020-10-12 01:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.026653
2020-10-12 01:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064842
2020-10-12 01:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000675
2020-10-12 01:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.038225
2020-10-12 01:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.026844
2020-10-12 01:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066059
2020-10-12 01:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:38:42 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.982 | nll_loss 5.616 | ppl 49.05 | wps 44027.4 | wpb 2306.9 | bsz 92.7 | num_updates 5568 | best_loss 6.982
2020-10-12 01:38:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:38:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 32 @ 5568 updates, score 6.982) (writing took 1.4934905700065428 seconds)
2020-10-12 01:38:44 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 01:38:44 | INFO | train | epoch 032 | loss 6.433 | nll_loss 5.115 | ppl 34.64 | wps 17471.2 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 5568 | lr 0.000169516 | gnorm 1.293 | clip 0 | train_wall 57 | wall 1985
2020-10-12 01:38:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 01:38:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 01:38:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:38:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000452
2020-10-12 01:38:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005948
2020-10-12 01:38:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:38:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 01:38:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:38:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100330
2020-10-12 01:38:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106871
2020-10-12 01:38:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:38:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:38:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004359
2020-10-12 01:38:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:38:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-12 01:38:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:38:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094127
2020-10-12 01:38:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098990
2020-10-12 01:38:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:38:44 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 01:38:54 | INFO | train_inner | epoch 033:     32 / 174 loss=6.386, nll_loss=5.059, ppl=33.33, wps=16443.2, ups=2.68, wpb=6139.2, bsz=263.8, num_updates=5600, lr=0.000169031, gnorm=1.331, clip=0, train_wall=32, wall=1995
2020-10-12 01:39:28 | INFO | train_inner | epoch 033:    132 / 174 loss=6.349, nll_loss=5.016, ppl=32.35, wps=18833.1, ups=3, wpb=6269.6, bsz=248.1, num_updates=5700, lr=0.000167542, gnorm=1.27, clip=0, train_wall=33, wall=2029
2020-10-12 01:39:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000822
2020-10-12 01:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037703
2020-10-12 01:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.026532
2020-10-12 01:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065413
2020-10-12 01:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000649
2020-10-12 01:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037085
2020-10-12 01:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.026903
2020-10-12 01:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064953
2020-10-12 01:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:39:44 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.875 | nll_loss 5.487 | ppl 44.84 | wps 44012.1 | wpb 2306.9 | bsz 92.7 | num_updates 5742 | best_loss 6.875
2020-10-12 01:39:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:39:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 33 @ 5742 updates, score 6.875) (writing took 1.500462684998638 seconds)
2020-10-12 01:39:46 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 01:39:46 | INFO | train | epoch 033 | loss 6.338 | nll_loss 5.002 | ppl 32.04 | wps 17469.5 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 5742 | lr 0.000166928 | gnorm 1.317 | clip 0 | train_wall 57 | wall 2047
2020-10-12 01:39:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 01:39:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 01:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:39:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000436
2020-10-12 01:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005753
2020-10-12 01:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 01:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098084
2020-10-12 01:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104410
2020-10-12 01:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004233
2020-10-12 01:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 01:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094381
2020-10-12 01:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099099
2020-10-12 01:39:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:39:46 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 01:40:05 | INFO | train_inner | epoch 034:     58 / 174 loss=6.265, nll_loss=4.918, ppl=30.23, wps=16627.4, ups=2.66, wpb=6243, bsz=262.6, num_updates=5800, lr=0.000166091, gnorm=1.307, clip=0, train_wall=33, wall=2066
2020-10-12 01:40:38 | INFO | train_inner | epoch 034:    158 / 174 loss=6.223, nll_loss=4.868, ppl=29.2, wps=18596.2, ups=3.02, wpb=6162.4, bsz=254.2, num_updates=5900, lr=0.000164677, gnorm=1.291, clip=0, train_wall=33, wall=2099
2020-10-12 01:40:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.006978
2020-10-12 01:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037337
2020-10-12 01:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027251
2020-10-12 01:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071949
2020-10-12 01:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000673
2020-10-12 01:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.038272
2020-10-12 01:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027383
2020-10-12 01:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066643
2020-10-12 01:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:40:46 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.827 | nll_loss 5.416 | ppl 42.71 | wps 44435 | wpb 2306.9 | bsz 92.7 | num_updates 5916 | best_loss 6.827
2020-10-12 01:40:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:40:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 34 @ 5916 updates, score 6.827) (writing took 1.5012997320009163 seconds)
2020-10-12 01:40:48 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 01:40:48 | INFO | train | epoch 034 | loss 6.226 | nll_loss 4.871 | ppl 29.27 | wps 17450.4 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 5916 | lr 0.000164455 | gnorm 1.279 | clip 0 | train_wall 57 | wall 2109
2020-10-12 01:40:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 01:40:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 01:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:40:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000448
2020-10-12 01:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005995
2020-10-12 01:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 01:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100195
2020-10-12 01:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106778
2020-10-12 01:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004237
2020-10-12 01:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000164
2020-10-12 01:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096946
2020-10-12 01:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101677
2020-10-12 01:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:40:48 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 01:41:16 | INFO | train_inner | epoch 035:     84 / 174 loss=6.149, nll_loss=4.782, ppl=27.51, wps=16667.1, ups=2.68, wpb=6230.1, bsz=246.5, num_updates=6000, lr=0.000163299, gnorm=1.305, clip=0, train_wall=33, wall=2137
2020-10-12 01:41:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.006998
2020-10-12 01:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.038377
2020-10-12 01:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027691
2020-10-12 01:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073449
2020-10-12 01:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000671
2020-10-12 01:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.038217
2020-10-12 01:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027778
2020-10-12 01:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066991
2020-10-12 01:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:41:49 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.815 | nll_loss 5.405 | ppl 42.36 | wps 43847.7 | wpb 2306.9 | bsz 92.7 | num_updates 6090 | best_loss 6.815
2020-10-12 01:41:49 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:41:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 35 @ 6090 updates, score 6.815) (writing took 1.5024432849895675 seconds)
2020-10-12 01:41:50 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 01:41:50 | INFO | train | epoch 035 | loss 6.138 | nll_loss 4.768 | ppl 27.25 | wps 17430.8 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 6090 | lr 0.000162088 | gnorm 1.319 | clip 0 | train_wall 57 | wall 2171
2020-10-12 01:41:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 01:41:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 01:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:41:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000517
2020-10-12 01:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006010
2020-10-12 01:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-12 01:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098929
2020-10-12 01:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105539
2020-10-12 01:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004381
2020-10-12 01:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-12 01:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095512
2020-10-12 01:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100410
2020-10-12 01:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:41:50 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 01:41:54 | INFO | train_inner | epoch 036:     10 / 174 loss=6.148, nll_loss=4.778, ppl=27.43, wps=16585, ups=2.65, wpb=6269.9, bsz=260.2, num_updates=6100, lr=0.000161955, gnorm=1.322, clip=0, train_wall=33, wall=2175
2020-10-12 01:42:27 | INFO | train_inner | epoch 036:    110 / 174 loss=6.036, nll_loss=4.649, ppl=25.08, wps=18707.4, ups=3, wpb=6230, bsz=261.7, num_updates=6200, lr=0.000160644, gnorm=1.307, clip=0, train_wall=33, wall=2208
2020-10-12 01:42:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000836
2020-10-12 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037803
2020-10-12 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027239
2020-10-12 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066225
2020-10-12 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000668
2020-10-12 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037068
2020-10-12 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027543
2020-10-12 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065605
2020-10-12 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:42:51 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.769 | nll_loss 5.35 | ppl 40.8 | wps 44387.3 | wpb 2306.9 | bsz 92.7 | num_updates 6264 | best_loss 6.769
2020-10-12 01:42:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:42:52 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 36 @ 6264 updates, score 6.769) (writing took 1.5028675650100922 seconds)
2020-10-12 01:42:52 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 01:42:52 | INFO | train | epoch 036 | loss 6.047 | nll_loss 4.661 | ppl 25.3 | wps 17404.2 | ups 2.79 | wpb 6233 | bsz 255.7 | num_updates 6264 | lr 0.000159821 | gnorm 1.283 | clip 0 | train_wall 57 | wall 2233
2020-10-12 01:42:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 01:42:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 01:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:42:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000507
2020-10-12 01:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005719
2020-10-12 01:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000250
2020-10-12 01:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098202
2020-10-12 01:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104508
2020-10-12 01:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:42:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:42:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004344
2020-10-12 01:42:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:42:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 01:42:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:42:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096413
2020-10-12 01:42:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101259
2020-10-12 01:42:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:42:53 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 01:43:05 | INFO | train_inner | epoch 037:     36 / 174 loss=6.012, nll_loss=4.62, ppl=24.59, wps=16654.6, ups=2.65, wpb=6281.4, bsz=255.5, num_updates=6300, lr=0.000159364, gnorm=1.263, clip=0, train_wall=33, wall=2246
2020-10-12 01:43:38 | INFO | train_inner | epoch 037:    136 / 174 loss=5.986, nll_loss=4.588, ppl=24.05, wps=18676.8, ups=3.02, wpb=6194.1, bsz=240.5, num_updates=6400, lr=0.000158114, gnorm=1.314, clip=0, train_wall=33, wall=2279
2020-10-12 01:43:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000767
2020-10-12 01:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037556
2020-10-12 01:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027152
2020-10-12 01:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065815
2020-10-12 01:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000697
2020-10-12 01:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037637
2020-10-12 01:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:43:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027094
2020-10-12 01:43:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065743
2020-10-12 01:43:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:43:53 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.789 | nll_loss 5.367 | ppl 41.27 | wps 44203.9 | wpb 2306.9 | bsz 92.7 | num_updates 6438 | best_loss 6.769
2020-10-12 01:43:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:43:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_last.pt (epoch 37 @ 6438 updates, score 6.789) (writing took 1.0294454090035288 seconds)
2020-10-12 01:43:54 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 01:43:54 | INFO | train | epoch 037 | loss 5.968 | nll_loss 4.567 | ppl 23.71 | wps 17595.9 | ups 2.82 | wpb 6233 | bsz 255.7 | num_updates 6438 | lr 0.000157647 | gnorm 1.323 | clip 0 | train_wall 57 | wall 2295
2020-10-12 01:43:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 01:43:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 01:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:43:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000433
2020-10-12 01:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005297
2020-10-12 01:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 01:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095864
2020-10-12 01:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101653
2020-10-12 01:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004237
2020-10-12 01:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000164
2020-10-12 01:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094880
2020-10-12 01:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099611
2020-10-12 01:43:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:43:54 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 01:44:15 | INFO | train_inner | epoch 038:     62 / 174 loss=5.875, nll_loss=4.46, ppl=22.01, wps=16822.3, ups=2.69, wpb=6263.5, bsz=286.5, num_updates=6500, lr=0.000156893, gnorm=1.331, clip=0, train_wall=33, wall=2316
2020-10-12 01:44:48 | INFO | train_inner | epoch 038:    162 / 174 loss=5.922, nll_loss=4.511, ppl=22.81, wps=18644, ups=3.03, wpb=6162.6, bsz=236.5, num_updates=6600, lr=0.0001557, gnorm=1.346, clip=0, train_wall=33, wall=2349
2020-10-12 01:44:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000764
2020-10-12 01:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037595
2020-10-12 01:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027404
2020-10-12 01:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066095
2020-10-12 01:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000671
2020-10-12 01:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037968
2020-10-12 01:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027272
2020-10-12 01:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066230
2020-10-12 01:44:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:44:55 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.734 | nll_loss 5.295 | ppl 39.27 | wps 44004.3 | wpb 2306.9 | bsz 92.7 | num_updates 6612 | best_loss 6.734
2020-10-12 01:44:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:44:56 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 38 @ 6612 updates, score 6.734) (writing took 1.4972707609995268 seconds)
2020-10-12 01:44:56 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 01:44:56 | INFO | train | epoch 038 | loss 5.887 | nll_loss 4.473 | ppl 22.21 | wps 17431.6 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 6612 | lr 0.000155558 | gnorm 1.326 | clip 0 | train_wall 57 | wall 2357
2020-10-12 01:44:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 01:44:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 01:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:44:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000663
2020-10-12 01:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005949
2020-10-12 01:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000243
2020-10-12 01:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097915
2020-10-12 01:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104438
2020-10-12 01:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004273
2020-10-12 01:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-12 01:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095416
2020-10-12 01:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100185
2020-10-12 01:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:44:56 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 01:45:26 | INFO | train_inner | epoch 039:     88 / 174 loss=5.853, nll_loss=4.431, ppl=21.58, wps=16915.5, ups=2.65, wpb=6390.6, bsz=246, num_updates=6700, lr=0.000154533, gnorm=1.376, clip=0, train_wall=33, wall=2387
2020-10-12 01:45:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000773
2020-10-12 01:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037761
2020-10-12 01:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027951
2020-10-12 01:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066831
2020-10-12 01:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000670
2020-10-12 01:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037098
2020-10-12 01:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027722
2020-10-12 01:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065832
2020-10-12 01:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:45:57 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.734 | nll_loss 5.285 | ppl 38.99 | wps 44161.7 | wpb 2306.9 | bsz 92.7 | num_updates 6786 | best_loss 6.734
2020-10-12 01:45:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:45:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 39 @ 6786 updates, score 6.734) (writing took 1.4963160289917141 seconds)
2020-10-12 01:45:58 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 01:45:58 | INFO | train | epoch 039 | loss 5.821 | nll_loss 4.394 | ppl 21.02 | wps 17424.7 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 6786 | lr 0.000153551 | gnorm 1.378 | clip 0 | train_wall 57 | wall 2419
2020-10-12 01:45:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 01:45:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 01:45:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:45:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000573
2020-10-12 01:45:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006101
2020-10-12 01:45:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:45:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 01:45:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096989
2020-10-12 01:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103685
2020-10-12 01:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004281
2020-10-12 01:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 01:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095335
2020-10-12 01:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100114
2020-10-12 01:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:45:59 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 01:46:03 | INFO | train_inner | epoch 040:     14 / 174 loss=5.781, nll_loss=4.348, ppl=20.37, wps=16298.5, ups=2.66, wpb=6126.5, bsz=265.5, num_updates=6800, lr=0.000153393, gnorm=1.342, clip=0, train_wall=33, wall=2424
2020-10-12 01:46:37 | INFO | train_inner | epoch 040:    114 / 174 loss=5.719, nll_loss=4.276, ppl=19.37, wps=18664, ups=3.01, wpb=6198.3, bsz=251, num_updates=6900, lr=0.000152277, gnorm=1.305, clip=0, train_wall=33, wall=2458
2020-10-12 01:46:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:46:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:46:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000765
2020-10-12 01:46:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:46:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.038007
2020-10-12 01:46:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:46:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027243
2020-10-12 01:46:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066353
2020-10-12 01:46:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:46:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:46:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000670
2020-10-12 01:46:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:46:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036879
2020-10-12 01:46:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:46:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027067
2020-10-12 01:46:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064931
2020-10-12 01:46:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:46:59 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.712 | nll_loss 5.253 | ppl 38.12 | wps 43732.7 | wpb 2306.9 | bsz 92.7 | num_updates 6960 | best_loss 6.712
2020-10-12 01:46:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:47:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/O2M/checkpoint_best.pt (epoch 40 @ 6960 updates, score 6.712) (writing took 1.4961230030021397 seconds)
2020-10-12 01:47:01 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 01:47:01 | INFO | train | epoch 040 | loss 5.732 | nll_loss 4.289 | ppl 19.55 | wps 17435.7 | ups 2.8 | wpb 6233 | bsz 255.7 | num_updates 6960 | lr 0.00015162 | gnorm 1.303 | clip 0 | train_wall 57 | wall 2482
2020-10-12 01:47:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 01:47:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 01:47:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:47:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000620
2020-10-12 01:47:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005905
2020-10-12 01:47:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:47:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 01:47:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:47:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096037
2020-10-12 01:47:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102441
2020-10-12 01:47:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:47:01 | INFO | fairseq_cli.train | done training in 2481.6 seconds
