2020-10-12 00:24:18 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_belpol_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='bel-eng,pol-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_belpol_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 00:24:18 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 00:24:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['bel', 'eng', 'pol']
2020-10-12 00:24:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bel] dictionary: 22227 types
2020-10-12 00:24:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 22227 types
2020-10-12 00:24:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | [pol] dictionary: 22227 types
2020-10-12 00:24:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 00:24:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:24:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 00:24:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:bel-eng': 1, 'main:pol-eng': 1}
2020-10-12 00:24:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-12 00:24:18 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belpol_sepspm8000/M2O/valid.bel-eng.bel
2020-10-12 00:24:18 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belpol_sepspm8000/M2O/valid.bel-eng.eng
2020-10-12 00:24:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belpol_sepspm8000/M2O/ valid bel-eng 248 examples
2020-10-12 00:24:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:pol-eng src_langtok: None; tgt_langtok: None
2020-10-12 00:24:18 | INFO | fairseq.data.data_utils | loaded 4108 examples from: fairseq/data-bin/ted_belpol_sepspm8000/M2O/valid.pol-eng.pol
2020-10-12 00:24:18 | INFO | fairseq.data.data_utils | loaded 4108 examples from: fairseq/data-bin/ted_belpol_sepspm8000/M2O/valid.pol-eng.eng
2020-10-12 00:24:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belpol_sepspm8000/M2O/ valid pol-eng 4108 examples
2020-10-12 00:24:18 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22227, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22227, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=22227, bias=False)
  )
)
2020-10-12 00:24:18 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 00:24:18 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 00:24:18 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 00:24:18 | INFO | fairseq_cli.train | num. model params: 42923520 (num. trained: 42923520)
2020-10-12 00:24:20 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 00:24:20 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 00:24:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 00:24:20 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.732 GB ; name = GeForce RTX 2080 Ti                     
2020-10-12 00:24:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 00:24:20 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 00:24:20 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 00:24:20 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_last.pt
2020-10-12 00:24:20 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 00:24:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 00:24:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:24:20 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 00:24:20 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:bel-eng': 1, 'main:pol-eng': 1}
2020-10-12 00:24:20 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-12 00:24:20 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belpol_sepspm8000/M2O/train.bel-eng.bel
2020-10-12 00:24:20 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belpol_sepspm8000/M2O/train.bel-eng.eng
2020-10-12 00:24:20 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belpol_sepspm8000/M2O/ train bel-eng 4509 examples
2020-10-12 00:24:20 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:pol-eng src_langtok: None; tgt_langtok: None
2020-10-12 00:24:20 | INFO | fairseq.data.data_utils | loaded 39985 examples from: fairseq/data-bin/ted_belpol_sepspm8000/M2O/train.pol-eng.pol
2020-10-12 00:24:20 | INFO | fairseq.data.data_utils | loaded 39985 examples from: fairseq/data-bin/ted_belpol_sepspm8000/M2O/train.pol-eng.eng
2020-10-12 00:24:20 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belpol_sepspm8000/M2O/ train pol-eng 39985 examples
2020-10-12 00:24:20 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:bel-eng', 4509), ('main:pol-eng', 39985)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 00:24:20 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 00:24:20 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 44494
2020-10-12 00:24:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 44494; virtual dataset size 44494
2020-10-12 00:24:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 00:24:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:bel-eng': 4509, 'main:pol-eng': 39985}; raw total size: 44494
2020-10-12 00:24:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:bel-eng': 4509, 'main:pol-eng': 39985}; resampled total size: 44494
2020-10-12 00:24:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 00:24:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.003063
2020-10-12 00:24:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:24:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000445
2020-10-12 00:24:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005161
2020-10-12 00:24:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:24:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 00:24:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:24:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099847
2020-10-12 00:24:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105533
2020-10-12 00:24:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:24:20 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 00:24:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:24:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004204
2020-10-12 00:24:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:24:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 00:24:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:24:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097819
2020-10-12 00:24:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102527
2020-10-12 00:24:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:24:20 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 00:24:50 | INFO | train_inner | epoch 001:    100 / 177 loss=14.134, nll_loss=14.019, ppl=16606.8, wps=23818.9, ups=3.41, wpb=6980.3, bsz=262.2, num_updates=100, lr=5.0975e-06, gnorm=4.396, clip=0, train_wall=29, wall=30
2020-10-12 00:25:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000742
2020-10-12 00:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035929
2020-10-12 00:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027446
2020-10-12 00:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064464
2020-10-12 00:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000656
2020-10-12 00:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035427
2020-10-12 00:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027098
2020-10-12 00:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.063508
2020-10-12 00:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
/home/han/Documents/11737-hw/assign2/fairseq/fairseq/utils.py:340: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2020-10-12 00:25:15 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.944 | nll_loss 11.572 | ppl 3044.12 | wps 52770.3 | wpb 2352.5 | bsz 87.1 | num_updates 177
2020-10-12 00:25:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:25:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 1 @ 177 updates, score 11.944) (writing took 0.9578446780069498 seconds)
2020-10-12 00:25:16 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 00:25:16 | INFO | train | epoch 001 | loss 13.48 | nll_loss 13.291 | ppl 10022.1 | wps 21754.5 | ups 3.22 | wpb 6763.8 | bsz 251.4 | num_updates 177 | lr 8.94558e-06 | gnorm 3.475 | clip 0 | train_wall 51 | wall 56
2020-10-12 00:25:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 00:25:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 00:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:25:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000547
2020-10-12 00:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005544
2020-10-12 00:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 00:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099791
2020-10-12 00:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105858
2020-10-12 00:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004214
2020-10-12 00:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 00:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099345
2020-10-12 00:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104051
2020-10-12 00:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:25:16 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 00:25:23 | INFO | train_inner | epoch 002:     23 / 177 loss=12.442, nll_loss=12.135, ppl=4498.11, wps=19924, ups=3.06, wpb=6516.1, bsz=246.5, num_updates=200, lr=1.0095e-05, gnorm=2.21, clip=0, train_wall=28, wall=63
2020-10-12 00:25:52 | INFO | train_inner | epoch 002:    123 / 177 loss=11.605, nll_loss=11.2, ppl=2352.62, wps=22629.3, ups=3.37, wpb=6722.4, bsz=244.4, num_updates=300, lr=1.50925e-05, gnorm=1.689, clip=0, train_wall=29, wall=92
2020-10-12 00:26:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000780
2020-10-12 00:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036290
2020-10-12 00:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027959
2020-10-12 00:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065371
2020-10-12 00:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000658
2020-10-12 00:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036215
2020-10-12 00:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027749
2020-10-12 00:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064980
2020-10-12 00:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:26:11 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.324 | nll_loss 9.722 | ppl 844.38 | wps 51896.9 | wpb 2352.5 | bsz 87.1 | num_updates 354 | best_loss 10.324
2020-10-12 00:26:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:26:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 2 @ 354 updates, score 10.324) (writing took 1.497631984995678 seconds)
2020-10-12 00:26:13 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 00:26:13 | INFO | train | epoch 002 | loss 11.44 | nll_loss 11.013 | ppl 2067.21 | wps 21036.4 | ups 3.11 | wpb 6763.8 | bsz 251.4 | num_updates 354 | lr 1.77911e-05 | gnorm 1.701 | clip 0 | train_wall 52 | wall 113
2020-10-12 00:26:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 00:26:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 00:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:26:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000570
2020-10-12 00:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006019
2020-10-12 00:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 00:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100350
2020-10-12 00:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106968
2020-10-12 00:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004222
2020-10-12 00:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 00:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099945
2020-10-12 00:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104665
2020-10-12 00:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:26:13 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 00:26:27 | INFO | train_inner | epoch 003:     46 / 177 loss=10.602, nll_loss=10.062, ppl=1069, wps=20025.8, ups=2.92, wpb=6869.2, bsz=244.7, num_updates=400, lr=2.009e-05, gnorm=1.665, clip=0, train_wall=30, wall=127
2020-10-12 00:26:57 | INFO | train_inner | epoch 003:    146 / 177 loss=9.676, nll_loss=8.968, ppl=500.63, wps=22185.2, ups=3.33, wpb=6658.1, bsz=251.7, num_updates=500, lr=2.50875e-05, gnorm=1.514, clip=0, train_wall=29, wall=157
2020-10-12 00:27:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000772
2020-10-12 00:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036316
2020-10-12 00:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.028412
2020-10-12 00:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065841
2020-10-12 00:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000652
2020-10-12 00:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035891
2020-10-12 00:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.028019
2020-10-12 00:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064891
2020-10-12 00:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:27:09 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.247 | nll_loss 8.387 | ppl 334.83 | wps 51353.4 | wpb 2352.5 | bsz 87.1 | num_updates 531 | best_loss 9.247
2020-10-12 00:27:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:27:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 3 @ 531 updates, score 9.247) (writing took 1.5021154190035304 seconds)
2020-10-12 00:27:10 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 00:27:10 | INFO | train | epoch 003 | loss 9.774 | nll_loss 9.084 | ppl 542.72 | wps 20781.5 | ups 3.07 | wpb 6763.8 | bsz 251.4 | num_updates 531 | lr 2.66367e-05 | gnorm 1.501 | clip 0 | train_wall 52 | wall 170
2020-10-12 00:27:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 00:27:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 00:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:27:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000457
2020-10-12 00:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005915
2020-10-12 00:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 00:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099284
2020-10-12 00:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105818
2020-10-12 00:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004216
2020-10-12 00:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000164
2020-10-12 00:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095631
2020-10-12 00:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100336
2020-10-12 00:27:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:27:10 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 00:27:32 | INFO | train_inner | epoch 004:     69 / 177 loss=9.343, nll_loss=8.545, ppl=373.43, wps=19817.7, ups=2.87, wpb=6900.1, bsz=256.2, num_updates=600, lr=3.0085e-05, gnorm=1.251, clip=0, train_wall=30, wall=191
2020-10-12 00:28:02 | INFO | train_inner | epoch 004:    169 / 177 loss=9.133, nll_loss=8.279, ppl=310.56, wps=22088.1, ups=3.28, wpb=6725.9, bsz=255.2, num_updates=700, lr=3.50825e-05, gnorm=1.397, clip=0, train_wall=30, wall=222
2020-10-12 00:28:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:28:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:28:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000772
2020-10-12 00:28:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:28:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036290
2020-10-12 00:28:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:28:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.028088
2020-10-12 00:28:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065489
2020-10-12 00:28:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:28:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:28:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000691
2020-10-12 00:28:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035821
2020-10-12 00:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027482
2020-10-12 00:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064318
2020-10-12 00:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:28:07 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.983 | nll_loss 8.047 | ppl 264.47 | wps 50935.6 | wpb 2352.5 | bsz 87.1 | num_updates 708 | best_loss 8.983
2020-10-12 00:28:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:28:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 4 @ 708 updates, score 8.983) (writing took 1.495847139012767 seconds)
2020-10-12 00:28:08 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 00:28:08 | INFO | train | epoch 004 | loss 9.203 | nll_loss 8.366 | ppl 329.84 | wps 20585.1 | ups 3.04 | wpb 6763.8 | bsz 251.4 | num_updates 708 | lr 3.54823e-05 | gnorm 1.363 | clip 0 | train_wall 53 | wall 228
2020-10-12 00:28:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 00:28:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 00:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:28:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000523
2020-10-12 00:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005619
2020-10-12 00:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 00:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099993
2020-10-12 00:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106127
2020-10-12 00:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004387
2020-10-12 00:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-12 00:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097950
2020-10-12 00:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102859
2020-10-12 00:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:28:09 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 00:28:37 | INFO | train_inner | epoch 005:     92 / 177 loss=9.002, nll_loss=8.121, ppl=278.32, wps=19362.2, ups=2.87, wpb=6749.3, bsz=253.8, num_updates=800, lr=4.008e-05, gnorm=1.469, clip=0, train_wall=30, wall=257
2020-10-12 00:29:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000841
2020-10-12 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036603
2020-10-12 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.028043
2020-10-12 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065843
2020-10-12 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000638
2020-10-12 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036908
2020-10-12 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027825
2020-10-12 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065709
2020-10-12 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:29:06 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.701 | nll_loss 7.73 | ppl 212.37 | wps 49966 | wpb 2352.5 | bsz 87.1 | num_updates 885 | best_loss 8.701
2020-10-12 00:29:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:29:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 5 @ 885 updates, score 8.701) (writing took 1.497633435006719 seconds)
2020-10-12 00:29:07 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 00:29:07 | INFO | train | epoch 005 | loss 8.964 | nll_loss 8.076 | ppl 269.77 | wps 20407.4 | ups 3.02 | wpb 6763.8 | bsz 251.4 | num_updates 885 | lr 4.43279e-05 | gnorm 1.493 | clip 0 | train_wall 53 | wall 287
2020-10-12 00:29:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 00:29:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 00:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:29:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000488
2020-10-12 00:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006069
2020-10-12 00:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 00:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099683
2020-10-12 00:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106353
2020-10-12 00:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004240
2020-10-12 00:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 00:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098993
2020-10-12 00:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103740
2020-10-12 00:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:29:07 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 00:29:12 | INFO | train_inner | epoch 006:     15 / 177 loss=8.905, nll_loss=8.008, ppl=257.43, wps=19152.7, ups=2.85, wpb=6721.2, bsz=251.3, num_updates=900, lr=4.50775e-05, gnorm=1.522, clip=0, train_wall=30, wall=292
2020-10-12 00:29:43 | INFO | train_inner | epoch 006:    115 / 177 loss=8.775, nll_loss=7.859, ppl=232.13, wps=21675.5, ups=3.24, wpb=6699.5, bsz=238.2, num_updates=1000, lr=5.0075e-05, gnorm=1.497, clip=0, train_wall=30, wall=323
2020-10-12 00:30:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:30:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:30:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000831
2020-10-12 00:30:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:30:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037086
2020-10-12 00:30:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:30:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027336
2020-10-12 00:30:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065604
2020-10-12 00:30:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:30:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:30:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000635
2020-10-12 00:30:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:30:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036186
2020-10-12 00:30:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:30:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.026929
2020-10-12 00:30:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064085
2020-10-12 00:30:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:30:05 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.444 | nll_loss 7.432 | ppl 172.67 | wps 49263.5 | wpb 2352.5 | bsz 87.1 | num_updates 1062 | best_loss 8.444
2020-10-12 00:30:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:30:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 6 @ 1062 updates, score 8.444) (writing took 1.4905285090062534 seconds)
2020-10-12 00:30:06 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 00:30:06 | INFO | train | epoch 006 | loss 8.695 | nll_loss 7.769 | ppl 218.05 | wps 20196.8 | ups 2.99 | wpb 6763.8 | bsz 251.4 | num_updates 1062 | lr 5.31735e-05 | gnorm 1.432 | clip 0 | train_wall 54 | wall 346
2020-10-12 00:30:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 00:30:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 00:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:30:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000440
2020-10-12 00:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005864
2020-10-12 00:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000243
2020-10-12 00:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101045
2020-10-12 00:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107496
2020-10-12 00:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004336
2020-10-12 00:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-12 00:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:30:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101506
2020-10-12 00:30:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106340
2020-10-12 00:30:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:30:07 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 00:30:19 | INFO | train_inner | epoch 007:     38 / 177 loss=8.562, nll_loss=7.616, ppl=196.21, wps=19478.7, ups=2.81, wpb=6942.5, bsz=268.3, num_updates=1100, lr=5.50725e-05, gnorm=1.337, clip=0, train_wall=31, wall=358
2020-10-12 00:30:50 | INFO | train_inner | epoch 007:    138 / 177 loss=8.417, nll_loss=7.451, ppl=174.99, wps=21252.4, ups=3.21, wpb=6614, bsz=247.1, num_updates=1200, lr=6.007e-05, gnorm=1.51, clip=0, train_wall=31, wall=390
2020-10-12 00:31:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000821
2020-10-12 00:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036176
2020-10-12 00:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027785
2020-10-12 00:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065130
2020-10-12 00:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000660
2020-10-12 00:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036053
2020-10-12 00:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027710
2020-10-12 00:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064745
2020-10-12 00:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:31:05 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.242 | nll_loss 7.206 | ppl 147.67 | wps 48962.3 | wpb 2352.5 | bsz 87.1 | num_updates 1239 | best_loss 8.242
2020-10-12 00:31:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:31:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 7 @ 1239 updates, score 8.242) (writing took 1.5124148719914956 seconds)
2020-10-12 00:31:06 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 00:31:06 | INFO | train | epoch 007 | loss 8.463 | nll_loss 7.502 | ppl 181.3 | wps 20001.7 | ups 2.96 | wpb 6763.8 | bsz 251.4 | num_updates 1239 | lr 6.2019e-05 | gnorm 1.472 | clip 0 | train_wall 54 | wall 406
2020-10-12 00:31:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 00:31:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 00:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:31:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000476
2020-10-12 00:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005721
2020-10-12 00:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 00:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099614
2020-10-12 00:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105845
2020-10-12 00:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004247
2020-10-12 00:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 00:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098986
2020-10-12 00:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103736
2020-10-12 00:31:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:31:06 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 00:31:26 | INFO | train_inner | epoch 008:     61 / 177 loss=8.354, nll_loss=7.379, ppl=166.42, wps=19076.9, ups=2.77, wpb=6892.1, bsz=254.7, num_updates=1300, lr=6.50675e-05, gnorm=1.403, clip=0, train_wall=31, wall=426
2020-10-12 00:31:57 | INFO | train_inner | epoch 008:    161 / 177 loss=8.295, nll_loss=7.31, ppl=158.72, wps=21129.6, ups=3.16, wpb=6693.9, bsz=238.9, num_updates=1400, lr=7.0065e-05, gnorm=1.434, clip=0, train_wall=31, wall=457
2020-10-12 00:32:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.006945
2020-10-12 00:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036623
2020-10-12 00:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.028234
2020-10-12 00:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072178
2020-10-12 00:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000658
2020-10-12 00:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035810
2020-10-12 00:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027727
2020-10-12 00:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064516
2020-10-12 00:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:32:05 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.045 | nll_loss 6.988 | ppl 126.98 | wps 48206.6 | wpb 2352.5 | bsz 87.1 | num_updates 1416 | best_loss 8.045
2020-10-12 00:32:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:32:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 8 @ 1416 updates, score 8.045) (writing took 1.4957396609970601 seconds)
2020-10-12 00:32:07 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 00:32:07 | INFO | train | epoch 008 | loss 8.269 | nll_loss 7.281 | ppl 155.57 | wps 19826.6 | ups 2.93 | wpb 6763.8 | bsz 251.4 | num_updates 1416 | lr 7.08646e-05 | gnorm 1.414 | clip 0 | train_wall 55 | wall 466
2020-10-12 00:32:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 00:32:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 00:32:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:32:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000541
2020-10-12 00:32:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005842
2020-10-12 00:32:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:32:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 00:32:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:32:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100184
2020-10-12 00:32:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106586
2020-10-12 00:32:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:32:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:32:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004424
2020-10-12 00:32:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:32:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 00:32:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:32:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097456
2020-10-12 00:32:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102404
2020-10-12 00:32:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:32:07 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 00:32:33 | INFO | train_inner | epoch 009:     84 / 177 loss=8.096, nll_loss=7.085, ppl=135.76, wps=18939.1, ups=2.78, wpb=6804.9, bsz=258, num_updates=1500, lr=7.50625e-05, gnorm=1.321, clip=0, train_wall=31, wall=493
2020-10-12 00:33:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000777
2020-10-12 00:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035477
2020-10-12 00:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027890
2020-10-12 00:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064485
2020-10-12 00:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000670
2020-10-12 00:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035033
2020-10-12 00:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027580
2020-10-12 00:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.063607
2020-10-12 00:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:33:06 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.862 | nll_loss 6.773 | ppl 109.33 | wps 48214.4 | wpb 2352.5 | bsz 87.1 | num_updates 1593 | best_loss 7.862
2020-10-12 00:33:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:33:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 9 @ 1593 updates, score 7.862) (writing took 1.493750485999044 seconds)
2020-10-12 00:33:07 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 00:33:07 | INFO | train | epoch 009 | loss 8.089 | nll_loss 7.075 | ppl 134.84 | wps 19732.5 | ups 2.92 | wpb 6763.8 | bsz 251.4 | num_updates 1593 | lr 7.97102e-05 | gnorm 1.357 | clip 0 | train_wall 55 | wall 527
2020-10-12 00:33:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 00:33:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 00:33:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:33:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000585
2020-10-12 00:33:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005984
2020-10-12 00:33:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:33:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 00:33:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:33:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100445
2020-10-12 00:33:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106940
2020-10-12 00:33:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:33:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:33:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004224
2020-10-12 00:33:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:33:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000164
2020-10-12 00:33:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:33:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098675
2020-10-12 00:33:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103395
2020-10-12 00:33:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:33:07 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 00:33:10 | INFO | train_inner | epoch 010:      7 / 177 loss=8.076, nll_loss=7.061, ppl=133.5, wps=18552.4, ups=2.75, wpb=6750.8, bsz=248, num_updates=1600, lr=8.006e-05, gnorm=1.378, clip=0, train_wall=31, wall=530
2020-10-12 00:33:42 | INFO | train_inner | epoch 010:    107 / 177 loss=7.893, nll_loss=6.852, ppl=115.52, wps=21295.1, ups=3.14, wpb=6780.2, bsz=268.3, num_updates=1700, lr=8.50575e-05, gnorm=1.407, clip=0, train_wall=31, wall=562
2020-10-12 00:34:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000768
2020-10-12 00:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036390
2020-10-12 00:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027768
2020-10-12 00:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065266
2020-10-12 00:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000672
2020-10-12 00:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035175
2020-10-12 00:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027697
2020-10-12 00:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.063880
2020-10-12 00:34:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:34:07 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.732 | nll_loss 6.634 | ppl 99.34 | wps 47924.8 | wpb 2352.5 | bsz 87.1 | num_updates 1770 | best_loss 7.732
2020-10-12 00:34:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:34:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 10 @ 1770 updates, score 7.732) (writing took 1.4915320300060557 seconds)
2020-10-12 00:34:08 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 00:34:08 | INFO | train | epoch 010 | loss 7.94 | nll_loss 6.905 | ppl 119.81 | wps 19610 | ups 2.9 | wpb 6763.8 | bsz 251.4 | num_updates 1770 | lr 8.85558e-05 | gnorm 1.316 | clip 0 | train_wall 56 | wall 588
2020-10-12 00:34:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 00:34:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 00:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:34:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000518
2020-10-12 00:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005924
2020-10-12 00:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-12 00:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101041
2020-10-12 00:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107541
2020-10-12 00:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004222
2020-10-12 00:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 00:34:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:34:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096273
2020-10-12 00:34:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100988
2020-10-12 00:34:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:34:09 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 00:34:18 | INFO | train_inner | epoch 011:     30 / 177 loss=7.921, nll_loss=6.884, ppl=118.1, wps=18283.4, ups=2.75, wpb=6651.7, bsz=237.3, num_updates=1800, lr=9.0055e-05, gnorm=1.26, clip=0, train_wall=31, wall=598
2020-10-12 00:34:50 | INFO | train_inner | epoch 011:    130 / 177 loss=7.828, nll_loss=6.777, ppl=109.7, wps=21185.6, ups=3.1, wpb=6844.5, bsz=262.1, num_updates=1900, lr=9.50525e-05, gnorm=1.218, clip=0, train_wall=32, wall=630
2020-10-12 00:35:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:35:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:35:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000708
2020-10-12 00:35:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036349
2020-10-12 00:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027223
2020-10-12 00:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064618
2020-10-12 00:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000662
2020-10-12 00:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036081
2020-10-12 00:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027175
2020-10-12 00:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064242
2020-10-12 00:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:35:08 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.653 | nll_loss 6.523 | ppl 91.99 | wps 47423.4 | wpb 2352.5 | bsz 87.1 | num_updates 1947 | best_loss 7.653
2020-10-12 00:35:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:35:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 11 @ 1947 updates, score 7.653) (writing took 1.4956990030041197 seconds)
2020-10-12 00:35:10 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 00:35:10 | INFO | train | epoch 011 | loss 7.808 | nll_loss 6.754 | ppl 107.95 | wps 19521.1 | ups 2.89 | wpb 6763.8 | bsz 251.4 | num_updates 1947 | lr 9.74013e-05 | gnorm 1.267 | clip 0 | train_wall 56 | wall 650
2020-10-12 00:35:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 00:35:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 00:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:35:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000560
2020-10-12 00:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005967
2020-10-12 00:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 00:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096702
2020-10-12 00:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103187
2020-10-12 00:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004254
2020-10-12 00:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-12 00:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096349
2020-10-12 00:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101097
2020-10-12 00:35:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:35:10 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 00:35:27 | INFO | train_inner | epoch 012:     53 / 177 loss=7.746, nll_loss=6.681, ppl=102.61, wps=18622.6, ups=2.74, wpb=6801.1, bsz=238.5, num_updates=2000, lr=0.00010005, gnorm=1.339, clip=0, train_wall=32, wall=667
2020-10-12 00:35:59 | INFO | train_inner | epoch 012:    153 / 177 loss=7.696, nll_loss=6.625, ppl=98.7, wps=20685.5, ups=3.1, wpb=6681.2, bsz=249.7, num_updates=2100, lr=0.000105048, gnorm=1.202, clip=0, train_wall=32, wall=699
2020-10-12 00:36:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000850
2020-10-12 00:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035283
2020-10-12 00:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027418
2020-10-12 00:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.063899
2020-10-12 00:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000654
2020-10-12 00:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036168
2020-10-12 00:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027369
2020-10-12 00:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064514
2020-10-12 00:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:36:10 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.562 | nll_loss 6.402 | ppl 84.57 | wps 47440.6 | wpb 2352.5 | bsz 87.1 | num_updates 2124 | best_loss 7.562
2020-10-12 00:36:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:36:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 12 @ 2124 updates, score 7.562) (writing took 1.4962153020023834 seconds)
2020-10-12 00:36:11 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 00:36:11 | INFO | train | epoch 012 | loss 7.692 | nll_loss 6.621 | ppl 98.43 | wps 19465.5 | ups 2.88 | wpb 6763.8 | bsz 251.4 | num_updates 2124 | lr 0.000106247 | gnorm 1.251 | clip 0 | train_wall 56 | wall 711
2020-10-12 00:36:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 00:36:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 00:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:36:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000487
2020-10-12 00:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006093
2020-10-12 00:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000257
2020-10-12 00:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102336
2020-10-12 00:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109032
2020-10-12 00:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004274
2020-10-12 00:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-12 00:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099101
2020-10-12 00:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103868
2020-10-12 00:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:36:11 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 00:36:36 | INFO | train_inner | epoch 013:     76 / 177 loss=7.598, nll_loss=6.514, ppl=91.36, wps=18417, ups=2.72, wpb=6775.4, bsz=257, num_updates=2200, lr=0.000110045, gnorm=1.195, clip=0, train_wall=32, wall=736
2020-10-12 00:37:08 | INFO | train_inner | epoch 013:    176 / 177 loss=7.565, nll_loss=6.474, ppl=88.91, wps=20940.3, ups=3.08, wpb=6801.3, bsz=251.1, num_updates=2300, lr=0.000115043, gnorm=1.166, clip=0, train_wall=32, wall=768
2020-10-12 00:37:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000856
2020-10-12 00:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036088
2020-10-12 00:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027580
2020-10-12 00:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064877
2020-10-12 00:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000677
2020-10-12 00:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036660
2020-10-12 00:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027211
2020-10-12 00:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064873
2020-10-12 00:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:37:11 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.433 | nll_loss 6.27 | ppl 77.18 | wps 47357.6 | wpb 2352.5 | bsz 87.1 | num_updates 2301 | best_loss 7.433
2020-10-12 00:37:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:37:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 13 @ 2301 updates, score 7.433) (writing took 1.4984387879958376 seconds)
2020-10-12 00:37:13 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 00:37:13 | INFO | train | epoch 013 | loss 7.571 | nll_loss 6.483 | ppl 89.42 | wps 19402 | ups 2.87 | wpb 6763.8 | bsz 251.4 | num_updates 2301 | lr 0.000115092 | gnorm 1.178 | clip 0 | train_wall 56 | wall 773
2020-10-12 00:37:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 00:37:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 00:37:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:37:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000484
2020-10-12 00:37:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005981
2020-10-12 00:37:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:37:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000242
2020-10-12 00:37:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:37:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097537
2020-10-12 00:37:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104178
2020-10-12 00:37:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:37:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:37:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004287
2020-10-12 00:37:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:37:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 00:37:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:37:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093818
2020-10-12 00:37:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098606
2020-10-12 00:37:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:37:13 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 00:37:45 | INFO | train_inner | epoch 014:     99 / 177 loss=7.433, nll_loss=6.324, ppl=80.11, wps=18442.6, ups=2.75, wpb=6711.3, bsz=264.2, num_updates=2400, lr=0.00012004, gnorm=1.219, clip=0, train_wall=31, wall=805
2020-10-12 00:38:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:38:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:38:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000788
2020-10-12 00:38:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:38:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036100
2020-10-12 00:38:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:38:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027726
2020-10-12 00:38:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064957
2020-10-12 00:38:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:38:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:38:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000667
2020-10-12 00:38:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:38:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035851
2020-10-12 00:38:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:38:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027266
2020-10-12 00:38:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064107
2020-10-12 00:38:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:38:13 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.364 | nll_loss 6.178 | ppl 72.42 | wps 46888.4 | wpb 2352.5 | bsz 87.1 | num_updates 2478 | best_loss 7.364
2020-10-12 00:38:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:38:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 14 @ 2478 updates, score 7.364) (writing took 1.4937101639952743 seconds)
2020-10-12 00:38:14 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 00:38:14 | INFO | train | epoch 014 | loss 7.47 | nll_loss 6.366 | ppl 82.46 | wps 19437.6 | ups 2.87 | wpb 6763.8 | bsz 251.4 | num_updates 2478 | lr 0.000123938 | gnorm 1.194 | clip 0 | train_wall 56 | wall 834
2020-10-12 00:38:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 00:38:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 00:38:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:38:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000463
2020-10-12 00:38:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005713
2020-10-12 00:38:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:38:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 00:38:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102431
2020-10-12 00:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108725
2020-10-12 00:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004326
2020-10-12 00:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 00:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099055
2020-10-12 00:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103925
2020-10-12 00:38:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:38:15 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 00:38:22 | INFO | train_inner | epoch 015:     22 / 177 loss=7.507, nll_loss=6.407, ppl=84.87, wps=18434.1, ups=2.7, wpb=6831.7, bsz=238.6, num_updates=2500, lr=0.000125037, gnorm=1.163, clip=0, train_wall=32, wall=842
2020-10-12 00:38:54 | INFO | train_inner | epoch 015:    122 / 177 loss=7.371, nll_loss=6.252, ppl=76.24, wps=20902.7, ups=3.07, wpb=6802, bsz=247.2, num_updates=2600, lr=0.000130035, gnorm=1.113, clip=0, train_wall=32, wall=874
2020-10-12 00:39:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000784
2020-10-12 00:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036029
2020-10-12 00:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027989
2020-10-12 00:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065143
2020-10-12 00:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000666
2020-10-12 00:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035855
2020-10-12 00:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027815
2020-10-12 00:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064664
2020-10-12 00:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:39:15 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.28 | nll_loss 6.103 | ppl 68.74 | wps 47092.6 | wpb 2352.5 | bsz 87.1 | num_updates 2655 | best_loss 7.28
2020-10-12 00:39:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:39:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 15 @ 2655 updates, score 7.28) (writing took 1.4987756620103028 seconds)
2020-10-12 00:39:16 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 00:39:16 | INFO | train | epoch 015 | loss 7.361 | nll_loss 6.24 | ppl 75.6 | wps 19373.6 | ups 2.86 | wpb 6763.8 | bsz 251.4 | num_updates 2655 | lr 0.000132784 | gnorm 1.135 | clip 0 | train_wall 56 | wall 896
2020-10-12 00:39:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 00:39:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 00:39:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:39:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000505
2020-10-12 00:39:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005732
2020-10-12 00:39:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:39:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 00:39:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:39:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099705
2020-10-12 00:39:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105946
2020-10-12 00:39:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:39:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:39:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004337
2020-10-12 00:39:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:39:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-12 00:39:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:39:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098886
2020-10-12 00:39:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103723
2020-10-12 00:39:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:39:16 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 00:39:31 | INFO | train_inner | epoch 016:     45 / 177 loss=7.279, nll_loss=6.147, ppl=70.88, wps=18202.6, ups=2.73, wpb=6662.4, bsz=267.3, num_updates=2700, lr=0.000135032, gnorm=1.193, clip=0, train_wall=32, wall=911
2020-10-12 00:40:04 | INFO | train_inner | epoch 016:    145 / 177 loss=7.265, nll_loss=6.13, ppl=70.02, wps=20838.8, ups=3.07, wpb=6778.6, bsz=247.1, num_updates=2800, lr=0.00014003, gnorm=1.152, clip=0, train_wall=32, wall=943
2020-10-12 00:40:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000773
2020-10-12 00:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035742
2020-10-12 00:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.028006
2020-10-12 00:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064865
2020-10-12 00:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000686
2020-10-12 00:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036093
2020-10-12 00:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027376
2020-10-12 00:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064476
2020-10-12 00:40:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:40:17 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.208 | nll_loss 6.004 | ppl 64.2 | wps 47257.7 | wpb 2352.5 | bsz 87.1 | num_updates 2832 | best_loss 7.208
2020-10-12 00:40:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:40:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 16 @ 2832 updates, score 7.208) (writing took 1.499845890008146 seconds)
2020-10-12 00:40:18 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 00:40:18 | INFO | train | epoch 016 | loss 7.27 | nll_loss 6.135 | ppl 70.3 | wps 19361.8 | ups 2.86 | wpb 6763.8 | bsz 251.4 | num_updates 2832 | lr 0.000141629 | gnorm 1.161 | clip 0 | train_wall 56 | wall 958
2020-10-12 00:40:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 00:40:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 00:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:40:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000471
2020-10-12 00:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006022
2020-10-12 00:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 00:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100912
2020-10-12 00:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107465
2020-10-12 00:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004390
2020-10-12 00:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 00:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097594
2020-10-12 00:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102490
2020-10-12 00:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:40:18 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 00:40:40 | INFO | train_inner | epoch 017:     68 / 177 loss=7.21, nll_loss=6.066, ppl=67.01, wps=18485.4, ups=2.71, wpb=6833.6, bsz=257.1, num_updates=2900, lr=0.000145028, gnorm=1.113, clip=0, train_wall=32, wall=980
2020-10-12 00:41:13 | INFO | train_inner | epoch 017:    168 / 177 loss=7.168, nll_loss=6.018, ppl=64.78, wps=20871.3, ups=3.08, wpb=6779.3, bsz=243.1, num_updates=3000, lr=0.000150025, gnorm=1.135, clip=0, train_wall=32, wall=1013
2020-10-12 00:41:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000784
2020-10-12 00:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036545
2020-10-12 00:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.028500
2020-10-12 00:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066192
2020-10-12 00:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000676
2020-10-12 00:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037141
2020-10-12 00:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027484
2020-10-12 00:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065634
2020-10-12 00:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:41:18 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.15 | nll_loss 5.935 | ppl 61.17 | wps 46807.9 | wpb 2352.5 | bsz 87.1 | num_updates 3009 | best_loss 7.15
2020-10-12 00:41:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:41:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 17 @ 3009 updates, score 7.15) (writing took 1.5261004790081643 seconds)
2020-10-12 00:41:20 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 00:41:20 | INFO | train | epoch 017 | loss 7.174 | nll_loss 6.025 | ppl 65.11 | wps 19321.7 | ups 2.86 | wpb 6763.8 | bsz 251.4 | num_updates 3009 | lr 0.000150475 | gnorm 1.133 | clip 0 | train_wall 56 | wall 1020
2020-10-12 00:41:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 00:41:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 00:41:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:41:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000511
2020-10-12 00:41:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005907
2020-10-12 00:41:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:41:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 00:41:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:41:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099778
2020-10-12 00:41:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106216
2020-10-12 00:41:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:41:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:41:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004313
2020-10-12 00:41:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:41:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 00:41:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:41:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098715
2020-10-12 00:41:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103534
2020-10-12 00:41:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:41:20 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 00:41:50 | INFO | train_inner | epoch 018:     91 / 177 loss=7.056, nll_loss=5.891, ppl=59.35, wps=18272.8, ups=2.71, wpb=6744.7, bsz=255.4, num_updates=3100, lr=0.000155023, gnorm=1.1, clip=0, train_wall=32, wall=1050
2020-10-12 00:42:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000835
2020-10-12 00:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036123
2020-10-12 00:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.028002
2020-10-12 00:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065310
2020-10-12 00:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000632
2020-10-12 00:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036160
2020-10-12 00:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027738
2020-10-12 00:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064861
2020-10-12 00:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:42:21 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.103 | nll_loss 5.888 | ppl 59.24 | wps 46787.4 | wpb 2352.5 | bsz 87.1 | num_updates 3186 | best_loss 7.103
2020-10-12 00:42:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:42:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 18 @ 3186 updates, score 7.103) (writing took 1.4970010959950741 seconds)
2020-10-12 00:42:22 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 00:42:22 | INFO | train | epoch 018 | loss 7.078 | nll_loss 5.914 | ppl 60.3 | wps 19302.9 | ups 2.85 | wpb 6763.8 | bsz 251.4 | num_updates 3186 | lr 0.00015932 | gnorm 1.128 | clip 0 | train_wall 57 | wall 1082
2020-10-12 00:42:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 00:42:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 00:42:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:42:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000541
2020-10-12 00:42:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006050
2020-10-12 00:42:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:42:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 00:42:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:42:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100719
2020-10-12 00:42:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107298
2020-10-12 00:42:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:42:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:42:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004238
2020-10-12 00:42:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:42:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 00:42:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:42:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095651
2020-10-12 00:42:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100397
2020-10-12 00:42:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:42:22 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 00:42:27 | INFO | train_inner | epoch 019:     14 / 177 loss=7.112, nll_loss=5.951, ppl=61.87, wps=18124.7, ups=2.71, wpb=6698.1, bsz=235.2, num_updates=3200, lr=0.00016002, gnorm=1.164, clip=0, train_wall=32, wall=1087
2020-10-12 00:42:59 | INFO | train_inner | epoch 019:    114 / 177 loss=6.998, nll_loss=5.823, ppl=56.62, wps=20847.2, ups=3.07, wpb=6797.6, bsz=263.5, num_updates=3300, lr=0.000165018, gnorm=1.158, clip=0, train_wall=32, wall=1119
2020-10-12 00:43:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000848
2020-10-12 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036133
2020-10-12 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.028433
2020-10-12 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065774
2020-10-12 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000667
2020-10-12 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035286
2020-10-12 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027459
2020-10-12 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.063739
2020-10-12 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:43:22 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.008 | nll_loss 5.763 | ppl 54.31 | wps 47230 | wpb 2352.5 | bsz 87.1 | num_updates 3363 | best_loss 7.008
2020-10-12 00:43:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:43:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 19 @ 3363 updates, score 7.008) (writing took 1.5017123090074165 seconds)
2020-10-12 00:43:24 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 00:43:24 | INFO | train | epoch 019 | loss 6.987 | nll_loss 5.81 | ppl 56.09 | wps 19319.7 | ups 2.86 | wpb 6763.8 | bsz 251.4 | num_updates 3363 | lr 0.000168166 | gnorm 1.139 | clip 0 | train_wall 57 | wall 1144
2020-10-12 00:43:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 00:43:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 00:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:43:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000533
2020-10-12 00:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005924
2020-10-12 00:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 00:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103960
2020-10-12 00:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110398
2020-10-12 00:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004294
2020-10-12 00:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 00:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096764
2020-10-12 00:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101563
2020-10-12 00:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:43:24 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 00:43:36 | INFO | train_inner | epoch 020:     37 / 177 loss=6.957, nll_loss=5.774, ppl=54.73, wps=18177.7, ups=2.73, wpb=6654.5, bsz=238.1, num_updates=3400, lr=0.000170015, gnorm=1.104, clip=0, train_wall=32, wall=1156
2020-10-12 00:44:09 | INFO | train_inner | epoch 020:    137 / 177 loss=6.857, nll_loss=5.66, ppl=50.57, wps=20879.2, ups=3.07, wpb=6792.5, bsz=249, num_updates=3500, lr=0.000175013, gnorm=1.1, clip=0, train_wall=32, wall=1188
2020-10-12 00:44:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:44:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:44:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000854
2020-10-12 00:44:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:44:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036302
2020-10-12 00:44:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:44:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.028112
2020-10-12 00:44:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065618
2020-10-12 00:44:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:44:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:44:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000666
2020-10-12 00:44:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:44:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036351
2020-10-12 00:44:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:44:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027746
2020-10-12 00:44:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065087
2020-10-12 00:44:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:44:24 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.952 | nll_loss 5.688 | ppl 51.57 | wps 46914.4 | wpb 2352.5 | bsz 87.1 | num_updates 3540 | best_loss 6.952
2020-10-12 00:44:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:44:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 20 @ 3540 updates, score 6.952) (writing took 1.5067969649971928 seconds)
2020-10-12 00:44:26 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 00:44:26 | INFO | train | epoch 020 | loss 6.875 | nll_loss 5.681 | ppl 51.3 | wps 19315.6 | ups 2.86 | wpb 6763.8 | bsz 251.4 | num_updates 3540 | lr 0.000177012 | gnorm 1.112 | clip 0 | train_wall 57 | wall 1206
2020-10-12 00:44:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 00:44:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 00:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:44:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000541
2020-10-12 00:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006002
2020-10-12 00:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 00:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105624
2020-10-12 00:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112164
2020-10-12 00:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004436
2020-10-12 00:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 00:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100223
2020-10-12 00:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105186
2020-10-12 00:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:44:26 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 00:44:46 | INFO | train_inner | epoch 021:     60 / 177 loss=6.832, nll_loss=5.631, ppl=49.56, wps=18503, ups=2.67, wpb=6919.8, bsz=265, num_updates=3600, lr=0.00018001, gnorm=1.119, clip=0, train_wall=32, wall=1226
2020-10-12 00:45:18 | INFO | train_inner | epoch 021:    160 / 177 loss=6.76, nll_loss=5.547, ppl=46.76, wps=20450.2, ups=3.1, wpb=6597.4, bsz=233.5, num_updates=3700, lr=0.000185008, gnorm=1.166, clip=0, train_wall=32, wall=1258
2020-10-12 00:45:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000844
2020-10-12 00:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037198
2020-10-12 00:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027912
2020-10-12 00:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066308
2020-10-12 00:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000660
2020-10-12 00:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036017
2020-10-12 00:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027234
2020-10-12 00:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064239
2020-10-12 00:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:45:27 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.876 | nll_loss 5.606 | ppl 48.7 | wps 46817.1 | wpb 2352.5 | bsz 87.1 | num_updates 3717 | best_loss 6.876
2020-10-12 00:45:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:45:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 21 @ 3717 updates, score 6.876) (writing took 1.5082613650010899 seconds)
2020-10-12 00:45:28 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 00:45:28 | INFO | train | epoch 021 | loss 6.768 | nll_loss 5.557 | ppl 47.06 | wps 19283.7 | ups 2.85 | wpb 6763.8 | bsz 251.4 | num_updates 3717 | lr 0.000185857 | gnorm 1.148 | clip 0 | train_wall 57 | wall 1268
2020-10-12 00:45:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 00:45:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 00:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:45:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000584
2020-10-12 00:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006056
2020-10-12 00:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 00:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103690
2020-10-12 00:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110258
2020-10-12 00:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004406
2020-10-12 00:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 00:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099277
2020-10-12 00:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104189
2020-10-12 00:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:45:28 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 00:45:55 | INFO | train_inner | epoch 022:     83 / 177 loss=6.67, nll_loss=5.445, ppl=43.55, wps=18744.2, ups=2.69, wpb=6969.2, bsz=275, num_updates=3800, lr=0.000190005, gnorm=1.183, clip=0, train_wall=32, wall=1295
2020-10-12 00:46:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000778
2020-10-12 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037031
2020-10-12 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.028137
2020-10-12 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066294
2020-10-12 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000632
2020-10-12 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036169
2020-10-12 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027169
2020-10-12 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064292
2020-10-12 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:46:29 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.747 | nll_loss 5.459 | ppl 44 | wps 46760.8 | wpb 2352.5 | bsz 87.1 | num_updates 3894 | best_loss 6.747
2020-10-12 00:46:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:46:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 22 @ 3894 updates, score 6.747) (writing took 1.4910407100105658 seconds)
2020-10-12 00:46:30 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 00:46:30 | INFO | train | epoch 022 | loss 6.663 | nll_loss 5.436 | ppl 43.28 | wps 19300.5 | ups 2.85 | wpb 6763.8 | bsz 251.4 | num_updates 3894 | lr 0.000194703 | gnorm 1.166 | clip 0 | train_wall 57 | wall 1330
2020-10-12 00:46:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 00:46:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 00:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:46:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000433
2020-10-12 00:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005963
2020-10-12 00:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000242
2020-10-12 00:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099429
2020-10-12 00:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105974
2020-10-12 00:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004299
2020-10-12 00:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 00:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095649
2020-10-12 00:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100443
2020-10-12 00:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:46:30 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 00:46:32 | INFO | train_inner | epoch 023:      6 / 177 loss=6.643, nll_loss=5.411, ppl=42.56, wps=18035.8, ups=2.71, wpb=6652.2, bsz=239, num_updates=3900, lr=0.000195003, gnorm=1.15, clip=0, train_wall=32, wall=1332
2020-10-12 00:47:05 | INFO | train_inner | epoch 023:    106 / 177 loss=6.518, nll_loss=5.27, ppl=38.57, wps=20771.5, ups=3.1, wpb=6707.3, bsz=248.9, num_updates=4000, lr=0.0002, gnorm=1.154, clip=0, train_wall=32, wall=1365
2020-10-12 00:47:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000781
2020-10-12 00:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036523
2020-10-12 00:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027632
2020-10-12 00:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065280
2020-10-12 00:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000672
2020-10-12 00:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036344
2020-10-12 00:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027500
2020-10-12 00:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064841
2020-10-12 00:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:47:31 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.726 | nll_loss 5.419 | ppl 42.79 | wps 46969.5 | wpb 2352.5 | bsz 87.1 | num_updates 4071 | best_loss 6.726
2020-10-12 00:47:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:47:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 23 @ 4071 updates, score 6.726) (writing took 1.494492600002559 seconds)
2020-10-12 00:47:32 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 00:47:32 | INFO | train | epoch 023 | loss 6.552 | nll_loss 5.308 | ppl 39.61 | wps 19272.6 | ups 2.85 | wpb 6763.8 | bsz 251.4 | num_updates 4071 | lr 0.000198248 | gnorm 1.169 | clip 0 | train_wall 57 | wall 1392
2020-10-12 00:47:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 00:47:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 00:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:47:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000514
2020-10-12 00:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006019
2020-10-12 00:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 00:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101748
2020-10-12 00:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108286
2020-10-12 00:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004265
2020-10-12 00:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 00:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096275
2020-10-12 00:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101035
2020-10-12 00:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:47:32 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 00:47:42 | INFO | train_inner | epoch 024:     29 / 177 loss=6.561, nll_loss=5.318, ppl=39.88, wps=18285.1, ups=2.67, wpb=6848, bsz=265.3, num_updates=4100, lr=0.000197546, gnorm=1.162, clip=0, train_wall=32, wall=1402
2020-10-12 00:48:14 | INFO | train_inner | epoch 024:    129 / 177 loss=6.423, nll_loss=5.158, ppl=35.69, wps=20911.1, ups=3.09, wpb=6776.6, bsz=247, num_updates=4200, lr=0.00019518, gnorm=1.164, clip=0, train_wall=32, wall=1434
2020-10-12 00:48:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000705
2020-10-12 00:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036920
2020-10-12 00:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027473
2020-10-12 00:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065431
2020-10-12 00:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000620
2020-10-12 00:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.034971
2020-10-12 00:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.026757
2020-10-12 00:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.062667
2020-10-12 00:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:48:33 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.627 | nll_loss 5.31 | ppl 39.68 | wps 46823.6 | wpb 2352.5 | bsz 87.1 | num_updates 4248 | best_loss 6.627
2020-10-12 00:48:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:48:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 24 @ 4248 updates, score 6.627) (writing took 1.501921796996612 seconds)
2020-10-12 00:48:34 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 00:48:34 | INFO | train | epoch 024 | loss 6.43 | nll_loss 5.167 | ppl 35.92 | wps 19307.9 | ups 2.85 | wpb 6763.8 | bsz 251.4 | num_updates 4248 | lr 0.000194074 | gnorm 1.137 | clip 0 | train_wall 57 | wall 1454
2020-10-12 00:48:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 00:48:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 00:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:48:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000543
2020-10-12 00:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005939
2020-10-12 00:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000241
2020-10-12 00:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102321
2020-10-12 00:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108846
2020-10-12 00:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004292
2020-10-12 00:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 00:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098482
2020-10-12 00:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103260
2020-10-12 00:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:48:34 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 00:48:51 | INFO | train_inner | epoch 025:     52 / 177 loss=6.328, nll_loss=5.049, ppl=33.11, wps=18292.8, ups=2.7, wpb=6763.9, bsz=259.4, num_updates=4300, lr=0.000192897, gnorm=1.11, clip=0, train_wall=32, wall=1471
2020-10-12 00:49:24 | INFO | train_inner | epoch 025:    152 / 177 loss=6.341, nll_loss=5.062, ppl=33.39, wps=20764.5, ups=3.06, wpb=6780.9, bsz=239.8, num_updates=4400, lr=0.000190693, gnorm=1.138, clip=0, train_wall=32, wall=1504
2020-10-12 00:49:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000841
2020-10-12 00:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036390
2020-10-12 00:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027823
2020-10-12 00:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065409
2020-10-12 00:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000762
2020-10-12 00:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036072
2020-10-12 00:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027345
2020-10-12 00:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064506
2020-10-12 00:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:49:35 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.549 | nll_loss 5.206 | ppl 36.92 | wps 46948.6 | wpb 2352.5 | bsz 87.1 | num_updates 4425 | best_loss 6.549
2020-10-12 00:49:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:49:36 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 25 @ 4425 updates, score 6.549) (writing took 1.4908616049942793 seconds)
2020-10-12 00:49:36 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 00:49:36 | INFO | train | epoch 025 | loss 6.302 | nll_loss 5.019 | ppl 32.42 | wps 19289.4 | ups 2.85 | wpb 6763.8 | bsz 251.4 | num_updates 4425 | lr 0.000190153 | gnorm 1.134 | clip 0 | train_wall 57 | wall 1516
2020-10-12 00:49:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 00:49:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 00:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:49:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000442
2020-10-12 00:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006008
2020-10-12 00:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 00:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098543
2020-10-12 00:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105137
2020-10-12 00:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004257
2020-10-12 00:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000164
2020-10-12 00:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094741
2020-10-12 00:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099495
2020-10-12 00:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:49:37 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 00:50:01 | INFO | train_inner | epoch 026:     75 / 177 loss=6.229, nll_loss=4.935, ppl=30.6, wps=18137.9, ups=2.72, wpb=6680.3, bsz=243.3, num_updates=4500, lr=0.000188562, gnorm=1.185, clip=0, train_wall=32, wall=1541
2020-10-12 00:50:33 | INFO | train_inner | epoch 026:    175 / 177 loss=6.176, nll_loss=4.872, ppl=29.28, wps=20800.7, ups=3.08, wpb=6762.1, bsz=254.4, num_updates=4600, lr=0.000186501, gnorm=1.109, clip=0, train_wall=32, wall=1573
2020-10-12 00:50:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000841
2020-10-12 00:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035957
2020-10-12 00:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027039
2020-10-12 00:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064184
2020-10-12 00:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000673
2020-10-12 00:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036058
2020-10-12 00:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027116
2020-10-12 00:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064172
2020-10-12 00:50:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:50:37 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.454 | nll_loss 5.09 | ppl 34.06 | wps 46540 | wpb 2352.5 | bsz 87.1 | num_updates 4602 | best_loss 6.454
2020-10-12 00:50:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:50:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 26 @ 4602 updates, score 6.454) (writing took 1.5010372339893365 seconds)
2020-10-12 00:50:38 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 00:50:38 | INFO | train | epoch 026 | loss 6.19 | nll_loss 4.889 | ppl 29.62 | wps 19304.5 | ups 2.85 | wpb 6763.8 | bsz 251.4 | num_updates 4602 | lr 0.00018646 | gnorm 1.135 | clip 0 | train_wall 57 | wall 1578
2020-10-12 00:50:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 00:50:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 00:50:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:50:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000475
2020-10-12 00:50:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006097
2020-10-12 00:50:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:50:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000247
2020-10-12 00:50:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:50:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097504
2020-10-12 00:50:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104188
2020-10-12 00:50:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:50:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:50:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004396
2020-10-12 00:50:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:50:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000164
2020-10-12 00:50:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097206
2020-10-12 00:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102091
2020-10-12 00:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:50:39 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 00:51:11 | INFO | train_inner | epoch 027:     98 / 177 loss=6.064, nll_loss=4.744, ppl=26.8, wps=18261.2, ups=2.69, wpb=6790.8, bsz=261.7, num_updates=4700, lr=0.000184506, gnorm=1.149, clip=0, train_wall=32, wall=1611
2020-10-12 00:51:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000840
2020-10-12 00:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035438
2020-10-12 00:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027856
2020-10-12 00:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064480
2020-10-12 00:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000613
2020-10-12 00:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036249
2020-10-12 00:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027741
2020-10-12 00:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064923
2020-10-12 00:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:51:39 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.4 | nll_loss 5.022 | ppl 32.48 | wps 46476.6 | wpb 2352.5 | bsz 87.1 | num_updates 4779 | best_loss 6.4
2020-10-12 00:51:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:51:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 27 @ 4779 updates, score 6.4) (writing took 1.5030786939896643 seconds)
2020-10-12 00:51:41 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 00:51:41 | INFO | train | epoch 027 | loss 6.075 | nll_loss 4.755 | ppl 27 | wps 19246 | ups 2.85 | wpb 6763.8 | bsz 251.4 | num_updates 4779 | lr 0.000182975 | gnorm 1.148 | clip 0 | train_wall 57 | wall 1640
2020-10-12 00:51:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 00:51:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 00:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:51:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000456
2020-10-12 00:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005808
2020-10-12 00:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 00:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099447
2020-10-12 00:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105833
2020-10-12 00:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004278
2020-10-12 00:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000164
2020-10-12 00:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098495
2020-10-12 00:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103257
2020-10-12 00:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:51:41 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 00:51:48 | INFO | train_inner | epoch 028:     21 / 177 loss=6.042, nll_loss=4.715, ppl=26.27, wps=18407.5, ups=2.7, wpb=6808.6, bsz=248.7, num_updates=4800, lr=0.000182574, gnorm=1.128, clip=0, train_wall=32, wall=1648
2020-10-12 00:52:20 | INFO | train_inner | epoch 028:    121 / 177 loss=5.977, nll_loss=4.642, ppl=24.97, wps=20438, ups=3.08, wpb=6645.1, bsz=243.5, num_updates=4900, lr=0.000180702, gnorm=1.164, clip=0, train_wall=32, wall=1680
2020-10-12 00:52:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:52:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:52:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000785
2020-10-12 00:52:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:52:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035754
2020-10-12 00:52:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:52:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027496
2020-10-12 00:52:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064376
2020-10-12 00:52:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:52:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:52:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000677
2020-10-12 00:52:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:52:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036073
2020-10-12 00:52:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:52:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027273
2020-10-12 00:52:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064346
2020-10-12 00:52:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:52:41 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.337 | nll_loss 4.946 | ppl 30.82 | wps 46367.2 | wpb 2352.5 | bsz 87.1 | num_updates 4956 | best_loss 6.337
2020-10-12 00:52:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:52:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 28 @ 4956 updates, score 6.337) (writing took 1.491218385999673 seconds)
2020-10-12 00:52:43 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 00:52:43 | INFO | train | epoch 028 | loss 5.971 | nll_loss 4.635 | ppl 24.84 | wps 19228.4 | ups 2.84 | wpb 6763.8 | bsz 251.4 | num_updates 4956 | lr 0.000179678 | gnorm 1.144 | clip 0 | train_wall 57 | wall 1703
2020-10-12 00:52:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 00:52:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 00:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:52:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000629
2020-10-12 00:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006141
2020-10-12 00:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 00:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100529
2020-10-12 00:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107187
2020-10-12 00:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004412
2020-10-12 00:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 00:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100380
2020-10-12 00:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105311
2020-10-12 00:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:52:43 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 00:52:57 | INFO | train_inner | epoch 029:     44 / 177 loss=5.911, nll_loss=4.565, ppl=23.67, wps=18481, ups=2.68, wpb=6904.1, bsz=263.6, num_updates=5000, lr=0.000178885, gnorm=1.124, clip=0, train_wall=32, wall=1717
2020-10-12 00:53:30 | INFO | train_inner | epoch 029:    144 / 177 loss=5.88, nll_loss=4.529, ppl=23.08, wps=20393.1, ups=3.07, wpb=6636.1, bsz=246.9, num_updates=5100, lr=0.000177123, gnorm=1.127, clip=0, train_wall=32, wall=1750
2020-10-12 00:53:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000771
2020-10-12 00:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035662
2020-10-12 00:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027671
2020-10-12 00:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064442
2020-10-12 00:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000615
2020-10-12 00:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036737
2020-10-12 00:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027437
2020-10-12 00:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065111
2020-10-12 00:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:53:43 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.312 | nll_loss 4.918 | ppl 30.24 | wps 46942.9 | wpb 2352.5 | bsz 87.1 | num_updates 5133 | best_loss 6.312
2020-10-12 00:53:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:53:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 29 @ 5133 updates, score 6.312) (writing took 1.4935162070032675 seconds)
2020-10-12 00:53:45 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 00:53:45 | INFO | train | epoch 029 | loss 5.869 | nll_loss 4.515 | ppl 22.87 | wps 19252.9 | ups 2.85 | wpb 6763.8 | bsz 251.4 | num_updates 5133 | lr 0.000176553 | gnorm 1.138 | clip 0 | train_wall 57 | wall 1765
2020-10-12 00:53:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 00:53:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 00:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:53:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000460
2020-10-12 00:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005754
2020-10-12 00:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 00:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104106
2020-10-12 00:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110415
2020-10-12 00:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004434
2020-10-12 00:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 00:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097713
2020-10-12 00:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102662
2020-10-12 00:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:53:45 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 00:54:07 | INFO | train_inner | epoch 030:     67 / 177 loss=5.825, nll_loss=4.465, ppl=22.08, wps=18421.8, ups=2.7, wpb=6834.2, bsz=256.5, num_updates=5200, lr=0.000175412, gnorm=1.153, clip=0, train_wall=32, wall=1787
2020-10-12 00:54:40 | INFO | train_inner | epoch 030:    167 / 177 loss=5.778, nll_loss=4.409, ppl=21.24, wps=20878.1, ups=3.07, wpb=6790.5, bsz=239.3, num_updates=5300, lr=0.000173749, gnorm=1.15, clip=0, train_wall=32, wall=1820
2020-10-12 00:54:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:54:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:54:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000781
2020-10-12 00:54:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:54:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036272
2020-10-12 00:54:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:54:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.028223
2020-10-12 00:54:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065618
2020-10-12 00:54:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:54:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:54:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000669
2020-10-12 00:54:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:54:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037251
2020-10-12 00:54:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:54:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027650
2020-10-12 00:54:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065901
2020-10-12 00:54:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:54:46 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.264 | nll_loss 4.851 | ppl 28.86 | wps 46782 | wpb 2352.5 | bsz 87.1 | num_updates 5310 | best_loss 6.264
2020-10-12 00:54:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:54:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 30 @ 5310 updates, score 6.264) (writing took 1.519940417012549 seconds)
2020-10-12 00:54:47 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 00:54:47 | INFO | train | epoch 030 | loss 5.777 | nll_loss 4.409 | ppl 21.24 | wps 19253.4 | ups 2.85 | wpb 6763.8 | bsz 251.4 | num_updates 5310 | lr 0.000173585 | gnorm 1.145 | clip 0 | train_wall 57 | wall 1827
2020-10-12 00:54:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 00:54:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 00:54:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:54:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000445
2020-10-12 00:54:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005787
2020-10-12 00:54:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:54:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 00:54:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:54:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101559
2020-10-12 00:54:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107892
2020-10-12 00:54:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:54:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:54:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004359
2020-10-12 00:54:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:54:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-12 00:54:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:54:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099406
2020-10-12 00:54:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104281
2020-10-12 00:54:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:54:47 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 00:55:17 | INFO | train_inner | epoch 031:     90 / 177 loss=5.741, nll_loss=4.366, ppl=20.62, wps=17840.2, ups=2.71, wpb=6581.5, bsz=237.3, num_updates=5400, lr=0.000172133, gnorm=1.177, clip=0, train_wall=32, wall=1856
2020-10-12 00:55:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000843
2020-10-12 00:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035972
2020-10-12 00:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027489
2020-10-12 00:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064650
2020-10-12 00:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000654
2020-10-12 00:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035170
2020-10-12 00:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.026787
2020-10-12 00:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.062927
2020-10-12 00:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:55:48 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.222 | nll_loss 4.801 | ppl 27.87 | wps 46639.6 | wpb 2352.5 | bsz 87.1 | num_updates 5487 | best_loss 6.222
2020-10-12 00:55:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:55:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 31 @ 5487 updates, score 6.222) (writing took 1.499202360995696 seconds)
2020-10-12 00:55:49 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 00:55:49 | INFO | train | epoch 031 | loss 5.693 | nll_loss 4.311 | ppl 19.84 | wps 19259.7 | ups 2.85 | wpb 6763.8 | bsz 251.4 | num_updates 5487 | lr 0.000170763 | gnorm 1.157 | clip 0 | train_wall 57 | wall 1889
2020-10-12 00:55:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 00:55:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 00:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:55:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000560
2020-10-12 00:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006041
2020-10-12 00:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 00:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100363
2020-10-12 00:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106997
2020-10-12 00:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004288
2020-10-12 00:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000164
2020-10-12 00:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:55:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093844
2020-10-12 00:55:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098630
2020-10-12 00:55:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:55:50 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 00:55:54 | INFO | train_inner | epoch 032:     13 / 177 loss=5.635, nll_loss=4.244, ppl=18.94, wps=18564.1, ups=2.68, wpb=6920, bsz=271.8, num_updates=5500, lr=0.000170561, gnorm=1.156, clip=0, train_wall=32, wall=1894
2020-10-12 00:56:26 | INFO | train_inner | epoch 032:    113 / 177 loss=5.608, nll_loss=4.211, ppl=18.52, wps=21058.5, ups=3.08, wpb=6847.9, bsz=244.9, num_updates=5600, lr=0.000169031, gnorm=1.165, clip=0, train_wall=32, wall=1926
2020-10-12 00:56:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000848
2020-10-12 00:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035485
2020-10-12 00:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027540
2020-10-12 00:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064215
2020-10-12 00:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000672
2020-10-12 00:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035769
2020-10-12 00:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027150
2020-10-12 00:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.063908
2020-10-12 00:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:56:50 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.183 | nll_loss 4.747 | ppl 26.85 | wps 46808.8 | wpb 2352.5 | bsz 87.1 | num_updates 5664 | best_loss 6.183
2020-10-12 00:56:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:56:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 32 @ 5664 updates, score 6.183) (writing took 1.5035835970047629 seconds)
2020-10-12 00:56:51 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 00:56:51 | INFO | train | epoch 032 | loss 5.614 | nll_loss 4.218 | ppl 18.61 | wps 19255 | ups 2.85 | wpb 6763.8 | bsz 251.4 | num_updates 5664 | lr 0.000168073 | gnorm 1.175 | clip 0 | train_wall 57 | wall 1951
2020-10-12 00:56:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 00:56:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:56:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000488
2020-10-12 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005809
2020-10-12 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:56:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099798
2020-10-12 00:56:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106252
2020-10-12 00:56:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:56:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:56:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005971
2020-10-12 00:56:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:56:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 00:56:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:56:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097409
2020-10-12 00:56:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103876
2020-10-12 00:56:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:56:52 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 00:57:03 | INFO | train_inner | epoch 033:     36 / 177 loss=5.564, nll_loss=4.161, ppl=17.88, wps=17976.9, ups=2.69, wpb=6676.8, bsz=267.1, num_updates=5700, lr=0.000167542, gnorm=1.138, clip=0, train_wall=32, wall=1963
2020-10-12 00:57:36 | INFO | train_inner | epoch 033:    136 / 177 loss=5.559, nll_loss=4.153, ppl=17.79, wps=20812.8, ups=3.06, wpb=6809.9, bsz=241.3, num_updates=5800, lr=0.000166091, gnorm=1.182, clip=0, train_wall=32, wall=1996
2020-10-12 00:57:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:57:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:57:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000845
2020-10-12 00:57:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:57:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036667
2020-10-12 00:57:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:57:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027364
2020-10-12 00:57:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065220
2020-10-12 00:57:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:57:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:57:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000651
2020-10-12 00:57:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:57:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036349
2020-10-12 00:57:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:57:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.026843
2020-10-12 00:57:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064167
2020-10-12 00:57:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:57:52 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.163 | nll_loss 4.719 | ppl 26.33 | wps 46801.4 | wpb 2352.5 | bsz 87.1 | num_updates 5841 | best_loss 6.163
2020-10-12 00:57:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:57:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 33 @ 5841 updates, score 6.163) (writing took 1.4936176479968708 seconds)
2020-10-12 00:57:54 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 00:57:54 | INFO | train | epoch 033 | loss 5.53 | nll_loss 4.12 | ppl 17.38 | wps 19244.3 | ups 2.85 | wpb 6763.8 | bsz 251.4 | num_updates 5841 | lr 0.000165507 | gnorm 1.155 | clip 0 | train_wall 57 | wall 2014
2020-10-12 00:57:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 00:57:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 00:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:57:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000441
2020-10-12 00:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005794
2020-10-12 00:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000256
2020-10-12 00:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102134
2020-10-12 00:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108529
2020-10-12 00:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004332
2020-10-12 00:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 00:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099223
2020-10-12 00:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104042
2020-10-12 00:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:57:54 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 00:58:13 | INFO | train_inner | epoch 034:     59 / 177 loss=5.455, nll_loss=4.033, ppl=16.37, wps=18168.1, ups=2.72, wpb=6687, bsz=252.8, num_updates=5900, lr=0.000164677, gnorm=1.119, clip=0, train_wall=32, wall=2033
2020-10-12 00:58:46 | INFO | train_inner | epoch 034:    159 / 177 loss=5.467, nll_loss=4.045, ppl=16.51, wps=20496.5, ups=3.04, wpb=6745.2, bsz=255.6, num_updates=6000, lr=0.000163299, gnorm=1.169, clip=0, train_wall=32, wall=2066
2020-10-12 00:58:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.006999
2020-10-12 00:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035394
2020-10-12 00:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027784
2020-10-12 00:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070562
2020-10-12 00:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000678
2020-10-12 00:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036475
2020-10-12 00:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027903
2020-10-12 00:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065382
2020-10-12 00:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:58:54 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.129 | nll_loss 4.67 | ppl 25.46 | wps 46870.4 | wpb 2352.5 | bsz 87.1 | num_updates 6018 | best_loss 6.129
2020-10-12 00:58:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:58:56 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 34 @ 6018 updates, score 6.129) (writing took 1.4962744939984987 seconds)
2020-10-12 00:58:56 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 00:58:56 | INFO | train | epoch 034 | loss 5.455 | nll_loss 4.032 | ppl 16.36 | wps 19217.2 | ups 2.84 | wpb 6763.8 | bsz 251.4 | num_updates 6018 | lr 0.000163055 | gnorm 1.138 | clip 0 | train_wall 57 | wall 2076
2020-10-12 00:58:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 00:58:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 00:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:58:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000431
2020-10-12 00:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005726
2020-10-12 00:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 00:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104469
2020-10-12 00:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110784
2020-10-12 00:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004291
2020-10-12 00:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 00:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100191
2020-10-12 00:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104989
2020-10-12 00:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:58:56 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 00:59:23 | INFO | train_inner | epoch 035:     82 / 177 loss=5.423, nll_loss=3.993, ppl=15.92, wps=18672.5, ups=2.66, wpb=7010.9, bsz=252.9, num_updates=6100, lr=0.000161955, gnorm=1.194, clip=0, train_wall=32, wall=2103
2020-10-12 00:59:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 00:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000770
2020-10-12 00:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036447
2020-10-12 00:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.028284
2020-10-12 00:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065846
2020-10-12 00:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000679
2020-10-12 00:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036840
2020-10-12 00:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027557
2020-10-12 00:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065406
2020-10-12 00:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:59:57 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.13 | nll_loss 4.671 | ppl 25.48 | wps 46633.6 | wpb 2352.5 | bsz 87.1 | num_updates 6195 | best_loss 6.129
2020-10-12 00:59:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 00:59:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_last.pt (epoch 35 @ 6195 updates, score 6.13) (writing took 1.0372780700126896 seconds)
2020-10-12 00:59:58 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 00:59:58 | INFO | train | epoch 035 | loss 5.389 | nll_loss 3.955 | ppl 15.51 | wps 19369 | ups 2.86 | wpb 6763.8 | bsz 251.4 | num_updates 6195 | lr 0.000160709 | gnorm 1.189 | clip 0 | train_wall 57 | wall 2138
2020-10-12 00:59:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 00:59:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 00:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:59:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000476
2020-10-12 00:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005410
2020-10-12 00:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-12 00:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101022
2020-10-12 00:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106922
2020-10-12 00:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 00:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004300
2020-10-12 00:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-12 00:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095236
2020-10-12 00:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100024
2020-10-12 00:59:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 00:59:58 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 00:59:59 | INFO | train_inner | epoch 036:      5 / 177 loss=5.384, nll_loss=3.949, ppl=15.44, wps=17995.2, ups=2.77, wpb=6486.1, bsz=235.3, num_updates=6200, lr=0.000160644, gnorm=1.181, clip=0, train_wall=31, wall=2139
2020-10-12 01:00:32 | INFO | train_inner | epoch 036:    105 / 177 loss=5.322, nll_loss=3.876, ppl=14.69, wps=20736.4, ups=3.04, wpb=6812.9, bsz=260.1, num_updates=6300, lr=0.000159364, gnorm=1.184, clip=0, train_wall=32, wall=2172
2020-10-12 01:00:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000777
2020-10-12 01:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036079
2020-10-12 01:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.028082
2020-10-12 01:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065276
2020-10-12 01:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000669
2020-10-12 01:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035819
2020-10-12 01:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027768
2020-10-12 01:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064588
2020-10-12 01:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:00:59 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.101 | nll_loss 4.633 | ppl 24.8 | wps 46514.6 | wpb 2352.5 | bsz 87.1 | num_updates 6372 | best_loss 6.101
2020-10-12 01:00:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:01:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 36 @ 6372 updates, score 6.101) (writing took 1.4996518179978011 seconds)
2020-10-12 01:01:00 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 01:01:00 | INFO | train | epoch 036 | loss 5.322 | nll_loss 3.876 | ppl 14.68 | wps 19189.5 | ups 2.84 | wpb 6763.8 | bsz 251.4 | num_updates 6372 | lr 0.000158461 | gnorm 1.191 | clip 0 | train_wall 57 | wall 2200
2020-10-12 01:01:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 01:01:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 01:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:01:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000505
2020-10-12 01:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005846
2020-10-12 01:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000224
2020-10-12 01:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101811
2020-10-12 01:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108228
2020-10-12 01:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004305
2020-10-12 01:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 01:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097298
2020-10-12 01:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102107
2020-10-12 01:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:01:00 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 01:01:10 | INFO | train_inner | epoch 037:     28 / 177 loss=5.281, nll_loss=3.829, ppl=14.21, wps=18151, ups=2.69, wpb=6748.9, bsz=252, num_updates=6400, lr=0.000158114, gnorm=1.152, clip=0, train_wall=32, wall=2209
2020-10-12 01:01:42 | INFO | train_inner | epoch 037:    128 / 177 loss=5.278, nll_loss=3.824, ppl=14.17, wps=20845.6, ups=3.04, wpb=6849.8, bsz=246.7, num_updates=6500, lr=0.000156893, gnorm=1.202, clip=0, train_wall=32, wall=2242
2020-10-12 01:01:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000713
2020-10-12 01:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037160
2020-10-12 01:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027818
2020-10-12 01:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.066031
2020-10-12 01:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000641
2020-10-12 01:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035613
2020-10-12 01:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027409
2020-10-12 01:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.063982
2020-10-12 01:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:02:01 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.104 | nll_loss 4.626 | ppl 24.7 | wps 46682.7 | wpb 2352.5 | bsz 87.1 | num_updates 6549 | best_loss 6.101
2020-10-12 01:02:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:02:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_last.pt (epoch 37 @ 6549 updates, score 6.104) (writing took 1.016456790996017 seconds)
2020-10-12 01:02:02 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 01:02:02 | INFO | train | epoch 037 | loss 5.25 | nll_loss 3.792 | ppl 13.85 | wps 19332 | ups 2.86 | wpb 6763.8 | bsz 251.4 | num_updates 6549 | lr 0.000156305 | gnorm 1.148 | clip 0 | train_wall 57 | wall 2262
2020-10-12 01:02:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 01:02:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 01:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:02:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000493
2020-10-12 01:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005528
2020-10-12 01:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000190
2020-10-12 01:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097770
2020-10-12 01:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103819
2020-10-12 01:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004266
2020-10-12 01:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 01:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098205
2020-10-12 01:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102973
2020-10-12 01:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:02:02 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 01:02:19 | INFO | train_inner | epoch 038:     51 / 177 loss=5.176, nll_loss=3.707, ppl=13.06, wps=18476.7, ups=2.74, wpb=6740.7, bsz=260.6, num_updates=6600, lr=0.0001557, gnorm=1.175, clip=0, train_wall=32, wall=2279
2020-10-12 01:02:52 | INFO | train_inner | epoch 038:    151 / 177 loss=5.22, nll_loss=3.755, ppl=13.5, wps=20486.2, ups=3.05, wpb=6709.8, bsz=249, num_updates=6700, lr=0.000154533, gnorm=1.178, clip=0, train_wall=32, wall=2312
2020-10-12 01:03:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000849
2020-10-12 01:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.037129
2020-10-12 01:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027367
2020-10-12 01:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065691
2020-10-12 01:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000649
2020-10-12 01:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036003
2020-10-12 01:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.026974
2020-10-12 01:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.063946
2020-10-12 01:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:03:03 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.061 | nll_loss 4.579 | ppl 23.91 | wps 46531 | wpb 2352.5 | bsz 87.1 | num_updates 6726 | best_loss 6.061
2020-10-12 01:03:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:03:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 38 @ 6726 updates, score 6.061) (writing took 1.4898115170071833 seconds)
2020-10-12 01:03:04 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 01:03:04 | INFO | train | epoch 038 | loss 5.194 | nll_loss 3.727 | ppl 13.24 | wps 19236.5 | ups 2.84 | wpb 6763.8 | bsz 251.4 | num_updates 6726 | lr 0.000154235 | gnorm 1.195 | clip 0 | train_wall 57 | wall 2324
2020-10-12 01:03:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 01:03:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 01:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:03:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000561
2020-10-12 01:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006002
2020-10-12 01:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000286
2020-10-12 01:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099913
2020-10-12 01:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106554
2020-10-12 01:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004290
2020-10-12 01:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 01:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097734
2020-10-12 01:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102537
2020-10-12 01:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:03:05 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 01:03:29 | INFO | train_inner | epoch 039:     74 / 177 loss=5.148, nll_loss=3.673, ppl=12.76, wps=18349.3, ups=2.71, wpb=6775, bsz=236, num_updates=6800, lr=0.000153393, gnorm=1.158, clip=0, train_wall=32, wall=2348
2020-10-12 01:04:01 | INFO | train_inner | epoch 039:    174 / 177 loss=5.148, nll_loss=3.671, ppl=12.74, wps=20562.5, ups=3.04, wpb=6763.6, bsz=263.1, num_updates=6900, lr=0.000152277, gnorm=1.217, clip=0, train_wall=32, wall=2381
2020-10-12 01:04:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000835
2020-10-12 01:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036395
2020-10-12 01:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027516
2020-10-12 01:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.065087
2020-10-12 01:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000656
2020-10-12 01:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036707
2020-10-12 01:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027235
2020-10-12 01:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064923
2020-10-12 01:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:04:05 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.034 | nll_loss 4.54 | ppl 23.26 | wps 46677.8 | wpb 2352.5 | bsz 87.1 | num_updates 6903 | best_loss 6.034
2020-10-12 01:04:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:04:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 39 @ 6903 updates, score 6.034) (writing took 1.5127086089923978 seconds)
2020-10-12 01:04:07 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 01:04:07 | INFO | train | epoch 039 | loss 5.138 | nll_loss 3.66 | ppl 12.64 | wps 19215.4 | ups 2.84 | wpb 6763.8 | bsz 251.4 | num_updates 6903 | lr 0.000152244 | gnorm 1.193 | clip 0 | train_wall 57 | wall 2387
2020-10-12 01:04:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 01:04:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 01:04:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:04:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000481
2020-10-12 01:04:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005416
2020-10-12 01:04:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:04:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 01:04:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:04:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097023
2020-10-12 01:04:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102931
2020-10-12 01:04:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:04:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:04:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004375
2020-10-12 01:04:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:04:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 01:04:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:04:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098995
2020-10-12 01:04:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103872
2020-10-12 01:04:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:04:07 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 01:04:38 | INFO | train_inner | epoch 040:     97 / 177 loss=5.066, nll_loss=3.578, ppl=11.94, wps=18351.4, ups=2.7, wpb=6802.6, bsz=250.5, num_updates=7000, lr=0.000151186, gnorm=1.185, clip=0, train_wall=32, wall=2418
2020-10-12 01:05:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000788
2020-10-12 01:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.036024
2020-10-12 01:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027504
2020-10-12 01:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064661
2020-10-12 01:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000667
2020-10-12 01:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.035561
2020-10-12 01:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.027732
2020-10-12 01:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.064286
2020-10-12 01:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:05:07 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.025 | nll_loss 4.528 | ppl 23.07 | wps 46363.2 | wpb 2352.5 | bsz 87.1 | num_updates 7080 | best_loss 6.025
2020-10-12 01:05:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:05:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belpol_sepspm8000/M2O/checkpoint_best.pt (epoch 40 @ 7080 updates, score 6.025) (writing took 1.5025519549963064 seconds)
2020-10-12 01:05:09 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 01:05:09 | INFO | train | epoch 040 | loss 5.074 | nll_loss 3.586 | ppl 12.01 | wps 19209.9 | ups 2.84 | wpb 6763.8 | bsz 251.4 | num_updates 7080 | lr 0.000150329 | gnorm 1.183 | clip 0 | train_wall 57 | wall 2449
2020-10-12 01:05:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 01:05:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 01:05:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:05:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000481
2020-10-12 01:05:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005659
2020-10-12 01:05:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:05:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 01:05:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:05:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100027
2020-10-12 01:05:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106231
2020-10-12 01:05:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:05:09 | INFO | fairseq_cli.train | done training in 2448.9 seconds
