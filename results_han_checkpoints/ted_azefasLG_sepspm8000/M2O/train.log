2020-10-12 22:05:32 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:11011
2020-10-12 22:05:32 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:11011
2020-10-12 22:05:32 | INFO | fairseq.distributed_utils | initialized host ip-172-31-31-94 as rank 2
2020-10-12 22:05:32 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:11011
2020-10-12 22:05:32 | INFO | fairseq.distributed_utils | initialized host ip-172-31-31-94 as rank 1
2020-10-12 22:05:32 | INFO | fairseq.distributed_utils | initialized host ip-172-31-31-94 as rank 0
2020-10-12 22:05:36 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 22:05:36 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 22:05:36 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_azefasLG_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:11011', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='aze-eng,fas-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=3, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 22:05:36 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 22:05:36 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['aze', 'eng', 'fas']
2020-10-12 22:05:36 | INFO | fairseq.data.multilingual.multilingual_data_manager | [aze] dictionary: 22355 types
2020-10-12 22:05:36 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 22355 types
2020-10-12 22:05:36 | INFO | fairseq.data.multilingual.multilingual_data_manager | [fas] dictionary: 22355 types
2020-10-12 22:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 22:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7563.7890625Mb; avail=481011.07421875Mb
2020-10-12 22:05:36 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 22:05:36 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:aze-eng': 1, 'main:fas-eng': 1}
2020-10-12 22:05:36 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-12 22:05:36 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azefasLG_sepspm8000/M2O/valid.aze-eng.aze
2020-10-12 22:05:36 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azefasLG_sepspm8000/M2O/valid.aze-eng.eng
2020-10-12 22:05:36 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azefasLG_sepspm8000/M2O/ valid aze-eng 671 examples
2020-10-12 22:05:36 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:fas-eng src_langtok: None; tgt_langtok: None
2020-10-12 22:05:36 | INFO | fairseq.data.data_utils | loaded 3930 examples from: fairseq/data-bin/ted_azefasLG_sepspm8000/M2O/valid.fas-eng.fas
2020-10-12 22:05:36 | INFO | fairseq.data.data_utils | loaded 3930 examples from: fairseq/data-bin/ted_azefasLG_sepspm8000/M2O/valid.fas-eng.eng
2020-10-12 22:05:36 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azefasLG_sepspm8000/M2O/ valid fas-eng 3930 examples
2020-10-12 22:05:37 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22355, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22355, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=22355, bias=False)
  )
)
2020-10-12 22:05:37 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 22:05:37 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 22:05:37 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 22:05:37 | INFO | fairseq_cli.train | num. model params: 42989056 (num. trained: 42989056)
2020-10-12 22:05:37 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 22:05:37 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 22:05:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-10-12 22:05:37 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 22:05:37 | INFO | fairseq.utils | rank   1: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 22:05:37 | INFO | fairseq.utils | rank   2: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 22:05:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-10-12 22:05:37 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2020-10-12 22:05:37 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 22:05:37 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_last.pt
2020-10-12 22:05:37 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 22:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 22:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7932.3359375Mb; avail=480642.52734375Mb
2020-10-12 22:05:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 22:05:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:aze-eng': 1, 'main:fas-eng': 1}
2020-10-12 22:05:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-12 22:05:37 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azefasLG_sepspm8000/M2O/train.aze-eng.aze
2020-10-12 22:05:37 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azefasLG_sepspm8000/M2O/train.aze-eng.eng
2020-10-12 22:05:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azefasLG_sepspm8000/M2O/ train aze-eng 5946 examples
2020-10-12 22:05:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:fas-eng src_langtok: None; tgt_langtok: None
2020-10-12 22:05:37 | INFO | fairseq.data.data_utils | loaded 150817 examples from: fairseq/data-bin/ted_azefasLG_sepspm8000/M2O/train.fas-eng.fas
2020-10-12 22:05:37 | INFO | fairseq.data.data_utils | loaded 150817 examples from: fairseq/data-bin/ted_azefasLG_sepspm8000/M2O/train.fas-eng.eng
2020-10-12 22:05:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azefasLG_sepspm8000/M2O/ train fas-eng 150817 examples
2020-10-12 22:05:37 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 156763
2020-10-12 22:05:37 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 156763
2020-10-12 22:05:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:aze-eng', 5946), ('main:fas-eng', 150817)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 22:05:37 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 22:05:37 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 156763
2020-10-12 22:05:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 156763; virtual dataset size 156763
2020-10-12 22:05:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 22:05:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:aze-eng': 5946, 'main:fas-eng': 150817}; raw total size: 156763
2020-10-12 22:05:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:aze-eng': 5946, 'main:fas-eng': 150817}; resampled total size: 156763
2020-10-12 22:05:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 22:05:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.017193
2020-10-12 22:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7936.03125Mb; avail=480638.83203125Mb
2020-10-12 22:05:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002587
2020-10-12 22:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.030348
2020-10-12 22:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7936.03125Mb; avail=480638.83203125Mb
2020-10-12 22:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001132
2020-10-12 22:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7936.03125Mb; avail=480638.83203125Mb
2020-10-12 22:05:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.547485
2020-10-12 22:05:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.580068
2020-10-12 22:05:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7951.8515625Mb; avail=480623.01171875Mb
2020-10-12 22:05:38 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 22:05:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7950.22265625Mb; avail=480624.640625Mb
2020-10-12 22:05:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023787
2020-10-12 22:05:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7943.82421875Mb; avail=480631.0390625Mb
2020-10-12 22:05:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001112
2020-10-12 22:05:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7943.82421875Mb; avail=480631.0390625Mb
2020-10-12 22:05:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.549096
2020-10-12 22:05:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.575378
2020-10-12 22:05:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7946.65625Mb; avail=480628.20703125Mb
2020-10-12 22:05:38 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 22:06:13 | INFO | train_inner | epoch 001:    100 / 197 loss=14.133, nll_loss=14.017, ppl=16579.8, wps=64967.1, ups=3.03, wpb=21406.9, bsz=821.3, num_updates=100, lr=5.0975e-06, gnorm=3.896, clip=0, train_wall=33, wall=36
2020-10-12 22:06:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:06:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20847.01171875Mb; avail=467656.42578125Mb
2020-10-12 22:06:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001750
2020-10-12 22:06:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20847.01171875Mb; avail=467656.42578125Mb
2020-10-12 22:06:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071965
2020-10-12 22:06:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20846.4375Mb; avail=467656.67578125Mb
2020-10-12 22:06:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048749
2020-10-12 22:06:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123261
2020-10-12 22:06:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20846.3046875Mb; avail=467656.91796875Mb
2020-10-12 22:06:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20846.296875Mb; avail=467656.92578125Mb
2020-10-12 22:06:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001125
2020-10-12 22:06:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20846.296875Mb; avail=467656.92578125Mb
2020-10-12 22:06:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071270
2020-10-12 22:06:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20846.17578125Mb; avail=467657.05078125Mb
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2020-10-12 22:06:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048881
2020-10-12 22:06:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122061
2020-10-12 22:06:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20848.11328125Mb; avail=467655.11328125Mb
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2020-10-12 22:06:47 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.603 | nll_loss 11.19 | ppl 2335.77 | wps 126446 | wpb 6713.4 | bsz 255.6 | num_updates 197
2020-10-12 22:06:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:06:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 1 @ 197 updates, score 11.603) (writing took 1.638065146000372 seconds)
2020-10-12 22:06:49 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 22:06:49 | INFO | train | epoch 001 | loss 13.302 | nll_loss 13.091 | ppl 8726.15 | wps 61284.5 | ups 2.89 | wpb 21229.6 | bsz 795.8 | num_updates 197 | lr 9.94508e-06 | gnorm 2.834 | clip 0 | train_wall 62 | wall 72
2020-10-12 22:06:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 22:06:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 22:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20951.45703125Mb; avail=467536.78515625Mb
2020-10-12 22:06:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004423
2020-10-12 22:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032482
2020-10-12 22:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20961.65625Mb; avail=467526.5859375Mb
2020-10-12 22:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001180
2020-10-12 22:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20962.8671875Mb; avail=467525.375Mb
2020-10-12 22:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.574578
2020-10-12 22:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.609233
2020-10-12 22:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21048.73828125Mb; avail=467439.94921875Mb
2020-10-12 22:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21046.84375Mb; avail=467441.84375Mb
2020-10-12 22:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024414
2020-10-12 22:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21046.84375Mb; avail=467441.84375Mb
2020-10-12 22:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001339
2020-10-12 22:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21046.84375Mb; avail=467441.84375Mb
2020-10-12 22:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.573563
2020-10-12 22:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.600276
2020-10-12 22:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21109.9765625Mb; avail=467377.20703125Mb
2020-10-12 22:06:50 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 22:06:52 | INFO | train_inner | epoch 002:      3 / 197 loss=12.417, nll_loss=12.105, ppl=4406.1, wps=54417.7, ups=2.58, wpb=21068.5, bsz=776.4, num_updates=200, lr=1.0095e-05, gnorm=1.737, clip=0, train_wall=31, wall=75
2020-10-12 22:07:24 | INFO | train_inner | epoch 002:    103 / 197 loss=11.459, nll_loss=11.034, ppl=2097.26, wps=65935.2, ups=3.09, wpb=21350.8, bsz=792.3, num_updates=300, lr=1.50925e-05, gnorm=1.533, clip=0, train_wall=31, wall=107
2020-10-12 22:07:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20997.8671875Mb; avail=467490.40625Mb
2020-10-12 22:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001512
2020-10-12 22:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20997.8671875Mb; avail=467490.40625Mb
2020-10-12 22:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070591
2020-10-12 22:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20997.87890625Mb; avail=467490.390625Mb
2020-10-12 22:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049797
2020-10-12 22:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122725
2020-10-12 22:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20996.26171875Mb; avail=467492.3515625Mb
2020-10-12 22:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20996.70703125Mb; avail=467491.984375Mb
2020-10-12 22:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001158
2020-10-12 22:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20996.328125Mb; avail=467491.87109375Mb
2020-10-12 22:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070737
2020-10-12 22:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20995.83984375Mb; avail=467492.890625Mb
2020-10-12 22:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049047
2020-10-12 22:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121701
2020-10-12 22:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20995.640625Mb; avail=467493.08984375Mb
2020-10-12 22:07:58 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.557 | nll_loss 8.795 | ppl 444.29 | wps 124227 | wpb 6713.4 | bsz 255.6 | num_updates 394 | best_loss 9.557
2020-10-12 22:07:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:08:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 2 @ 394 updates, score 9.557) (writing took 4.469781547000821 seconds)
2020-10-12 22:08:02 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 22:08:02 | INFO | train | epoch 002 | loss 10.912 | nll_loss 10.407 | ppl 1357.34 | wps 56887.3 | ups 2.68 | wpb 21229.6 | bsz 795.8 | num_updates 394 | lr 1.97901e-05 | gnorm 1.425 | clip 0 | train_wall 62 | wall 145
2020-10-12 22:08:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 22:08:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 22:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21203.39453125Mb; avail=467284.8203125Mb
2020-10-12 22:08:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004900
2020-10-12 22:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035849
2020-10-12 22:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21204.7265625Mb; avail=467283.48828125Mb
2020-10-12 22:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001258
2020-10-12 22:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21204.7265625Mb; avail=467283.48828125Mb
2020-10-12 22:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.685888
2020-10-12 22:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.724047
2020-10-12 22:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21361.21484375Mb; avail=467127.30859375Mb
2020-10-12 22:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21365.6015625Mb; avail=467122.921875Mb
2020-10-12 22:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023328
2020-10-12 22:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21367.30859375Mb; avail=467121.21484375Mb
2020-10-12 22:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001328
2020-10-12 22:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21367.30859375Mb; avail=467121.21484375Mb
2020-10-12 22:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.545470
2020-10-12 22:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.570993
2020-10-12 22:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21436.83203125Mb; avail=467050.234375Mb
2020-10-12 22:08:03 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 22:08:07 | INFO | train_inner | epoch 003:      6 / 197 loss=10.261, nll_loss=9.66, ppl=808.73, wps=49934.8, ups=2.37, wpb=21091.3, bsz=790.2, num_updates=400, lr=2.009e-05, gnorm=1.268, clip=0, train_wall=31, wall=150
2020-10-12 22:08:39 | INFO | train_inner | epoch 003:    106 / 197 loss=9.549, nll_loss=8.814, ppl=450.2, wps=65133.8, ups=3.09, wpb=21093.1, bsz=761.4, num_updates=500, lr=2.50875e-05, gnorm=1.03, clip=0, train_wall=31, wall=182
2020-10-12 22:09:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21114.609375Mb; avail=467372.89453125Mb
2020-10-12 22:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001594
2020-10-12 22:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21114.48828125Mb; avail=467373.015625Mb
2020-10-12 22:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070885
2020-10-12 22:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21114.390625Mb; avail=467373.015625Mb
2020-10-12 22:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049648
2020-10-12 22:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123983
2020-10-12 22:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21114.46875Mb; avail=467373.015625Mb
2020-10-12 22:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21114.46875Mb; avail=467373.015625Mb
2020-10-12 22:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001096
2020-10-12 22:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21114.46875Mb; avail=467373.015625Mb
2020-10-12 22:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072134
2020-10-12 22:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21114.4609375Mb; avail=467373.0234375Mb
2020-10-12 22:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048943
2020-10-12 22:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122972
2020-10-12 22:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21114.4609375Mb; avail=467373.0234375Mb
2020-10-12 22:09:11 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.832 | nll_loss 7.894 | ppl 237.89 | wps 132868 | wpb 6713.4 | bsz 255.6 | num_updates 591 | best_loss 8.832
2020-10-12 22:09:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:09:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 3 @ 591 updates, score 8.832) (writing took 4.465414062000491 seconds)
2020-10-12 22:09:16 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 22:09:16 | INFO | train | epoch 003 | loss 9.396 | nll_loss 8.622 | ppl 394.05 | wps 56684.9 | ups 2.67 | wpb 21229.6 | bsz 795.8 | num_updates 591 | lr 2.96352e-05 | gnorm 1.092 | clip 0 | train_wall 62 | wall 219
2020-10-12 22:09:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 22:09:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 22:09:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21277.96875Mb; avail=467209.5390625Mb
2020-10-12 22:09:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004620
2020-10-12 22:09:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.039313
2020-10-12 22:09:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21279.015625Mb; avail=467208.8125Mb
2020-10-12 22:09:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001317
2020-10-12 22:09:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21279.015625Mb; avail=467208.8125Mb
2020-10-12 22:09:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.554085
2020-10-12 22:09:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.595852
2020-10-12 22:09:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21283.5390625Mb; avail=467204.08984375Mb
2020-10-12 22:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21280.12109375Mb; avail=467207.29296875Mb
2020-10-12 22:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022932
2020-10-12 22:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21280.31640625Mb; avail=467207.4140625Mb
2020-10-12 22:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001169
2020-10-12 22:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21280.31640625Mb; avail=467207.4140625Mb
2020-10-12 22:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.548487
2020-10-12 22:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.573442
2020-10-12 22:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21280.015625Mb; avail=467207.76953125Mb
2020-10-12 22:09:17 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 22:09:21 | INFO | train_inner | epoch 004:      9 / 197 loss=9.185, nll_loss=8.359, ppl=328.29, wps=50663.9, ups=2.37, wpb=21416.2, bsz=848, num_updates=600, lr=3.0085e-05, gnorm=1.159, clip=0, train_wall=31, wall=224
2020-10-12 22:09:54 | INFO | train_inner | epoch 004:    109 / 197 loss=8.975, nll_loss=8.097, ppl=273.79, wps=65830.6, ups=3.08, wpb=21359.2, bsz=759.5, num_updates=700, lr=3.50825e-05, gnorm=1.063, clip=0, train_wall=31, wall=257
2020-10-12 22:10:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21130.17578125Mb; avail=467358.41796875Mb
2020-10-12 22:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001562
2020-10-12 22:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21130.17578125Mb; avail=467358.41796875Mb
2020-10-12 22:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071074
2020-10-12 22:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21130.17578125Mb; avail=467358.41796875Mb
2020-10-12 22:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049889
2020-10-12 22:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123324
2020-10-12 22:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21130.17578125Mb; avail=467358.41796875Mb
2020-10-12 22:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21130.17578125Mb; avail=467358.41796875Mb
2020-10-12 22:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001154
2020-10-12 22:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21130.17578125Mb; avail=467358.41796875Mb
2020-10-12 22:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072019
2020-10-12 22:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21130.17578125Mb; avail=467358.41796875Mb
2020-10-12 22:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050102
2020-10-12 22:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124050
2020-10-12 22:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21130.0625Mb; avail=467358.5390625Mb
2020-10-12 22:10:25 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.393 | nll_loss 7.38 | ppl 166.52 | wps 125942 | wpb 6713.4 | bsz 255.6 | num_updates 788 | best_loss 8.393
2020-10-12 22:10:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:10:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 4 @ 788 updates, score 8.393) (writing took 4.487666773999081 seconds)
2020-10-12 22:10:29 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 22:10:29 | INFO | train | epoch 004 | loss 8.904 | nll_loss 8.014 | ppl 258.57 | wps 57032.7 | ups 2.69 | wpb 21229.6 | bsz 795.8 | num_updates 788 | lr 3.94803e-05 | gnorm 1.132 | clip 0 | train_wall 61 | wall 292
2020-10-12 22:10:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 22:10:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 22:10:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21297.3359375Mb; avail=467190.4140625Mb
2020-10-12 22:10:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004198
2020-10-12 22:10:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033350
2020-10-12 22:10:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21297.94140625Mb; avail=467189.80859375Mb
2020-10-12 22:10:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001243
2020-10-12 22:10:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21297.94140625Mb; avail=467189.80859375Mb
2020-10-12 22:10:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.543730
2020-10-12 22:10:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.579218
2020-10-12 22:10:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21301.69140625Mb; avail=467186.0859375Mb
2020-10-12 22:10:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21298.69921875Mb; avail=467188.68359375Mb
2020-10-12 22:10:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022870
2020-10-12 22:10:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21298.8203125Mb; avail=467188.68359375Mb
2020-10-12 22:10:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001163
2020-10-12 22:10:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21298.8203125Mb; avail=467188.68359375Mb
2020-10-12 22:10:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.542566
2020-10-12 22:10:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.567448
2020-10-12 22:10:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21298.890625Mb; avail=467189.23828125Mb
2020-10-12 22:10:30 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 22:10:36 | INFO | train_inner | epoch 005:     12 / 197 loss=8.799, nll_loss=7.891, ppl=237.3, wps=50013.8, ups=2.39, wpb=20947.4, bsz=806.7, num_updates=800, lr=4.008e-05, gnorm=1.222, clip=0, train_wall=31, wall=299
2020-10-12 22:11:08 | INFO | train_inner | epoch 005:    112 / 197 loss=8.603, nll_loss=7.664, ppl=202.83, wps=65917.2, ups=3.09, wpb=21348.7, bsz=794.4, num_updates=900, lr=4.50775e-05, gnorm=1.078, clip=0, train_wall=31, wall=331
2020-10-12 22:11:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21136.82421875Mb; avail=467350.375Mb
2020-10-12 22:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001582
2020-10-12 22:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21136.82421875Mb; avail=467350.375Mb
2020-10-12 22:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076636
2020-10-12 22:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21136.5625Mb; avail=467350.3828125Mb
2020-10-12 22:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049897
2020-10-12 22:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128961
2020-10-12 22:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21136.5703125Mb; avail=467350.26171875Mb
2020-10-12 22:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21136.5703125Mb; avail=467350.26171875Mb
2020-10-12 22:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001202
2020-10-12 22:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21136.5703125Mb; avail=467350.26171875Mb
2020-10-12 22:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071697
2020-10-12 22:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21136.5703125Mb; avail=467350.26171875Mb
2020-10-12 22:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049178
2020-10-12 22:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122845
2020-10-12 22:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21135.640625Mb; avail=467352.9296875Mb
2020-10-12 22:11:38 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.058 | nll_loss 7 | ppl 128.02 | wps 117573 | wpb 6713.4 | bsz 255.6 | num_updates 985 | best_loss 8.058
2020-10-12 22:11:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:11:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 5 @ 985 updates, score 8.058) (writing took 4.478305366999848 seconds)
2020-10-12 22:11:43 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 22:11:43 | INFO | train | epoch 005 | loss 8.532 | nll_loss 7.583 | ppl 191.76 | wps 56745 | ups 2.67 | wpb 21229.6 | bsz 795.8 | num_updates 985 | lr 4.93254e-05 | gnorm 1.133 | clip 0 | train_wall 62 | wall 366
2020-10-12 22:11:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 22:11:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 22:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21561.81640625Mb; avail=466925.5Mb
2020-10-12 22:11:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004629
2020-10-12 22:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.038453
2020-10-12 22:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21563.0078125Mb; avail=466924.41015625Mb
2020-10-12 22:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001408
2020-10-12 22:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21563.0078125Mb; avail=466924.41015625Mb
2020-10-12 22:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.545829
2020-10-12 22:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.586898
2020-10-12 22:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21567.55078125Mb; avail=466920.421875Mb
2020-10-12 22:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21564.10546875Mb; avail=466923.8671875Mb
2020-10-12 22:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023842
2020-10-12 22:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21542.44921875Mb; avail=466947.0Mb
2020-10-12 22:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001427
2020-10-12 22:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21537.03515625Mb; avail=466951.4296875Mb
2020-10-12 22:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.590815
2020-10-12 22:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.617158
2020-10-12 22:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21658.00390625Mb; avail=466831.13671875Mb
2020-10-12 22:11:44 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 22:11:50 | INFO | train_inner | epoch 006:     15 / 197 loss=8.415, nll_loss=7.449, ppl=174.75, wps=50421.9, ups=2.36, wpb=21322, bsz=806.1, num_updates=1000, lr=5.0075e-05, gnorm=1.17, clip=0, train_wall=31, wall=373
2020-10-12 22:12:23 | INFO | train_inner | epoch 006:    115 / 197 loss=8.286, nll_loss=7.301, ppl=157.74, wps=65267.6, ups=3.1, wpb=21028.6, bsz=782.2, num_updates=1100, lr=5.50725e-05, gnorm=1.143, clip=0, train_wall=31, wall=406
2020-10-12 22:12:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21152.98828125Mb; avail=467334.80859375Mb
2020-10-12 22:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001634
2020-10-12 22:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21153.00390625Mb; avail=467334.6875Mb
2020-10-12 22:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070443
2020-10-12 22:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21152.97265625Mb; avail=467334.9296875Mb
2020-10-12 22:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049648
2020-10-12 22:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122505
2020-10-12 22:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21152.97265625Mb; avail=467334.9296875Mb
2020-10-12 22:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21152.97265625Mb; avail=467334.9296875Mb
2020-10-12 22:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001094
2020-10-12 22:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21152.97265625Mb; avail=467334.9296875Mb
2020-10-12 22:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072130
2020-10-12 22:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21152.97265625Mb; avail=467334.9296875Mb
2020-10-12 22:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049587
2020-10-12 22:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123568
2020-10-12 22:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21152.87109375Mb; avail=467334.9296875Mb
2020-10-12 22:12:52 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.777 | nll_loss 6.67 | ppl 101.8 | wps 127040 | wpb 6713.4 | bsz 255.6 | num_updates 1182 | best_loss 7.777
2020-10-12 22:12:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:12:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 6 @ 1182 updates, score 7.777) (writing took 4.480110713999238 seconds)
2020-10-12 22:12:57 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 22:12:57 | INFO | train | epoch 006 | loss 8.234 | nll_loss 7.241 | ppl 151.29 | wps 56743.3 | ups 2.67 | wpb 21229.6 | bsz 795.8 | num_updates 1182 | lr 5.91705e-05 | gnorm 1.123 | clip 0 | train_wall 62 | wall 440
2020-10-12 22:12:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 22:12:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 22:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21321.109375Mb; avail=467166.453125Mb
2020-10-12 22:12:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004758
2020-10-12 22:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.036076
2020-10-12 22:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21321.6953125Mb; avail=467165.8671875Mb
2020-10-12 22:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001466
2020-10-12 22:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21321.5859375Mb; avail=467165.8671875Mb
2020-10-12 22:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.588307
2020-10-12 22:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.627093
2020-10-12 22:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21320.95703125Mb; avail=467167.078125Mb
2020-10-12 22:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21320.95703125Mb; avail=467167.078125Mb
2020-10-12 22:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023006
2020-10-12 22:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21320.97265625Mb; avail=467166.95703125Mb
2020-10-12 22:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001285
2020-10-12 22:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21320.97265625Mb; avail=467166.95703125Mb
2020-10-12 22:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.548608
2020-10-12 22:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.573844
2020-10-12 22:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21320.3828125Mb; avail=467166.8359375Mb
2020-10-12 22:12:58 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 22:13:05 | INFO | train_inner | epoch 007:     18 / 197 loss=8.126, nll_loss=7.118, ppl=138.87, wps=50558.3, ups=2.35, wpb=21472.1, bsz=810.2, num_updates=1200, lr=6.007e-05, gnorm=1.075, clip=0, train_wall=31, wall=448
2020-10-12 22:13:37 | INFO | train_inner | epoch 007:    118 / 197 loss=8.015, nll_loss=6.99, ppl=127.08, wps=66021, ups=3.08, wpb=21436.4, bsz=834.8, num_updates=1300, lr=6.50675e-05, gnorm=1.189, clip=0, train_wall=31, wall=480
2020-10-12 22:14:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21171.91015625Mb; avail=467316.88671875Mb
2020-10-12 22:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001616
2020-10-12 22:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21172.0234375Mb; avail=467316.7734375Mb
2020-10-12 22:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071547
2020-10-12 22:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21171.63671875Mb; avail=467316.78125Mb
2020-10-12 22:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050975
2020-10-12 22:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124925
2020-10-12 22:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21171.5703125Mb; avail=467317.0234375Mb
2020-10-12 22:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21171.5703125Mb; avail=467317.0234375Mb
2020-10-12 22:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001108
2020-10-12 22:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21171.5703125Mb; avail=467317.0234375Mb
2020-10-12 22:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071105
2020-10-12 22:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21171.5703125Mb; avail=467317.0234375Mb
2020-10-12 22:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049794
2020-10-12 22:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122764
2020-10-12 22:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21171.5703125Mb; avail=467317.0234375Mb
2020-10-12 22:14:06 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.547 | nll_loss 6.406 | ppl 84.78 | wps 126322 | wpb 6713.4 | bsz 255.6 | num_updates 1379 | best_loss 7.547
2020-10-12 22:14:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:14:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 7 @ 1379 updates, score 7.547) (writing took 4.472299856000973 seconds)
2020-10-12 22:14:11 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 22:14:11 | INFO | train | epoch 007 | loss 7.986 | nll_loss 6.956 | ppl 124.18 | wps 56580.9 | ups 2.67 | wpb 21229.6 | bsz 795.8 | num_updates 1379 | lr 6.90155e-05 | gnorm 1.071 | clip 0 | train_wall 62 | wall 514
2020-10-12 22:14:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 22:14:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 22:14:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21333.421875Mb; avail=467154.09765625Mb
2020-10-12 22:14:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003348
2020-10-12 22:14:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.031754
2020-10-12 22:14:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21333.2421875Mb; avail=467154.296875Mb
2020-10-12 22:14:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001304
2020-10-12 22:14:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21333.2421875Mb; avail=467154.296875Mb
2020-10-12 22:14:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.550136
2020-10-12 22:14:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.584315
2020-10-12 22:14:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21333.4453125Mb; avail=467154.81640625Mb
2020-10-12 22:14:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21334.05078125Mb; avail=467154.2109375Mb
2020-10-12 22:14:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024231
2020-10-12 22:14:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21334.05078125Mb; avail=467154.2109375Mb
2020-10-12 22:14:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001328
2020-10-12 22:14:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21334.05078125Mb; avail=467154.2109375Mb
2020-10-12 22:14:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.543851
2020-10-12 22:14:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.570359
2020-10-12 22:14:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21333.91796875Mb; avail=467154.08984375Mb
2020-10-12 22:14:12 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 22:14:20 | INFO | train_inner | epoch 008:     21 / 197 loss=7.906, nll_loss=6.864, ppl=116.46, wps=49486.1, ups=2.37, wpb=20920.4, bsz=768.9, num_updates=1400, lr=7.0065e-05, gnorm=1.015, clip=0, train_wall=31, wall=523
2020-10-12 22:14:52 | INFO | train_inner | epoch 008:    121 / 197 loss=7.832, nll_loss=6.779, ppl=109.8, wps=66302.8, ups=3.1, wpb=21378.4, bsz=787.7, num_updates=1500, lr=7.50625e-05, gnorm=0.968, clip=0, train_wall=31, wall=555
2020-10-12 22:15:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:15:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21175.2265625Mb; avail=467313.17578125Mb
2020-10-12 22:15:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001589
2020-10-12 22:15:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21175.2265625Mb; avail=467313.17578125Mb
2020-10-12 22:15:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071036
2020-10-12 22:15:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21175.2265625Mb; avail=467313.17578125Mb
2020-10-12 22:15:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049418
2020-10-12 22:15:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122859
2020-10-12 22:15:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21175.2265625Mb; avail=467313.17578125Mb
2020-10-12 22:15:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21175.2265625Mb; avail=467313.17578125Mb
2020-10-12 22:15:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001145
2020-10-12 22:15:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21175.2265625Mb; avail=467313.17578125Mb
2020-10-12 22:15:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072737
2020-10-12 22:15:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21175.2265625Mb; avail=467313.17578125Mb
2020-10-12 22:15:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049712
2020-10-12 22:15:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124445
2020-10-12 22:15:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21175.2265625Mb; avail=467313.17578125Mb
2020-10-12 22:15:20 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.404 | nll_loss 6.245 | ppl 75.85 | wps 129171 | wpb 6713.4 | bsz 255.6 | num_updates 1576 | best_loss 7.404
2020-10-12 22:15:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:15:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 8 @ 1576 updates, score 7.404) (writing took 5.719356149000305 seconds)
2020-10-12 22:15:25 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 22:15:25 | INFO | train | epoch 008 | loss 7.778 | nll_loss 6.717 | ppl 105.17 | wps 55873.4 | ups 2.63 | wpb 21229.6 | bsz 795.8 | num_updates 1576 | lr 7.88606e-05 | gnorm 1.018 | clip 0 | train_wall 62 | wall 588
2020-10-12 22:15:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 22:15:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 22:15:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21878.7890625Mb; avail=466609.1640625Mb
2020-10-12 22:15:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003271
2020-10-12 22:15:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.031746
2020-10-12 22:15:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21879.39453125Mb; avail=466608.55859375Mb
2020-10-12 22:15:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001227
2020-10-12 22:15:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21879.39453125Mb; avail=466608.55859375Mb
2020-10-12 22:15:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.555509
2020-10-12 22:15:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.589627
2020-10-12 22:15:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21881.1171875Mb; avail=466606.97265625Mb
2020-10-12 22:15:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21880.625Mb; avail=466607.46484375Mb
2020-10-12 22:15:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024296
2020-10-12 22:15:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21880.625Mb; avail=466607.46484375Mb
2020-10-12 22:15:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001208
2020-10-12 22:15:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21880.625Mb; avail=466607.46484375Mb
2020-10-12 22:15:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.550638
2020-10-12 22:15:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.577030
2020-10-12 22:15:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21309.78125Mb; avail=467177.734375Mb
2020-10-12 22:15:27 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 22:15:36 | INFO | train_inner | epoch 009:     24 / 197 loss=7.687, nll_loss=6.613, ppl=97.87, wps=48558.8, ups=2.29, wpb=21200.8, bsz=807.1, num_updates=1600, lr=8.006e-05, gnorm=1.035, clip=0, train_wall=31, wall=599
2020-10-12 22:16:08 | INFO | train_inner | epoch 009:    124 / 197 loss=7.618, nll_loss=6.533, ppl=92.6, wps=65775.5, ups=3.08, wpb=21332.2, bsz=787.1, num_updates=1700, lr=8.50575e-05, gnorm=1.013, clip=0, train_wall=31, wall=631
2020-10-12 22:16:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:16:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21178.66015625Mb; avail=467309.390625Mb
2020-10-12 22:16:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001308
2020-10-12 22:16:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21178.5234375Mb; avail=467309.6328125Mb
2020-10-12 22:16:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072074
2020-10-12 22:16:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21178.62109375Mb; avail=467309.6328125Mb
2020-10-12 22:16:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050356
2020-10-12 22:16:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124621
2020-10-12 22:16:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21178.62109375Mb; avail=467309.6328125Mb
2020-10-12 22:16:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21178.62109375Mb; avail=467309.6328125Mb
2020-10-12 22:16:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001090
2020-10-12 22:16:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21178.62109375Mb; avail=467309.6328125Mb
2020-10-12 22:16:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071707
2020-10-12 22:16:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21178.87890625Mb; avail=467309.1484375Mb
2020-10-12 22:16:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050547
2020-10-12 22:16:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124152
2020-10-12 22:16:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21178.35546875Mb; avail=467310.20703125Mb
2020-10-12 22:16:35 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.198 | nll_loss 5.994 | ppl 63.75 | wps 125121 | wpb 6713.4 | bsz 255.6 | num_updates 1773 | best_loss 7.198
2020-10-12 22:16:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:16:39 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 9 @ 1773 updates, score 7.198) (writing took 4.517292319998887 seconds)
2020-10-12 22:16:39 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 22:16:39 | INFO | train | epoch 009 | loss 7.598 | nll_loss 6.51 | ppl 91.13 | wps 56738.6 | ups 2.67 | wpb 21229.6 | bsz 795.8 | num_updates 1773 | lr 8.87057e-05 | gnorm 1.021 | clip 0 | train_wall 62 | wall 662
2020-10-12 22:16:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 22:16:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 22:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21343.03515625Mb; avail=467144.859375Mb
2020-10-12 22:16:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003351
2020-10-12 22:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.031962
2020-10-12 22:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21342.5625Mb; avail=467144.89453125Mb
2020-10-12 22:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001318
2020-10-12 22:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21342.5625Mb; avail=467144.89453125Mb
2020-10-12 22:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.545716
2020-10-12 22:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.580150
2020-10-12 22:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21342.9453125Mb; avail=467144.89453125Mb
2020-10-12 22:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21342.984375Mb; avail=467144.53125Mb
2020-10-12 22:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024103
2020-10-12 22:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21343.0234375Mb; avail=467145.015625Mb
2020-10-12 22:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001276
2020-10-12 22:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21343.0234375Mb; avail=467145.015625Mb
2020-10-12 22:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.541176
2020-10-12 22:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.567491
2020-10-12 22:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21343.3125Mb; avail=467144.53125Mb
2020-10-12 22:16:40 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 22:16:50 | INFO | train_inner | epoch 010:     27 / 197 loss=7.529, nll_loss=6.431, ppl=86.27, wps=49656.2, ups=2.37, wpb=20917.8, bsz=784.6, num_updates=1800, lr=9.0055e-05, gnorm=1.005, clip=0, train_wall=31, wall=673
2020-10-12 22:17:23 | INFO | train_inner | epoch 010:    127 / 197 loss=7.43, nll_loss=6.317, ppl=79.73, wps=65495.8, ups=3.08, wpb=21294.7, bsz=832.5, num_updates=1900, lr=9.50525e-05, gnorm=0.98, clip=0, train_wall=31, wall=706
2020-10-12 22:17:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:17:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21183.8984375Mb; avail=467303.7578125Mb
2020-10-12 22:17:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001703
2020-10-12 22:17:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21183.8984375Mb; avail=467303.7578125Mb
2020-10-12 22:17:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071227
2020-10-12 22:17:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21183.8984375Mb; avail=467303.7578125Mb
2020-10-12 22:17:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050324
2020-10-12 22:17:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124100
2020-10-12 22:17:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21183.8984375Mb; avail=467303.7578125Mb
2020-10-12 22:17:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21183.8984375Mb; avail=467303.7578125Mb
2020-10-12 22:17:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001121
2020-10-12 22:17:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21183.8984375Mb; avail=467303.7578125Mb
2020-10-12 22:17:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072182
2020-10-12 22:17:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21183.828125Mb; avail=467303.63671875Mb
2020-10-12 22:17:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050070
2020-10-12 22:17:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124326
2020-10-12 22:17:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21183.73046875Mb; avail=467303.7578125Mb
2020-10-12 22:17:48 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.04 | nll_loss 5.823 | ppl 56.6 | wps 125010 | wpb 6713.4 | bsz 255.6 | num_updates 1970 | best_loss 7.04
2020-10-12 22:17:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:17:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 10 @ 1970 updates, score 7.04) (writing took 4.5334690420004335 seconds)
2020-10-12 22:17:53 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 22:17:53 | INFO | train | epoch 010 | loss 7.429 | nll_loss 6.315 | ppl 79.6 | wps 56762.3 | ups 2.67 | wpb 21229.6 | bsz 795.8 | num_updates 1970 | lr 9.85508e-05 | gnorm 0.98 | clip 0 | train_wall 62 | wall 736
2020-10-12 22:17:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 22:17:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 22:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21342.53515625Mb; avail=467144.84765625Mb
2020-10-12 22:17:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003340
2020-10-12 22:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033362
2020-10-12 22:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21343.03515625Mb; avail=467144.34765625Mb
2020-10-12 22:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001210
2020-10-12 22:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21343.03515625Mb; avail=467144.34765625Mb
2020-10-12 22:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.549389
2020-10-12 22:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.585097
2020-10-12 22:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21343.265625Mb; avail=467144.70703125Mb
2020-10-12 22:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21343.265625Mb; avail=467144.70703125Mb
2020-10-12 22:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024143
2020-10-12 22:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21343.265625Mb; avail=467144.70703125Mb
2020-10-12 22:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001263
2020-10-12 22:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21343.265625Mb; avail=467144.70703125Mb
2020-10-12 22:17:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.543608
2020-10-12 22:17:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.569952
2020-10-12 22:17:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21343.00390625Mb; avail=467144.46875Mb
2020-10-12 22:17:54 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 22:18:05 | INFO | train_inner | epoch 011:     30 / 197 loss=7.375, nll_loss=6.253, ppl=76.25, wps=50155.8, ups=2.37, wpb=21167.3, bsz=762.1, num_updates=2000, lr=0.00010005, gnorm=0.973, clip=0, train_wall=31, wall=748
2020-10-12 22:18:37 | INFO | train_inner | epoch 011:    130 / 197 loss=7.267, nll_loss=6.129, ppl=69.98, wps=66634.1, ups=3.09, wpb=21570.4, bsz=812.1, num_updates=2100, lr=0.000105048, gnorm=0.975, clip=0, train_wall=31, wall=780
2020-10-12 22:18:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21084.43359375Mb; avail=467403.9453125Mb
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001629
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21087.4609375Mb; avail=467401.5234375Mb
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070105
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21099.6640625Mb; avail=467389.27734375Mb
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049703
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122356
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21098.48828125Mb; avail=467390.453125Mb
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21099.09375Mb; avail=467389.84765625Mb
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001140
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21099.09375Mb; avail=467389.2421875Mb
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.143298
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21107.58984375Mb; avail=467381.7578125Mb
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049356
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.194579
2020-10-12 22:18:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21111.71484375Mb; avail=467377.6328125Mb
2020-10-12 22:19:02 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.833 | nll_loss 5.568 | ppl 47.43 | wps 111808 | wpb 6713.4 | bsz 255.6 | num_updates 2167 | best_loss 6.833
2020-10-12 22:19:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:19:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 11 @ 2167 updates, score 6.833) (writing took 6.815758434999225 seconds)
2020-10-12 22:19:09 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 22:19:09 | INFO | train | epoch 011 | loss 7.249 | nll_loss 6.109 | ppl 69 | wps 55099 | ups 2.6 | wpb 21229.6 | bsz 795.8 | num_updates 2167 | lr 0.000108396 | gnorm 0.973 | clip 0 | train_wall 62 | wall 812
2020-10-12 22:19:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 22:19:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 22:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21310.5546875Mb; avail=467177.5390625Mb
2020-10-12 22:19:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003794
2020-10-12 22:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035474
2020-10-12 22:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21311.203125Mb; avail=467176.8046875Mb
2020-10-12 22:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001322
2020-10-12 22:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21311.203125Mb; avail=467176.8046875Mb
2020-10-12 22:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.619964
2020-10-12 22:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.658482
2020-10-12 22:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21312.57421875Mb; avail=467175.53125Mb
2020-10-12 22:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21312.05859375Mb; avail=467176.046875Mb
2020-10-12 22:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027188
2020-10-12 22:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21312.05859375Mb; avail=467176.046875Mb
2020-10-12 22:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001267
2020-10-12 22:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21312.07421875Mb; avail=467176.03125Mb
2020-10-12 22:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.609151
2020-10-12 22:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.638631
2020-10-12 22:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21311.40234375Mb; avail=467176.6640625Mb
2020-10-12 22:19:10 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 22:19:22 | INFO | train_inner | epoch 012:     33 / 197 loss=7.195, nll_loss=6.046, ppl=66.07, wps=46909.5, ups=2.24, wpb=20979.2, bsz=775.3, num_updates=2200, lr=0.000110045, gnorm=0.965, clip=0, train_wall=31, wall=825
2020-10-12 22:19:54 | INFO | train_inner | epoch 012:    133 / 197 loss=7.063, nll_loss=5.895, ppl=59.5, wps=66014.3, ups=3.09, wpb=21338.3, bsz=807.2, num_updates=2300, lr=0.000115043, gnorm=0.982, clip=0, train_wall=31, wall=857
2020-10-12 22:20:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21524.828125Mb; avail=466964.02734375Mb
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001742
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21524.828125Mb; avail=466964.02734375Mb
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070615
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21524.828125Mb; avail=466964.02734375Mb
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048896
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122081
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21524.6796875Mb; avail=466963.90625Mb
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21523.8125Mb; avail=466964.40625Mb
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001123
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21522.828125Mb; avail=466965.390625Mb
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071516
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21205.3515625Mb; avail=467286.44140625Mb
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048378
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121776
2020-10-12 22:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21078.859375Mb; avail=467409.9765625Mb
2020-10-12 22:20:18 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.659 | nll_loss 5.366 | ppl 41.23 | wps 127173 | wpb 6713.4 | bsz 255.6 | num_updates 2364 | best_loss 6.659
2020-10-12 22:20:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:20:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 12 @ 2364 updates, score 6.659) (writing took 4.472828168998603 seconds)
2020-10-12 22:20:22 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 22:20:22 | INFO | train | epoch 012 | loss 7.067 | nll_loss 5.9 | ppl 59.71 | wps 56864.9 | ups 2.68 | wpb 21229.6 | bsz 795.8 | num_updates 2364 | lr 0.000118241 | gnorm 0.949 | clip 0 | train_wall 62 | wall 885
2020-10-12 22:20:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 22:20:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 22:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21344.71484375Mb; avail=467143.015625Mb
2020-10-12 22:20:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004599
2020-10-12 22:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.036660
2020-10-12 22:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21345.27734375Mb; avail=467142.41015625Mb
2020-10-12 22:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001494
2020-10-12 22:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21345.20703125Mb; avail=467142.41015625Mb
2020-10-12 22:20:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.555402
2020-10-12 22:20:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.594624
2020-10-12 22:20:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21348.546875Mb; avail=467139.375Mb
2020-10-12 22:20:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21347.01171875Mb; avail=467140.609375Mb
2020-10-12 22:20:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023035
2020-10-12 22:20:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21346.5390625Mb; avail=467141.31640625Mb
2020-10-12 22:20:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001136
2020-10-12 22:20:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21346.5390625Mb; avail=467141.31640625Mb
2020-10-12 22:20:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.545741
2020-10-12 22:20:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.570775
2020-10-12 22:20:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21346.6796875Mb; avail=467141.1953125Mb
2020-10-12 22:20:23 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 22:20:36 | INFO | train_inner | epoch 013:     36 / 197 loss=6.992, nll_loss=5.813, ppl=56.23, wps=50367.1, ups=2.39, wpb=21110, bsz=791, num_updates=2400, lr=0.00012004, gnorm=0.929, clip=0, train_wall=31, wall=899
2020-10-12 22:21:09 | INFO | train_inner | epoch 013:    136 / 197 loss=6.9, nll_loss=5.707, ppl=52.25, wps=65534.9, ups=3.09, wpb=21202.8, bsz=801.1, num_updates=2500, lr=0.000125037, gnorm=0.939, clip=0, train_wall=31, wall=932
2020-10-12 22:21:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21613.87109375Mb; avail=466875.34765625Mb
2020-10-12 22:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001510
2020-10-12 22:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21615.6875Mb; avail=466873.53125Mb
2020-10-12 22:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069852
2020-10-12 22:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21648.26953125Mb; avail=466840.94921875Mb
2020-10-12 22:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048564
2020-10-12 22:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120735
2020-10-12 22:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21648.875Mb; avail=466840.34375Mb
2020-10-12 22:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21648.8828125Mb; avail=466839.36328125Mb
2020-10-12 22:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001223
2020-10-12 22:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21649.23046875Mb; avail=466839.62890625Mb
2020-10-12 22:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070214
2020-10-12 22:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21649.515625Mb; avail=466839.6015625Mb
2020-10-12 22:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049124
2020-10-12 22:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121551
2020-10-12 22:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21648.8671875Mb; avail=466839.75390625Mb
2020-10-12 22:21:31 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.507 | nll_loss 5.179 | ppl 36.24 | wps 110367 | wpb 6713.4 | bsz 255.6 | num_updates 2561 | best_loss 6.507
2020-10-12 22:21:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:21:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 13 @ 2561 updates, score 6.507) (writing took 5.909404349000397 seconds)
2020-10-12 22:21:37 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 22:21:37 | INFO | train | epoch 013 | loss 6.887 | nll_loss 5.692 | ppl 51.71 | wps 55928.3 | ups 2.63 | wpb 21229.6 | bsz 795.8 | num_updates 2561 | lr 0.000128086 | gnorm 0.933 | clip 0 | train_wall 62 | wall 960
2020-10-12 22:21:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 22:21:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 22:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21349.5859375Mb; avail=467138.1796875Mb
2020-10-12 22:21:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004826
2020-10-12 22:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.037539
2020-10-12 22:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21350.19140625Mb; avail=467137.57421875Mb
2020-10-12 22:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001311
2020-10-12 22:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21350.19140625Mb; avail=467137.57421875Mb
2020-10-12 22:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.547392
2020-10-12 22:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.587187
2020-10-12 22:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21349.890625Mb; avail=467137.453125Mb
2020-10-12 22:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21349.9609375Mb; avail=467136.96875Mb
2020-10-12 22:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023017
2020-10-12 22:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21349.92578125Mb; avail=467137.2109375Mb
2020-10-12 22:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001135
2020-10-12 22:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21349.92578125Mb; avail=467137.2109375Mb
2020-10-12 22:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.546851
2020-10-12 22:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.571992
2020-10-12 22:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21349.69140625Mb; avail=467137.9375Mb
2020-10-12 22:21:38 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 22:21:52 | INFO | train_inner | epoch 014:     39 / 197 loss=6.811, nll_loss=5.605, ppl=48.66, wps=49323.4, ups=2.31, wpb=21382.8, bsz=795.4, num_updates=2600, lr=0.000130035, gnorm=0.939, clip=0, train_wall=31, wall=975
2020-10-12 22:22:24 | INFO | train_inner | epoch 014:    139 / 197 loss=6.728, nll_loss=5.51, ppl=45.56, wps=66383.6, ups=3.1, wpb=21395.1, bsz=807.6, num_updates=2700, lr=0.000135032, gnorm=0.939, clip=0, train_wall=31, wall=1007
2020-10-12 22:22:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21223.25390625Mb; avail=467265.7421875Mb
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003139
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21224.45703125Mb; avail=467264.5390625Mb
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.121580
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21245.078125Mb; avail=467244.43359375Mb
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050876
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.176957
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21268.26953125Mb; avail=467221.2421875Mb
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21274.32421875Mb; avail=467215.1875Mb
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001386
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21274.9296875Mb; avail=467214.58203125Mb
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071745
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21307.62109375Mb; avail=467181.8046875Mb
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051037
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125140
2020-10-12 22:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21324.05859375Mb; avail=467165.3671875Mb
2020-10-12 22:22:46 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.374 | nll_loss 5.021 | ppl 32.47 | wps 129804 | wpb 6713.4 | bsz 255.6 | num_updates 2758 | best_loss 6.374
2020-10-12 22:22:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:22:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 14 @ 2758 updates, score 6.374) (writing took 12.877574905000074 seconds)
2020-10-12 22:22:58 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 22:22:58 | INFO | train | epoch 014 | loss 6.715 | nll_loss 5.494 | ppl 45.07 | wps 51323.1 | ups 2.42 | wpb 21229.6 | bsz 795.8 | num_updates 2758 | lr 0.000137931 | gnorm 0.977 | clip 0 | train_wall 61 | wall 1042
2020-10-12 22:22:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 22:22:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 22:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21338.54296875Mb; avail=467149.07421875Mb
2020-10-12 22:22:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003451
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032602
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21338.54296875Mb; avail=467149.07421875Mb
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001174
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21338.54296875Mb; avail=467149.07421875Mb
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.551350
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.586072
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21251.1015625Mb; avail=467236.69140625Mb
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21251.1015625Mb; avail=467236.69140625Mb
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022968
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21251.1015625Mb; avail=467236.69140625Mb
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001134
2020-10-12 22:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21251.1015625Mb; avail=467236.69140625Mb
2020-10-12 22:23:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.543017
2020-10-12 22:23:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.568007
2020-10-12 22:23:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21274.703125Mb; avail=467213.3046875Mb
2020-10-12 22:23:00 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 22:23:14 | INFO | train_inner | epoch 015:     42 / 197 loss=6.632, nll_loss=5.4, ppl=42.22, wps=41659.5, ups=1.99, wpb=20959.8, bsz=772, num_updates=2800, lr=0.00014003, gnorm=1.015, clip=0, train_wall=31, wall=1058
2020-10-12 22:23:47 | INFO | train_inner | epoch 015:    142 / 197 loss=6.536, nll_loss=5.29, ppl=39.11, wps=64923.7, ups=3.07, wpb=21149.9, bsz=814.4, num_updates=2900, lr=0.000145028, gnorm=0.983, clip=0, train_wall=31, wall=1090
2020-10-12 22:24:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21183.84765625Mb; avail=467304.6953125Mb
2020-10-12 22:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002069
2020-10-12 22:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21183.84765625Mb; avail=467304.6953125Mb
2020-10-12 22:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077134
2020-10-12 22:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21183.9296875Mb; avail=467304.58203125Mb
2020-10-12 22:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049971
2020-10-12 22:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130106
2020-10-12 22:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21183.80859375Mb; avail=467304.953125Mb
2020-10-12 22:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21183.80859375Mb; avail=467304.953125Mb
2020-10-12 22:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001089
2020-10-12 22:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21183.80859375Mb; avail=467304.953125Mb
2020-10-12 22:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070666
2020-10-12 22:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21183.75Mb; avail=467304.58984375Mb
2020-10-12 22:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050716
2020-10-12 22:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123235
2020-10-12 22:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21183.80078125Mb; avail=467304.953125Mb
2020-10-12 22:24:08 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.22 | nll_loss 4.835 | ppl 28.54 | wps 130572 | wpb 6713.4 | bsz 255.6 | num_updates 2955 | best_loss 6.22
2020-10-12 22:24:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:24:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 15 @ 2955 updates, score 6.22) (writing took 4.836076930998388 seconds)
2020-10-12 22:24:12 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 22:24:12 | INFO | train | epoch 015 | loss 6.545 | nll_loss 5.299 | ppl 39.38 | wps 56541.2 | ups 2.66 | wpb 21229.6 | bsz 795.8 | num_updates 2955 | lr 0.000147776 | gnorm 0.974 | clip 0 | train_wall 62 | wall 1115
2020-10-12 22:24:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 22:24:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 22:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21866.62109375Mb; avail=466621.60546875Mb
2020-10-12 22:24:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004645
2020-10-12 22:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.039115
2020-10-12 22:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21870.84765625Mb; avail=466616.95703125Mb
2020-10-12 22:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001280
2020-10-12 22:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21871.453125Mb; avail=466616.3515625Mb
2020-10-12 22:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.616606
2020-10-12 22:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.658036
2020-10-12 22:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21909.2109375Mb; avail=466579.32421875Mb
2020-10-12 22:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21909.2109375Mb; avail=466579.32421875Mb
2020-10-12 22:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025720
2020-10-12 22:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21909.2109375Mb; avail=466579.32421875Mb
2020-10-12 22:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001323
2020-10-12 22:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21909.2109375Mb; avail=466579.32421875Mb
2020-10-12 22:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.615090
2020-10-12 22:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.643605
2020-10-12 22:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21908.53515625Mb; avail=466579.96875Mb
2020-10-12 22:24:14 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 22:24:29 | INFO | train_inner | epoch 016:     45 / 197 loss=6.467, nll_loss=5.209, ppl=37, wps=50448.8, ups=2.36, wpb=21377, bsz=787.7, num_updates=3000, lr=0.000150025, gnorm=0.98, clip=0, train_wall=31, wall=1132
2020-10-12 22:25:02 | INFO | train_inner | epoch 016:    145 / 197 loss=6.402, nll_loss=5.135, ppl=35.13, wps=64819.3, ups=3.06, wpb=21167.9, bsz=801.1, num_updates=3100, lr=0.000155023, gnorm=1.015, clip=0, train_wall=32, wall=1165
2020-10-12 22:25:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:25:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21196.4453125Mb; avail=467290.8828125Mb
2020-10-12 22:25:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001684
2020-10-12 22:25:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21196.4453125Mb; avail=467290.8828125Mb
2020-10-12 22:25:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071941
2020-10-12 22:25:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21194.75390625Mb; avail=467293.39453125Mb
2020-10-12 22:25:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055379
2020-10-12 22:25:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129830
2020-10-12 22:25:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21194.5625Mb; avail=467293.40234375Mb
2020-10-12 22:25:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21194.47265625Mb; avail=467294.12890625Mb
2020-10-12 22:25:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001105
2020-10-12 22:25:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21194.47265625Mb; avail=467294.12890625Mb
2020-10-12 22:25:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071246
2020-10-12 22:25:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21194.47265625Mb; avail=467294.12890625Mb
2020-10-12 22:25:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051183
2020-10-12 22:25:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124292
2020-10-12 22:25:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21194.47265625Mb; avail=467294.12890625Mb
2020-10-12 22:25:22 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.078 | nll_loss 4.671 | ppl 25.48 | wps 132300 | wpb 6713.4 | bsz 255.6 | num_updates 3152 | best_loss 6.078
2020-10-12 22:25:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:25:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 16 @ 3152 updates, score 6.078) (writing took 4.474778320000041 seconds)
2020-10-12 22:25:26 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 22:25:26 | INFO | train | epoch 016 | loss 6.39 | nll_loss 5.12 | ppl 34.78 | wps 56840.7 | ups 2.68 | wpb 21229.6 | bsz 795.8 | num_updates 3152 | lr 0.000157621 | gnorm 0.991 | clip 0 | train_wall 61 | wall 1189
2020-10-12 22:25:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 22:25:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 22:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21354.1796875Mb; avail=467133.7421875Mb
2020-10-12 22:25:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004626
2020-10-12 22:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.037251
2020-10-12 22:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21354.78515625Mb; avail=467133.13671875Mb
2020-10-12 22:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001274
2020-10-12 22:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21354.78515625Mb; avail=467133.13671875Mb
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.551612
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.591195
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21356.8984375Mb; avail=467131.13671875Mb
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21355.515625Mb; avail=467133.234375Mb
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023114
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21354.90234375Mb; avail=467133.35546875Mb
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001131
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21354.90234375Mb; avail=467133.35546875Mb
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.622120
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.647229
2020-10-12 22:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21436.94921875Mb; avail=467051.34375Mb
2020-10-12 22:25:27 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 22:25:44 | INFO | train_inner | epoch 017:     48 / 197 loss=6.333, nll_loss=5.054, ppl=33.22, wps=49869.7, ups=2.38, wpb=20911.2, bsz=749.3, num_updates=3200, lr=0.00016002, gnorm=0.966, clip=0, train_wall=31, wall=1207
2020-10-12 22:26:16 | INFO | train_inner | epoch 017:    148 / 197 loss=6.203, nll_loss=4.905, ppl=29.96, wps=66609.5, ups=3.09, wpb=21591.3, bsz=845.8, num_updates=3300, lr=0.000165018, gnorm=0.963, clip=0, train_wall=31, wall=1239
2020-10-12 22:26:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21519.38671875Mb; avail=466969.203125Mb
2020-10-12 22:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001631
2020-10-12 22:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21519.38671875Mb; avail=466969.203125Mb
2020-10-12 22:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072942
2020-10-12 22:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21429.8671875Mb; avail=467058.72265625Mb
2020-10-12 22:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.097678
2020-10-12 22:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.173339
2020-10-12 22:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21066.0234375Mb; avail=467422.78125Mb
2020-10-12 22:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21066.64453125Mb; avail=467422.41796875Mb
2020-10-12 22:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001155
2020-10-12 22:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21066.64453125Mb; avail=467422.41796875Mb
2020-10-12 22:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072355
2020-10-12 22:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21066.64453125Mb; avail=467422.41796875Mb
2020-10-12 22:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052659
2020-10-12 22:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127046
2020-10-12 22:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21076.7109375Mb; avail=467412.3515625Mb
2020-10-12 22:26:35 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 5.966 | nll_loss 4.53 | ppl 23.11 | wps 125054 | wpb 6713.4 | bsz 255.6 | num_updates 3349 | best_loss 5.966
2020-10-12 22:26:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:26:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 17 @ 3349 updates, score 5.966) (writing took 4.48088320900024 seconds)
2020-10-12 22:26:40 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 22:26:40 | INFO | train | epoch 017 | loss 6.242 | nll_loss 4.95 | ppl 30.9 | wps 56789.1 | ups 2.67 | wpb 21229.6 | bsz 795.8 | num_updates 3349 | lr 0.000167466 | gnorm 0.982 | clip 0 | train_wall 62 | wall 1263
2020-10-12 22:26:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 22:26:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 22:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21359.828125Mb; avail=467127.85546875Mb
2020-10-12 22:26:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004566
2020-10-12 22:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.036521
2020-10-12 22:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21359.828125Mb; avail=467127.85546875Mb
2020-10-12 22:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001191
2020-10-12 22:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21359.828125Mb; avail=467127.85546875Mb
2020-10-12 22:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.547653
2020-10-12 22:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.586522
2020-10-12 22:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21360.09765625Mb; avail=467127.76171875Mb
2020-10-12 22:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21360.0Mb; avail=467127.8828125Mb
2020-10-12 22:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023334
2020-10-12 22:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21360.0Mb; avail=467127.8828125Mb
2020-10-12 22:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001184
2020-10-12 22:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21360.0Mb; avail=467127.8828125Mb
2020-10-12 22:26:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.544014
2020-10-12 22:26:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.569487
2020-10-12 22:26:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21359.40625Mb; avail=467128.24609375Mb
2020-10-12 22:26:41 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 22:26:59 | INFO | train_inner | epoch 018:     51 / 197 loss=6.186, nll_loss=4.885, ppl=29.54, wps=49657.4, ups=2.36, wpb=21032.7, bsz=766.3, num_updates=3400, lr=0.000170015, gnorm=0.961, clip=0, train_wall=32, wall=1282
2020-10-12 22:27:31 | INFO | train_inner | epoch 018:    151 / 197 loss=6.079, nll_loss=4.762, ppl=27.13, wps=65437.8, ups=3.06, wpb=21361.8, bsz=811.5, num_updates=3500, lr=0.000175013, gnorm=0.957, clip=0, train_wall=32, wall=1314
2020-10-12 22:27:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21199.28125Mb; avail=467289.07421875Mb
2020-10-12 22:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001679
2020-10-12 22:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21199.28125Mb; avail=467289.07421875Mb
2020-10-12 22:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.116103
2020-10-12 22:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21199.25390625Mb; avail=467289.07421875Mb
2020-10-12 22:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050194
2020-10-12 22:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.169019
2020-10-12 22:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21199.25390625Mb; avail=467289.07421875Mb
2020-10-12 22:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21199.25390625Mb; avail=467289.07421875Mb
2020-10-12 22:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001104
2020-10-12 22:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21199.25390625Mb; avail=467289.07421875Mb
2020-10-12 22:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070909
2020-10-12 22:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21199.25390625Mb; avail=467289.07421875Mb
2020-10-12 22:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048847
2020-10-12 22:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121627
2020-10-12 22:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21204.4765625Mb; avail=467283.8515625Mb
2020-10-12 22:27:49 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 5.859 | nll_loss 4.401 | ppl 21.13 | wps 132322 | wpb 6713.4 | bsz 255.6 | num_updates 3546 | best_loss 5.859
2020-10-12 22:27:49 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:27:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 18 @ 3546 updates, score 5.859) (writing took 4.488925036999717 seconds)
2020-10-12 22:27:53 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 22:27:53 | INFO | train | epoch 018 | loss 6.084 | nll_loss 4.767 | ppl 27.23 | wps 56653.9 | ups 2.67 | wpb 21229.6 | bsz 795.8 | num_updates 3546 | lr 0.000177311 | gnorm 0.928 | clip 0 | train_wall 62 | wall 1337
2020-10-12 22:27:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 22:27:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 22:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21361.26953125Mb; avail=467126.625Mb
2020-10-12 22:27:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007600
2020-10-12 22:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.047279
2020-10-12 22:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21361.796875Mb; avail=467125.77734375Mb
2020-10-12 22:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001297
2020-10-12 22:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21361.796875Mb; avail=467125.77734375Mb
2020-10-12 22:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.576027
2020-10-12 22:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.625871
2020-10-12 22:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21362.265625Mb; avail=467124.82421875Mb
2020-10-12 22:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21362.37890625Mb; avail=467124.7109375Mb
2020-10-12 22:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024477
2020-10-12 22:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21362.37890625Mb; avail=467124.7109375Mb
2020-10-12 22:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001167
2020-10-12 22:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21362.37890625Mb; avail=467124.7109375Mb
2020-10-12 22:27:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.545476
2020-10-12 22:27:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.571972
2020-10-12 22:27:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21362.32421875Mb; avail=467124.12109375Mb
2020-10-12 22:27:55 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 22:28:14 | INFO | train_inner | epoch 019:     54 / 197 loss=6.003, nll_loss=4.673, ppl=25.52, wps=50349.2, ups=2.37, wpb=21218.6, bsz=791.7, num_updates=3600, lr=0.00018001, gnorm=0.844, clip=0, train_wall=31, wall=1357
2020-10-12 22:28:46 | INFO | train_inner | epoch 019:    154 / 197 loss=5.936, nll_loss=4.595, ppl=24.17, wps=66391.2, ups=3.11, wpb=21321.4, bsz=812.2, num_updates=3700, lr=0.000185008, gnorm=0.974, clip=0, train_wall=31, wall=1389
2020-10-12 22:29:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21204.43359375Mb; avail=467283.72265625Mb
2020-10-12 22:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001556
2020-10-12 22:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21204.43359375Mb; avail=467283.72265625Mb
2020-10-12 22:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070719
2020-10-12 22:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21204.3125Mb; avail=467283.84375Mb
2020-10-12 22:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049330
2020-10-12 22:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122395
2020-10-12 22:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21204.3125Mb; avail=467283.84375Mb
2020-10-12 22:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21204.3125Mb; avail=467283.84375Mb
2020-10-12 22:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001145
2020-10-12 22:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21204.3125Mb; avail=467283.84375Mb
2020-10-12 22:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070416
2020-10-12 22:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21204.3125Mb; avail=467283.84375Mb
2020-10-12 22:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048717
2020-10-12 22:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121030
2020-10-12 22:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21204.0859375Mb; avail=467283.8515625Mb
2020-10-12 22:29:02 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 5.717 | nll_loss 4.241 | ppl 18.91 | wps 130899 | wpb 6713.4 | bsz 255.6 | num_updates 3743 | best_loss 5.717
2020-10-12 22:29:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:29:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 19 @ 3743 updates, score 5.717) (writing took 4.566810014999646 seconds)
2020-10-12 22:29:07 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 22:29:07 | INFO | train | epoch 019 | loss 5.938 | nll_loss 4.598 | ppl 24.21 | wps 57034.2 | ups 2.69 | wpb 21229.6 | bsz 795.8 | num_updates 3743 | lr 0.000187156 | gnorm 0.909 | clip 0 | train_wall 61 | wall 1410
2020-10-12 22:29:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 22:29:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 22:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21360.046875Mb; avail=467127.0078125Mb
2020-10-12 22:29:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003318
2020-10-12 22:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.031813
2020-10-12 22:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21360.65234375Mb; avail=467126.40234375Mb
2020-10-12 22:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001149
2020-10-12 22:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21360.65234375Mb; avail=467126.40234375Mb
2020-10-12 22:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.550824
2020-10-12 22:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.584674
2020-10-12 22:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21362.72265625Mb; avail=467124.3203125Mb
2020-10-12 22:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21362.34375Mb; avail=467124.69921875Mb
2020-10-12 22:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023042
2020-10-12 22:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21362.34375Mb; avail=467124.69921875Mb
2020-10-12 22:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001130
2020-10-12 22:29:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21362.34375Mb; avail=467124.69921875Mb
2020-10-12 22:29:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.556022
2020-10-12 22:29:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.581331
2020-10-12 22:29:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21362.2109375Mb; avail=467125.5546875Mb
2020-10-12 22:29:08 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 22:29:28 | INFO | train_inner | epoch 020:     57 / 197 loss=5.848, nll_loss=4.494, ppl=22.54, wps=50699.3, ups=2.38, wpb=21301, bsz=804.7, num_updates=3800, lr=0.000190005, gnorm=0.922, clip=0, train_wall=31, wall=1431
2020-10-12 22:30:00 | INFO | train_inner | epoch 020:    157 / 197 loss=5.777, nll_loss=4.412, ppl=21.29, wps=65020, ups=3.1, wpb=20992.9, bsz=801.1, num_updates=3900, lr=0.000195003, gnorm=0.839, clip=0, train_wall=31, wall=1463
2020-10-12 22:30:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21166.30078125Mb; avail=467322.68359375Mb
2020-10-12 22:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001312
2020-10-12 22:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21166.30078125Mb; avail=467322.68359375Mb
2020-10-12 22:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070820
2020-10-12 22:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21166.9296875Mb; avail=467321.859375Mb
2020-10-12 22:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050202
2020-10-12 22:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123133
2020-10-12 22:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21182.71484375Mb; avail=467306.30859375Mb
2020-10-12 22:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21187.39453125Mb; avail=467301.38671875Mb
2020-10-12 22:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001215
2020-10-12 22:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21187.9921875Mb; avail=467301.03125Mb
2020-10-12 22:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071014
2020-10-12 22:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21200.80078125Mb; avail=467288.21484375Mb
2020-10-12 22:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048166
2020-10-12 22:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121194
2020-10-12 22:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21201.36328125Mb; avail=467287.25390625Mb
2020-10-12 22:30:16 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 5.699 | nll_loss 4.211 | ppl 18.52 | wps 131198 | wpb 6713.4 | bsz 255.6 | num_updates 3940 | best_loss 5.699
2020-10-12 22:30:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:30:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 20 @ 3940 updates, score 5.699) (writing took 4.472263273000863 seconds)
2020-10-12 22:30:20 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 22:30:20 | INFO | train | epoch 020 | loss 5.791 | nll_loss 4.428 | ppl 21.52 | wps 56880.8 | ups 2.68 | wpb 21229.6 | bsz 795.8 | num_updates 3940 | lr 0.000197002 | gnorm 0.869 | clip 0 | train_wall 62 | wall 1483
2020-10-12 22:30:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 22:30:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 22:30:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21369.2578125Mb; avail=467118.46875Mb
2020-10-12 22:30:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008345
2020-10-12 22:30:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.058640
2020-10-12 22:30:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21369.3515625Mb; avail=467118.375Mb
2020-10-12 22:30:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001849
2020-10-12 22:30:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21369.3515625Mb; avail=467118.375Mb
2020-10-12 22:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.560640
2020-10-12 22:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.622733
2020-10-12 22:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21369.23828125Mb; avail=467118.62109375Mb
2020-10-12 22:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21369.23828125Mb; avail=467118.62109375Mb
2020-10-12 22:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024202
2020-10-12 22:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21369.23828125Mb; avail=467118.62109375Mb
2020-10-12 22:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001166
2020-10-12 22:30:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21369.23828125Mb; avail=467118.62109375Mb
2020-10-12 22:30:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.554300
2020-10-12 22:30:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.580633
2020-10-12 22:30:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21369.328125Mb; avail=467117.89453125Mb
2020-10-12 22:30:22 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 22:30:43 | INFO | train_inner | epoch 021:     60 / 197 loss=5.725, nll_loss=4.351, ppl=20.4, wps=49761.9, ups=2.34, wpb=21227.8, bsz=777.8, num_updates=4000, lr=0.0002, gnorm=0.82, clip=0, train_wall=32, wall=1506
2020-10-12 22:31:15 | INFO | train_inner | epoch 021:    160 / 197 loss=5.636, nll_loss=4.249, ppl=19.01, wps=66409.2, ups=3.1, wpb=21440, bsz=795.6, num_updates=4100, lr=0.000197546, gnorm=0.879, clip=0, train_wall=31, wall=1538
2020-10-12 22:31:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:31:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21200.78125Mb; avail=467286.8671875Mb
2020-10-12 22:31:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001597
2020-10-12 22:31:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21200.78125Mb; avail=467286.8671875Mb
2020-10-12 22:31:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070320
2020-10-12 22:31:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21199.50390625Mb; avail=467289.1015625Mb
2020-10-12 22:31:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051984
2020-10-12 22:31:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124702
2020-10-12 22:31:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21199.50390625Mb; avail=467289.1015625Mb
2020-10-12 22:31:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21199.50390625Mb; avail=467289.1015625Mb
2020-10-12 22:31:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001174
2020-10-12 22:31:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21199.50390625Mb; avail=467289.1015625Mb
2020-10-12 22:31:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069982
2020-10-12 22:31:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21199.50390625Mb; avail=467289.1015625Mb
2020-10-12 22:31:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049817
2020-10-12 22:31:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121733
2020-10-12 22:31:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21199.50390625Mb; avail=467289.1015625Mb
2020-10-12 22:31:30 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 5.506 | nll_loss 3.977 | ppl 15.74 | wps 126197 | wpb 6713.4 | bsz 255.6 | num_updates 4137 | best_loss 5.506
2020-10-12 22:31:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:31:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 21 @ 4137 updates, score 5.506) (writing took 4.478764313998909 seconds)
2020-10-12 22:31:34 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 22:31:34 | INFO | train | epoch 021 | loss 5.646 | nll_loss 4.26 | ppl 19.16 | wps 56757.3 | ups 2.67 | wpb 21229.6 | bsz 795.8 | num_updates 4137 | lr 0.000196661 | gnorm 0.842 | clip 0 | train_wall 62 | wall 1557
2020-10-12 22:31:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 22:31:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 22:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21366.53515625Mb; avail=467121.2578125Mb
2020-10-12 22:31:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004667
2020-10-12 22:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.036461
2020-10-12 22:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21366.13671875Mb; avail=467121.65625Mb
2020-10-12 22:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001200
2020-10-12 22:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21366.13671875Mb; avail=467121.65625Mb
2020-10-12 22:31:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.553125
2020-10-12 22:31:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.591971
2020-10-12 22:31:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21366.48046875Mb; avail=467121.30859375Mb
2020-10-12 22:31:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21366.48046875Mb; avail=467121.30859375Mb
2020-10-12 22:31:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023371
2020-10-12 22:31:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21366.48046875Mb; avail=467121.30859375Mb
2020-10-12 22:31:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001239
2020-10-12 22:31:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21366.49609375Mb; avail=467121.203125Mb
2020-10-12 22:31:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.546456
2020-10-12 22:31:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.572091
2020-10-12 22:31:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21366.9765625Mb; avail=467120.48046875Mb
2020-10-12 22:31:35 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 22:31:57 | INFO | train_inner | epoch 022:     63 / 197 loss=5.56, nll_loss=4.161, ppl=17.88, wps=49848.5, ups=2.37, wpb=20989.8, bsz=786.5, num_updates=4200, lr=0.00019518, gnorm=0.777, clip=0, train_wall=31, wall=1580
2020-10-12 22:32:29 | INFO | train_inner | epoch 022:    163 / 197 loss=5.52, nll_loss=4.114, ppl=17.31, wps=65682.7, ups=3.09, wpb=21243.2, bsz=779, num_updates=4300, lr=0.000192897, gnorm=0.805, clip=0, train_wall=31, wall=1612
2020-10-12 22:32:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21205.04296875Mb; avail=467283.73828125Mb
2020-10-12 22:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001613
2020-10-12 22:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21205.04296875Mb; avail=467283.73828125Mb
2020-10-12 22:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074454
2020-10-12 22:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21205.04296875Mb; avail=467283.73828125Mb
2020-10-12 22:32:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049810
2020-10-12 22:32:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126830
2020-10-12 22:32:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21204.96875Mb; avail=467284.22265625Mb
2020-10-12 22:32:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21204.96875Mb; avail=467284.22265625Mb
2020-10-12 22:32:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001146
2020-10-12 22:32:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21204.96875Mb; avail=467284.22265625Mb
2020-10-12 22:32:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072523
2020-10-12 22:32:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21204.84765625Mb; avail=467284.34375Mb
2020-10-12 22:32:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051168
2020-10-12 22:32:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125705
2020-10-12 22:32:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21204.84765625Mb; avail=467284.34375Mb
2020-10-12 22:32:43 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 5.418 | nll_loss 3.875 | ppl 14.67 | wps 131569 | wpb 6713.4 | bsz 255.6 | num_updates 4334 | best_loss 5.418
2020-10-12 22:32:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:32:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 22 @ 4334 updates, score 5.418) (writing took 4.5110945270007505 seconds)
2020-10-12 22:32:48 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 22:32:48 | INFO | train | epoch 022 | loss 5.506 | nll_loss 4.099 | ppl 17.13 | wps 56839 | ups 2.68 | wpb 21229.6 | bsz 795.8 | num_updates 4334 | lr 0.000192139 | gnorm 0.795 | clip 0 | train_wall 61 | wall 1631
2020-10-12 22:32:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 22:32:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 22:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21365.328125Mb; avail=467122.44140625Mb
2020-10-12 22:32:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004660
2020-10-12 22:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.036387
2020-10-12 22:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21365.30078125Mb; avail=467122.46875Mb
2020-10-12 22:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001443
2020-10-12 22:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21365.30078125Mb; avail=467122.46875Mb
2020-10-12 22:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.547539
2020-10-12 22:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.586350
2020-10-12 22:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21365.43359375Mb; avail=467122.47265625Mb
2020-10-12 22:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21365.43359375Mb; avail=467122.47265625Mb
2020-10-12 22:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023036
2020-10-12 22:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21365.43359375Mb; avail=467122.47265625Mb
2020-10-12 22:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001140
2020-10-12 22:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21365.43359375Mb; avail=467122.47265625Mb
2020-10-12 22:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.551415
2020-10-12 22:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.576435
2020-10-12 22:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21365.109375Mb; avail=467122.3515625Mb
2020-10-12 22:32:49 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 22:33:11 | INFO | train_inner | epoch 023:     66 / 197 loss=5.422, nll_loss=4.002, ppl=16.02, wps=50099.8, ups=2.38, wpb=21053.1, bsz=797.5, num_updates=4400, lr=0.000190693, gnorm=0.767, clip=0, train_wall=31, wall=1654
2020-10-12 22:33:44 | INFO | train_inner | epoch 023:    166 / 197 loss=5.378, nll_loss=3.951, ppl=15.47, wps=66523.2, ups=3.12, wpb=21345.2, bsz=800.2, num_updates=4500, lr=0.000188562, gnorm=0.785, clip=0, train_wall=31, wall=1687
2020-10-12 22:33:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:33:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21147.53125Mb; avail=467340.9921875Mb
2020-10-12 22:33:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001603
2020-10-12 22:33:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21147.53125Mb; avail=467340.9921875Mb
2020-10-12 22:33:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069936
2020-10-12 22:33:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21153.5859375Mb; avail=467334.9375Mb
2020-10-12 22:33:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049263
2020-10-12 22:33:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121585
2020-10-12 22:33:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21157.00390625Mb; avail=467332.02734375Mb
2020-10-12 22:33:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21157.72265625Mb; avail=467331.30859375Mb
2020-10-12 22:33:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001136
2020-10-12 22:33:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21157.72265625Mb; avail=467331.30859375Mb
2020-10-12 22:33:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073263
2020-10-12 22:33:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21159.0546875Mb; avail=467329.9765625Mb
2020-10-12 22:33:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049050
2020-10-12 22:33:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124311
2020-10-12 22:33:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21160.87109375Mb; avail=467328.16015625Mb
2020-10-12 22:33:56 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 5.346 | nll_loss 3.785 | ppl 13.79 | wps 122836 | wpb 6713.4 | bsz 255.6 | num_updates 4531 | best_loss 5.346
2020-10-12 22:33:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:34:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 23 @ 4531 updates, score 5.346) (writing took 4.4947890090006695 seconds)
2020-10-12 22:34:01 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 22:34:01 | INFO | train | epoch 023 | loss 5.39 | nll_loss 3.964 | ppl 15.61 | wps 57006.8 | ups 2.69 | wpb 21229.6 | bsz 795.8 | num_updates 4531 | lr 0.000187916 | gnorm 0.772 | clip 0 | train_wall 61 | wall 1704
2020-10-12 22:34:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 22:34:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 22:34:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21366.96875Mb; avail=467120.81640625Mb
2020-10-12 22:34:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003741
2020-10-12 22:34:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035985
2020-10-12 22:34:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21367.57421875Mb; avail=467120.2109375Mb
2020-10-12 22:34:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001283
2020-10-12 22:34:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21367.57421875Mb; avail=467120.2109375Mb
2020-10-12 22:34:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.594393
2020-10-12 22:34:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.632912
2020-10-12 22:34:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21370.2890625Mb; avail=467117.66796875Mb
2020-10-12 22:34:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21369.3046875Mb; avail=467118.65234375Mb
2020-10-12 22:34:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024360
2020-10-12 22:34:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21369.3046875Mb; avail=467118.65234375Mb
2020-10-12 22:34:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001210
2020-10-12 22:34:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21369.3046875Mb; avail=467118.65234375Mb
2020-10-12 22:34:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.549501
2020-10-12 22:34:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.576031
2020-10-12 22:34:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21368.02734375Mb; avail=467119.84765625Mb
2020-10-12 22:34:02 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 22:34:26 | INFO | train_inner | epoch 024:     69 / 197 loss=5.311, nll_loss=3.873, ppl=14.65, wps=50654.1, ups=2.36, wpb=21477.2, bsz=813.8, num_updates=4600, lr=0.000186501, gnorm=0.766, clip=0, train_wall=31, wall=1729
2020-10-12 22:34:59 | INFO | train_inner | epoch 024:    169 / 197 loss=5.313, nll_loss=3.875, ppl=14.68, wps=65144.6, ups=3.07, wpb=21242.9, bsz=765.8, num_updates=4700, lr=0.000184506, gnorm=0.778, clip=0, train_wall=32, wall=1762
2020-10-12 22:35:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21202.11328125Mb; avail=467286.046875Mb
2020-10-12 22:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001554
2020-10-12 22:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21202.11328125Mb; avail=467286.046875Mb
2020-10-12 22:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071529
2020-10-12 22:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21202.23046875Mb; avail=467285.92578125Mb
2020-10-12 22:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049618
2020-10-12 22:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123563
2020-10-12 22:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21202.23046875Mb; avail=467285.92578125Mb
2020-10-12 22:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21202.23046875Mb; avail=467285.92578125Mb
2020-10-12 22:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001118
2020-10-12 22:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21202.23046875Mb; avail=467285.92578125Mb
2020-10-12 22:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074763
2020-10-12 22:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21202.53125Mb; avail=467285.8125Mb
2020-10-12 22:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052026
2020-10-12 22:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129031
2020-10-12 22:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21201.28125Mb; avail=467287.29296875Mb
2020-10-12 22:35:10 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.287 | nll_loss 3.717 | ppl 13.15 | wps 130501 | wpb 6713.4 | bsz 255.6 | num_updates 4728 | best_loss 5.287
2020-10-12 22:35:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:35:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 24 @ 4728 updates, score 5.287) (writing took 4.502200696999353 seconds)
2020-10-12 22:35:15 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 22:35:15 | INFO | train | epoch 024 | loss 5.285 | nll_loss 3.844 | ppl 14.36 | wps 56557.6 | ups 2.66 | wpb 21229.6 | bsz 795.8 | num_updates 4728 | lr 0.000183959 | gnorm 0.776 | clip 0 | train_wall 62 | wall 1778
2020-10-12 22:35:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 22:35:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 22:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21365.15234375Mb; avail=467122.671875Mb
2020-10-12 22:35:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004921
2020-10-12 22:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.036965
2020-10-12 22:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21364.66015625Mb; avail=467123.1640625Mb
2020-10-12 22:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001172
2020-10-12 22:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21364.66015625Mb; avail=467123.1640625Mb
2020-10-12 22:35:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.562138
2020-10-12 22:35:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.601260
2020-10-12 22:35:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21364.78515625Mb; avail=467122.78515625Mb
2020-10-12 22:35:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21365.3515625Mb; avail=467122.30078125Mb
2020-10-12 22:35:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022953
2020-10-12 22:35:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21365.3515625Mb; avail=467122.30078125Mb
2020-10-12 22:35:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001134
2020-10-12 22:35:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21365.3515625Mb; avail=467122.30078125Mb
2020-10-12 22:35:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.538972
2020-10-12 22:35:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.563904
2020-10-12 22:35:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21365.34375Mb; avail=467122.5390625Mb
2020-10-12 22:35:16 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 22:35:41 | INFO | train_inner | epoch 025:     72 / 197 loss=5.189, nll_loss=3.735, ppl=13.32, wps=49782.8, ups=2.35, wpb=21148, bsz=830.6, num_updates=4800, lr=0.000182574, gnorm=0.721, clip=0, train_wall=32, wall=1804
2020-10-12 22:36:14 | INFO | train_inner | epoch 025:    172 / 197 loss=5.188, nll_loss=3.732, ppl=13.29, wps=64894.3, ups=3.06, wpb=21175.2, bsz=806.9, num_updates=4900, lr=0.000180702, gnorm=0.756, clip=0, train_wall=32, wall=1837
2020-10-12 22:36:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21209.71484375Mb; avail=467277.484375Mb
2020-10-12 22:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001710
2020-10-12 22:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21209.71484375Mb; avail=467277.484375Mb
2020-10-12 22:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.126562
2020-10-12 22:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21209.69140625Mb; avail=467277.7265625Mb
2020-10-12 22:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049244
2020-10-12 22:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.178634
2020-10-12 22:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21209.69140625Mb; avail=467277.7265625Mb
2020-10-12 22:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21206.73046875Mb; avail=467281.8203125Mb
2020-10-12 22:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001178
2020-10-12 22:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21206.73046875Mb; avail=467281.8203125Mb
2020-10-12 22:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083014
2020-10-12 22:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21206.9921875Mb; avail=467281.5859375Mb
2020-10-12 22:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.089448
2020-10-12 22:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.174643
2020-10-12 22:36:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21218.04296875Mb; avail=467270.3125Mb
2020-10-12 22:36:24 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.224 | nll_loss 3.651 | ppl 12.57 | wps 129396 | wpb 6713.4 | bsz 255.6 | num_updates 4925 | best_loss 5.224
2020-10-12 22:36:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:36:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 25 @ 4925 updates, score 5.224) (writing took 4.4884762940000655 seconds)
2020-10-12 22:36:29 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 22:36:29 | INFO | train | epoch 025 | loss 5.193 | nll_loss 3.738 | ppl 13.34 | wps 56521.1 | ups 2.66 | wpb 21229.6 | bsz 795.8 | num_updates 4925 | lr 0.000180242 | gnorm 0.737 | clip 0 | train_wall 62 | wall 1852
2020-10-12 22:36:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 22:36:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 22:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21370.19921875Mb; avail=467117.703125Mb
2020-10-12 22:36:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004082
2020-10-12 22:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033205
2020-10-12 22:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21370.8046875Mb; avail=467117.09765625Mb
2020-10-12 22:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001132
2020-10-12 22:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21370.8046875Mb; avail=467117.09765625Mb
2020-10-12 22:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.556597
2020-10-12 22:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.591964
2020-10-12 22:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21370.578125Mb; avail=467117.06640625Mb
2020-10-12 22:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21371.0625Mb; avail=467116.58203125Mb
2020-10-12 22:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023100
2020-10-12 22:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21371.0625Mb; avail=467116.58203125Mb
2020-10-12 22:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001130
2020-10-12 22:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21371.0625Mb; avail=467116.58203125Mb
2020-10-12 22:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.557210
2020-10-12 22:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.582358
2020-10-12 22:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21370.59765625Mb; avail=467116.703125Mb
2020-10-12 22:36:30 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 22:36:56 | INFO | train_inner | epoch 026:     75 / 197 loss=5.134, nll_loss=3.671, ppl=12.74, wps=50698.6, ups=2.38, wpb=21287.8, bsz=769.6, num_updates=5000, lr=0.000178885, gnorm=0.699, clip=0, train_wall=31, wall=1879
2020-10-12 22:37:28 | INFO | train_inner | epoch 026:    175 / 197 loss=5.099, nll_loss=3.63, ppl=12.38, wps=65048.5, ups=3.08, wpb=21105.5, bsz=794.4, num_updates=5100, lr=0.000177123, gnorm=0.702, clip=0, train_wall=31, wall=1911
2020-10-12 22:37:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:37:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21206.4765625Mb; avail=467281.93359375Mb
2020-10-12 22:37:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001612
2020-10-12 22:37:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21206.4765625Mb; avail=467281.93359375Mb
2020-10-12 22:37:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.125528
2020-10-12 22:37:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21206.5703125Mb; avail=467281.8203125Mb
2020-10-12 22:37:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049284
2020-10-12 22:37:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.177267
2020-10-12 22:37:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21206.5234375Mb; avail=467281.45703125Mb
2020-10-12 22:37:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21206.5234375Mb; avail=467281.45703125Mb
2020-10-12 22:37:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001087
2020-10-12 22:37:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21206.5234375Mb; avail=467281.45703125Mb
2020-10-12 22:37:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070478
2020-10-12 22:37:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21206.5234375Mb; avail=467281.45703125Mb
2020-10-12 22:37:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049297
2020-10-12 22:37:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121617
2020-10-12 22:37:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21208.83203125Mb; avail=467279.1484375Mb
2020-10-12 22:37:38 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.163 | nll_loss 3.571 | ppl 11.88 | wps 131352 | wpb 6713.4 | bsz 255.6 | num_updates 5122 | best_loss 5.163
2020-10-12 22:37:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:37:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 26 @ 5122 updates, score 5.163) (writing took 4.475406387000476 seconds)
2020-10-12 22:37:43 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 22:37:43 | INFO | train | epoch 026 | loss 5.1 | nll_loss 3.632 | ppl 12.4 | wps 56833.9 | ups 2.68 | wpb 21229.6 | bsz 795.8 | num_updates 5122 | lr 0.000176742 | gnorm 0.696 | clip 0 | train_wall 61 | wall 1926
2020-10-12 22:37:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 22:37:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 22:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21368.2109375Mb; avail=467119.6171875Mb
2020-10-12 22:37:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003720
2020-10-12 22:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035189
2020-10-12 22:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21368.3046875Mb; avail=467119.5234375Mb
2020-10-12 22:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001199
2020-10-12 22:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21368.3046875Mb; avail=467119.5234375Mb
2020-10-12 22:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.553852
2020-10-12 22:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.591378
2020-10-12 22:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21367.52734375Mb; avail=467120.2890625Mb
2020-10-12 22:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21367.52734375Mb; avail=467120.2890625Mb
2020-10-12 22:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024561
2020-10-12 22:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21367.52734375Mb; avail=467120.2890625Mb
2020-10-12 22:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001169
2020-10-12 22:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21367.52734375Mb; avail=467120.2890625Mb
2020-10-12 22:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.546736
2020-10-12 22:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.573438
2020-10-12 22:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21366.46875Mb; avail=467121.37890625Mb
2020-10-12 22:37:44 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 22:38:10 | INFO | train_inner | epoch 027:     78 / 197 loss=5.03, nll_loss=3.551, ppl=11.72, wps=50150.3, ups=2.37, wpb=21199, bsz=797, num_updates=5200, lr=0.000175412, gnorm=0.678, clip=0, train_wall=31, wall=1953
2020-10-12 22:38:43 | INFO | train_inner | epoch 027:    178 / 197 loss=5.027, nll_loss=3.548, ppl=11.69, wps=66467.9, ups=3.11, wpb=21380.1, bsz=796.4, num_updates=5300, lr=0.000173749, gnorm=0.685, clip=0, train_wall=31, wall=1986
2020-10-12 22:38:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:38:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21209.171875Mb; avail=467280.63671875Mb
2020-10-12 22:38:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001662
2020-10-12 22:38:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21209.171875Mb; avail=467280.63671875Mb
2020-10-12 22:38:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071543
2020-10-12 22:38:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21208.79296875Mb; avail=467280.03125Mb
2020-10-12 22:38:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049176
2020-10-12 22:38:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123193
2020-10-12 22:38:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21208.96484375Mb; avail=467280.2734375Mb
2020-10-12 22:38:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21208.96484375Mb; avail=467280.2734375Mb
2020-10-12 22:38:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001078
2020-10-12 22:38:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21208.96484375Mb; avail=467280.2734375Mb
2020-10-12 22:38:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070573
2020-10-12 22:38:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21208.77734375Mb; avail=467280.63671875Mb
2020-10-12 22:38:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048943
2020-10-12 22:38:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121375
2020-10-12 22:38:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21208.77734375Mb; avail=467280.63671875Mb
2020-10-12 22:38:51 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.114 | nll_loss 3.514 | ppl 11.42 | wps 128262 | wpb 6713.4 | bsz 255.6 | num_updates 5319 | best_loss 5.114
2020-10-12 22:38:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:38:56 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 27 @ 5319 updates, score 5.114) (writing took 4.518752043000859 seconds)
2020-10-12 22:38:56 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 22:38:56 | INFO | train | epoch 027 | loss 5.022 | nll_loss 3.541 | ppl 11.64 | wps 57036.1 | ups 2.69 | wpb 21229.6 | bsz 795.8 | num_updates 5319 | lr 0.000173438 | gnorm 0.681 | clip 0 | train_wall 61 | wall 1999
2020-10-12 22:38:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 22:38:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 22:38:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21373.1796875Mb; avail=467115.01953125Mb
2020-10-12 22:38:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003562
2020-10-12 22:38:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032916
2020-10-12 22:38:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21372.4453125Mb; avail=467115.75390625Mb
2020-10-12 22:38:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001213
2020-10-12 22:38:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21372.4453125Mb; avail=467115.75390625Mb
2020-10-12 22:38:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.557181
2020-10-12 22:38:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.592204
2020-10-12 22:38:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21372.765625Mb; avail=467114.7890625Mb
2020-10-12 22:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21372.4296875Mb; avail=467115.046875Mb
2020-10-12 22:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023726
2020-10-12 22:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21372.2890625Mb; avail=467115.046875Mb
2020-10-12 22:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001243
2020-10-12 22:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21372.2890625Mb; avail=467115.046875Mb
2020-10-12 22:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.557347
2020-10-12 22:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.583459
2020-10-12 22:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21370.9453125Mb; avail=467116.484375Mb
2020-10-12 22:38:57 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 22:39:25 | INFO | train_inner | epoch 028:     81 / 197 loss=4.961, nll_loss=3.473, ppl=11.1, wps=50487, ups=2.36, wpb=21381.2, bsz=812.2, num_updates=5400, lr=0.000172133, gnorm=0.701, clip=0, train_wall=31, wall=2028
2020-10-12 22:39:57 | INFO | train_inner | epoch 028:    181 / 197 loss=4.949, nll_loss=3.458, ppl=10.99, wps=65627.3, ups=3.09, wpb=21265.6, bsz=801.1, num_updates=5500, lr=0.000170561, gnorm=0.673, clip=0, train_wall=31, wall=2060
2020-10-12 22:40:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:40:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21210.47265625Mb; avail=467277.6796875Mb
2020-10-12 22:40:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001706
2020-10-12 22:40:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21210.47265625Mb; avail=467277.6796875Mb
2020-10-12 22:40:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071783
2020-10-12 22:40:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21210.47265625Mb; avail=467277.6796875Mb
2020-10-12 22:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049056
2020-10-12 22:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123347
2020-10-12 22:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21210.47265625Mb; avail=467277.6796875Mb
2020-10-12 22:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21210.47265625Mb; avail=467277.6796875Mb
2020-10-12 22:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001080
2020-10-12 22:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21210.47265625Mb; avail=467277.6796875Mb
2020-10-12 22:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070310
2020-10-12 22:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21210.47265625Mb; avail=467277.6796875Mb
2020-10-12 22:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048020
2020-10-12 22:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120192
2020-10-12 22:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21210.51953125Mb; avail=467277.4375Mb
2020-10-12 22:40:05 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.062 | nll_loss 3.466 | ppl 11.05 | wps 127979 | wpb 6713.4 | bsz 255.6 | num_updates 5516 | best_loss 5.062
2020-10-12 22:40:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:40:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 28 @ 5516 updates, score 5.062) (writing took 4.477172024000538 seconds)
2020-10-12 22:40:10 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 22:40:10 | INFO | train | epoch 028 | loss 4.953 | nll_loss 3.463 | ppl 11.03 | wps 56718 | ups 2.67 | wpb 21229.6 | bsz 795.8 | num_updates 5516 | lr 0.000170313 | gnorm 0.685 | clip 0 | train_wall 62 | wall 2073
2020-10-12 22:40:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 22:40:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 22:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21371.60546875Mb; avail=467116.6640625Mb
2020-10-12 22:40:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004153
2020-10-12 22:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035086
2020-10-12 22:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21371.69921875Mb; avail=467116.5703125Mb
2020-10-12 22:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001163
2020-10-12 22:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21371.69921875Mb; avail=467116.5703125Mb
2020-10-12 22:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.553039
2020-10-12 22:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.590499
2020-10-12 22:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21371.55859375Mb; avail=467116.3359375Mb
2020-10-12 22:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21371.56640625Mb; avail=467116.09375Mb
2020-10-12 22:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025595
2020-10-12 22:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21371.59765625Mb; avail=467115.97265625Mb
2020-10-12 22:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001159
2020-10-12 22:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21371.59765625Mb; avail=467115.97265625Mb
2020-10-12 22:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.542570
2020-10-12 22:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.570221
2020-10-12 22:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21371.35546875Mb; avail=467117.0625Mb
2020-10-12 22:40:11 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 22:40:39 | INFO | train_inner | epoch 029:     84 / 197 loss=4.903, nll_loss=3.406, ppl=10.6, wps=50201.9, ups=2.4, wpb=20951.1, bsz=791.4, num_updates=5600, lr=0.000169031, gnorm=0.717, clip=0, train_wall=31, wall=2102
2020-10-12 22:41:12 | INFO | train_inner | epoch 029:    184 / 197 loss=4.898, nll_loss=3.399, ppl=10.55, wps=65360.1, ups=3.07, wpb=21318.1, bsz=777.8, num_updates=5700, lr=0.000167542, gnorm=0.657, clip=0, train_wall=31, wall=2135
2020-10-12 22:41:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21214.08984375Mb; avail=467274.3046875Mb
2020-10-12 22:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001631
2020-10-12 22:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21214.08984375Mb; avail=467274.3046875Mb
2020-10-12 22:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070557
2020-10-12 22:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21213.04296875Mb; avail=467275.58984375Mb
2020-10-12 22:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051552
2020-10-12 22:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124570
2020-10-12 22:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21212.9375Mb; avail=467275.484375Mb
2020-10-12 22:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21212.7734375Mb; avail=467275.84765625Mb
2020-10-12 22:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001128
2020-10-12 22:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21212.7734375Mb; avail=467275.84765625Mb
2020-10-12 22:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072331
2020-10-12 22:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21213.0Mb; avail=467275.60546875Mb
2020-10-12 22:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049479
2020-10-12 22:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123683
2020-10-12 22:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21213.0Mb; avail=467275.60546875Mb
2020-10-12 22:41:19 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.057 | nll_loss 3.453 | ppl 10.95 | wps 129143 | wpb 6713.4 | bsz 255.6 | num_updates 5713 | best_loss 5.057
2020-10-12 22:41:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:41:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 29 @ 5713 updates, score 5.057) (writing took 4.490854980000222 seconds)
2020-10-12 22:41:23 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 22:41:23 | INFO | train | epoch 029 | loss 4.89 | nll_loss 3.391 | ppl 10.49 | wps 56912.9 | ups 2.68 | wpb 21229.6 | bsz 795.8 | num_updates 5713 | lr 0.000167351 | gnorm 0.688 | clip 0 | train_wall 61 | wall 2146
2020-10-12 22:41:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 22:41:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 22:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21374.0234375Mb; avail=467113.75Mb
2020-10-12 22:41:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005413
2020-10-12 22:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035311
2020-10-12 22:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21374.0234375Mb; avail=467113.75Mb
2020-10-12 22:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001195
2020-10-12 22:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21374.0234375Mb; avail=467113.75Mb
2020-10-12 22:41:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.554586
2020-10-12 22:41:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.592191
2020-10-12 22:41:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21374.16796875Mb; avail=467113.87109375Mb
2020-10-12 22:41:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21374.73828125Mb; avail=467113.30078125Mb
2020-10-12 22:41:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022781
2020-10-12 22:41:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21374.73828125Mb; avail=467113.30078125Mb
2020-10-12 22:41:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001144
2020-10-12 22:41:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21374.73828125Mb; avail=467113.30078125Mb
2020-10-12 22:41:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.550718
2020-10-12 22:41:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.575473
2020-10-12 22:41:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21374.13671875Mb; avail=467114.02734375Mb
2020-10-12 22:41:24 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 22:41:54 | INFO | train_inner | epoch 030:     87 / 197 loss=4.834, nll_loss=3.327, ppl=10.04, wps=50524.5, ups=2.38, wpb=21259.3, bsz=818, num_updates=5800, lr=0.000166091, gnorm=0.705, clip=0, train_wall=31, wall=2177
2020-10-12 22:42:26 | INFO | train_inner | epoch 030:    187 / 197 loss=4.833, nll_loss=3.326, ppl=10.03, wps=65717.6, ups=3.09, wpb=21257.8, bsz=779.4, num_updates=5900, lr=0.000164677, gnorm=0.631, clip=0, train_wall=31, wall=2209
2020-10-12 22:42:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:42:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21206.5390625Mb; avail=467281.77734375Mb
2020-10-12 22:42:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001607
2020-10-12 22:42:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21206.5390625Mb; avail=467281.77734375Mb
2020-10-12 22:42:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071324
2020-10-12 22:42:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21206.71484375Mb; avail=467281.29296875Mb
2020-10-12 22:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.090991
2020-10-12 22:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.164850
2020-10-12 22:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21206.72265625Mb; avail=467281.171875Mb
2020-10-12 22:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21206.72265625Mb; avail=467281.171875Mb
2020-10-12 22:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001138
2020-10-12 22:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21206.72265625Mb; avail=467281.171875Mb
2020-10-12 22:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071091
2020-10-12 22:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21206.72265625Mb; avail=467281.171875Mb
2020-10-12 22:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048798
2020-10-12 22:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121810
2020-10-12 22:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21211.375Mb; avail=467276.3125Mb
2020-10-12 22:42:32 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.013 | nll_loss 3.406 | ppl 10.6 | wps 130165 | wpb 6713.4 | bsz 255.6 | num_updates 5910 | best_loss 5.013
2020-10-12 22:42:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:42:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 30 @ 5910 updates, score 5.013) (writing took 4.5695786270007375 seconds)
2020-10-12 22:42:37 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 22:42:37 | INFO | train | epoch 030 | loss 4.832 | nll_loss 3.325 | ppl 10.02 | wps 56780.5 | ups 2.67 | wpb 21229.6 | bsz 795.8 | num_updates 5910 | lr 0.000164538 | gnorm 0.669 | clip 0 | train_wall 62 | wall 2220
2020-10-12 22:42:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 22:42:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 22:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21372.5703125Mb; avail=467115.390625Mb
2020-10-12 22:42:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003721
2020-10-12 22:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035813
2020-10-12 22:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21373.16015625Mb; avail=467114.80078125Mb
2020-10-12 22:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001231
2020-10-12 22:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21373.16015625Mb; avail=467114.80078125Mb
2020-10-12 22:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.549109
2020-10-12 22:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.587184
2020-10-12 22:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21373.765625Mb; avail=467113.47265625Mb
2020-10-12 22:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21373.359375Mb; avail=467114.27734375Mb
2020-10-12 22:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023229
2020-10-12 22:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21373.359375Mb; avail=467114.27734375Mb
2020-10-12 22:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001139
2020-10-12 22:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21373.359375Mb; avail=467114.27734375Mb
2020-10-12 22:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.555683
2020-10-12 22:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.581194
2020-10-12 22:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21372.95703125Mb; avail=467114.9140625Mb
2020-10-12 22:42:38 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 22:43:08 | INFO | train_inner | epoch 031:     90 / 197 loss=4.782, nll_loss=3.268, ppl=9.63, wps=50069.6, ups=2.36, wpb=21180.4, bsz=782.2, num_updates=6000, lr=0.000163299, gnorm=0.674, clip=0, train_wall=31, wall=2251
2020-10-12 22:43:41 | INFO | train_inner | epoch 031:    190 / 197 loss=4.78, nll_loss=3.266, ppl=9.62, wps=65272.7, ups=3.08, wpb=21196.8, bsz=799.9, num_updates=6100, lr=0.000161955, gnorm=0.641, clip=0, train_wall=31, wall=2284
2020-10-12 22:43:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21217.55078125Mb; avail=467271.30078125Mb
2020-10-12 22:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001549
2020-10-12 22:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21217.55078125Mb; avail=467271.30078125Mb
2020-10-12 22:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071201
2020-10-12 22:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21217.48828125Mb; avail=467271.55078125Mb
2020-10-12 22:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053042
2020-10-12 22:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126649
2020-10-12 22:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21217.3125Mb; avail=467271.9140625Mb
2020-10-12 22:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21217.3125Mb; avail=467271.9140625Mb
2020-10-12 22:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001282
2020-10-12 22:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21217.3125Mb; avail=467271.9140625Mb
2020-10-12 22:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072361
2020-10-12 22:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21217.3125Mb; avail=467271.9140625Mb
2020-10-12 22:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049770
2020-10-12 22:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124236
2020-10-12 22:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21223.74609375Mb; avail=467265.48046875Mb
2020-10-12 22:43:46 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 4.969 | nll_loss 3.361 | ppl 10.27 | wps 131892 | wpb 6713.4 | bsz 255.6 | num_updates 6107 | best_loss 4.969
2020-10-12 22:43:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:43:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 31 @ 6107 updates, score 4.969) (writing took 4.483044999999038 seconds)
2020-10-12 22:43:50 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 22:43:50 | INFO | train | epoch 031 | loss 4.776 | nll_loss 3.261 | ppl 9.59 | wps 56808.7 | ups 2.68 | wpb 21229.6 | bsz 795.8 | num_updates 6107 | lr 0.000161862 | gnorm 0.654 | clip 0 | train_wall 62 | wall 2293
2020-10-12 22:43:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 22:43:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 22:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21377.21875Mb; avail=467110.5234375Mb
2020-10-12 22:43:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004483
2020-10-12 22:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.036084
2020-10-12 22:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21377.046875Mb; avail=467110.88671875Mb
2020-10-12 22:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001165
2020-10-12 22:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21377.046875Mb; avail=467110.88671875Mb
2020-10-12 22:43:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.547393
2020-10-12 22:43:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.585602
2020-10-12 22:43:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.91796875Mb; avail=467111.890625Mb
2020-10-12 22:43:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21375.3515625Mb; avail=467112.26171875Mb
2020-10-12 22:43:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023165
2020-10-12 22:43:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.44921875Mb; avail=467112.3828125Mb
2020-10-12 22:43:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001151
2020-10-12 22:43:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.33984375Mb; avail=467112.625Mb
2020-10-12 22:43:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.546547
2020-10-12 22:43:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.571718
2020-10-12 22:43:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.61328125Mb; avail=467112.140625Mb
2020-10-12 22:43:52 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 22:44:23 | INFO | train_inner | epoch 032:     93 / 197 loss=4.739, nll_loss=3.218, ppl=9.31, wps=50573.9, ups=2.37, wpb=21306.3, bsz=781.2, num_updates=6200, lr=0.000160644, gnorm=0.634, clip=0, train_wall=31, wall=2326
2020-10-12 22:44:56 | INFO | train_inner | epoch 032:    193 / 197 loss=4.708, nll_loss=3.184, ppl=9.09, wps=64924.6, ups=3.06, wpb=21228.7, bsz=824.3, num_updates=6300, lr=0.000159364, gnorm=0.642, clip=0, train_wall=32, wall=2359
2020-10-12 22:44:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:44:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21213.02734375Mb; avail=467275.5859375Mb
2020-10-12 22:44:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001608
2020-10-12 22:44:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21213.02734375Mb; avail=467275.5859375Mb
2020-10-12 22:44:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071690
2020-10-12 22:44:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21213.02734375Mb; avail=467275.5859375Mb
2020-10-12 22:44:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049221
2020-10-12 22:44:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123308
2020-10-12 22:44:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21212.90625Mb; avail=467275.70703125Mb
2020-10-12 22:44:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21212.80859375Mb; avail=467275.5859375Mb
2020-10-12 22:44:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001118
2020-10-12 22:44:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21212.80859375Mb; avail=467275.5859375Mb
2020-10-12 22:44:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071194
2020-10-12 22:44:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21213.05859375Mb; avail=467275.34375Mb
2020-10-12 22:44:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049446
2020-10-12 22:44:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122558
2020-10-12 22:44:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21213.05859375Mb; avail=467275.34375Mb
2020-10-12 22:45:00 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 4.928 | nll_loss 3.311 | ppl 9.92 | wps 130785 | wpb 6713.4 | bsz 255.6 | num_updates 6304 | best_loss 4.928
2020-10-12 22:45:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:45:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 32 @ 6304 updates, score 4.928) (writing took 4.462846289001391 seconds)
2020-10-12 22:45:04 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 22:45:04 | INFO | train | epoch 032 | loss 4.724 | nll_loss 3.202 | ppl 9.2 | wps 56633.5 | ups 2.67 | wpb 21229.6 | bsz 795.8 | num_updates 6304 | lr 0.000159313 | gnorm 0.638 | clip 0 | train_wall 62 | wall 2367
2020-10-12 22:45:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 22:45:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 22:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21372.77734375Mb; avail=467114.89453125Mb
2020-10-12 22:45:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003343
2020-10-12 22:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.031817
2020-10-12 22:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21372.28125Mb; avail=467115.5078125Mb
2020-10-12 22:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001121
2020-10-12 22:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21372.28125Mb; avail=467115.5078125Mb
2020-10-12 22:45:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.546290
2020-10-12 22:45:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.580082
2020-10-12 22:45:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21372.1640625Mb; avail=467115.3984375Mb
2020-10-12 22:45:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21372.265625Mb; avail=467114.79296875Mb
2020-10-12 22:45:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025180
2020-10-12 22:45:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21372.71484375Mb; avail=467114.45703125Mb
2020-10-12 22:45:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001193
2020-10-12 22:45:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21372.71484375Mb; avail=467114.45703125Mb
2020-10-12 22:45:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.549760
2020-10-12 22:45:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.577237
2020-10-12 22:45:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21374.90234375Mb; avail=467112.9375Mb
2020-10-12 22:45:05 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 22:45:38 | INFO | train_inner | epoch 033:     96 / 197 loss=4.679, nll_loss=3.15, ppl=8.88, wps=50748.6, ups=2.37, wpb=21401.3, bsz=786.7, num_updates=6400, lr=0.000158114, gnorm=0.643, clip=0, train_wall=31, wall=2401
2020-10-12 22:46:10 | INFO | train_inner | epoch 033:    196 / 197 loss=4.68, nll_loss=3.151, ppl=8.88, wps=64550.7, ups=3.06, wpb=21081.9, bsz=804.4, num_updates=6500, lr=0.000156893, gnorm=0.655, clip=0, train_wall=32, wall=2434
2020-10-12 22:46:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:46:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21186.99609375Mb; avail=467301.828125Mb
2020-10-12 22:46:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001601
2020-10-12 22:46:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21186.99609375Mb; avail=467301.828125Mb
2020-10-12 22:46:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070437
2020-10-12 22:46:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21190.17578125Mb; avail=467298.6484375Mb
2020-10-12 22:46:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049174
2020-10-12 22:46:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122004
2020-10-12 22:46:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21189.671875Mb; avail=467299.15234375Mb
2020-10-12 22:46:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21188.6640625Mb; avail=467300.29296875Mb
2020-10-12 22:46:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001205
2020-10-12 22:46:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21188.6640625Mb; avail=467300.29296875Mb
2020-10-12 22:46:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070175
2020-10-12 22:46:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21206.98046875Mb; avail=467281.9765625Mb
2020-10-12 22:46:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048906
2020-10-12 22:46:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121137
2020-10-12 22:46:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21213.96875Mb; avail=467274.8828125Mb
2020-10-12 22:46:14 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 4.911 | nll_loss 3.29 | ppl 9.78 | wps 129206 | wpb 6713.4 | bsz 255.6 | num_updates 6501 | best_loss 4.911
2020-10-12 22:46:14 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:46:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 33 @ 6501 updates, score 4.911) (writing took 4.471329068001069 seconds)
2020-10-12 22:46:18 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 22:46:18 | INFO | train | epoch 033 | loss 4.678 | nll_loss 3.15 | ppl 8.87 | wps 56649.4 | ups 2.67 | wpb 21229.6 | bsz 795.8 | num_updates 6501 | lr 0.000156881 | gnorm 0.65 | clip 0 | train_wall 62 | wall 2441
2020-10-12 22:46:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 22:46:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 22:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21377.4765625Mb; avail=467110.01953125Mb
2020-10-12 22:46:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004047
2020-10-12 22:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033083
2020-10-12 22:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21377.546875Mb; avail=467110.3828125Mb
2020-10-12 22:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001232
2020-10-12 22:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21377.546875Mb; avail=467110.3828125Mb
2020-10-12 22:46:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.545372
2020-10-12 22:46:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.580545
2020-10-12 22:46:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21377.7421875Mb; avail=467109.80078125Mb
2020-10-12 22:46:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21378.3203125Mb; avail=467109.22265625Mb
2020-10-12 22:46:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023042
2020-10-12 22:46:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21378.3203125Mb; avail=467109.22265625Mb
2020-10-12 22:46:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001140
2020-10-12 22:46:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21378.3203125Mb; avail=467109.22265625Mb
2020-10-12 22:46:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.545699
2020-10-12 22:46:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.570784
2020-10-12 22:46:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21378.6796875Mb; avail=467109.46875Mb
2020-10-12 22:46:19 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 22:46:53 | INFO | train_inner | epoch 034:     99 / 197 loss=4.623, nll_loss=3.087, ppl=8.5, wps=50654, ups=2.38, wpb=21306.4, bsz=799, num_updates=6600, lr=0.0001557, gnorm=0.632, clip=0, train_wall=31, wall=2476
2020-10-12 22:47:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:47:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21199.765625Mb; avail=467288.45703125Mb
2020-10-12 22:47:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001690
2020-10-12 22:47:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21199.5078125Mb; avail=467288.72265625Mb
2020-10-12 22:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.123699
2020-10-12 22:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21199.5Mb; avail=467289.46875Mb
2020-10-12 22:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.086664
2020-10-12 22:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.213223
2020-10-12 22:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21199.046875Mb; avail=467289.25Mb
2020-10-12 22:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21199.21875Mb; avail=467289.37109375Mb
2020-10-12 22:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001279
2020-10-12 22:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21199.6953125Mb; avail=467289.14453125Mb
2020-10-12 22:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070682
2020-10-12 22:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21206.1640625Mb; avail=467282.28515625Mb
2020-10-12 22:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048834
2020-10-12 22:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121838
2020-10-12 22:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21211.59375Mb; avail=467277.7890625Mb
2020-10-12 22:47:27 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.895 | nll_loss 3.271 | ppl 9.65 | wps 129575 | wpb 6713.4 | bsz 255.6 | num_updates 6698 | best_loss 4.895
2020-10-12 22:47:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:47:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 34 @ 6698 updates, score 4.895) (writing took 4.524963216998003 seconds)
2020-10-12 22:47:32 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 22:47:32 | INFO | train | epoch 034 | loss 4.634 | nll_loss 3.099 | ppl 8.57 | wps 56796.2 | ups 2.68 | wpb 21229.6 | bsz 795.8 | num_updates 6698 | lr 0.000154557 | gnorm 0.646 | clip 0 | train_wall 62 | wall 2515
2020-10-12 22:47:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 22:47:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 22:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21380.03125Mb; avail=467107.7421875Mb
2020-10-12 22:47:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003818
2020-10-12 22:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034954
2020-10-12 22:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21379.5390625Mb; avail=467108.234375Mb
2020-10-12 22:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001236
2020-10-12 22:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21379.5390625Mb; avail=467108.234375Mb
2020-10-12 22:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.552352
2020-10-12 22:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.589824
2020-10-12 22:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21381.265625Mb; avail=467106.41796875Mb
2020-10-12 22:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21380.765625Mb; avail=467106.91796875Mb
2020-10-12 22:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025671
2020-10-12 22:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21380.765625Mb; avail=467106.91796875Mb
2020-10-12 22:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001244
2020-10-12 22:47:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21380.765625Mb; avail=467106.91796875Mb
2020-10-12 22:47:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.541915
2020-10-12 22:47:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.569679
2020-10-12 22:47:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21380.20703125Mb; avail=467107.75Mb
2020-10-12 22:47:33 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 22:47:35 | INFO | train_inner | epoch 035:      2 / 197 loss=4.643, nll_loss=3.11, ppl=8.63, wps=49747.4, ups=2.36, wpb=21046.8, bsz=784.8, num_updates=6700, lr=0.000154533, gnorm=0.665, clip=0, train_wall=31, wall=2518
2020-10-12 22:48:07 | INFO | train_inner | epoch 035:    102 / 197 loss=4.578, nll_loss=3.036, ppl=8.2, wps=66158.5, ups=3.1, wpb=21323.1, bsz=801.3, num_updates=6800, lr=0.000153393, gnorm=0.624, clip=0, train_wall=31, wall=2550
2020-10-12 22:48:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21336.3046875Mb; avail=467152.53515625Mb
2020-10-12 22:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001548
2020-10-12 22:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21336.3046875Mb; avail=467152.53515625Mb
2020-10-12 22:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071481
2020-10-12 22:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21337.8046875Mb; avail=467151.03515625Mb
2020-10-12 22:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050294
2020-10-12 22:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124171
2020-10-12 22:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21337.30078125Mb; avail=467151.5390625Mb
2020-10-12 22:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21335.80078125Mb; avail=467153.0390625Mb
2020-10-12 22:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001155
2020-10-12 22:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21335.80078125Mb; avail=467153.0390625Mb
2020-10-12 22:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070906
2020-10-12 22:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21354.04296875Mb; avail=467134.79296875Mb
2020-10-12 22:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049939
2020-10-12 22:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122881
2020-10-12 22:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21377.03515625Mb; avail=467111.79296875Mb
2020-10-12 22:48:41 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.876 | nll_loss 3.25 | ppl 9.51 | wps 121352 | wpb 6713.4 | bsz 255.6 | num_updates 6895 | best_loss 4.876
2020-10-12 22:48:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:48:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 35 @ 6895 updates, score 4.876) (writing took 7.662787159999425 seconds)
2020-10-12 22:48:48 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 22:48:48 | INFO | train | epoch 035 | loss 4.592 | nll_loss 3.051 | ppl 8.29 | wps 54476.2 | ups 2.57 | wpb 21229.6 | bsz 795.8 | num_updates 6895 | lr 0.000152333 | gnorm 0.636 | clip 0 | train_wall 62 | wall 2591
2020-10-12 22:48:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 22:48:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 22:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21348.8359375Mb; avail=467139.34375Mb
2020-10-12 22:48:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003880
2020-10-12 22:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035743
2020-10-12 22:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21349.40234375Mb; avail=467138.77734375Mb
2020-10-12 22:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001202
2020-10-12 22:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21350.0078125Mb; avail=467138.171875Mb
2020-10-12 22:48:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.554523
2020-10-12 22:48:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.592484
2020-10-12 22:48:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21376.0390625Mb; avail=467111.42578125Mb
2020-10-12 22:48:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21375.96484375Mb; avail=467111.91015625Mb
2020-10-12 22:48:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025087
2020-10-12 22:48:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.96484375Mb; avail=467111.91015625Mb
2020-10-12 22:48:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001203
2020-10-12 22:48:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.96484375Mb; avail=467111.91015625Mb
2020-10-12 22:48:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.546413
2020-10-12 22:48:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.573649
2020-10-12 22:48:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.90625Mb; avail=467112.15234375Mb
2020-10-12 22:48:50 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 22:48:52 | INFO | train_inner | epoch 036:      5 / 197 loss=4.608, nll_loss=3.069, ppl=8.39, wps=46494.1, ups=2.2, wpb=21091.5, bsz=786.3, num_updates=6900, lr=0.000152277, gnorm=0.651, clip=0, train_wall=31, wall=2595
2020-10-12 22:49:25 | INFO | train_inner | epoch 036:    105 / 197 loss=4.539, nll_loss=2.991, ppl=7.95, wps=65788.7, ups=3.07, wpb=21436.2, bsz=804.3, num_updates=7000, lr=0.000151186, gnorm=0.615, clip=0, train_wall=32, wall=2628
2020-10-12 22:49:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21211.640625Mb; avail=467277.12109375Mb
2020-10-12 22:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001638
2020-10-12 22:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21211.640625Mb; avail=467277.12109375Mb
2020-10-12 22:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071881
2020-10-12 22:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21211.640625Mb; avail=467277.12109375Mb
2020-10-12 22:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050262
2020-10-12 22:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124624
2020-10-12 22:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21211.640625Mb; avail=467277.12109375Mb
2020-10-12 22:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21207.8515625Mb; avail=467280.74609375Mb
2020-10-12 22:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001113
2020-10-12 22:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21207.8515625Mb; avail=467280.74609375Mb
2020-10-12 22:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072094
2020-10-12 22:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21207.97265625Mb; avail=467280.02734375Mb
2020-10-12 22:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050304
2020-10-12 22:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124330
2020-10-12 22:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21207.95703125Mb; avail=467280.26953125Mb
2020-10-12 22:49:57 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 4.856 | nll_loss 3.226 | ppl 9.35 | wps 114201 | wpb 6713.4 | bsz 255.6 | num_updates 7092 | best_loss 4.856
2020-10-12 22:49:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:50:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 36 @ 7092 updates, score 4.856) (writing took 6.978236923001532 seconds)
2020-10-12 22:50:04 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 22:50:04 | INFO | train | epoch 036 | loss 4.552 | nll_loss 3.006 | ppl 8.04 | wps 55006.4 | ups 2.59 | wpb 21229.6 | bsz 795.8 | num_updates 7092 | lr 0.000150202 | gnorm 0.626 | clip 0 | train_wall 62 | wall 2667
2020-10-12 22:50:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 22:50:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 22:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21357.1875Mb; avail=467130.5390625Mb
2020-10-12 22:50:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003819
2020-10-12 22:50:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035789
2020-10-12 22:50:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21356.6953125Mb; avail=467131.03125Mb
2020-10-12 22:50:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001378
2020-10-12 22:50:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21356.6953125Mb; avail=467131.03125Mb
2020-10-12 22:50:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.617612
2020-10-12 22:50:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.656050
2020-10-12 22:50:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21357.34375Mb; avail=467130.3359375Mb
2020-10-12 22:50:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21357.34375Mb; avail=467130.3359375Mb
2020-10-12 22:50:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027100
2020-10-12 22:50:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21357.34375Mb; avail=467130.3359375Mb
2020-10-12 22:50:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001299
2020-10-12 22:50:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21357.34375Mb; avail=467130.3359375Mb
2020-10-12 22:50:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.611358
2020-10-12 22:50:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.640772
2020-10-12 22:50:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21357.51953125Mb; avail=467130.04296875Mb
2020-10-12 22:50:06 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 22:50:10 | INFO | train_inner | epoch 037:      8 / 197 loss=4.558, nll_loss=3.012, ppl=8.07, wps=47293.2, ups=2.24, wpb=21159, bsz=793.1, num_updates=7100, lr=0.000150117, gnorm=0.632, clip=0, train_wall=31, wall=2673
2020-10-12 22:50:42 | INFO | train_inner | epoch 037:    108 / 197 loss=4.503, nll_loss=2.951, ppl=7.73, wps=66270.1, ups=3.07, wpb=21600.2, bsz=819.5, num_updates=7200, lr=0.000149071, gnorm=0.632, clip=0, train_wall=31, wall=2705
2020-10-12 22:51:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21208.40625Mb; avail=467280.2109375Mb
2020-10-12 22:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001636
2020-10-12 22:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21208.40625Mb; avail=467280.2109375Mb
2020-10-12 22:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071666
2020-10-12 22:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21208.30859375Mb; avail=467280.453125Mb
2020-10-12 22:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049274
2020-10-12 22:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123388
2020-10-12 22:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21208.30859375Mb; avail=467280.453125Mb
2020-10-12 22:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21208.30859375Mb; avail=467280.453125Mb
2020-10-12 22:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001083
2020-10-12 22:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21208.30859375Mb; avail=467280.453125Mb
2020-10-12 22:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070848
2020-10-12 22:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21208.07421875Mb; avail=467280.4609375Mb
2020-10-12 22:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048477
2020-10-12 22:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121163
2020-10-12 22:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21208.07421875Mb; avail=467280.4609375Mb
2020-10-12 22:51:14 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 4.862 | nll_loss 3.241 | ppl 9.45 | wps 117087 | wpb 6713.4 | bsz 255.6 | num_updates 7289 | best_loss 4.856
2020-10-12 22:51:14 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:51:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_last.pt (epoch 37 @ 7289 updates, score 4.862) (writing took 2.2176348230022995 seconds)
2020-10-12 22:51:16 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 22:51:16 | INFO | train | epoch 037 | loss 4.518 | nll_loss 2.967 | ppl 7.82 | wps 58534 | ups 2.76 | wpb 21229.6 | bsz 795.8 | num_updates 7289 | lr 0.000148158 | gnorm 0.639 | clip 0 | train_wall 62 | wall 2739
2020-10-12 22:51:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 22:51:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 22:51:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21306.80859375Mb; avail=467182.5390625Mb
2020-10-12 22:51:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004042
2020-10-12 22:51:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033370
2020-10-12 22:51:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21306.578125Mb; avail=467182.76953125Mb
2020-10-12 22:51:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001130
2020-10-12 22:51:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21306.578125Mb; avail=467182.76953125Mb
2020-10-12 22:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.552713
2020-10-12 22:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.588414
2020-10-12 22:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21370.48828125Mb; avail=467117.57421875Mb
2020-10-12 22:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21370.57421875Mb; avail=467117.08984375Mb
2020-10-12 22:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023294
2020-10-12 22:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21370.625Mb; avail=467117.2109375Mb
2020-10-12 22:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001190
2020-10-12 22:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21370.625Mb; avail=467117.2109375Mb
2020-10-12 22:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.638986
2020-10-12 22:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.664634
2020-10-12 22:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21475.30078125Mb; avail=467012.85546875Mb
2020-10-12 22:51:17 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 22:51:22 | INFO | train_inner | epoch 038:     11 / 197 loss=4.534, nll_loss=2.985, ppl=7.92, wps=52334.2, ups=2.51, wpb=20812.3, bsz=767.5, num_updates=7300, lr=0.000148047, gnorm=0.653, clip=0, train_wall=31, wall=2745
2020-10-12 22:51:54 | INFO | train_inner | epoch 038:    111 / 197 loss=4.468, nll_loss=2.91, ppl=7.51, wps=65891.8, ups=3.1, wpb=21285.9, bsz=822.1, num_updates=7400, lr=0.000147043, gnorm=0.612, clip=0, train_wall=31, wall=2777
2020-10-12 22:52:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21218.59375Mb; avail=467269.62890625Mb
2020-10-12 22:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001625
2020-10-12 22:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21218.59375Mb; avail=467269.62890625Mb
2020-10-12 22:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071218
2020-10-12 22:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21218.59375Mb; avail=467269.625Mb
2020-10-12 22:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048991
2020-10-12 22:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122639
2020-10-12 22:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21218.50390625Mb; avail=467269.625Mb
2020-10-12 22:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21218.51953125Mb; avail=467269.50390625Mb
2020-10-12 22:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001099
2020-10-12 22:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21218.51953125Mb; avail=467269.50390625Mb
2020-10-12 22:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070654
2020-10-12 22:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21218.73046875Mb; avail=467269.50390625Mb
2020-10-12 22:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048755
2020-10-12 22:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121275
2020-10-12 22:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21218.73046875Mb; avail=467269.50390625Mb
2020-10-12 22:52:25 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 4.824 | nll_loss 3.19 | ppl 9.12 | wps 128425 | wpb 6713.4 | bsz 255.6 | num_updates 7486 | best_loss 4.824
2020-10-12 22:52:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:52:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 38 @ 7486 updates, score 4.824) (writing took 4.584922235000704 seconds)
2020-10-12 22:52:30 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 22:52:30 | INFO | train | epoch 038 | loss 4.481 | nll_loss 2.925 | ppl 7.6 | wps 56602.3 | ups 2.67 | wpb 21229.6 | bsz 795.8 | num_updates 7486 | lr 0.000146196 | gnorm 0.626 | clip 0 | train_wall 62 | wall 2813
2020-10-12 22:52:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 22:52:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 22:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21376.32421875Mb; avail=467111.4140625Mb
2020-10-12 22:52:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003716
2020-10-12 22:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.037659
2020-10-12 22:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21376.9296875Mb; avail=467110.80859375Mb
2020-10-12 22:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001260
2020-10-12 22:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21376.9296875Mb; avail=467110.80859375Mb
2020-10-12 22:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.549420
2020-10-12 22:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.589321
2020-10-12 22:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21378.484375Mb; avail=467109.35546875Mb
2020-10-12 22:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21378.59765625Mb; avail=467109.2421875Mb
2020-10-12 22:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024824
2020-10-12 22:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21378.59765625Mb; avail=467109.2421875Mb
2020-10-12 22:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001149
2020-10-12 22:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21378.59765625Mb; avail=467109.2421875Mb
2020-10-12 22:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.551598
2020-10-12 22:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.578533
2020-10-12 22:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21377.06640625Mb; avail=467110.56640625Mb
2020-10-12 22:52:31 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 22:52:37 | INFO | train_inner | epoch 039:     14 / 197 loss=4.48, nll_loss=2.925, ppl=7.59, wps=49881.6, ups=2.36, wpb=21153, bsz=775.7, num_updates=7500, lr=0.000146059, gnorm=0.624, clip=0, train_wall=31, wall=2820
2020-10-12 22:53:09 | INFO | train_inner | epoch 039:    114 / 197 loss=4.453, nll_loss=2.893, ppl=7.43, wps=65676.5, ups=3.12, wpb=21048.7, bsz=779.9, num_updates=7600, lr=0.000145095, gnorm=0.626, clip=0, train_wall=31, wall=2852
2020-10-12 22:53:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21214.546875Mb; avail=467274.3984375Mb
2020-10-12 22:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001800
2020-10-12 22:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21214.546875Mb; avail=467274.3984375Mb
2020-10-12 22:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074632
2020-10-12 22:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21214.97265625Mb; avail=467274.27734375Mb
2020-10-12 22:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052107
2020-10-12 22:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129540
2020-10-12 22:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21214.96875Mb; avail=467274.27734375Mb
2020-10-12 22:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21214.96875Mb; avail=467274.27734375Mb
2020-10-12 22:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001092
2020-10-12 22:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21214.96875Mb; avail=467274.27734375Mb
2020-10-12 22:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080116
2020-10-12 22:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21214.96875Mb; avail=467274.27734375Mb
2020-10-12 22:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050394
2020-10-12 22:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.132374
2020-10-12 22:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21214.99609375Mb; avail=467274.03515625Mb
2020-10-12 22:53:38 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 4.806 | nll_loss 3.169 | ppl 8.99 | wps 126497 | wpb 6713.4 | bsz 255.6 | num_updates 7683 | best_loss 4.806
2020-10-12 22:53:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:53:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 39 @ 7683 updates, score 4.806) (writing took 4.479467077002482 seconds)
2020-10-12 22:53:43 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 22:53:43 | INFO | train | epoch 039 | loss 4.448 | nll_loss 2.887 | ppl 7.4 | wps 57194.2 | ups 2.69 | wpb 21229.6 | bsz 795.8 | num_updates 7683 | lr 0.000144309 | gnorm 0.622 | clip 0 | train_wall 61 | wall 2886
2020-10-12 22:53:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 22:53:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 22:53:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21375.8046875Mb; avail=467112.0703125Mb
2020-10-12 22:53:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003396
2020-10-12 22:53:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032194
2020-10-12 22:53:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.8046875Mb; avail=467112.0703125Mb
2020-10-12 22:53:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001147
2020-10-12 22:53:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.8046875Mb; avail=467112.0703125Mb
2020-10-12 22:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.554866
2020-10-12 22:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.589470
2020-10-12 22:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.9140625Mb; avail=467111.72265625Mb
2020-10-12 22:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21375.79296875Mb; avail=467111.84375Mb
2020-10-12 22:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024056
2020-10-12 22:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.79296875Mb; avail=467111.84375Mb
2020-10-12 22:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001192
2020-10-12 22:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.79296875Mb; avail=467111.84375Mb
2020-10-12 22:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.546672
2020-10-12 22:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.572805
2020-10-12 22:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21376.81640625Mb; avail=467111.11328125Mb
2020-10-12 22:53:44 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 22:53:51 | INFO | train_inner | epoch 040:     17 / 197 loss=4.438, nll_loss=2.876, ppl=7.34, wps=50862.9, ups=2.36, wpb=21530.9, bsz=812.5, num_updates=7700, lr=0.00014415, gnorm=0.617, clip=0, train_wall=31, wall=2894
2020-10-12 22:54:24 | INFO | train_inner | epoch 040:    117 / 197 loss=4.408, nll_loss=2.842, ppl=7.17, wps=65949.4, ups=3.06, wpb=21545.3, bsz=813.8, num_updates=7800, lr=0.000143223, gnorm=0.612, clip=0, train_wall=32, wall=2927
2020-10-12 22:54:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21215.08984375Mb; avail=467273.2421875Mb
2020-10-12 22:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001632
2020-10-12 22:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21215.08984375Mb; avail=467273.2421875Mb
2020-10-12 22:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071951
2020-10-12 22:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21215.08984375Mb; avail=467273.2421875Mb
2020-10-12 22:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049447
2020-10-12 22:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123860
2020-10-12 22:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21215.37109375Mb; avail=467273.2421875Mb
2020-10-12 22:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21215.56640625Mb; avail=467273.2421875Mb
2020-10-12 22:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001186
2020-10-12 22:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21215.56640625Mb; avail=467273.2421875Mb
2020-10-12 22:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074267
2020-10-12 22:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21215.6953125Mb; avail=467273.0078125Mb
2020-10-12 22:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049959
2020-10-12 22:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126264
2020-10-12 22:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21213.66015625Mb; avail=467274.99609375Mb
2020-10-12 22:54:52 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 4.793 | nll_loss 3.158 | ppl 8.93 | wps 130664 | wpb 6713.4 | bsz 255.6 | num_updates 7880 | best_loss 4.793
2020-10-12 22:54:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:54:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/M2O/checkpoint_best.pt (epoch 40 @ 7880 updates, score 4.793) (writing took 4.502303304001543 seconds)
2020-10-12 22:54:57 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 22:54:57 | INFO | train | epoch 040 | loss 4.416 | nll_loss 2.851 | ppl 7.22 | wps 56497.3 | ups 2.66 | wpb 21229.6 | bsz 795.8 | num_updates 7880 | lr 0.000142494 | gnorm 0.622 | clip 0 | train_wall 62 | wall 2960
2020-10-12 22:54:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 22:54:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 22:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16353.40625Mb; avail=472159.26171875Mb
2020-10-12 22:54:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006580
2020-10-12 22:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.053294
2020-10-12 22:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16352.8828125Mb; avail=472159.78515625Mb
2020-10-12 22:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001867
2020-10-12 22:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16352.8828125Mb; avail=472159.78515625Mb
2020-10-12 22:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.553869
2020-10-12 22:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.610614
2020-10-12 22:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16353.0703125Mb; avail=472159.421875Mb
2020-10-12 22:54:58 | INFO | fairseq_cli.train | done training in 2960.0 seconds
