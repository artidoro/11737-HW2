2020-10-12 22:56:25 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:14989
2020-10-12 22:56:25 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:14989
2020-10-12 22:56:25 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:14989
2020-10-12 22:56:25 | INFO | fairseq.distributed_utils | initialized host ip-172-31-31-94 as rank 2
2020-10-12 22:56:26 | INFO | fairseq.distributed_utils | initialized host ip-172-31-31-94 as rank 1
2020-10-12 22:56:26 | INFO | fairseq.distributed_utils | initialized host ip-172-31-31-94 as rank 0
2020-10-12 22:56:30 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 22:56:30 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 22:56:30 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_azefasLG_sepspm8000/O2M/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:14989', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok='tgt', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='eng-aze,eng-fas', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=3, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 22:56:30 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 22:56:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['aze', 'eng', 'fas']
2020-10-12 22:56:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | [aze] dictionary: 22355 types
2020-10-12 22:56:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 22355 types
2020-10-12 22:56:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | [fas] dictionary: 22355 types
2020-10-12 22:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 22:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20752.8984375Mb; avail=467735.609375Mb
2020-10-12 22:56:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 22:56:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:eng-aze': 1, 'main:eng-fas': 1}
2020-10-12 22:56:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-aze src_langtok: 22352; tgt_langtok: None
2020-10-12 22:56:30 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azefasLG_sepspm8000/O2M/valid.eng-aze.eng
2020-10-12 22:56:30 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azefasLG_sepspm8000/O2M/valid.eng-aze.aze
2020-10-12 22:56:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azefasLG_sepspm8000/O2M/ valid eng-aze 671 examples
2020-10-12 22:56:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-fas src_langtok: 22354; tgt_langtok: None
2020-10-12 22:56:30 | INFO | fairseq.data.data_utils | loaded 3930 examples from: fairseq/data-bin/ted_azefasLG_sepspm8000/O2M/valid.eng-fas.eng
2020-10-12 22:56:30 | INFO | fairseq.data.data_utils | loaded 3930 examples from: fairseq/data-bin/ted_azefasLG_sepspm8000/O2M/valid.eng-fas.fas
2020-10-12 22:56:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azefasLG_sepspm8000/O2M/ valid eng-fas 3930 examples
2020-10-12 22:56:31 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22355, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22355, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=22355, bias=False)
  )
)
2020-10-12 22:56:31 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 22:56:31 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 22:56:31 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 22:56:31 | INFO | fairseq_cli.train | num. model params: 42989056 (num. trained: 42989056)
2020-10-12 22:56:31 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 22:56:31 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 22:56:31 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-10-12 22:56:31 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 22:56:31 | INFO | fairseq.utils | rank   1: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 22:56:31 | INFO | fairseq.utils | rank   2: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 22:56:31 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-10-12 22:56:31 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2020-10-12 22:56:31 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 22:56:31 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_last.pt
2020-10-12 22:56:31 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 22:56:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 22:56:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21126.3828125Mb; avail=467362.5546875Mb
2020-10-12 22:56:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 22:56:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:eng-aze': 1, 'main:eng-fas': 1}
2020-10-12 22:56:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-aze src_langtok: 22352; tgt_langtok: None
2020-10-12 22:56:31 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azefasLG_sepspm8000/O2M/train.eng-aze.eng
2020-10-12 22:56:31 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azefasLG_sepspm8000/O2M/train.eng-aze.aze
2020-10-12 22:56:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azefasLG_sepspm8000/O2M/ train eng-aze 5946 examples
2020-10-12 22:56:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-fas src_langtok: 22354; tgt_langtok: None
2020-10-12 22:56:31 | INFO | fairseq.data.data_utils | loaded 150817 examples from: fairseq/data-bin/ted_azefasLG_sepspm8000/O2M/train.eng-fas.eng
2020-10-12 22:56:31 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 156763
2020-10-12 22:56:31 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 156763
2020-10-12 22:56:31 | INFO | fairseq.data.data_utils | loaded 150817 examples from: fairseq/data-bin/ted_azefasLG_sepspm8000/O2M/train.eng-fas.fas
2020-10-12 22:56:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azefasLG_sepspm8000/O2M/ train eng-fas 150817 examples
2020-10-12 22:56:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:eng-aze', 5946), ('main:eng-fas', 150817)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 22:56:31 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 22:56:31 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 156763
2020-10-12 22:56:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 156763; virtual dataset size 156763
2020-10-12 22:56:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 22:56:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:eng-aze': 5946, 'main:eng-fas': 150817}; raw total size: 156763
2020-10-12 22:56:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:eng-aze': 5946, 'main:eng-fas': 150817}; resampled total size: 156763
2020-10-12 22:56:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 22:56:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.017388
2020-10-12 22:56:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21130.43359375Mb; avail=467358.50390625Mb
2020-10-12 22:56:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002633
2020-10-12 22:56:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.030919
2020-10-12 22:56:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21130.43359375Mb; avail=467358.50390625Mb
2020-10-12 22:56:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001242
2020-10-12 22:56:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21130.43359375Mb; avail=467358.50390625Mb
2020-10-12 22:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.604971
2020-10-12 22:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.638058
2020-10-12 22:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21138.0234375Mb; avail=467350.9140625Mb
2020-10-12 22:56:32 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 22:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21131.73828125Mb; avail=467357.19921875Mb
2020-10-12 22:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023078
2020-10-12 22:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21132.078125Mb; avail=467356.859375Mb
2020-10-12 22:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001219
2020-10-12 22:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21131.69921875Mb; avail=467357.23828125Mb
2020-10-12 22:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.597062
2020-10-12 22:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.622251
2020-10-12 22:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20739.93359375Mb; avail=467748.17578125Mb
2020-10-12 22:56:33 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 22:57:07 | INFO | train_inner | epoch 001:    100 / 205 loss=14.323, nll_loss=14.228, ppl=19192.9, wps=66656.8, ups=3.15, wpb=21182.4, bsz=768, num_updates=100, lr=5.0975e-06, gnorm=3.445, clip=0, train_wall=32, wall=35
2020-10-12 22:57:38 | INFO | train_inner | epoch 001:    200 / 205 loss=12.779, nll_loss=12.508, ppl=5825.74, wps=66307.5, ups=3.17, wpb=20900.3, bsz=774.1, num_updates=200, lr=1.0095e-05, gnorm=1.5, clip=0, train_wall=31, wall=67
2020-10-12 22:57:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:57:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21087.5625Mb; avail=467401.359375Mb
2020-10-12 22:57:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001845
2020-10-12 22:57:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21087.5625Mb; avail=467401.359375Mb
2020-10-12 22:57:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067477
2020-10-12 22:57:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21087.76953125Mb; avail=467401.359375Mb
2020-10-12 22:57:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048908
2020-10-12 22:57:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119044
2020-10-12 22:57:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21086.6953125Mb; avail=467401.9921875Mb
2020-10-12 22:57:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21086.57421875Mb; avail=467402.11328125Mb
2020-10-12 22:57:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001170
2020-10-12 22:57:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21086.57421875Mb; avail=467402.11328125Mb
2020-10-12 22:57:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066514
2020-10-12 22:57:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21086.4921875Mb; avail=467401.87109375Mb
2020-10-12 22:57:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050069
2020-10-12 22:57:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118565
2020-10-12 22:57:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21086.5546875Mb; avail=467402.11328125Mb
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2020-10-12 22:57:43 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.109 | nll_loss 11.75 | ppl 3443.73 | wps 100891 | wpb 5327.1 | bsz 200 | num_updates 205
2020-10-12 22:57:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:57:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 1 @ 205 updates, score 12.109) (writing took 1.6518134149991965 seconds)
2020-10-12 22:57:44 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 22:57:44 | INFO | train | epoch 001 | loss 13.532 | nll_loss 13.347 | ppl 10421.1 | wps 61893.5 | ups 2.95 | wpb 20980.5 | bsz 764.7 | num_updates 205 | lr 1.03449e-05 | gnorm 2.446 | clip 0 | train_wall 64 | wall 73
2020-10-12 22:57:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 22:57:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 22:57:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21398.640625Mb; avail=467089.7890625Mb
2020-10-12 22:57:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008771
2020-10-12 22:57:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.064690
2020-10-12 22:57:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21460.3984375Mb; avail=467028.03125Mb
2020-10-12 22:57:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002763
2020-10-12 22:57:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21464.03125Mb; avail=467024.3984375Mb
2020-10-12 22:57:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.610283
2020-10-12 22:57:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.679540
2020-10-12 22:57:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20984.24609375Mb; avail=467510.2578125Mb
2020-10-12 22:57:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19824.30078125Mb; avail=468673.0703125Mb
2020-10-12 22:57:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023917
2020-10-12 22:57:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19509.33984375Mb; avail=468979.578125Mb
2020-10-12 22:57:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001164
2020-10-12 22:57:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19509.6875Mb; avail=468979.73046875Mb
2020-10-12 22:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.642524
2020-10-12 22:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.668573
2020-10-12 22:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19425.3828125Mb; avail=469075.6015625Mb
2020-10-12 22:57:46 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 22:58:17 | INFO | train_inner | epoch 002:     95 / 205 loss=11.928, nll_loss=11.558, ppl=3015.85, wps=54230.1, ups=2.58, wpb=21031.1, bsz=750.7, num_updates=300, lr=1.50925e-05, gnorm=1.203, clip=0, train_wall=31, wall=106
2020-10-12 22:58:49 | INFO | train_inner | epoch 002:    195 / 205 loss=11.036, nll_loss=10.539, ppl=1488.02, wps=65428.9, ups=3.13, wpb=20907, bsz=771.7, num_updates=400, lr=2.009e-05, gnorm=1.069, clip=0, train_wall=31, wall=138
2020-10-12 22:58:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 22:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21123.77734375Mb; avail=467364.42578125Mb
2020-10-12 22:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001558
2020-10-12 22:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21123.77734375Mb; avail=467364.42578125Mb
2020-10-12 22:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066918
2020-10-12 22:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21124.05078125Mb; avail=467364.4609375Mb
2020-10-12 22:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048008
2020-10-12 22:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117346
2020-10-12 22:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21124.05078125Mb; avail=467364.4609375Mb
2020-10-12 22:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21123.74609375Mb; avail=467364.765625Mb
2020-10-12 22:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001152
2020-10-12 22:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21123.859375Mb; avail=467364.65234375Mb
2020-10-12 22:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066445
2020-10-12 22:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21123.53515625Mb; avail=467364.8671875Mb
2020-10-12 22:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047986
2020-10-12 22:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116433
2020-10-12 22:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21124.09765625Mb; avail=467364.625Mb
2020-10-12 22:58:55 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.55 | nll_loss 9.93 | ppl 975.52 | wps 102595 | wpb 5327.1 | bsz 200 | num_updates 410 | best_loss 10.55
2020-10-12 22:58:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 22:59:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 2 @ 410 updates, score 10.55) (writing took 4.5081565090004005 seconds)
2020-10-12 22:59:00 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 22:59:00 | INFO | train | epoch 002 | loss 11.424 | nll_loss 10.982 | ppl 2023.2 | wps 57162.1 | ups 2.72 | wpb 20980.5 | bsz 764.7 | num_updates 410 | lr 2.05898e-05 | gnorm 1.112 | clip 0 | train_wall 64 | wall 148
2020-10-12 22:59:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 22:59:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 22:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21291.3203125Mb; avail=467196.01953125Mb
2020-10-12 22:59:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005199
2020-10-12 22:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.036900
2020-10-12 22:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21292.89453125Mb; avail=467194.4453125Mb
2020-10-12 22:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001174
2020-10-12 22:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21292.89453125Mb; avail=467194.4453125Mb
2020-10-12 22:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.602460
2020-10-12 22:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.641480
2020-10-12 22:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21296.33203125Mb; avail=467191.4140625Mb
2020-10-12 22:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21294.87109375Mb; avail=467193.87109375Mb
2020-10-12 22:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022913
2020-10-12 22:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21294.37890625Mb; avail=467193.87109375Mb
2020-10-12 22:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001130
2020-10-12 22:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21294.37890625Mb; avail=467193.87109375Mb
2020-10-12 22:59:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.600695
2020-10-12 22:59:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.625962
2020-10-12 22:59:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21293.15234375Mb; avail=467195.06640625Mb
2020-10-12 22:59:01 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 22:59:31 | INFO | train_inner | epoch 003:     90 / 205 loss=10.489, nll_loss=9.881, ppl=942.63, wps=49726.3, ups=2.38, wpb=20896.6, bsz=717.3, num_updates=500, lr=2.50875e-05, gnorm=0.913, clip=0, train_wall=31, wall=180
2020-10-12 23:00:03 | INFO | train_inner | epoch 003:    190 / 205 loss=10.195, nll_loss=9.506, ppl=727.1, wps=66300.4, ups=3.14, wpb=21143, bsz=805.4, num_updates=600, lr=3.0085e-05, gnorm=1.203, clip=0, train_wall=31, wall=212
2020-10-12 23:00:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21165.39453125Mb; avail=467322.7734375Mb
2020-10-12 23:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001597
2020-10-12 23:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21165.39453125Mb; avail=467322.7734375Mb
2020-10-12 23:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067966
2020-10-12 23:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21165.39453125Mb; avail=467322.7734375Mb
2020-10-12 23:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049940
2020-10-12 23:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120359
2020-10-12 23:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21165.39453125Mb; avail=467322.7734375Mb
2020-10-12 23:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21166.015625Mb; avail=467322.2890625Mb
2020-10-12 23:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001154
2020-10-12 23:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21166.015625Mb; avail=467322.2890625Mb
2020-10-12 23:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066807
2020-10-12 23:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21164.1953125Mb; avail=467324.51953125Mb
2020-10-12 23:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050588
2020-10-12 23:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119427
2020-10-12 23:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21164.1953125Mb; avail=467324.51953125Mb
2020-10-12 23:00:11 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.123 | nll_loss 9.372 | ppl 662.63 | wps 110810 | wpb 5327.1 | bsz 200 | num_updates 615 | best_loss 10.123
2020-10-12 23:00:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:00:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 3 @ 615 updates, score 10.123) (writing took 4.493709420999949 seconds)
2020-10-12 23:00:15 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 23:00:15 | INFO | train | epoch 003 | loss 10.308 | nll_loss 9.651 | ppl 803.7 | wps 56908.4 | ups 2.71 | wpb 20980.5 | bsz 764.7 | num_updates 615 | lr 3.08346e-05 | gnorm 1.082 | clip 0 | train_wall 64 | wall 224
2020-10-12 23:00:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 23:00:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 23:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21334.42578125Mb; avail=467153.52734375Mb
2020-10-12 23:00:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003977
2020-10-12 23:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032993
2020-10-12 23:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21334.42578125Mb; avail=467153.52734375Mb
2020-10-12 23:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001157
2020-10-12 23:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21334.42578125Mb; avail=467153.52734375Mb
2020-10-12 23:00:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.594666
2020-10-12 23:00:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.629623
2020-10-12 23:00:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21339.734375Mb; avail=467147.90234375Mb
2020-10-12 23:00:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21336.78125Mb; avail=467150.85546875Mb
2020-10-12 23:00:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022481
2020-10-12 23:00:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21336.8203125Mb; avail=467151.21875Mb
2020-10-12 23:00:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001114
2020-10-12 23:00:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21336.8203125Mb; avail=467151.21875Mb
2020-10-12 23:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.588136
2020-10-12 23:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.612511
2020-10-12 23:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21336.77734375Mb; avail=467152.19140625Mb
2020-10-12 23:00:17 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 23:00:45 | INFO | train_inner | epoch 004:     85 / 205 loss=10.041, nll_loss=9.309, ppl=634.47, wps=49644.4, ups=2.39, wpb=20776, bsz=739.7, num_updates=700, lr=3.50825e-05, gnorm=1.001, clip=0, train_wall=31, wall=253
2020-10-12 23:01:17 | INFO | train_inner | epoch 004:    185 / 205 loss=9.884, nll_loss=9.121, ppl=556.74, wps=65667.6, ups=3.12, wpb=21070.2, bsz=789.1, num_updates=800, lr=4.008e-05, gnorm=0.963, clip=0, train_wall=31, wall=286
2020-10-12 23:01:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21173.30859375Mb; avail=467315.19140625Mb
2020-10-12 23:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001900
2020-10-12 23:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21173.30859375Mb; avail=467315.19140625Mb
2020-10-12 23:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067003
2020-10-12 23:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21173.88671875Mb; avail=467314.5859375Mb
2020-10-12 23:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049925
2020-10-12 23:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119670
2020-10-12 23:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21173.88671875Mb; avail=467314.5859375Mb
2020-10-12 23:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21174.0078125Mb; avail=467314.46484375Mb
2020-10-12 23:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001214
2020-10-12 23:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21174.0078125Mb; avail=467314.46484375Mb
2020-10-12 23:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067015
2020-10-12 23:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21174.0078125Mb; avail=467314.46484375Mb
2020-10-12 23:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.092803
2020-10-12 23:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.161972
2020-10-12 23:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21183.59375Mb; avail=467304.640625Mb
2020-10-12 23:01:26 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.614 | nll_loss 8.793 | ppl 443.56 | wps 110385 | wpb 5327.1 | bsz 200 | num_updates 820 | best_loss 9.614
2020-10-12 23:01:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:01:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 4 @ 820 updates, score 9.614) (writing took 4.496643299000425 seconds)
2020-10-12 23:01:31 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 23:01:31 | INFO | train | epoch 004 | loss 9.938 | nll_loss 9.185 | ppl 582.25 | wps 56919.7 | ups 2.71 | wpb 20980.5 | bsz 764.7 | num_updates 820 | lr 4.10795e-05 | gnorm 0.984 | clip 0 | train_wall 64 | wall 300
2020-10-12 23:01:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 23:01:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21341.6796875Mb; avail=467146.32421875Mb
2020-10-12 23:01:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008599
2020-10-12 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.064959
2020-10-12 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21342.09765625Mb; avail=467145.4765625Mb
2020-10-12 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002263
2020-10-12 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21342.09765625Mb; avail=467145.4765625Mb
2020-10-12 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.600003
2020-10-12 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.668731
2020-10-12 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21345.73046875Mb; avail=467142.93359375Mb
2020-10-12 23:01:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21342.7734375Mb; avail=467146.12890625Mb
2020-10-12 23:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023094
2020-10-12 23:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21342.8046875Mb; avail=467145.88671875Mb
2020-10-12 23:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001247
2020-10-12 23:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21342.8046875Mb; avail=467145.88671875Mb
2020-10-12 23:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.606941
2020-10-12 23:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.632259
2020-10-12 23:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21342.55078125Mb; avail=467145.14453125Mb
2020-10-12 23:01:32 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 23:01:59 | INFO | train_inner | epoch 005:     80 / 205 loss=9.72, nll_loss=8.929, ppl=487.53, wps=49740.7, ups=2.38, wpb=20914.3, bsz=769.1, num_updates=900, lr=4.50775e-05, gnorm=1.037, clip=0, train_wall=31, wall=328
2020-10-12 23:02:31 | INFO | train_inner | epoch 005:    180 / 205 loss=9.565, nll_loss=8.751, ppl=430.97, wps=66210.6, ups=3.15, wpb=21031.2, bsz=773.2, num_updates=1000, lr=5.0075e-05, gnorm=1.008, clip=0, train_wall=31, wall=359
2020-10-12 23:02:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21160.98828125Mb; avail=467327.65625Mb
2020-10-12 23:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001562
2020-10-12 23:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21162.3125Mb; avail=467326.33203125Mb
2020-10-12 23:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066841
2020-10-12 23:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21183.16015625Mb; avail=467305.25390625Mb
2020-10-12 23:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049692
2020-10-12 23:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118924
2020-10-12 23:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21183.9140625Mb; avail=467304.52734375Mb
2020-10-12 23:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21184.03515625Mb; avail=467304.40625Mb
2020-10-12 23:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001103
2020-10-12 23:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21183.93359375Mb; avail=467304.40625Mb
2020-10-12 23:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066906
2020-10-12 23:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21183.94140625Mb; avail=467304.28515625Mb
2020-10-12 23:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049538
2020-10-12 23:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118354
2020-10-12 23:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21183.94140625Mb; avail=467304.28515625Mb
2020-10-12 23:02:41 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.284 | nll_loss 8.416 | ppl 341.62 | wps 113209 | wpb 5327.1 | bsz 200 | num_updates 1025 | best_loss 9.284
2020-10-12 23:02:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:02:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 5 @ 1025 updates, score 9.284) (writing took 4.489403520998167 seconds)
2020-10-12 23:02:46 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 23:02:46 | INFO | train | epoch 005 | loss 9.61 | nll_loss 8.803 | ppl 446.8 | wps 57287.7 | ups 2.73 | wpb 20980.5 | bsz 764.7 | num_updates 1025 | lr 5.13244e-05 | gnorm 1 | clip 0 | train_wall 64 | wall 375
2020-10-12 23:02:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 23:02:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 23:02:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21350.68359375Mb; avail=467137.6484375Mb
2020-10-12 23:02:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005790
2020-10-12 23:02:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034176
2020-10-12 23:02:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21350.90625Mb; avail=467136.79296875Mb
2020-10-12 23:02:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001889
2020-10-12 23:02:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21350.875Mb; avail=467137.03515625Mb
2020-10-12 23:02:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.604705
2020-10-12 23:02:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.642116
2020-10-12 23:02:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21355.5546875Mb; avail=467132.37109375Mb
2020-10-12 23:02:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21352.32421875Mb; avail=467135.703125Mb
2020-10-12 23:02:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024791
2020-10-12 23:02:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21350.84765625Mb; avail=467137.1796875Mb
2020-10-12 23:02:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001127
2020-10-12 23:02:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21350.84765625Mb; avail=467137.1796875Mb
2020-10-12 23:02:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.585702
2020-10-12 23:02:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.612470
2020-10-12 23:02:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21351.12890625Mb; avail=467136.7578125Mb
2020-10-12 23:02:47 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 23:03:12 | INFO | train_inner | epoch 006:     75 / 205 loss=9.438, nll_loss=8.606, ppl=389.55, wps=50051.2, ups=2.4, wpb=20898.1, bsz=759.2, num_updates=1100, lr=5.50725e-05, gnorm=1.003, clip=0, train_wall=31, wall=401
2020-10-12 23:03:44 | INFO | train_inner | epoch 006:    175 / 205 loss=9.312, nll_loss=8.461, ppl=352.47, wps=66069.1, ups=3.14, wpb=21031.7, bsz=759.7, num_updates=1200, lr=6.007e-05, gnorm=0.914, clip=0, train_wall=31, wall=433
2020-10-12 23:03:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:03:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21109.24609375Mb; avail=467379.3984375Mb
2020-10-12 23:03:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003517
2020-10-12 23:03:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21109.24609375Mb; avail=467379.3984375Mb
2020-10-12 23:03:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.133013
2020-10-12 23:03:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21117.234375Mb; avail=467371.41015625Mb
2020-10-12 23:03:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051230
2020-10-12 23:03:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.189100
2020-10-12 23:03:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21119.68359375Mb; avail=467368.9609375Mb
2020-10-12 23:03:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21127.4140625Mb; avail=467360.60546875Mb
2020-10-12 23:03:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001220
2020-10-12 23:03:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21127.4140625Mb; avail=467360.60546875Mb
2020-10-12 23:03:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066642
2020-10-12 23:03:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21139.30078125Mb; avail=467349.1015625Mb
2020-10-12 23:03:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049919
2020-10-12 23:03:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118637
2020-10-12 23:03:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21138.5625Mb; avail=467350.109375Mb
2020-10-12 23:03:57 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.043 | nll_loss 8.143 | ppl 282.72 | wps 111230 | wpb 5327.1 | bsz 200 | num_updates 1230 | best_loss 9.043
2020-10-12 23:03:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:04:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 6 @ 1230 updates, score 9.043) (writing took 4.53058855999916 seconds)
2020-10-12 23:04:01 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 23:04:01 | INFO | train | epoch 006 | loss 9.337 | nll_loss 8.49 | ppl 359.5 | wps 57008.9 | ups 2.72 | wpb 20980.5 | bsz 764.7 | num_updates 1230 | lr 6.15693e-05 | gnorm 0.982 | clip 0 | train_wall 64 | wall 450
2020-10-12 23:04:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 23:04:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 23:04:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21364.7578125Mb; avail=467123.390625Mb
2020-10-12 23:04:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004330
2020-10-12 23:04:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.036305
2020-10-12 23:04:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21366.57421875Mb; avail=467121.57421875Mb
2020-10-12 23:04:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001256
2020-10-12 23:04:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21366.57421875Mb; avail=467121.57421875Mb
2020-10-12 23:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.611938
2020-10-12 23:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.650624
2020-10-12 23:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21369.7421875Mb; avail=467118.30859375Mb
2020-10-12 23:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21366.7890625Mb; avail=467121.26171875Mb
2020-10-12 23:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023310
2020-10-12 23:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21366.7890625Mb; avail=467121.26171875Mb
2020-10-12 23:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001252
2020-10-12 23:04:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21366.7890625Mb; avail=467121.26171875Mb
2020-10-12 23:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.594565
2020-10-12 23:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.620060
2020-10-12 23:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21367.03515625Mb; avail=467121.01953125Mb
2020-10-12 23:04:03 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 23:04:26 | INFO | train_inner | epoch 007:     70 / 205 loss=9.187, nll_loss=8.318, ppl=319.09, wps=51084, ups=2.38, wpb=21492.1, bsz=784.5, num_updates=1300, lr=6.50675e-05, gnorm=1.051, clip=0, train_wall=31, wall=475
2020-10-12 23:04:58 | INFO | train_inner | epoch 007:    170 / 205 loss=9.086, nll_loss=8.202, ppl=294.41, wps=64460.2, ups=3.12, wpb=20669.2, bsz=764.7, num_updates=1400, lr=7.0065e-05, gnorm=1.051, clip=0, train_wall=31, wall=507
2020-10-12 23:05:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21099.73828125Mb; avail=467388.765625Mb
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001798
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21099.73828125Mb; avail=467388.765625Mb
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065926
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21097.875Mb; avail=467390.85546875Mb
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047320
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115866
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21097.875Mb; avail=467390.85546875Mb
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21097.875Mb; avail=467390.85546875Mb
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001081
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21097.875Mb; avail=467390.85546875Mb
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066991
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21097.875Mb; avail=467390.85546875Mb
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047418
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116281
2020-10-12 23:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21097.875Mb; avail=467390.85546875Mb
2020-10-12 23:05:12 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.834 | nll_loss 7.899 | ppl 238.67 | wps 110296 | wpb 5327.1 | bsz 200 | num_updates 1435 | best_loss 8.834
2020-10-12 23:05:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:05:17 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 7 @ 1435 updates, score 8.834) (writing took 4.533872236999741 seconds)
2020-10-12 23:05:17 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 23:05:17 | INFO | train | epoch 007 | loss 9.108 | nll_loss 8.228 | ppl 299.75 | wps 56814.2 | ups 2.71 | wpb 20980.5 | bsz 764.7 | num_updates 1435 | lr 7.18141e-05 | gnorm 1.046 | clip 0 | train_wall 64 | wall 526
2020-10-12 23:05:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 23:05:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 23:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21375.85546875Mb; avail=467112.109375Mb
2020-10-12 23:05:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004403
2020-10-12 23:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035594
2020-10-12 23:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.96875Mb; avail=467111.99609375Mb
2020-10-12 23:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001253
2020-10-12 23:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.96875Mb; avail=467111.99609375Mb
2020-10-12 23:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.606335
2020-10-12 23:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.644454
2020-10-12 23:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.84765625Mb; avail=467112.25Mb
2020-10-12 23:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21375.84765625Mb; avail=467112.25Mb
2020-10-12 23:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022787
2020-10-12 23:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.84765625Mb; avail=467112.25Mb
2020-10-12 23:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001157
2020-10-12 23:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.84765625Mb; avail=467112.25Mb
2020-10-12 23:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.591428
2020-10-12 23:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.616175
2020-10-12 23:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21375.66796875Mb; avail=467112.37109375Mb
2020-10-12 23:05:18 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 23:05:40 | INFO | train_inner | epoch 008:     65 / 205 loss=8.98, nll_loss=8.081, ppl=270.75, wps=49917.6, ups=2.38, wpb=21003.4, bsz=755.1, num_updates=1500, lr=7.50625e-05, gnorm=0.987, clip=0, train_wall=31, wall=549
2020-10-12 23:06:13 | INFO | train_inner | epoch 008:    165 / 205 loss=8.854, nll_loss=7.936, ppl=244.83, wps=65299.2, ups=3.1, wpb=21038.5, bsz=771.4, num_updates=1600, lr=8.006e-05, gnorm=0.993, clip=0, train_wall=32, wall=581
2020-10-12 23:06:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21101.953125Mb; avail=467387.328125Mb
2020-10-12 23:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001623
2020-10-12 23:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21101.953125Mb; avail=467387.328125Mb
2020-10-12 23:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067624
2020-10-12 23:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21102.55078125Mb; avail=467386.73046875Mb
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048709
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118797
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21102.55078125Mb; avail=467386.73046875Mb
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21102.55078125Mb; avail=467386.73046875Mb
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001181
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21102.55078125Mb; avail=467386.73046875Mb
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066982
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21103.15625Mb; avail=467386.125Mb
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047980
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116950
2020-10-12 23:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21103.76171875Mb; avail=467385.51953125Mb
2020-10-12 23:06:28 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.605 | nll_loss 7.628 | ppl 197.82 | wps 98407.3 | wpb 5327.1 | bsz 200 | num_updates 1640 | best_loss 8.605
2020-10-12 23:06:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:06:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 8 @ 1640 updates, score 8.605) (writing took 6.7497297059999255 seconds)
2020-10-12 23:06:35 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 23:06:35 | INFO | train | epoch 008 | loss 8.881 | nll_loss 7.967 | ppl 250.19 | wps 55046.2 | ups 2.62 | wpb 20980.5 | bsz 764.7 | num_updates 1640 | lr 8.2059e-05 | gnorm 0.977 | clip 0 | train_wall 64 | wall 604
2020-10-12 23:06:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 23:06:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 23:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21351.44140625Mb; avail=467136.015625Mb
2020-10-12 23:06:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004168
2020-10-12 23:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.037436
2020-10-12 23:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21353.48828125Mb; avail=467134.89453125Mb
2020-10-12 23:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001989
2020-10-12 23:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21353.70703125Mb; avail=467134.93359375Mb
2020-10-12 23:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.632395
2020-10-12 23:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.673069
2020-10-12 23:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21353.140625Mb; avail=467135.1796875Mb
2020-10-12 23:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21353.0546875Mb; avail=467135.17578125Mb
2020-10-12 23:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023462
2020-10-12 23:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21352.9921875Mb; avail=467135.54296875Mb
2020-10-12 23:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001123
2020-10-12 23:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21352.9921875Mb; avail=467135.54296875Mb
2020-10-12 23:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.600611
2020-10-12 23:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.625978
2020-10-12 23:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21352.9765625Mb; avail=467135.3515625Mb
2020-10-12 23:06:36 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 23:06:57 | INFO | train_inner | epoch 009:     60 / 205 loss=8.763, nll_loss=7.832, ppl=227.78, wps=46843.2, ups=2.26, wpb=20753.7, bsz=746.6, num_updates=1700, lr=8.50575e-05, gnorm=0.995, clip=0, train_wall=31, wall=626
2020-10-12 23:07:29 | INFO | train_inner | epoch 009:    160 / 205 loss=8.676, nll_loss=7.731, ppl=212.39, wps=66951.3, ups=3.14, wpb=21321.8, bsz=780.6, num_updates=1800, lr=9.0055e-05, gnorm=0.947, clip=0, train_wall=31, wall=658
2020-10-12 23:07:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21145.80078125Mb; avail=467342.98828125Mb
2020-10-12 23:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001630
2020-10-12 23:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21145.80078125Mb; avail=467342.98828125Mb
2020-10-12 23:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067918
2020-10-12 23:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21153.7109375Mb; avail=467335.05859375Mb
2020-10-12 23:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048259
2020-10-12 23:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118636
2020-10-12 23:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21160.9765625Mb; avail=467327.79296875Mb
2020-10-12 23:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21165.8203125Mb; avail=467322.94921875Mb
2020-10-12 23:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001154
2020-10-12 23:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21165.8203125Mb; avail=467322.94921875Mb
2020-10-12 23:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068281
2020-10-12 23:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21168.9609375Mb; avail=467319.8046875Mb
2020-10-12 23:07:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048417
2020-10-12 23:07:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118672
2020-10-12 23:07:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21173.19921875Mb; avail=467315.56640625Mb
2020-10-12 23:07:46 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.412 | nll_loss 7.404 | ppl 169.41 | wps 111024 | wpb 5327.1 | bsz 200 | num_updates 1845 | best_loss 8.412
2020-10-12 23:07:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:07:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 9 @ 1845 updates, score 8.412) (writing took 4.494886436001252 seconds)
2020-10-12 23:07:51 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 23:07:51 | INFO | train | epoch 009 | loss 8.672 | nll_loss 7.726 | ppl 211.76 | wps 56984.7 | ups 2.72 | wpb 20980.5 | bsz 764.7 | num_updates 1845 | lr 9.23039e-05 | gnorm 0.982 | clip 0 | train_wall 64 | wall 679
2020-10-12 23:07:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 23:07:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 23:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21380.80078125Mb; avail=467107.015625Mb
2020-10-12 23:07:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003955
2020-10-12 23:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.037343
2020-10-12 23:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21380.84375Mb; avail=467107.62109375Mb
2020-10-12 23:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001348
2020-10-12 23:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21380.84375Mb; avail=467107.62109375Mb
2020-10-12 23:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.597391
2020-10-12 23:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.637166
2020-10-12 23:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21381.0234375Mb; avail=467107.13671875Mb
2020-10-12 23:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21381.0234375Mb; avail=467107.13671875Mb
2020-10-12 23:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023227
2020-10-12 23:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21381.0234375Mb; avail=467107.13671875Mb
2020-10-12 23:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001262
2020-10-12 23:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21381.0234375Mb; avail=467107.13671875Mb
2020-10-12 23:07:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.592768
2020-10-12 23:07:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.618132
2020-10-12 23:07:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21380.75390625Mb; avail=467107.33203125Mb
2020-10-12 23:07:52 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 23:08:11 | INFO | train_inner | epoch 010:     55 / 205 loss=8.571, nll_loss=7.61, ppl=195.41, wps=49416.6, ups=2.39, wpb=20682.5, bsz=755.8, num_updates=1900, lr=9.50525e-05, gnorm=0.992, clip=0, train_wall=31, wall=699
2020-10-12 23:08:43 | INFO | train_inner | epoch 010:    155 / 205 loss=8.447, nll_loss=7.468, ppl=177.08, wps=65087.5, ups=3.1, wpb=20984.9, bsz=782.3, num_updates=2000, lr=0.00010005, gnorm=0.961, clip=0, train_wall=32, wall=732
2020-10-12 23:08:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21118.3203125Mb; avail=467370.83203125Mb
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001570
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21118.3203125Mb; avail=467370.83203125Mb
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067033
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21118.6640625Mb; avail=467370.46875Mb
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047648
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117073
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21118.6640625Mb; avail=467370.46875Mb
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21118.6640625Mb; avail=467370.46875Mb
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001079
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21118.6640625Mb; avail=467370.46875Mb
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065791
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21118.6640625Mb; avail=467370.46875Mb
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047758
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115381
2020-10-12 23:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21118.6640625Mb; avail=467370.46875Mb
2020-10-12 23:09:02 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.218 | nll_loss 7.18 | ppl 145 | wps 103773 | wpb 5327.1 | bsz 200 | num_updates 2050 | best_loss 8.218
2020-10-12 23:09:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:09:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 10 @ 2050 updates, score 8.218) (writing took 4.489797094000096 seconds)
2020-10-12 23:09:06 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 23:09:06 | INFO | train | epoch 010 | loss 8.47 | nll_loss 7.494 | ppl 180.25 | wps 56724.1 | ups 2.7 | wpb 20980.5 | bsz 764.7 | num_updates 2050 | lr 0.000102549 | gnorm 0.955 | clip 0 | train_wall 64 | wall 755
2020-10-12 23:09:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 23:09:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 23:09:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21388.77734375Mb; avail=467099.296875Mb
2020-10-12 23:09:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004140
2020-10-12 23:09:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033943
2020-10-12 23:09:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21389.3828125Mb; avail=467098.69140625Mb
2020-10-12 23:09:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001211
2020-10-12 23:09:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21389.3828125Mb; avail=467098.69140625Mb
2020-10-12 23:09:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.598593
2020-10-12 23:09:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.634567
2020-10-12 23:09:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21390.453125Mb; avail=467097.484375Mb
2020-10-12 23:09:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21390.453125Mb; avail=467097.484375Mb
2020-10-12 23:09:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022661
2020-10-12 23:09:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21390.453125Mb; avail=467097.484375Mb
2020-10-12 23:09:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001126
2020-10-12 23:09:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21390.453125Mb; avail=467097.484375Mb
2020-10-12 23:09:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.589331
2020-10-12 23:09:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.613934
2020-10-12 23:09:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21390.10546875Mb; avail=467097.484375Mb
2020-10-12 23:09:08 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 23:09:25 | INFO | train_inner | epoch 011:     50 / 205 loss=8.368, nll_loss=7.377, ppl=166.21, wps=49922.9, ups=2.37, wpb=21087.1, bsz=744.6, num_updates=2100, lr=0.000105048, gnorm=0.92, clip=0, train_wall=31, wall=774
2020-10-12 23:09:57 | INFO | train_inner | epoch 011:    150 / 205 loss=8.266, nll_loss=7.261, ppl=153.34, wps=64998.8, ups=3.15, wpb=20636.7, bsz=754.1, num_updates=2200, lr=0.000110045, gnorm=0.969, clip=0, train_wall=31, wall=806
2020-10-12 23:10:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:10:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21651.63671875Mb; avail=466837.578125Mb
2020-10-12 23:10:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001580
2020-10-12 23:10:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21651.63671875Mb; avail=466837.578125Mb
2020-10-12 23:10:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067126
2020-10-12 23:10:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21651.63671875Mb; avail=466837.578125Mb
2020-10-12 23:10:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048118
2020-10-12 23:10:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117620
2020-10-12 23:10:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21651.63671875Mb; avail=466837.578125Mb
2020-10-12 23:10:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21651.63671875Mb; avail=466837.578125Mb
2020-10-12 23:10:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001099
2020-10-12 23:10:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21651.63671875Mb; avail=466837.578125Mb
2020-10-12 23:10:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065820
2020-10-12 23:10:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21649.53515625Mb; avail=466839.6796875Mb
2020-10-12 23:10:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047208
2020-10-12 23:10:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.114922
2020-10-12 23:10:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21649.55078125Mb; avail=466839.6640625Mb
2020-10-12 23:10:17 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.004 | nll_loss 6.928 | ppl 121.73 | wps 112738 | wpb 5327.1 | bsz 200 | num_updates 2255 | best_loss 8.004
2020-10-12 23:10:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:10:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 11 @ 2255 updates, score 8.004) (writing took 6.0786434440015 seconds)
2020-10-12 23:10:23 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 23:10:23 | INFO | train | epoch 011 | loss 8.252 | nll_loss 7.244 | ppl 151.6 | wps 55837.6 | ups 2.66 | wpb 20980.5 | bsz 764.7 | num_updates 2255 | lr 0.000112794 | gnorm 0.928 | clip 0 | train_wall 64 | wall 832
2020-10-12 23:10:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 23:10:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 23:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21386.51953125Mb; avail=467101.3828125Mb
2020-10-12 23:10:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005736
2020-10-12 23:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034777
2020-10-12 23:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21387.578125Mb; avail=467100.32421875Mb
2020-10-12 23:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.003473
2020-10-12 23:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21387.53515625Mb; avail=467100.56640625Mb
2020-10-12 23:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.611485
2020-10-12 23:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.650776
2020-10-12 23:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21387.26171875Mb; avail=467101.1484375Mb
2020-10-12 23:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21387.234375Mb; avail=467100.90625Mb
2020-10-12 23:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022776
2020-10-12 23:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21387.234375Mb; avail=467100.90625Mb
2020-10-12 23:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001135
2020-10-12 23:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21387.234375Mb; avail=467100.90625Mb
2020-10-12 23:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.601006
2020-10-12 23:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.626096
2020-10-12 23:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21387.24609375Mb; avail=467100.75390625Mb
2020-10-12 23:10:25 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 23:10:40 | INFO | train_inner | epoch 012:     45 / 205 loss=8.149, nll_loss=7.126, ppl=139.67, wps=49059.8, ups=2.3, wpb=21357.4, bsz=775.8, num_updates=2300, lr=0.000115043, gnorm=0.92, clip=0, train_wall=31, wall=849
2020-10-12 23:11:12 | INFO | train_inner | epoch 012:    145 / 205 loss=8.057, nll_loss=7.019, ppl=129.7, wps=65364.8, ups=3.13, wpb=20853.7, bsz=771.7, num_updates=2400, lr=0.00012004, gnorm=0.985, clip=0, train_wall=31, wall=881
2020-10-12 23:11:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21109.34375Mb; avail=467379.09375Mb
2020-10-12 23:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001680
2020-10-12 23:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21111.765625Mb; avail=467377.27734375Mb
2020-10-12 23:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066900
2020-10-12 23:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21193.83203125Mb; avail=467295.2109375Mb
2020-10-12 23:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048030
2020-10-12 23:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117462
2020-10-12 23:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21252.5625Mb; avail=467236.48046875Mb
2020-10-12 23:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21288.890625Mb; avail=467200.15234375Mb
2020-10-12 23:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001214
2020-10-12 23:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21290.70703125Mb; avail=467198.3359375Mb
2020-10-12 23:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066344
2020-10-12 23:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21376.69921875Mb; avail=467112.125Mb
2020-10-12 23:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047829
2020-10-12 23:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116148
2020-10-12 23:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21430.3828125Mb; avail=467058.8125Mb
2020-10-12 23:11:34 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.848 | nll_loss 6.74 | ppl 106.87 | wps 105128 | wpb 5327.1 | bsz 200 | num_updates 2460 | best_loss 7.848
2020-10-12 23:11:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:11:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 12 @ 2460 updates, score 7.848) (writing took 6.230081171997881 seconds)
2020-10-12 23:11:41 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 23:11:41 | INFO | train | epoch 012 | loss 8.049 | nll_loss 7.01 | ppl 128.89 | wps 55684.1 | ups 2.65 | wpb 20980.5 | bsz 764.7 | num_updates 2460 | lr 0.000123039 | gnorm 0.957 | clip 0 | train_wall 64 | wall 909
2020-10-12 23:11:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 23:11:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 23:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21383.1875Mb; avail=467104.74609375Mb
2020-10-12 23:11:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006996
2020-10-12 23:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.042733
2020-10-12 23:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.69921875Mb; avail=467103.234375Mb
2020-10-12 23:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001680
2020-10-12 23:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.69921875Mb; avail=467103.234375Mb
2020-10-12 23:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.592168
2020-10-12 23:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.637809
2020-10-12 23:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.84375Mb; avail=467102.65234375Mb
2020-10-12 23:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21384.86328125Mb; avail=467102.53125Mb
2020-10-12 23:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022965
2020-10-12 23:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.86328125Mb; avail=467102.53125Mb
2020-10-12 23:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001140
2020-10-12 23:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.86328125Mb; avail=467102.53125Mb
2020-10-12 23:11:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.589619
2020-10-12 23:11:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.614652
2020-10-12 23:11:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.47265625Mb; avail=467102.7734375Mb
2020-10-12 23:11:42 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 23:11:56 | INFO | train_inner | epoch 013:     40 / 205 loss=7.962, nll_loss=6.91, ppl=120.26, wps=47851.8, ups=2.29, wpb=20893.1, bsz=746.2, num_updates=2500, lr=0.000125037, gnorm=0.946, clip=0, train_wall=31, wall=925
2020-10-12 23:12:28 | INFO | train_inner | epoch 013:    140 / 205 loss=7.846, nll_loss=6.777, ppl=109.66, wps=64930.3, ups=3.1, wpb=20951, bsz=762, num_updates=2600, lr=0.000130035, gnorm=0.929, clip=0, train_wall=32, wall=957
2020-10-12 23:12:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21206.7265625Mb; avail=467281.33203125Mb
2020-10-12 23:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001876
2020-10-12 23:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21206.7265625Mb; avail=467281.33203125Mb
2020-10-12 23:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067560
2020-10-12 23:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21206.7265625Mb; avail=467281.82421875Mb
2020-10-12 23:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048607
2020-10-12 23:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118928
2020-10-12 23:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21206.65234375Mb; avail=467282.54296875Mb
2020-10-12 23:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21206.65234375Mb; avail=467282.54296875Mb
2020-10-12 23:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001316
2020-10-12 23:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21206.65234375Mb; avail=467282.54296875Mb
2020-10-12 23:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066249
2020-10-12 23:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21052.3359375Mb; avail=467436.859375Mb
2020-10-12 23:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048223
2020-10-12 23:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116762
2020-10-12 23:12:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21092.37109375Mb; avail=467396.82421875Mb
2020-10-12 23:12:52 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.612 | nll_loss 6.458 | ppl 87.89 | wps 109042 | wpb 5327.1 | bsz 200 | num_updates 2665 | best_loss 7.612
2020-10-12 23:12:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:12:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 13 @ 2665 updates, score 7.612) (writing took 6.41143545599698 seconds)
2020-10-12 23:12:58 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 23:12:58 | INFO | train | epoch 013 | loss 7.85 | nll_loss 6.781 | ppl 109.97 | wps 55419.1 | ups 2.64 | wpb 20980.5 | bsz 764.7 | num_updates 2665 | lr 0.000133283 | gnorm 0.944 | clip 0 | train_wall 64 | wall 987
2020-10-12 23:12:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 23:12:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 23:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21383.0390625Mb; avail=467105.0859375Mb
2020-10-12 23:12:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006404
2020-10-12 23:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.037055
2020-10-12 23:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21383.60546875Mb; avail=467104.51953125Mb
2020-10-12 23:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001130
2020-10-12 23:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21383.60546875Mb; avail=467104.51953125Mb
2020-10-12 23:12:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.591390
2020-10-12 23:12:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.630484
2020-10-12 23:12:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.26171875Mb; avail=467103.75390625Mb
2020-10-12 23:12:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21384.26171875Mb; avail=467103.75390625Mb
2020-10-12 23:12:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022784
2020-10-12 23:12:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.26171875Mb; avail=467103.75390625Mb
2020-10-12 23:12:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001128
2020-10-12 23:12:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.26171875Mb; avail=467103.75390625Mb
2020-10-12 23:13:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.591236
2020-10-12 23:13:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.616016
2020-10-12 23:13:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.53515625Mb; avail=467103.33203125Mb
2020-10-12 23:13:00 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 23:13:12 | INFO | train_inner | epoch 014:     35 / 205 loss=7.771, nll_loss=6.69, ppl=103.29, wps=48269.7, ups=2.28, wpb=21141, bsz=764.9, num_updates=2700, lr=0.000135032, gnorm=0.943, clip=0, train_wall=31, wall=1001
2020-10-12 23:13:44 | INFO | train_inner | epoch 014:    135 / 205 loss=7.661, nll_loss=6.564, ppl=94.6, wps=66067.2, ups=3.16, wpb=20920.2, bsz=773.7, num_updates=2800, lr=0.00014003, gnorm=0.94, clip=0, train_wall=31, wall=1032
2020-10-12 23:14:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21203.59375Mb; avail=467285.421875Mb
2020-10-12 23:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002401
2020-10-12 23:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21203.59375Mb; avail=467285.421875Mb
2020-10-12 23:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083250
2020-10-12 23:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21203.4375Mb; avail=467285.1796875Mb
2020-10-12 23:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060367
2020-10-12 23:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.147086
2020-10-12 23:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21203.546875Mb; avail=467285.1796875Mb
2020-10-12 23:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21203.56640625Mb; avail=467285.05859375Mb
2020-10-12 23:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001146
2020-10-12 23:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21203.56640625Mb; avail=467285.05859375Mb
2020-10-12 23:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078599
2020-10-12 23:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21203.58203125Mb; avail=467285.05859375Mb
2020-10-12 23:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053569
2020-10-12 23:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.134258
2020-10-12 23:14:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21209.34375Mb; avail=467279.109375Mb
2020-10-12 23:14:09 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.47 | nll_loss 6.286 | ppl 78.06 | wps 105803 | wpb 5327.1 | bsz 200 | num_updates 2870 | best_loss 7.47
2020-10-12 23:14:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:14:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 14 @ 2870 updates, score 7.47) (writing took 6.8836339869994845 seconds)
2020-10-12 23:14:16 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 23:14:16 | INFO | train | epoch 014 | loss 7.661 | nll_loss 6.563 | ppl 94.58 | wps 55429.4 | ups 2.64 | wpb 20980.5 | bsz 764.7 | num_updates 2870 | lr 0.000143528 | gnorm 0.95 | clip 0 | train_wall 64 | wall 1065
2020-10-12 23:14:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 23:14:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 23:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21384.5390625Mb; avail=467103.4140625Mb
2020-10-12 23:14:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004314
2020-10-12 23:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.037775
2020-10-12 23:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21385.14453125Mb; avail=467102.80859375Mb
2020-10-12 23:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002946
2020-10-12 23:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21385.14453125Mb; avail=467102.80859375Mb
2020-10-12 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.610268
2020-10-12 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.652355
2020-10-12 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21385.80078125Mb; avail=467102.4453125Mb
2020-10-12 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21385.93359375Mb; avail=467102.2109375Mb
2020-10-12 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025161
2020-10-12 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21385.171875Mb; avail=467102.953125Mb
2020-10-12 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001208
2020-10-12 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21385.171875Mb; avail=467102.953125Mb
2020-10-12 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.589615
2020-10-12 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.616863
2020-10-12 23:14:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21385.52734375Mb; avail=467102.48046875Mb
2020-10-12 23:14:17 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 23:14:28 | INFO | train_inner | epoch 015:     30 / 205 loss=7.597, nll_loss=6.49, ppl=89.89, wps=47094.5, ups=2.26, wpb=20873.2, bsz=769, num_updates=2900, lr=0.000145028, gnorm=0.962, clip=0, train_wall=31, wall=1077
2020-10-12 23:15:00 | INFO | train_inner | epoch 015:    130 / 205 loss=7.487, nll_loss=6.362, ppl=82.28, wps=66659, ups=3.14, wpb=21198, bsz=772.8, num_updates=3000, lr=0.000150025, gnorm=0.951, clip=0, train_wall=31, wall=1109
2020-10-12 23:15:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21201.75Mb; avail=467286.8671875Mb
2020-10-12 23:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001527
2020-10-12 23:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21201.75Mb; avail=467286.8671875Mb
2020-10-12 23:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071162
2020-10-12 23:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21201.75Mb; avail=467286.8671875Mb
2020-10-12 23:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051111
2020-10-12 23:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124646
2020-10-12 23:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21201.66015625Mb; avail=467286.8671875Mb
2020-10-12 23:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21201.76171875Mb; avail=467286.8671875Mb
2020-10-12 23:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001127
2020-10-12 23:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21201.76171875Mb; avail=467286.8671875Mb
2020-10-12 23:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069636
2020-10-12 23:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21201.890625Mb; avail=467287.1171875Mb
2020-10-12 23:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051679
2020-10-12 23:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123308
2020-10-12 23:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21201.921875Mb; avail=467286.875Mb
2020-10-12 23:15:27 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.303 | nll_loss 6.091 | ppl 68.18 | wps 107499 | wpb 5327.1 | bsz 200 | num_updates 3075 | best_loss 7.303
2020-10-12 23:15:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:15:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 15 @ 3075 updates, score 7.303) (writing took 6.802618401998188 seconds)
2020-10-12 23:15:34 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 23:15:34 | INFO | train | epoch 015 | loss 7.48 | nll_loss 6.355 | ppl 81.86 | wps 55359.1 | ups 2.64 | wpb 20980.5 | bsz 764.7 | num_updates 3075 | lr 0.000153773 | gnorm 0.981 | clip 0 | train_wall 64 | wall 1142
2020-10-12 23:15:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 23:15:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 23:15:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21385.94921875Mb; avail=467102.11328125Mb
2020-10-12 23:15:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007003
2020-10-12 23:15:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.042947
2020-10-12 23:15:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21386.81640625Mb; avail=467101.328125Mb
2020-10-12 23:15:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001178
2020-10-12 23:15:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21386.81640625Mb; avail=467101.328125Mb
2020-10-12 23:15:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.603976
2020-10-12 23:15:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.649149
2020-10-12 23:15:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21386.71484375Mb; avail=467101.1875Mb
2020-10-12 23:15:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21386.71484375Mb; avail=467101.1875Mb
2020-10-12 23:15:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023173
2020-10-12 23:15:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21386.71484375Mb; avail=467101.1875Mb
2020-10-12 23:15:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001147
2020-10-12 23:15:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21386.71484375Mb; avail=467101.1875Mb
2020-10-12 23:15:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.631593
2020-10-12 23:15:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.656829
2020-10-12 23:15:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21386.86328125Mb; avail=467101.06640625Mb
2020-10-12 23:15:35 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 23:15:44 | INFO | train_inner | epoch 016:     25 / 205 loss=7.429, nll_loss=6.295, ppl=78.52, wps=46491, ups=2.25, wpb=20692.8, bsz=736.5, num_updates=3100, lr=0.000155023, gnorm=0.997, clip=0, train_wall=31, wall=1153
2020-10-12 23:16:16 | INFO | train_inner | epoch 016:    125 / 205 loss=7.308, nll_loss=6.156, ppl=71.3, wps=66088.1, ups=3.12, wpb=21185.5, bsz=800.7, num_updates=3200, lr=0.00016002, gnorm=0.952, clip=0, train_wall=31, wall=1185
2020-10-12 23:16:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21375.39453125Mb; avail=467113.19921875Mb
2020-10-12 23:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001967
2020-10-12 23:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21378.421875Mb; avail=467110.77734375Mb
2020-10-12 23:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067007
2020-10-12 23:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21455.31640625Mb; avail=467033.27734375Mb
2020-10-12 23:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048212
2020-10-12 23:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118073
2020-10-12 23:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21511.01953125Mb; avail=466978.1796875Mb
2020-10-12 23:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21544.92578125Mb; avail=466944.2734375Mb
2020-10-12 23:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001192
2020-10-12 23:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21546.13671875Mb; avail=466943.0625Mb
2020-10-12 23:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066051
2020-10-12 23:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21595.51953125Mb; avail=466893.1875Mb
2020-10-12 23:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047992
2020-10-12 23:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116077
2020-10-12 23:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21595.51953125Mb; avail=466893.6796875Mb
2020-10-12 23:16:45 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.164 | nll_loss 5.926 | ppl 60.81 | wps 107980 | wpb 5327.1 | bsz 200 | num_updates 3280 | best_loss 7.164
2020-10-12 23:16:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:16:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 16 @ 3280 updates, score 7.164) (writing took 5.966706514998805 seconds)
2020-10-12 23:16:51 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 23:16:51 | INFO | train | epoch 016 | loss 7.305 | nll_loss 6.152 | ppl 71.13 | wps 55686 | ups 2.65 | wpb 20980.5 | bsz 764.7 | num_updates 3280 | lr 0.000164018 | gnorm 0.958 | clip 0 | train_wall 64 | wall 1220
2020-10-12 23:16:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 23:16:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 23:16:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21394.39453125Mb; avail=467093.19140625Mb
2020-10-12 23:16:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003777
2020-10-12 23:16:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032794
2020-10-12 23:16:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21395.04296875Mb; avail=467093.01171875Mb
2020-10-12 23:16:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001127
2020-10-12 23:16:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21395.04296875Mb; avail=467093.01171875Mb
2020-10-12 23:16:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.615866
2020-10-12 23:16:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.650926
2020-10-12 23:16:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21395.0859375Mb; avail=467093.01171875Mb
2020-10-12 23:16:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21395.3125Mb; avail=467092.78515625Mb
2020-10-12 23:16:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026461
2020-10-12 23:16:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21395.2734375Mb; avail=467092.82421875Mb
2020-10-12 23:16:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001204
2020-10-12 23:16:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21395.2734375Mb; avail=467092.82421875Mb
2020-10-12 23:16:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.591639
2020-10-12 23:16:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.620102
2020-10-12 23:16:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21394.71484375Mb; avail=467093.73828125Mb
2020-10-12 23:16:52 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 23:17:00 | INFO | train_inner | epoch 017:     20 / 205 loss=7.277, nll_loss=6.119, ppl=69.52, wps=47896.7, ups=2.31, wpb=20747.8, bsz=714.4, num_updates=3300, lr=0.000165018, gnorm=0.955, clip=0, train_wall=31, wall=1228
2020-10-12 23:17:32 | INFO | train_inner | epoch 017:    120 / 205 loss=7.161, nll_loss=5.986, ppl=63.37, wps=65869.2, ups=3.13, wpb=21070.8, bsz=768.7, num_updates=3400, lr=0.000170015, gnorm=0.979, clip=0, train_wall=31, wall=1260
2020-10-12 23:17:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21208.96875Mb; avail=467279.9609375Mb
2020-10-12 23:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001784
2020-10-12 23:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21208.4765625Mb; avail=467280.453125Mb
2020-10-12 23:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066004
2020-10-12 23:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21075.08984375Mb; avail=467413.234375Mb
2020-10-12 23:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047955
2020-10-12 23:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116666
2020-10-12 23:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21109.2265625Mb; avail=467379.546875Mb
2020-10-12 23:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21146.83984375Mb; avail=467341.28125Mb
2020-10-12 23:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001209
2020-10-12 23:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21148.6640625Mb; avail=467340.0703125Mb
2020-10-12 23:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066463
2020-10-12 23:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21225.1171875Mb; avail=467264.265625Mb
2020-10-12 23:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047754
2020-10-12 23:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116226
2020-10-12 23:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21285.6640625Mb; avail=467203.11328125Mb
2020-10-12 23:18:02 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.039 | nll_loss 5.772 | ppl 54.65 | wps 109957 | wpb 5327.1 | bsz 200 | num_updates 3485 | best_loss 7.039
2020-10-12 23:18:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:18:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 17 @ 3485 updates, score 7.039) (writing took 6.283927770000446 seconds)
2020-10-12 23:18:08 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 23:18:08 | INFO | train | epoch 017 | loss 7.135 | nll_loss 5.956 | ppl 62.06 | wps 55629.1 | ups 2.65 | wpb 20980.5 | bsz 764.7 | num_updates 3485 | lr 0.000174263 | gnorm 0.96 | clip 0 | train_wall 64 | wall 1297
2020-10-12 23:18:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 23:18:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 23:18:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21389.02734375Mb; avail=467098.63671875Mb
2020-10-12 23:18:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003734
2020-10-12 23:18:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032469
2020-10-12 23:18:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21389.6328125Mb; avail=467098.03125Mb
2020-10-12 23:18:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001127
2020-10-12 23:18:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21389.6328125Mb; avail=467098.03125Mb
2020-10-12 23:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.602297
2020-10-12 23:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.636880
2020-10-12 23:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21389.81640625Mb; avail=467098.2734375Mb
2020-10-12 23:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21390.43359375Mb; avail=467097.546875Mb
2020-10-12 23:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024835
2020-10-12 23:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21390.43359375Mb; avail=467097.546875Mb
2020-10-12 23:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001155
2020-10-12 23:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21390.43359375Mb; avail=467097.546875Mb
2020-10-12 23:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.589318
2020-10-12 23:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.616406
2020-10-12 23:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21390.2890625Mb; avail=467097.66796875Mb
2020-10-12 23:18:09 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 23:18:15 | INFO | train_inner | epoch 018:     15 / 205 loss=7.073, nll_loss=5.884, ppl=59.05, wps=47991.8, ups=2.29, wpb=20964.5, bsz=788.2, num_updates=3500, lr=0.000175013, gnorm=0.951, clip=0, train_wall=31, wall=1304
2020-10-12 23:18:47 | INFO | train_inner | epoch 018:    115 / 205 loss=6.988, nll_loss=5.786, ppl=55.19, wps=66137.1, ups=3.15, wpb=21022.2, bsz=761, num_updates=3600, lr=0.00018001, gnorm=0.946, clip=0, train_wall=31, wall=1336
2020-10-12 23:19:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21392.6328125Mb; avail=467095.859375Mb
2020-10-12 23:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002598
2020-10-12 23:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21392.9453125Mb; avail=467095.859375Mb
2020-10-12 23:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066520
2020-10-12 23:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21394.07421875Mb; avail=467095.01953125Mb
2020-10-12 23:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047965
2020-10-12 23:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117874
2020-10-12 23:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21393.72265625Mb; avail=467095.39453125Mb
2020-10-12 23:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21394.109375Mb; avail=467095.00390625Mb
2020-10-12 23:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001144
2020-10-12 23:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21394.109375Mb; avail=467095.00390625Mb
2020-10-12 23:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066025
2020-10-12 23:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21411.19921875Mb; avail=467078.1015625Mb
2020-10-12 23:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048360
2020-10-12 23:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116276
2020-10-12 23:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21416.796875Mb; avail=467072.87890625Mb
2020-10-12 23:19:19 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.871 | nll_loss 5.562 | ppl 47.23 | wps 108870 | wpb 5327.1 | bsz 200 | num_updates 3690 | best_loss 6.871
2020-10-12 23:19:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:19:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 18 @ 3690 updates, score 6.871) (writing took 11.189232143999106 seconds)
2020-10-12 23:19:30 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 23:19:30 | INFO | train | epoch 018 | loss 6.959 | nll_loss 5.752 | ppl 53.9 | wps 52576.6 | ups 2.51 | wpb 20980.5 | bsz 764.7 | num_updates 3690 | lr 0.000184508 | gnorm 0.913 | clip 0 | train_wall 64 | wall 1379
2020-10-12 23:19:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 23:19:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13895.93359375Mb; avail=474626.0703125Mb
2020-10-12 23:19:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004368
2020-10-12 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.037184
2020-10-12 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13896.05859375Mb; avail=474625.9453125Mb
2020-10-12 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001396
2020-10-12 23:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13896.05859375Mb; avail=474625.9453125Mb
2020-10-12 23:19:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.671020
2020-10-12 23:19:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.710533
2020-10-12 23:19:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13686.62109375Mb; avail=474847.63671875Mb
2020-10-12 23:19:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13691.70703125Mb; avail=474842.55078125Mb
2020-10-12 23:19:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024559
2020-10-12 23:19:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13691.69921875Mb; avail=474842.55859375Mb
2020-10-12 23:19:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001134
2020-10-12 23:19:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13691.69921875Mb; avail=474842.55859375Mb
2020-10-12 23:19:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.594098
2020-10-12 23:19:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.620786
2020-10-12 23:19:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13777.58203125Mb; avail=474756.4609375Mb
2020-10-12 23:19:31 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 23:19:36 | INFO | train_inner | epoch 019:     10 / 205 loss=6.915, nll_loss=5.701, ppl=52, wps=43290.6, ups=2.06, wpb=21038.9, bsz=762.3, num_updates=3700, lr=0.000185008, gnorm=0.89, clip=0, train_wall=31, wall=1385
2020-10-12 23:20:08 | INFO | train_inner | epoch 019:    110 / 205 loss=6.79, nll_loss=5.557, ppl=47.09, wps=65666, ups=3.14, wpb=20943, bsz=784.2, num_updates=3800, lr=0.000190005, gnorm=0.91, clip=0, train_wall=31, wall=1416
2020-10-12 23:20:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:20:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20997.79296875Mb; avail=467495.85546875Mb
2020-10-12 23:20:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002338
2020-10-12 23:20:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20970.72265625Mb; avail=467521.94140625Mb
2020-10-12 23:20:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066813
2020-10-12 23:20:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20929.54296875Mb; avail=467559.95703125Mb
2020-10-12 23:20:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.083724
2020-10-12 23:20:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.154056
2020-10-12 23:20:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21028.5234375Mb; avail=467460.76953125Mb
2020-10-12 23:20:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21061.82421875Mb; avail=467427.46875Mb
2020-10-12 23:20:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001197
2020-10-12 23:20:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21063.03515625Mb; avail=467426.2578125Mb
2020-10-12 23:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066851
2020-10-12 23:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21150.22265625Mb; avail=467339.0703125Mb
2020-10-12 23:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048011
2020-10-12 23:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116977
2020-10-12 23:20:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21215.94921875Mb; avail=467272.6953125Mb
2020-10-12 23:20:41 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.808 | nll_loss 5.487 | ppl 44.85 | wps 110384 | wpb 5327.1 | bsz 200 | num_updates 3895 | best_loss 6.808
2020-10-12 23:20:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:20:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 19 @ 3895 updates, score 6.808) (writing took 6.297705443997984 seconds)
2020-10-12 23:20:47 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 23:20:47 | INFO | train | epoch 019 | loss 6.786 | nll_loss 5.551 | ppl 46.9 | wps 55537.9 | ups 2.65 | wpb 20980.5 | bsz 764.7 | num_updates 3895 | lr 0.000194753 | gnorm 0.912 | clip 0 | train_wall 64 | wall 1456
2020-10-12 23:20:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 23:20:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 23:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21223.4921875Mb; avail=467264.55078125Mb
2020-10-12 23:20:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004768
2020-10-12 23:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.040691
2020-10-12 23:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21223.74609375Mb; avail=467264.08203125Mb
2020-10-12 23:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001164
2020-10-12 23:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21223.74609375Mb; avail=467264.08203125Mb
2020-10-12 23:20:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.603225
2020-10-12 23:20:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.645991
2020-10-12 23:20:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21223.4609375Mb; avail=467264.68359375Mb
2020-10-12 23:20:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21223.09765625Mb; avail=467265.046875Mb
2020-10-12 23:20:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022895
2020-10-12 23:20:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21223.015625Mb; avail=467264.92578125Mb
2020-10-12 23:20:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001118
2020-10-12 23:20:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21223.03125Mb; avail=467264.8046875Mb
2020-10-12 23:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.598364
2020-10-12 23:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.623504
2020-10-12 23:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21223.08203125Mb; avail=467265.0546875Mb
2020-10-12 23:20:49 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 23:20:52 | INFO | train_inner | epoch 020:      5 / 205 loss=6.765, nll_loss=5.527, ppl=46.11, wps=47543, ups=2.28, wpb=20896.8, bsz=751.4, num_updates=3900, lr=0.000195003, gnorm=0.917, clip=0, train_wall=31, wall=1460
2020-10-12 23:21:24 | INFO | train_inner | epoch 020:    105 / 205 loss=6.643, nll_loss=5.387, ppl=41.84, wps=64981.1, ups=3.1, wpb=20950.8, bsz=772.1, num_updates=4000, lr=0.0002, gnorm=0.903, clip=0, train_wall=31, wall=1493
2020-10-12 23:21:56 | INFO | train_inner | epoch 020:    205 / 205 loss=6.58, nll_loss=5.312, ppl=39.73, wps=65012.5, ups=3.09, wpb=21048.7, bsz=760.1, num_updates=4100, lr=0.000197546, gnorm=0.856, clip=0, train_wall=31, wall=1525
2020-10-12 23:21:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:21:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21286.35546875Mb; avail=467203.16015625Mb
2020-10-12 23:21:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001823
2020-10-12 23:21:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21288.171875Mb; avail=467201.34375Mb
2020-10-12 23:21:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067566
2020-10-12 23:21:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21360.22265625Mb; avail=467129.29296875Mb
2020-10-12 23:21:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047744
2020-10-12 23:21:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117959
2020-10-12 23:21:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21410.4765625Mb; avail=467079.0390625Mb
2020-10-12 23:21:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21433.87109375Mb; avail=467055.5390625Mb
2020-10-12 23:21:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001675
2020-10-12 23:21:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21433.37890625Mb; avail=467056.03125Mb
2020-10-12 23:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.088313
2020-10-12 23:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21432.765625Mb; avail=467055.75Mb
2020-10-12 23:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047715
2020-10-12 23:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138771
2020-10-12 23:21:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21433.38671875Mb; avail=467055.71484375Mb
2020-10-12 23:21:59 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.607 | nll_loss 5.246 | ppl 37.96 | wps 109628 | wpb 5327.1 | bsz 200 | num_updates 4100 | best_loss 6.607
2020-10-12 23:21:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:22:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 20 @ 4100 updates, score 6.607) (writing took 5.84773122399929 seconds)
2020-10-12 23:22:05 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 23:22:05 | INFO | train | epoch 020 | loss 6.614 | nll_loss 5.353 | ppl 40.87 | wps 55404.8 | ups 2.64 | wpb 20980.5 | bsz 764.7 | num_updates 4100 | lr 0.000197546 | gnorm 0.884 | clip 0 | train_wall 65 | wall 1534
2020-10-12 23:22:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 23:22:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 23:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21226.96875Mb; avail=467261.06640625Mb
2020-10-12 23:22:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004451
2020-10-12 23:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034151
2020-10-12 23:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21227.0625Mb; avail=467260.97265625Mb
2020-10-12 23:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001182
2020-10-12 23:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21227.0625Mb; avail=467260.97265625Mb
2020-10-12 23:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.601185
2020-10-12 23:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.637398
2020-10-12 23:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21226.7734375Mb; avail=467261.46484375Mb
2020-10-12 23:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21226.7734375Mb; avail=467261.46484375Mb
2020-10-12 23:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022903
2020-10-12 23:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21226.7734375Mb; avail=467261.46484375Mb
2020-10-12 23:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001124
2020-10-12 23:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21226.7734375Mb; avail=467261.46484375Mb
2020-10-12 23:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.590413
2020-10-12 23:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.615286
2020-10-12 23:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21226.48046875Mb; avail=467261.67578125Mb
2020-10-12 23:22:06 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 23:22:40 | INFO | train_inner | epoch 021:    100 / 205 loss=6.464, nll_loss=5.18, ppl=36.25, wps=48702.5, ups=2.31, wpb=21095.9, bsz=765.8, num_updates=4200, lr=0.00019518, gnorm=0.867, clip=0, train_wall=31, wall=1568
2020-10-12 23:23:12 | INFO | train_inner | epoch 021:    200 / 205 loss=6.443, nll_loss=5.155, ppl=35.63, wps=65108.1, ups=3.11, wpb=20935.3, bsz=760.4, num_updates=4300, lr=0.000192897, gnorm=0.85, clip=0, train_wall=31, wall=1600
2020-10-12 23:23:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:23:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21563.09765625Mb; avail=466924.75Mb
2020-10-12 23:23:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002639
2020-10-12 23:23:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21566.125Mb; avail=466922.328125Mb
2020-10-12 23:23:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066830
2020-10-12 23:23:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21575.29296875Mb; avail=466912.76953125Mb
2020-10-12 23:23:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049993
2020-10-12 23:23:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120582
2020-10-12 23:23:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21575.0703125Mb; avail=466913.8046875Mb
2020-10-12 23:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21575.1015625Mb; avail=466913.14453125Mb
2020-10-12 23:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001483
2020-10-12 23:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21575.3203125Mb; avail=466912.8046875Mb
2020-10-12 23:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067909
2020-10-12 23:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21574.92578125Mb; avail=466913.44921875Mb
2020-10-12 23:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048055
2020-10-12 23:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118347
2020-10-12 23:23:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21574.94921875Mb; avail=466912.9140625Mb
2020-10-12 23:23:16 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.498 | nll_loss 5.113 | ppl 34.61 | wps 112409 | wpb 5327.1 | bsz 200 | num_updates 4305 | best_loss 6.498
2020-10-12 23:23:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:23:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 21 @ 4305 updates, score 6.498) (writing took 5.811438742002792 seconds)
2020-10-12 23:23:22 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 23:23:22 | INFO | train | epoch 021 | loss 6.451 | nll_loss 5.164 | ppl 35.86 | wps 55885.3 | ups 2.66 | wpb 20980.5 | bsz 764.7 | num_updates 4305 | lr 0.000192785 | gnorm 0.857 | clip 0 | train_wall 64 | wall 1611
2020-10-12 23:23:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 23:23:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 23:23:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21371.33984375Mb; avail=467116.74609375Mb
2020-10-12 23:23:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003725
2020-10-12 23:23:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.041242
2020-10-12 23:23:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21371.91796875Mb; avail=467116.16796875Mb
2020-10-12 23:23:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002314
2020-10-12 23:23:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21371.91796875Mb; avail=467116.16796875Mb
2020-10-12 23:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.641073
2020-10-12 23:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.686078
2020-10-12 23:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21371.5546875Mb; avail=467116.2890625Mb
2020-10-12 23:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21371.3125Mb; avail=467116.53125Mb
2020-10-12 23:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022986
2020-10-12 23:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21371.3125Mb; avail=467116.53125Mb
2020-10-12 23:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001165
2020-10-12 23:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21371.3125Mb; avail=467116.53125Mb
2020-10-12 23:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.602346
2020-10-12 23:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.627649
2020-10-12 23:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21370.44921875Mb; avail=467117.65234375Mb
2020-10-12 23:23:23 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 23:23:55 | INFO | train_inner | epoch 022:     95 / 205 loss=6.336, nll_loss=5.032, ppl=32.72, wps=48510.6, ups=2.31, wpb=20960.6, bsz=753.7, num_updates=4400, lr=0.000190693, gnorm=0.831, clip=0, train_wall=31, wall=1644
2020-10-12 23:24:27 | INFO | train_inner | epoch 022:    195 / 205 loss=6.284, nll_loss=4.97, ppl=31.35, wps=65004.8, ups=3.1, wpb=20942.2, bsz=779.6, num_updates=4500, lr=0.000188562, gnorm=0.835, clip=0, train_wall=31, wall=1676
2020-10-12 23:24:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21505.89453125Mb; avail=466983.55859375Mb
2020-10-12 23:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001628
2020-10-12 23:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21507.7109375Mb; avail=466981.7421875Mb
2020-10-12 23:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067384
2020-10-12 23:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21575.85546875Mb; avail=466913.59765625Mb
2020-10-12 23:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049660
2020-10-12 23:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119589
2020-10-12 23:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21575.85546875Mb; avail=466913.59765625Mb
2020-10-12 23:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21575.36328125Mb; avail=466914.08984375Mb
2020-10-12 23:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001242
2020-10-12 23:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21575.36328125Mb; avail=466914.08984375Mb
2020-10-12 23:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067111
2020-10-12 23:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21575.8671875Mb; avail=466913.29296875Mb
2020-10-12 23:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049288
2020-10-12 23:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118484
2020-10-12 23:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21575.5703125Mb; avail=466913.96484375Mb
2020-10-12 23:24:33 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.392 | nll_loss 4.994 | ppl 31.87 | wps 110841 | wpb 5327.1 | bsz 200 | num_updates 4510 | best_loss 6.392
2020-10-12 23:24:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:24:39 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 22 @ 4510 updates, score 6.392) (writing took 5.6981338469995535 seconds)
2020-10-12 23:24:39 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 23:24:39 | INFO | train | epoch 022 | loss 6.307 | nll_loss 4.997 | ppl 31.94 | wps 55742.2 | ups 2.66 | wpb 20980.5 | bsz 764.7 | num_updates 4510 | lr 0.000188353 | gnorm 0.837 | clip 0 | train_wall 64 | wall 1688
2020-10-12 23:24:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 23:24:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 23:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21365.28515625Mb; avail=467122.54296875Mb
2020-10-12 23:24:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003612
2020-10-12 23:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033474
2020-10-12 23:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21365.5234375Mb; avail=467122.39453125Mb
2020-10-12 23:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001173
2020-10-12 23:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21365.5234375Mb; avail=467122.39453125Mb
2020-10-12 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.595099
2020-10-12 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.630668
2020-10-12 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21366.59765625Mb; avail=467120.94140625Mb
2020-10-12 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21366.5390625Mb; avail=467121.42578125Mb
2020-10-12 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023150
2020-10-12 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21366.41796875Mb; avail=467121.546875Mb
2020-10-12 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001176
2020-10-12 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21366.41796875Mb; avail=467121.546875Mb
2020-10-12 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.594424
2020-10-12 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.619640
2020-10-12 23:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21366.53125Mb; avail=467121.546875Mb
2020-10-12 23:24:40 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 23:25:11 | INFO | train_inner | epoch 023:     90 / 205 loss=6.196, nll_loss=4.87, ppl=29.24, wps=48590.2, ups=2.3, wpb=21114.8, bsz=754.9, num_updates=4600, lr=0.000186501, gnorm=0.803, clip=0, train_wall=31, wall=1719
2020-10-12 23:25:43 | INFO | train_inner | epoch 023:    190 / 205 loss=6.171, nll_loss=4.84, ppl=28.64, wps=65262.7, ups=3.13, wpb=20834.9, bsz=772.3, num_updates=4700, lr=0.000184506, gnorm=0.793, clip=0, train_wall=31, wall=1751
2020-10-12 23:25:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:25:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21575.5390625Mb; avail=466914.03515625Mb
2020-10-12 23:25:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002062
2020-10-12 23:25:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21575.0Mb; avail=466913.7109375Mb
2020-10-12 23:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067964
2020-10-12 23:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21575.30078125Mb; avail=466913.9609375Mb
2020-10-12 23:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048665
2020-10-12 23:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119534
2020-10-12 23:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21575.30859375Mb; avail=466914.296875Mb
2020-10-12 23:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21575.4921875Mb; avail=466914.31640625Mb
2020-10-12 23:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001097
2020-10-12 23:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21575.234375Mb; avail=466913.484375Mb
2020-10-12 23:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067664
2020-10-12 23:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21574.64453125Mb; avail=466914.2109375Mb
2020-10-12 23:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047343
2020-10-12 23:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116880
2020-10-12 23:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21574.66015625Mb; avail=466914.765625Mb
2020-10-12 23:25:51 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.304 | nll_loss 4.888 | ppl 29.61 | wps 94595.5 | wpb 5327.1 | bsz 200 | num_updates 4715 | best_loss 6.304
2020-10-12 23:25:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:25:59 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 23 @ 4715 updates, score 6.304) (writing took 8.180291411001235 seconds)
2020-10-12 23:25:59 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 23:25:59 | INFO | train | epoch 023 | loss 6.177 | nll_loss 4.848 | ppl 28.8 | wps 54048.3 | ups 2.58 | wpb 20980.5 | bsz 764.7 | num_updates 4715 | lr 0.000184213 | gnorm 0.8 | clip 0 | train_wall 64 | wall 1767
2020-10-12 23:25:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 23:25:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 23:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21342.42578125Mb; avail=467145.85546875Mb
2020-10-12 23:25:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003636
2020-10-12 23:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032403
2020-10-12 23:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21342.45703125Mb; avail=467145.82421875Mb
2020-10-12 23:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001132
2020-10-12 23:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21342.3359375Mb; avail=467145.9453125Mb
2020-10-12 23:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.607789
2020-10-12 23:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.642182
2020-10-12 23:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21342.19140625Mb; avail=467146.08984375Mb
2020-10-12 23:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21342.28125Mb; avail=467145.57421875Mb
2020-10-12 23:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022726
2020-10-12 23:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21342.2109375Mb; avail=467146.04296875Mb
2020-10-12 23:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001140
2020-10-12 23:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21342.2109375Mb; avail=467146.04296875Mb
2020-10-12 23:26:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.597930
2020-10-12 23:26:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.622914
2020-10-12 23:26:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21341.75Mb; avail=467146.390625Mb
2020-10-12 23:26:00 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 23:26:28 | INFO | train_inner | epoch 024:     85 / 205 loss=6.079, nll_loss=4.735, ppl=26.63, wps=45976.2, ups=2.18, wpb=21122.9, bsz=764.8, num_updates=4800, lr=0.000182574, gnorm=0.774, clip=0, train_wall=31, wall=1797
2020-10-12 23:27:00 | INFO | train_inner | epoch 024:    185 / 205 loss=6.051, nll_loss=4.702, ppl=26.03, wps=65885.8, ups=3.13, wpb=21060.7, bsz=772.2, num_updates=4900, lr=0.000180702, gnorm=0.791, clip=0, train_wall=31, wall=1829
2020-10-12 23:27:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21458.40625Mb; avail=467029.984375Mb
2020-10-12 23:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001666
2020-10-12 23:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21458.40625Mb; avail=467029.984375Mb
2020-10-12 23:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071043
2020-10-12 23:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21457.81640625Mb; avail=467031.09765625Mb
2020-10-12 23:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051571
2020-10-12 23:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125237
2020-10-12 23:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21457.7734375Mb; avail=467031.21875Mb
2020-10-12 23:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21457.8046875Mb; avail=467031.1875Mb
2020-10-12 23:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001360
2020-10-12 23:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21457.8046875Mb; avail=467031.1875Mb
2020-10-12 23:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067836
2020-10-12 23:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21457.8203125Mb; avail=467031.171875Mb
2020-10-12 23:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050760
2020-10-12 23:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120931
2020-10-12 23:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21457.8203125Mb; avail=467031.171875Mb
2020-10-12 23:27:10 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.218 | nll_loss 4.775 | ppl 27.39 | wps 107223 | wpb 5327.1 | bsz 200 | num_updates 4920 | best_loss 6.218
2020-10-12 23:27:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:27:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 24 @ 4920 updates, score 6.218) (writing took 4.51617429199905 seconds)
2020-10-12 23:27:14 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 23:27:14 | INFO | train | epoch 024 | loss 6.058 | nll_loss 4.711 | ppl 26.19 | wps 56919.2 | ups 2.71 | wpb 20980.5 | bsz 764.7 | num_updates 4920 | lr 0.000180334 | gnorm 0.768 | clip 0 | train_wall 64 | wall 1843
2020-10-12 23:27:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 23:27:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 23:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21373.69140625Mb; avail=467114.33984375Mb
2020-10-12 23:27:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003524
2020-10-12 23:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032524
2020-10-12 23:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21373.3125Mb; avail=467114.71875Mb
2020-10-12 23:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001139
2020-10-12 23:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21373.3125Mb; avail=467114.71875Mb
2020-10-12 23:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.604439
2020-10-12 23:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.638971
2020-10-12 23:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21374.26171875Mb; avail=467113.875Mb
2020-10-12 23:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21374.26171875Mb; avail=467113.875Mb
2020-10-12 23:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022826
2020-10-12 23:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21373.87890625Mb; avail=467114.48046875Mb
2020-10-12 23:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001148
2020-10-12 23:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21373.87890625Mb; avail=467114.48046875Mb
2020-10-12 23:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.605942
2020-10-12 23:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.631127
2020-10-12 23:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21373.8125Mb; avail=467114.23828125Mb
2020-10-12 23:27:16 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 23:27:42 | INFO | train_inner | epoch 025:     80 / 205 loss=5.974, nll_loss=4.614, ppl=24.49, wps=49980.2, ups=2.38, wpb=20969.8, bsz=778.8, num_updates=5000, lr=0.000178885, gnorm=0.787, clip=0, train_wall=31, wall=1871
2020-10-12 23:28:14 | INFO | train_inner | epoch 025:    180 / 205 loss=5.956, nll_loss=4.593, ppl=24.14, wps=66003.4, ups=3.15, wpb=20980.4, bsz=764.1, num_updates=5100, lr=0.000177123, gnorm=0.752, clip=0, train_wall=31, wall=1903
2020-10-12 23:28:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21470.17578125Mb; avail=467018.81640625Mb
2020-10-12 23:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001760
2020-10-12 23:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21469.7890625Mb; avail=467018.4765625Mb
2020-10-12 23:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066780
2020-10-12 23:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21466.578125Mb; avail=467022.22265625Mb
2020-10-12 23:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048002
2020-10-12 23:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117370
2020-10-12 23:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21466.83984375Mb; avail=467021.97265625Mb
2020-10-12 23:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21467.28515625Mb; avail=467021.91015625Mb
2020-10-12 23:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001199
2020-10-12 23:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21467.02734375Mb; avail=467021.68359375Mb
2020-10-12 23:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067110
2020-10-12 23:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21466.21875Mb; avail=467021.8359375Mb
2020-10-12 23:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048042
2020-10-12 23:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117158
2020-10-12 23:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21466.3125Mb; avail=467021.9765625Mb
2020-10-12 23:28:25 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.16 | nll_loss 4.703 | ppl 26.05 | wps 111568 | wpb 5327.1 | bsz 200 | num_updates 5125 | best_loss 6.16
2020-10-12 23:28:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:28:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 25 @ 5125 updates, score 6.16) (writing took 4.536239397999452 seconds)
2020-10-12 23:28:29 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 23:28:29 | INFO | train | epoch 025 | loss 5.955 | nll_loss 4.593 | ppl 24.13 | wps 57214.6 | ups 2.73 | wpb 20980.5 | bsz 764.7 | num_updates 5125 | lr 0.00017669 | gnorm 0.775 | clip 0 | train_wall 64 | wall 1918
2020-10-12 23:28:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 23:28:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 23:28:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21384.6953125Mb; avail=467103.375Mb
2020-10-12 23:28:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003380
2020-10-12 23:28:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032155
2020-10-12 23:28:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.80859375Mb; avail=467103.26171875Mb
2020-10-12 23:28:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001128
2020-10-12 23:28:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.80859375Mb; avail=467103.26171875Mb
2020-10-12 23:28:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.600684
2020-10-12 23:28:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.634831
2020-10-12 23:28:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.23046875Mb; avail=467103.921875Mb
2020-10-12 23:28:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21384.22265625Mb; avail=467103.921875Mb
2020-10-12 23:28:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022744
2020-10-12 23:28:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.22265625Mb; avail=467103.921875Mb
2020-10-12 23:28:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001122
2020-10-12 23:28:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.22265625Mb; avail=467103.921875Mb
2020-10-12 23:28:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.599668
2020-10-12 23:28:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.624380
2020-10-12 23:28:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21384.68359375Mb; avail=467103.1953125Mb
2020-10-12 23:28:31 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 23:28:56 | INFO | train_inner | epoch 026:     75 / 205 loss=5.895, nll_loss=4.524, ppl=23, wps=50173.4, ups=2.41, wpb=20824.6, bsz=738.8, num_updates=5200, lr=0.000175412, gnorm=0.755, clip=0, train_wall=31, wall=1944
2020-10-12 23:29:27 | INFO | train_inner | epoch 026:    175 / 205 loss=5.856, nll_loss=4.479, ppl=22.29, wps=65800.4, ups=3.15, wpb=20908.8, bsz=768.6, num_updates=5300, lr=0.000173749, gnorm=0.779, clip=0, train_wall=31, wall=1976
2020-10-12 23:29:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:29:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21940.73828125Mb; avail=466548.6328125Mb
2020-10-12 23:29:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001608
2020-10-12 23:29:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21940.95703125Mb; avail=466549.27734375Mb
2020-10-12 23:29:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066370
2020-10-12 23:29:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21946.41015625Mb; avail=466543.2109375Mb
2020-10-12 23:29:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049211
2020-10-12 23:29:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118119
2020-10-12 23:29:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21952.328125Mb; avail=466537.078125Mb
2020-10-12 23:29:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21955.01171875Mb; avail=466534.6796875Mb
2020-10-12 23:29:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001096
2020-10-12 23:29:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21955.26953125Mb; avail=466534.9453125Mb
2020-10-12 23:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.140165
2020-10-12 23:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21464.48046875Mb; avail=467032.4375Mb
2020-10-12 23:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049399
2020-10-12 23:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.191664
2020-10-12 23:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21405.46875Mb; avail=467085.1015625Mb
2020-10-12 23:29:40 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.092 | nll_loss 4.636 | ppl 24.87 | wps 112904 | wpb 5327.1 | bsz 200 | num_updates 5330 | best_loss 6.092
2020-10-12 23:29:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:29:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 26 @ 5330 updates, score 6.092) (writing took 8.381939117000002 seconds)
2020-10-12 23:29:49 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 23:29:49 | INFO | train | epoch 026 | loss 5.863 | nll_loss 4.487 | ppl 22.42 | wps 54228.4 | ups 2.58 | wpb 20980.5 | bsz 764.7 | num_updates 5330 | lr 0.000173259 | gnorm 0.763 | clip 0 | train_wall 64 | wall 1998
2020-10-12 23:29:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 23:29:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 23:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21393.7578125Mb; avail=467093.12109375Mb
2020-10-12 23:29:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004362
2020-10-12 23:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034228
2020-10-12 23:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21394.4453125Mb; avail=467093.5Mb
2020-10-12 23:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001182
2020-10-12 23:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21394.4453125Mb; avail=467093.5Mb
2020-10-12 23:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.603965
2020-10-12 23:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.640598
2020-10-12 23:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21395.64453125Mb; avail=467092.40625Mb
2020-10-12 23:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21395.5546875Mb; avail=467092.40625Mb
2020-10-12 23:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024531
2020-10-12 23:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21395.5546875Mb; avail=467092.40625Mb
2020-10-12 23:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001222
2020-10-12 23:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21395.5546875Mb; avail=467092.40625Mb
2020-10-12 23:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.594993
2020-10-12 23:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.621589
2020-10-12 23:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21395.4609375Mb; avail=467092.52734375Mb
2020-10-12 23:29:50 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 23:30:14 | INFO | train_inner | epoch 027:     70 / 205 loss=5.791, nll_loss=4.404, ppl=21.17, wps=45594.4, ups=2.16, wpb=21075.6, bsz=765.8, num_updates=5400, lr=0.000172133, gnorm=0.727, clip=0, train_wall=31, wall=2022
2020-10-12 23:30:46 | INFO | train_inner | epoch 027:    170 / 205 loss=5.789, nll_loss=4.401, ppl=21.13, wps=66162, ups=3.14, wpb=21070, bsz=767.4, num_updates=5500, lr=0.000170561, gnorm=0.764, clip=0, train_wall=31, wall=2054
2020-10-12 23:30:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21066.47265625Mb; avail=467422.46484375Mb
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001837
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21066.47265625Mb; avail=467422.46484375Mb
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080102
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21069.1015625Mb; avail=467419.8359375Mb
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.154943
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.238108
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21079.59765625Mb; avail=467409.015625Mb
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21088.56640625Mb; avail=467400.65234375Mb
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001196
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21089.77734375Mb; avail=467399.44140625Mb
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066420
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21102.40234375Mb; avail=467386.21484375Mb
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048728
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117310
2020-10-12 23:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21118.01953125Mb; avail=467371.203125Mb
2020-10-12 23:31:00 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.031 | nll_loss 4.561 | ppl 23.6 | wps 110817 | wpb 5327.1 | bsz 200 | num_updates 5535 | best_loss 6.031
2020-10-12 23:31:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:31:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 27 @ 5535 updates, score 6.031) (writing took 8.307505414002662 seconds)
2020-10-12 23:31:08 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 23:31:08 | INFO | train | epoch 027 | loss 5.773 | nll_loss 4.384 | ppl 20.88 | wps 54286.3 | ups 2.59 | wpb 20980.5 | bsz 764.7 | num_updates 5535 | lr 0.00017002 | gnorm 0.744 | clip 0 | train_wall 64 | wall 2077
2020-10-12 23:31:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 23:31:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 23:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21375.81640625Mb; avail=467112.578125Mb
2020-10-12 23:31:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003600
2020-10-12 23:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032110
2020-10-12 23:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21385.1328125Mb; avail=467103.26171875Mb
2020-10-12 23:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001146
2020-10-12 23:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21385.73828125Mb; avail=467102.65625Mb
2020-10-12 23:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.605292
2020-10-12 23:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.639437
2020-10-12 23:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21397.44140625Mb; avail=467090.65234375Mb
2020-10-12 23:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21397.44140625Mb; avail=467090.65234375Mb
2020-10-12 23:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022867
2020-10-12 23:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21397.3203125Mb; avail=467090.7734375Mb
2020-10-12 23:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001143
2020-10-12 23:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21397.3203125Mb; avail=467090.7734375Mb
2020-10-12 23:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.598402
2020-10-12 23:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.623547
2020-10-12 23:31:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21397.796875Mb; avail=467090.29296875Mb
2020-10-12 23:31:09 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 23:31:31 | INFO | train_inner | epoch 028:     65 / 205 loss=5.727, nll_loss=4.33, ppl=20.12, wps=45275.5, ups=2.18, wpb=20736.7, bsz=745.3, num_updates=5600, lr=0.000169031, gnorm=0.737, clip=0, train_wall=31, wall=2100
2020-10-12 23:32:03 | INFO | train_inner | epoch 028:    165 / 205 loss=5.692, nll_loss=4.29, ppl=19.56, wps=66246.1, ups=3.14, wpb=21130.3, bsz=776.8, num_updates=5700, lr=0.000167542, gnorm=0.735, clip=0, train_wall=31, wall=2132
2020-10-12 23:32:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21082.0859375Mb; avail=467407.8125Mb
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001640
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21081.8046875Mb; avail=467407.99609375Mb
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069309
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21094.36328125Mb; avail=467395.4375Mb
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048905
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120705
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21109.5Mb; avail=467380.30078125Mb
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21120.27734375Mb; avail=467369.5234375Mb
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001188
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21120.26953125Mb; avail=467369.53125Mb
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066360
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21129.3515625Mb; avail=467360.44921875Mb
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048611
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116998
2020-10-12 23:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21140.25Mb; avail=467349.55078125Mb
2020-10-12 23:32:19 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.989 | nll_loss 4.507 | ppl 22.73 | wps 106714 | wpb 5327.1 | bsz 200 | num_updates 5740 | best_loss 5.989
2020-10-12 23:32:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:32:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 28 @ 5740 updates, score 5.989) (writing took 8.15945579199979 seconds)
2020-10-12 23:32:27 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 23:32:27 | INFO | train | epoch 028 | loss 5.698 | nll_loss 4.298 | ppl 19.66 | wps 54384.3 | ups 2.59 | wpb 20980.5 | bsz 764.7 | num_updates 5740 | lr 0.000166957 | gnorm 0.735 | clip 0 | train_wall 64 | wall 2156
2020-10-12 23:32:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 23:32:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 23:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21369.4453125Mb; avail=467118.953125Mb
2020-10-12 23:32:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004386
2020-10-12 23:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.037290
2020-10-12 23:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21370.55859375Mb; avail=467117.83984375Mb
2020-10-12 23:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001179
2020-10-12 23:32:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21370.55859375Mb; avail=467117.83984375Mb
2020-10-12 23:32:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.607480
2020-10-12 23:32:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.647028
2020-10-12 23:32:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21397.26953125Mb; avail=467090.6171875Mb
2020-10-12 23:32:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21397.1796875Mb; avail=467090.49609375Mb
2020-10-12 23:32:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025836
2020-10-12 23:32:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21397.34765625Mb; avail=467090.02734375Mb
2020-10-12 23:32:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001249
2020-10-12 23:32:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21397.25Mb; avail=467090.02734375Mb
2020-10-12 23:32:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.595906
2020-10-12 23:32:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.624024
2020-10-12 23:32:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21398.0234375Mb; avail=467090.2734375Mb
2020-10-12 23:32:28 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 23:32:49 | INFO | train_inner | epoch 029:     60 / 205 loss=5.652, nll_loss=4.245, ppl=18.96, wps=46086.4, ups=2.21, wpb=20884, bsz=737, num_updates=5800, lr=0.000166091, gnorm=0.714, clip=0, train_wall=31, wall=2177
2020-10-12 23:33:20 | INFO | train_inner | epoch 029:    160 / 205 loss=5.625, nll_loss=4.214, ppl=18.55, wps=66293.5, ups=3.15, wpb=21064.9, bsz=796.1, num_updates=5900, lr=0.000164677, gnorm=0.712, clip=0, train_wall=31, wall=2209
2020-10-12 23:33:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21055.46484375Mb; avail=467433.84375Mb
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001614
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21055.46484375Mb; avail=467433.84375Mb
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066938
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21055.46484375Mb; avail=467433.84375Mb
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048892
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118367
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21064.65234375Mb; avail=467424.61328125Mb
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21066.91015625Mb; avail=467422.3125Mb
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001256
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21066.91015625Mb; avail=467422.3125Mb
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066376
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21064.56640625Mb; avail=467424.3515625Mb
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.106810
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.175338
2020-10-12 23:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21076.94921875Mb; avail=467412.4609375Mb
2020-10-12 23:33:38 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.949 | nll_loss 4.466 | ppl 22.09 | wps 104086 | wpb 5327.1 | bsz 200 | num_updates 5945 | best_loss 5.949
2020-10-12 23:33:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:33:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 29 @ 5945 updates, score 5.949) (writing took 8.496624421000888 seconds)
2020-10-12 23:33:46 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 23:33:46 | INFO | train | epoch 029 | loss 5.624 | nll_loss 4.212 | ppl 18.54 | wps 54349.1 | ups 2.59 | wpb 20980.5 | bsz 764.7 | num_updates 5945 | lr 0.000164053 | gnorm 0.713 | clip 0 | train_wall 63 | wall 2235
2020-10-12 23:33:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 23:33:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 23:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21360.6484375Mb; avail=467127.1171875Mb
2020-10-12 23:33:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004090
2020-10-12 23:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034138
2020-10-12 23:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21364.453125Mb; avail=467124.1328125Mb
2020-10-12 23:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001149
2020-10-12 23:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21365.05859375Mb; avail=467123.52734375Mb
2020-10-12 23:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.598851
2020-10-12 23:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.635354
2020-10-12 23:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21398.875Mb; avail=467089.1875Mb
2020-10-12 23:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21398.875Mb; avail=467089.1875Mb
2020-10-12 23:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025393
2020-10-12 23:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21398.609375Mb; avail=467089.453125Mb
2020-10-12 23:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001176
2020-10-12 23:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21398.609375Mb; avail=467089.453125Mb
2020-10-12 23:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.591757
2020-10-12 23:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.619150
2020-10-12 23:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21398.85546875Mb; avail=467089.33203125Mb
2020-10-12 23:33:48 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 23:34:06 | INFO | train_inner | epoch 030:     55 / 205 loss=5.574, nll_loss=4.155, ppl=17.82, wps=45560.3, ups=2.18, wpb=20921.8, bsz=782.5, num_updates=6000, lr=0.000163299, gnorm=0.727, clip=0, train_wall=31, wall=2255
2020-10-12 23:34:38 | INFO | train_inner | epoch 030:    155 / 205 loss=5.574, nll_loss=4.155, ppl=17.81, wps=65964.9, ups=3.12, wpb=21125.5, bsz=740.8, num_updates=6100, lr=0.000161955, gnorm=0.722, clip=0, train_wall=31, wall=2287
2020-10-12 23:34:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21080.66015625Mb; avail=467408.32421875Mb
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002834
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21081.15234375Mb; avail=467408.32421875Mb
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067987
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21092.046875Mb; avail=467397.44921875Mb
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048707
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120461
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21111.1796875Mb; avail=467378.31640625Mb
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21119.05078125Mb; avail=467370.4453125Mb
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001172
2020-10-12 23:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21119.65625Mb; avail=467369.83984375Mb
2020-10-12 23:34:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067819
2020-10-12 23:34:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21129.67578125Mb; avail=467359.71875Mb
2020-10-12 23:34:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048542
2020-10-12 23:34:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118375
2020-10-12 23:34:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21139.4140625Mb; avail=467349.91015625Mb
2020-10-12 23:34:57 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.92 | nll_loss 4.43 | ppl 21.55 | wps 104887 | wpb 5327.1 | bsz 200 | num_updates 6150 | best_loss 5.92
2020-10-12 23:34:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:35:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 30 @ 6150 updates, score 5.92) (writing took 8.106559181000193 seconds)
2020-10-12 23:35:05 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 23:35:05 | INFO | train | epoch 030 | loss 5.56 | nll_loss 4.139 | ppl 17.61 | wps 54364.2 | ups 2.59 | wpb 20980.5 | bsz 764.7 | num_updates 6150 | lr 0.000161296 | gnorm 0.719 | clip 0 | train_wall 64 | wall 2314
2020-10-12 23:35:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 23:35:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 23:35:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21363.75390625Mb; avail=467125.5390625Mb
2020-10-12 23:35:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004532
2020-10-12 23:35:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.037579
2020-10-12 23:35:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21379.73828125Mb; avail=467109.5546875Mb
2020-10-12 23:35:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001330
2020-10-12 23:35:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21380.94921875Mb; avail=467108.34375Mb
2020-10-12 23:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.600259
2020-10-12 23:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.640197
2020-10-12 23:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21401.5Mb; avail=467087.453125Mb
2020-10-12 23:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21409.83203125Mb; avail=467079.03515625Mb
2020-10-12 23:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024358
2020-10-12 23:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21415.88671875Mb; avail=467072.98046875Mb
2020-10-12 23:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001145
2020-10-12 23:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21417.09765625Mb; avail=467071.76953125Mb
2020-10-12 23:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.691971
2020-10-12 23:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.718704
2020-10-12 23:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21561.0078125Mb; avail=466927.55078125Mb
2020-10-12 23:35:07 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 23:35:24 | INFO | train_inner | epoch 031:     50 / 205 loss=5.525, nll_loss=4.098, ppl=17.13, wps=45855.9, ups=2.19, wpb=20952.3, bsz=762.5, num_updates=6200, lr=0.000160644, gnorm=0.715, clip=0, train_wall=31, wall=2333
2020-10-12 23:35:56 | INFO | train_inner | epoch 031:    150 / 205 loss=5.504, nll_loss=4.075, ppl=16.86, wps=66519.6, ups=3.15, wpb=21136.1, bsz=757.6, num_updates=6300, lr=0.000159364, gnorm=0.732, clip=0, train_wall=31, wall=2365
2020-10-12 23:36:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21143.0234375Mb; avail=467345.82421875Mb
2020-10-12 23:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002541
2020-10-12 23:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21143.62890625Mb; avail=467345.82421875Mb
2020-10-12 23:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066598
2020-10-12 23:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21156.34375Mb; avail=467333.109375Mb
2020-10-12 23:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047610
2020-10-12 23:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117600
2020-10-12 23:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21163.359375Mb; avail=467326.09375Mb
2020-10-12 23:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21164.17578125Mb; avail=467325.26953125Mb
2020-10-12 23:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001186
2020-10-12 23:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21163.69140625Mb; avail=467325.75390625Mb
2020-10-12 23:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066583
2020-10-12 23:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21170.28125Mb; avail=467319.09375Mb
2020-10-12 23:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047499
2020-10-12 23:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116047
2020-10-12 23:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21172.703125Mb; avail=467316.671875Mb
2020-10-12 23:36:16 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 5.884 | nll_loss 4.392 | ppl 21 | wps 108086 | wpb 5327.1 | bsz 200 | num_updates 6355 | best_loss 5.884
2020-10-12 23:36:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:36:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 31 @ 6355 updates, score 5.884) (writing took 7.900652181000623 seconds)
2020-10-12 23:36:24 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 23:36:24 | INFO | train | epoch 031 | loss 5.501 | nll_loss 4.071 | ppl 16.81 | wps 54478.7 | ups 2.6 | wpb 20980.5 | bsz 764.7 | num_updates 6355 | lr 0.000158673 | gnorm 0.726 | clip 0 | train_wall 64 | wall 2393
2020-10-12 23:36:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 23:36:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 23:36:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21399.69140625Mb; avail=467088.54296875Mb
2020-10-12 23:36:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004436
2020-10-12 23:36:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033735
2020-10-12 23:36:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21399.6953125Mb; avail=467088.171875Mb
2020-10-12 23:36:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001124
2020-10-12 23:36:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21399.6953125Mb; avail=467088.171875Mb
2020-10-12 23:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.601448
2020-10-12 23:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.637330
2020-10-12 23:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21402.8984375Mb; avail=467085.1171875Mb
2020-10-12 23:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21401.4296875Mb; avail=467086.3515625Mb
2020-10-12 23:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022837
2020-10-12 23:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21401.4296875Mb; avail=467086.3515625Mb
2020-10-12 23:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001126
2020-10-12 23:36:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21401.4296875Mb; avail=467086.3515625Mb
2020-10-12 23:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.590861
2020-10-12 23:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.615710
2020-10-12 23:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21401.55859375Mb; avail=467085.484375Mb
2020-10-12 23:36:26 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 23:36:41 | INFO | train_inner | epoch 032:     45 / 205 loss=5.486, nll_loss=4.054, ppl=16.61, wps=45367, ups=2.21, wpb=20529.5, bsz=746.2, num_updates=6400, lr=0.000158114, gnorm=0.725, clip=0, train_wall=31, wall=2410
2020-10-12 23:37:13 | INFO | train_inner | epoch 032:    145 / 205 loss=5.435, nll_loss=3.995, ppl=15.95, wps=66767.8, ups=3.13, wpb=21322.2, bsz=783.3, num_updates=6500, lr=0.000156893, gnorm=0.696, clip=0, train_wall=31, wall=2442
2020-10-12 23:37:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21060.40625Mb; avail=467428.27734375Mb
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001564
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21060.40625Mb; avail=467428.27734375Mb
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066898
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21059.62890625Mb; avail=467429.28515625Mb
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048395
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118380
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21059.62890625Mb; avail=467429.28515625Mb
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21059.62890625Mb; avail=467429.28515625Mb
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001156
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21059.62890625Mb; avail=467429.28515625Mb
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066305
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21059.62890625Mb; avail=467429.28515625Mb
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048853
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117088
2020-10-12 23:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21059.62890625Mb; avail=467429.28515625Mb
2020-10-12 23:37:35 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.838 | nll_loss 4.33 | ppl 20.11 | wps 109870 | wpb 5327.1 | bsz 200 | num_updates 6560 | best_loss 5.838
2020-10-12 23:37:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:37:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 32 @ 6560 updates, score 5.838) (writing took 8.942074593000143 seconds)
2020-10-12 23:37:44 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 23:37:44 | INFO | train | epoch 032 | loss 5.444 | nll_loss 4.006 | ppl 16.07 | wps 53756.4 | ups 2.56 | wpb 20980.5 | bsz 764.7 | num_updates 6560 | lr 0.000156174 | gnorm 0.712 | clip 0 | train_wall 64 | wall 2473
2020-10-12 23:37:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 23:37:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 23:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21290.11328125Mb; avail=467198.26953125Mb
2020-10-12 23:37:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003703
2020-10-12 23:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032837
2020-10-12 23:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21290.83203125Mb; avail=467197.55078125Mb
2020-10-12 23:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001114
2020-10-12 23:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21290.83203125Mb; avail=467197.55078125Mb
2020-10-12 23:37:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.670495
2020-10-12 23:37:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.705306
2020-10-12 23:37:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21339.04296875Mb; avail=467149.46484375Mb
2020-10-12 23:37:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21344.4921875Mb; avail=467144.015625Mb
2020-10-12 23:37:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022670
2020-10-12 23:37:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21346.30859375Mb; avail=467142.19921875Mb
2020-10-12 23:37:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001107
2020-10-12 23:37:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21346.30859375Mb; avail=467142.19921875Mb
2020-10-12 23:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.597094
2020-10-12 23:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.621718
2020-10-12 23:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21394.71875Mb; avail=467093.79296875Mb
2020-10-12 23:37:46 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 23:38:00 | INFO | train_inner | epoch 033:     40 / 205 loss=5.418, nll_loss=3.976, ppl=15.74, wps=44925.4, ups=2.15, wpb=20940, bsz=775.1, num_updates=6600, lr=0.0001557, gnorm=0.713, clip=0, train_wall=31, wall=2488
2020-10-12 23:38:31 | INFO | train_inner | epoch 033:    140 / 205 loss=5.387, nll_loss=3.941, ppl=15.36, wps=65871.8, ups=3.14, wpb=20958.5, bsz=777.6, num_updates=6700, lr=0.000154533, gnorm=0.693, clip=0, train_wall=31, wall=2520
2020-10-12 23:38:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:38:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21528.765625Mb; avail=466960.359375Mb
2020-10-12 23:38:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002715
2020-10-12 23:38:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21528.765625Mb; avail=466960.359375Mb
2020-10-12 23:38:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066655
2020-10-12 23:38:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21529.4765625Mb; avail=466959.55859375Mb
2020-10-12 23:38:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048711
2020-10-12 23:38:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118888
2020-10-12 23:38:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21529.5Mb; avail=466959.53515625Mb
2020-10-12 23:38:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21532.63671875Mb; avail=466956.5Mb
2020-10-12 23:38:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001163
2020-10-12 23:38:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21532.14453125Mb; avail=466956.9921875Mb
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066472
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21563.87890625Mb; avail=466925.2578125Mb
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047991
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116375
2020-10-12 23:38:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21564.58203125Mb; avail=466924.82421875Mb
2020-10-12 23:38:55 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 5.812 | nll_loss 4.308 | ppl 19.81 | wps 113554 | wpb 5327.1 | bsz 200 | num_updates 6765 | best_loss 5.812
2020-10-12 23:38:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:39:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 33 @ 6765 updates, score 5.812) (writing took 7.4774350650004635 seconds)
2020-10-12 23:39:03 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 23:39:03 | INFO | train | epoch 033 | loss 5.391 | nll_loss 3.945 | ppl 15.4 | wps 54942.3 | ups 2.62 | wpb 20980.5 | bsz 764.7 | num_updates 6765 | lr 0.000153789 | gnorm 0.704 | clip 0 | train_wall 64 | wall 2551
2020-10-12 23:39:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 23:39:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 23:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21658.4140625Mb; avail=466829.7109375Mb
2020-10-12 23:39:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003610
2020-10-12 23:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032382
2020-10-12 23:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21658.4140625Mb; avail=466829.7109375Mb
2020-10-12 23:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001132
2020-10-12 23:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21658.4140625Mb; avail=466829.7109375Mb
2020-10-12 23:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.620281
2020-10-12 23:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.654964
2020-10-12 23:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21658.01171875Mb; avail=466830.21484375Mb
2020-10-12 23:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21658.01171875Mb; avail=466830.21484375Mb
2020-10-12 23:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.024195
2020-10-12 23:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21658.01171875Mb; avail=466830.21484375Mb
2020-10-12 23:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001218
2020-10-12 23:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21658.01171875Mb; avail=466830.21484375Mb
2020-10-12 23:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.597145
2020-10-12 23:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.623383
2020-10-12 23:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21657.875Mb; avail=466830.578125Mb
2020-10-12 23:39:04 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 23:39:16 | INFO | train_inner | epoch 034:     35 / 205 loss=5.372, nll_loss=3.923, ppl=15.17, wps=46636.8, ups=2.22, wpb=20970.1, bsz=771.7, num_updates=6800, lr=0.000153393, gnorm=0.725, clip=0, train_wall=31, wall=2565
2020-10-12 23:39:48 | INFO | train_inner | epoch 034:    135 / 205 loss=5.348, nll_loss=3.896, ppl=14.88, wps=66597.3, ups=3.13, wpb=21244.4, bsz=760.5, num_updates=6900, lr=0.000152277, gnorm=0.7, clip=0, train_wall=31, wall=2597
2020-10-12 23:40:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21232.78515625Mb; avail=467255.703125Mb
2020-10-12 23:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001628
2020-10-12 23:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21232.78515625Mb; avail=467255.703125Mb
2020-10-12 23:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065541
2020-10-12 23:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21232.6640625Mb; avail=467255.82421875Mb
2020-10-12 23:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048632
2020-10-12 23:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116650
2020-10-12 23:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21232.6640625Mb; avail=467255.82421875Mb
2020-10-12 23:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21232.6640625Mb; avail=467255.82421875Mb
2020-10-12 23:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001206
2020-10-12 23:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21232.6640625Mb; avail=467255.82421875Mb
2020-10-12 23:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065704
2020-10-12 23:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21232.6640625Mb; avail=467255.82421875Mb
2020-10-12 23:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048288
2020-10-12 23:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116021
2020-10-12 23:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21232.7578125Mb; avail=467255.4609375Mb
2020-10-12 23:40:14 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 5.789 | nll_loss 4.276 | ppl 19.37 | wps 106046 | wpb 5327.1 | bsz 200 | num_updates 6970 | best_loss 5.789
2020-10-12 23:40:14 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:40:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 34 @ 6970 updates, score 5.789) (writing took 5.687524696997571 seconds)
2020-10-12 23:40:19 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 23:40:19 | INFO | train | epoch 034 | loss 5.341 | nll_loss 3.888 | ppl 14.8 | wps 56004.4 | ups 2.67 | wpb 20980.5 | bsz 764.7 | num_updates 6970 | lr 0.000151511 | gnorm 0.709 | clip 0 | train_wall 64 | wall 2628
2020-10-12 23:40:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 23:40:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 23:40:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21864.60546875Mb; avail=466623.765625Mb
2020-10-12 23:40:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004423
2020-10-12 23:40:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.041327
2020-10-12 23:40:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21865.3515625Mb; avail=466623.40234375Mb
2020-10-12 23:40:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001247
2020-10-12 23:40:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21865.3515625Mb; avail=466623.40234375Mb
2020-10-12 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.606678
2020-10-12 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.650490
2020-10-12 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21864.4453125Mb; avail=466624.06640625Mb
2020-10-12 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21864.4453125Mb; avail=466624.06640625Mb
2020-10-12 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023096
2020-10-12 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21864.4453125Mb; avail=466624.06640625Mb
2020-10-12 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001137
2020-10-12 23:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21864.4453125Mb; avail=466624.06640625Mb
2020-10-12 23:40:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.598407
2020-10-12 23:40:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.623767
2020-10-12 23:40:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21651.8046875Mb; avail=466836.7734375Mb
2020-10-12 23:40:21 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 23:40:31 | INFO | train_inner | epoch 035:     30 / 205 loss=5.313, nll_loss=3.856, ppl=14.48, wps=47705, ups=2.31, wpb=20626.2, bsz=767.7, num_updates=7000, lr=0.000151186, gnorm=0.703, clip=0, train_wall=31, wall=2640
2020-10-12 23:41:04 | INFO | train_inner | epoch 035:    130 / 205 loss=5.295, nll_loss=3.835, ppl=14.27, wps=65190.6, ups=3.11, wpb=20933.1, bsz=745.1, num_updates=7100, lr=0.000150117, gnorm=0.707, clip=0, train_wall=31, wall=2672
2020-10-12 23:41:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21235.90625Mb; avail=467252.41015625Mb
2020-10-12 23:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001748
2020-10-12 23:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21235.90625Mb; avail=467252.41015625Mb
2020-10-12 23:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067557
2020-10-12 23:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21235.90625Mb; avail=467252.41015625Mb
2020-10-12 23:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049551
2020-10-12 23:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119693
2020-10-12 23:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21235.8984375Mb; avail=467252.41796875Mb
2020-10-12 23:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21235.8984375Mb; avail=467252.41796875Mb
2020-10-12 23:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001236
2020-10-12 23:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21235.8984375Mb; avail=467252.41796875Mb
2020-10-12 23:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068894
2020-10-12 23:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21235.5234375Mb; avail=467252.41796875Mb
2020-10-12 23:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049463
2020-10-12 23:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120614
2020-10-12 23:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21235.68359375Mb; avail=467252.66015625Mb
2020-10-12 23:41:30 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 5.772 | nll_loss 4.261 | ppl 19.18 | wps 111568 | wpb 5327.1 | bsz 200 | num_updates 7175 | best_loss 5.772
2020-10-12 23:41:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:41:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 35 @ 7175 updates, score 5.772) (writing took 4.485869801999797 seconds)
2020-10-12 23:41:35 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 23:41:35 | INFO | train | epoch 035 | loss 5.295 | nll_loss 3.835 | ppl 14.28 | wps 56930.1 | ups 2.71 | wpb 20980.5 | bsz 764.7 | num_updates 7175 | lr 0.000149331 | gnorm 0.701 | clip 0 | train_wall 64 | wall 2704
2020-10-12 23:41:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 23:41:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 23:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21313.49609375Mb; avail=467174.5703125Mb
2020-10-12 23:41:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.009909
2020-10-12 23:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.054298
2020-10-12 23:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21378.265625Mb; avail=467110.51171875Mb
2020-10-12 23:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001312
2020-10-12 23:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21380.08203125Mb; avail=467108.6953125Mb
2020-10-12 23:41:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.617428
2020-10-12 23:41:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.674058
2020-10-12 23:41:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21782.6640625Mb; avail=466706.234375Mb
2020-10-12 23:41:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21782.2578125Mb; avail=466706.34765625Mb
2020-10-12 23:41:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023784
2020-10-12 23:41:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21782.26171875Mb; avail=466706.62890625Mb
2020-10-12 23:41:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001132
2020-10-12 23:41:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21781.875Mb; avail=466706.78125Mb
2020-10-12 23:41:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.681709
2020-10-12 23:41:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.707832
2020-10-12 23:41:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21827.16796875Mb; avail=466661.52734375Mb
2020-10-12 23:41:36 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 23:41:46 | INFO | train_inner | epoch 036:     25 / 205 loss=5.298, nll_loss=3.838, ppl=14.3, wps=50221.4, ups=2.38, wpb=21074, bsz=761.4, num_updates=7200, lr=0.000149071, gnorm=0.698, clip=0, train_wall=31, wall=2714
2020-10-12 23:42:18 | INFO | train_inner | epoch 036:    125 / 205 loss=5.249, nll_loss=3.782, ppl=13.76, wps=65841.4, ups=3.12, wpb=21078.7, bsz=765.5, num_updates=7300, lr=0.000148047, gnorm=0.69, clip=0, train_wall=31, wall=2746
2020-10-12 23:42:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21245.01171875Mb; avail=467243.515625Mb
2020-10-12 23:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001784
2020-10-12 23:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21245.01171875Mb; avail=467243.515625Mb
2020-10-12 23:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066735
2020-10-12 23:42:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21245.0078125Mb; avail=467243.40234375Mb
2020-10-12 23:42:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048668
2020-10-12 23:42:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118298
2020-10-12 23:42:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21245.0078125Mb; avail=467243.40234375Mb
2020-10-12 23:42:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21245.0078125Mb; avail=467243.40234375Mb
2020-10-12 23:42:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001193
2020-10-12 23:42:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21245.0078125Mb; avail=467243.40234375Mb
2020-10-12 23:42:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066517
2020-10-12 23:42:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21245.0078125Mb; avail=467243.40234375Mb
2020-10-12 23:42:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048304
2020-10-12 23:42:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116786
2020-10-12 23:42:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21245.0078125Mb; avail=467243.40234375Mb
2020-10-12 23:42:46 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 5.743 | nll_loss 4.223 | ppl 18.67 | wps 107287 | wpb 5327.1 | bsz 200 | num_updates 7380 | best_loss 5.743
2020-10-12 23:42:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:42:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 36 @ 7380 updates, score 5.743) (writing took 4.533667092000542 seconds)
2020-10-12 23:42:51 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 23:42:51 | INFO | train | epoch 036 | loss 5.251 | nll_loss 3.785 | ppl 13.79 | wps 56668 | ups 2.7 | wpb 20980.5 | bsz 764.7 | num_updates 7380 | lr 0.000147242 | gnorm 0.698 | clip 0 | train_wall 64 | wall 2780
2020-10-12 23:42:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 23:42:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 23:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21398.8359375Mb; avail=467089.1640625Mb
2020-10-12 23:42:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003810
2020-10-12 23:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032704
2020-10-12 23:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21400.3671875Mb; avail=467087.91015625Mb
2020-10-12 23:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001156
2020-10-12 23:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21400.3671875Mb; avail=467087.91015625Mb
2020-10-12 23:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.606429
2020-10-12 23:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.641441
2020-10-12 23:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21400.515625Mb; avail=467087.43359375Mb
2020-10-12 23:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21400.3125Mb; avail=467087.43359375Mb
2020-10-12 23:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023985
2020-10-12 23:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21400.21484375Mb; avail=467087.8046875Mb
2020-10-12 23:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001231
2020-10-12 23:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21400.21484375Mb; avail=467087.8046875Mb
2020-10-12 23:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.613785
2020-10-12 23:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.639960
2020-10-12 23:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21727.8828125Mb; avail=466760.84765625Mb
2020-10-12 23:42:52 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 23:43:00 | INFO | train_inner | epoch 037:     20 / 205 loss=5.24, nll_loss=3.773, ppl=13.67, wps=49507.1, ups=2.37, wpb=20905.3, bsz=777.7, num_updates=7400, lr=0.000147043, gnorm=0.696, clip=0, train_wall=31, wall=2789
2020-10-12 23:43:32 | INFO | train_inner | epoch 037:    120 / 205 loss=5.196, nll_loss=3.723, ppl=13.2, wps=65991.3, ups=3.13, wpb=21086.1, bsz=779.9, num_updates=7500, lr=0.000146059, gnorm=0.683, clip=0, train_wall=31, wall=2821
2020-10-12 23:43:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21237.03125Mb; avail=467252.015625Mb
2020-10-12 23:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001749
2020-10-12 23:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21237.03125Mb; avail=467252.015625Mb
2020-10-12 23:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066421
2020-10-12 23:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21237.109375Mb; avail=467252.14453125Mb
2020-10-12 23:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048125
2020-10-12 23:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117124
2020-10-12 23:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21237.109375Mb; avail=467252.14453125Mb
2020-10-12 23:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21237.1015625Mb; avail=467252.15234375Mb
2020-10-12 23:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001160
2020-10-12 23:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21237.1015625Mb; avail=467252.15234375Mb
2020-10-12 23:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065844
2020-10-12 23:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21236.453125Mb; avail=467252.515625Mb
2020-10-12 23:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047309
2020-10-12 23:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115088
2020-10-12 23:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21236.453125Mb; avail=467252.515625Mb
2020-10-12 23:44:02 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 5.709 | nll_loss 4.187 | ppl 18.21 | wps 107995 | wpb 5327.1 | bsz 200 | num_updates 7585 | best_loss 5.709
2020-10-12 23:44:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:44:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 37 @ 7585 updates, score 5.709) (writing took 4.503756747002626 seconds)
2020-10-12 23:44:06 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 23:44:06 | INFO | train | epoch 037 | loss 5.21 | nll_loss 3.738 | ppl 13.34 | wps 57049.5 | ups 2.72 | wpb 20980.5 | bsz 764.7 | num_updates 7585 | lr 0.000145239 | gnorm 0.684 | clip 0 | train_wall 64 | wall 2855
2020-10-12 23:44:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 23:44:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21239.98046875Mb; avail=467248.8125Mb
2020-10-12 23:44:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004278
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.038431
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21241.078125Mb; avail=467247.26171875Mb
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001242
2020-10-12 23:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21241.078125Mb; avail=467247.26171875Mb
2020-10-12 23:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.800044
2020-10-12 23:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.840808
2020-10-12 23:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21351.37890625Mb; avail=467137.46875Mb
2020-10-12 23:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21353.21484375Mb; avail=467135.61328125Mb
2020-10-12 23:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.023039
2020-10-12 23:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21353.71484375Mb; avail=467135.11328125Mb
2020-10-12 23:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001199
2020-10-12 23:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21353.71484375Mb; avail=467135.11328125Mb
2020-10-12 23:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.603036
2020-10-12 23:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.628491
2020-10-12 23:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21393.94140625Mb; avail=467094.53515625Mb
2020-10-12 23:44:08 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 23:44:14 | INFO | train_inner | epoch 038:     15 / 205 loss=5.216, nll_loss=3.745, ppl=13.41, wps=49425.5, ups=2.38, wpb=20779.7, bsz=753.1, num_updates=7600, lr=0.000145095, gnorm=0.69, clip=0, train_wall=31, wall=2863
2020-10-12 23:44:46 | INFO | train_inner | epoch 038:    115 / 205 loss=5.165, nll_loss=3.687, ppl=12.88, wps=65448.9, ups=3.1, wpb=21088.2, bsz=765.4, num_updates=7700, lr=0.00014415, gnorm=0.705, clip=0, train_wall=31, wall=2895
2020-10-12 23:45:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21233.73828125Mb; avail=467255.0390625Mb
2020-10-12 23:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002543
2020-10-12 23:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21233.73828125Mb; avail=467255.0390625Mb
2020-10-12 23:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066589
2020-10-12 23:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21233.73828125Mb; avail=467255.0390625Mb
2020-10-12 23:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048520
2020-10-12 23:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118459
2020-10-12 23:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21233.73828125Mb; avail=467255.0390625Mb
2020-10-12 23:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21233.76953125Mb; avail=467254.796875Mb
2020-10-12 23:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001173
2020-10-12 23:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21233.8125Mb; avail=467254.67578125Mb
2020-10-12 23:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.097100
2020-10-12 23:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21233.80859375Mb; avail=467255.16015625Mb
2020-10-12 23:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.074614
2020-10-12 23:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.174022
2020-10-12 23:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21240.28515625Mb; avail=467248.484375Mb
2020-10-12 23:45:18 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 5.705 | nll_loss 4.179 | ppl 18.11 | wps 109688 | wpb 5327.1 | bsz 200 | num_updates 7790 | best_loss 5.705
2020-10-12 23:45:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:45:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 38 @ 7790 updates, score 5.705) (writing took 4.476467819000391 seconds)
2020-10-12 23:45:22 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 23:45:22 | INFO | train | epoch 038 | loss 5.17 | nll_loss 3.692 | ppl 12.93 | wps 56702 | ups 2.7 | wpb 20980.5 | bsz 764.7 | num_updates 7790 | lr 0.000143315 | gnorm 0.702 | clip 0 | train_wall 64 | wall 2931
2020-10-12 23:45:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 23:45:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 23:45:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21401.515625Mb; avail=467086.78125Mb
2020-10-12 23:45:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003420
2020-10-12 23:45:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032199
2020-10-12 23:45:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21401.0234375Mb; avail=467087.2734375Mb
2020-10-12 23:45:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001868
2020-10-12 23:45:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21401.0234375Mb; avail=467087.2734375Mb
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.656776
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.692132
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21224.265625Mb; avail=467264.13671875Mb
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21224.84375Mb; avail=467263.55859375Mb
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022664
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21224.84375Mb; avail=467263.55859375Mb
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001131
2020-10-12 23:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21224.84375Mb; avail=467263.55859375Mb
2020-10-12 23:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.697661
2020-10-12 23:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.722321
2020-10-12 23:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21321.7734375Mb; avail=467166.74609375Mb
2020-10-12 23:45:24 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 23:45:28 | INFO | train_inner | epoch 039:     10 / 205 loss=5.178, nll_loss=3.701, ppl=13.01, wps=49694.8, ups=2.38, wpb=20867.1, bsz=746.4, num_updates=7800, lr=0.000143223, gnorm=0.707, clip=0, train_wall=31, wall=2937
2020-10-12 23:46:00 | INFO | train_inner | epoch 039:    110 / 205 loss=5.126, nll_loss=3.641, ppl=12.48, wps=66046, ups=3.14, wpb=21047.5, bsz=765.4, num_updates=7900, lr=0.000142314, gnorm=0.686, clip=0, train_wall=31, wall=2969
2020-10-12 23:46:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21231.6015625Mb; avail=467257.05078125Mb
2020-10-12 23:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001698
2020-10-12 23:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21231.6015625Mb; avail=467257.05078125Mb
2020-10-12 23:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066166
2020-10-12 23:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21231.4921875Mb; avail=467257.05078125Mb
2020-10-12 23:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047445
2020-10-12 23:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116854
2020-10-12 23:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21231.44921875Mb; avail=467257.171875Mb
2020-10-12 23:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21231.44921875Mb; avail=467257.171875Mb
2020-10-12 23:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001176
2020-10-12 23:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21231.44921875Mb; avail=467257.171875Mb
2020-10-12 23:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066373
2020-10-12 23:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21231.44921875Mb; avail=467257.171875Mb
2020-10-12 23:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047313
2020-10-12 23:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115630
2020-10-12 23:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21231.44921875Mb; avail=467257.171875Mb
2020-10-12 23:46:33 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 5.679 | nll_loss 4.149 | ppl 17.74 | wps 108673 | wpb 5327.1 | bsz 200 | num_updates 7995 | best_loss 5.679
2020-10-12 23:46:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:46:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 39 @ 7995 updates, score 5.679) (writing took 4.505682154998794 seconds)
2020-10-12 23:46:38 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 23:46:38 | INFO | train | epoch 039 | loss 5.134 | nll_loss 3.651 | ppl 12.56 | wps 56789.8 | ups 2.71 | wpb 20980.5 | bsz 764.7 | num_updates 7995 | lr 0.000141466 | gnorm 0.699 | clip 0 | train_wall 64 | wall 3007
2020-10-12 23:46:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 23:46:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 23:46:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21398.41796875Mb; avail=467089.55078125Mb
2020-10-12 23:46:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004178
2020-10-12 23:46:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035409
2020-10-12 23:46:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21398.5390625Mb; avail=467089.44921875Mb
2020-10-12 23:46:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001171
2020-10-12 23:46:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21398.5390625Mb; avail=467089.44921875Mb
2020-10-12 23:46:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.599124
2020-10-12 23:46:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.636597
2020-10-12 23:46:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21399.0078125Mb; avail=467089.0859375Mb
2020-10-12 23:46:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21398.6875Mb; avail=467089.44921875Mb
2020-10-12 23:46:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.022894
2020-10-12 23:46:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21398.6875Mb; avail=467089.44921875Mb
2020-10-12 23:46:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001135
2020-10-12 23:46:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21398.6875Mb; avail=467089.44921875Mb
2020-10-12 23:46:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.593033
2020-10-12 23:46:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.617989
2020-10-12 23:46:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21399.0234375Mb; avail=467089.33984375Mb
2020-10-12 23:46:39 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 23:46:42 | INFO | train_inner | epoch 040:      5 / 205 loss=5.14, nll_loss=3.657, ppl=12.62, wps=49689, ups=2.37, wpb=20943.9, bsz=778.2, num_updates=8000, lr=0.000141421, gnorm=0.708, clip=0, train_wall=31, wall=3011
2020-10-12 23:47:14 | INFO | train_inner | epoch 040:    105 / 205 loss=5.094, nll_loss=3.605, ppl=12.17, wps=65786.2, ups=3.13, wpb=20988.6, bsz=759, num_updates=8100, lr=0.000140546, gnorm=0.702, clip=0, train_wall=31, wall=3043
2020-10-12 23:47:46 | INFO | train_inner | epoch 040:    205 / 205 loss=5.101, nll_loss=3.613, ppl=12.23, wps=66105.1, ups=3.15, wpb=20969.6, bsz=767.6, num_updates=8200, lr=0.000139686, gnorm=0.678, clip=0, train_wall=31, wall=3074
2020-10-12 23:47:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 23:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21232.3984375Mb; avail=467256.30859375Mb
2020-10-12 23:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001249
2020-10-12 23:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21232.3984375Mb; avail=467256.30859375Mb
2020-10-12 23:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067286
2020-10-12 23:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21232.29296875Mb; avail=467256.4140625Mb
2020-10-12 23:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047002
2020-10-12 23:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116324
2020-10-12 23:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21232.29296875Mb; avail=467256.4140625Mb
2020-10-12 23:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21232.29296875Mb; avail=467256.4140625Mb
2020-10-12 23:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001107
2020-10-12 23:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21232.29296875Mb; avail=467256.4140625Mb
2020-10-12 23:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065974
2020-10-12 23:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21232.33984375Mb; avail=467256.4140625Mb
2020-10-12 23:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047457
2020-10-12 23:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115319
2020-10-12 23:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21232.27734375Mb; avail=467256.171875Mb
2020-10-12 23:47:49 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 5.661 | nll_loss 4.125 | ppl 17.44 | wps 111004 | wpb 5327.1 | bsz 200 | num_updates 8200 | best_loss 5.661
2020-10-12 23:47:49 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 23:47:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefasLG_sepspm8000/O2M/checkpoint_best.pt (epoch 40 @ 8200 updates, score 5.661) (writing took 4.511452089001978 seconds)
2020-10-12 23:47:53 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 23:47:53 | INFO | train | epoch 040 | loss 5.096 | nll_loss 3.607 | ppl 12.19 | wps 57099.8 | ups 2.72 | wpb 20980.5 | bsz 764.7 | num_updates 8200 | lr 0.000139686 | gnorm 0.689 | clip 0 | train_wall 64 | wall 3082
2020-10-12 23:47:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 23:47:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 23:47:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16365.0546875Mb; avail=472147.9140625Mb
2020-10-12 23:47:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004225
2020-10-12 23:47:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033696
2020-10-12 23:47:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16365.66015625Mb; avail=472147.30859375Mb
2020-10-12 23:47:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001129
2020-10-12 23:47:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16365.66015625Mb; avail=472147.30859375Mb
2020-10-12 23:47:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.605419
2020-10-12 23:47:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.641320
2020-10-12 23:47:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16365.82421875Mb; avail=472147.203125Mb
2020-10-12 23:47:54 | INFO | fairseq_cli.train | done training in 3081.9 seconds
