2020-10-12 02:27:46 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_bellit_sepspm8000/O2M/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok='tgt', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='eng-bel,eng-lit', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_bellit_sepspm8000/O2M/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 02:27:46 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 02:27:46 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['bel', 'eng', 'lit']
2020-10-12 02:27:46 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bel] dictionary: 22083 types
2020-10-12 02:27:46 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 22083 types
2020-10-12 02:27:46 | INFO | fairseq.data.multilingual.multilingual_data_manager | [lit] dictionary: 22083 types
2020-10-12 02:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 02:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:27:46 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 02:27:46 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:eng-bel': 1, 'main:eng-lit': 1}
2020-10-12 02:27:46 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-bel src_langtok: 22080; tgt_langtok: None
2020-10-12 02:27:46 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_bellit_sepspm8000/O2M/valid.eng-bel.eng
2020-10-12 02:27:46 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_bellit_sepspm8000/O2M/valid.eng-bel.bel
2020-10-12 02:27:46 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_bellit_sepspm8000/O2M/ valid eng-bel 248 examples
2020-10-12 02:27:46 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-lit src_langtok: 22082; tgt_langtok: None
2020-10-12 02:27:46 | INFO | fairseq.data.data_utils | loaded 1791 examples from: fairseq/data-bin/ted_bellit_sepspm8000/O2M/valid.eng-lit.eng
2020-10-12 02:27:46 | INFO | fairseq.data.data_utils | loaded 1791 examples from: fairseq/data-bin/ted_bellit_sepspm8000/O2M/valid.eng-lit.lit
2020-10-12 02:27:46 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_bellit_sepspm8000/O2M/ valid eng-lit 1791 examples
2020-10-12 02:27:47 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22083, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22083, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=22083, bias=False)
  )
)
2020-10-12 02:27:47 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 02:27:47 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 02:27:47 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 02:27:47 | INFO | fairseq_cli.train | num. model params: 42849792 (num. trained: 42849792)
2020-10-12 02:27:48 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 02:27:48 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 02:27:48 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 02:27:48 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.732 GB ; name = GeForce RTX 2080 Ti                     
2020-10-12 02:27:48 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 02:27:48 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 02:27:48 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 02:27:48 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_last.pt
2020-10-12 02:27:48 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 02:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 02:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:27:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 02:27:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:eng-bel': 1, 'main:eng-lit': 1}
2020-10-12 02:27:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-bel src_langtok: 22080; tgt_langtok: None
2020-10-12 02:27:48 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_bellit_sepspm8000/O2M/train.eng-bel.eng
2020-10-12 02:27:48 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_bellit_sepspm8000/O2M/train.eng-bel.bel
2020-10-12 02:27:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_bellit_sepspm8000/O2M/ train eng-bel 4509 examples
2020-10-12 02:27:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-lit src_langtok: 22082; tgt_langtok: None
2020-10-12 02:27:48 | INFO | fairseq.data.data_utils | loaded 39992 examples from: fairseq/data-bin/ted_bellit_sepspm8000/O2M/train.eng-lit.eng
2020-10-12 02:27:48 | INFO | fairseq.data.data_utils | loaded 39992 examples from: fairseq/data-bin/ted_bellit_sepspm8000/O2M/train.eng-lit.lit
2020-10-12 02:27:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_bellit_sepspm8000/O2M/ train eng-lit 39992 examples
2020-10-12 02:27:48 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:eng-bel', 4509), ('main:eng-lit', 39992)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 02:27:48 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 02:27:48 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 44501
2020-10-12 02:27:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 44501; virtual dataset size 44501
2020-10-12 02:27:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 02:27:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:eng-bel': 4509, 'main:eng-lit': 39992}; raw total size: 44501
2020-10-12 02:27:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:eng-bel': 4509, 'main:eng-lit': 39992}; resampled total size: 44501
2020-10-12 02:27:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 02:27:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.003128
2020-10-12 02:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:27:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000416
2020-10-12 02:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005164
2020-10-12 02:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000188
2020-10-12 02:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:27:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104159
2020-10-12 02:27:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109841
2020-10-12 02:27:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:27:49 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 02:27:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:27:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004236
2020-10-12 02:27:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:27:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 02:27:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:27:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101830
2020-10-12 02:27:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106580
2020-10-12 02:27:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:27:49 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 02:28:21 | INFO | train_inner | epoch 001:    100 / 169 loss=14.157, nll_loss=14.041, ppl=16850.9, wps=20205.8, ups=3.12, wpb=6462.2, bsz=271, num_updates=100, lr=5.0975e-06, gnorm=4.195, clip=0, train_wall=32, wall=33
2020-10-12 02:28:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000611
2020-10-12 02:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017574
2020-10-12 02:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013494
2020-10-12 02:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032014
2020-10-12 02:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000434
2020-10-12 02:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017307
2020-10-12 02:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013025
2020-10-12 02:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031090
2020-10-12 02:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
/home/han/Documents/11737-hw/assign2/fairseq/fairseq/utils.py:340: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2020-10-12 02:28:45 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.683 | nll_loss 12.371 | ppl 5296.95 | wps 43751.2 | wpb 2341.5 | bsz 92.7 | num_updates 169
2020-10-12 02:28:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:28:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 1 @ 169 updates, score 12.683) (writing took 0.9567686439986574 seconds)
2020-10-12 02:28:46 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 02:28:46 | INFO | train | epoch 001 | loss 13.743 | nll_loss 13.577 | ppl 12221.2 | wps 18955.9 | ups 2.98 | wpb 6357.8 | bsz 263.3 | num_updates 169 | lr 8.54578e-06 | gnorm 3.15 | clip 0 | train_wall 54 | wall 57
2020-10-12 02:28:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 02:28:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 02:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:28:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000568
2020-10-12 02:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005497
2020-10-12 02:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-12 02:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104220
2020-10-12 02:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110204
2020-10-12 02:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004265
2020-10-12 02:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-12 02:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106356
2020-10-12 02:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111107
2020-10-12 02:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:28:46 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 02:28:56 | INFO | train_inner | epoch 002:     31 / 169 loss=13.006, nll_loss=12.751, ppl=6895.18, wps=17791.7, ups=2.84, wpb=6267.6, bsz=268.1, num_updates=200, lr=1.0095e-05, gnorm=1.581, clip=0, train_wall=32, wall=68
2020-10-12 02:29:29 | INFO | train_inner | epoch 002:    131 / 169 loss=12.461, nll_loss=12.148, ppl=4538.74, wps=19193.4, ups=3.03, wpb=6336.8, bsz=250.3, num_updates=300, lr=1.50925e-05, gnorm=1.142, clip=0, train_wall=33, wall=101
2020-10-12 02:29:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000592
2020-10-12 02:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017315
2020-10-12 02:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013565
2020-10-12 02:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031809
2020-10-12 02:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000446
2020-10-12 02:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017070
2020-10-12 02:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013369
2020-10-12 02:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031204
2020-10-12 02:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:29:43 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.598 | nll_loss 11.16 | ppl 2288.9 | wps 43608.2 | wpb 2341.5 | bsz 92.7 | num_updates 338 | best_loss 11.598
2020-10-12 02:29:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:29:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 2 @ 338 updates, score 11.598) (writing took 1.5107622729992727 seconds)
2020-10-12 02:29:45 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 02:29:45 | INFO | train | epoch 002 | loss 12.403 | nll_loss 12.082 | ppl 4336.25 | wps 18210.1 | ups 2.86 | wpb 6357.8 | bsz 263.3 | num_updates 338 | lr 1.69915e-05 | gnorm 1.333 | clip 0 | train_wall 55 | wall 116
2020-10-12 02:29:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 02:29:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 02:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:29:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000630
2020-10-12 02:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005921
2020-10-12 02:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 02:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.107373
2020-10-12 02:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113800
2020-10-12 02:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004221
2020-10-12 02:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 02:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104726
2020-10-12 02:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109443
2020-10-12 02:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:29:45 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 02:30:06 | INFO | train_inner | epoch 003:     62 / 169 loss=11.749, nll_loss=11.345, ppl=2601.89, wps=17534, ups=2.76, wpb=6361.7, bsz=261.4, num_updates=400, lr=2.009e-05, gnorm=1.572, clip=0, train_wall=33, wall=137
2020-10-12 02:30:39 | INFO | train_inner | epoch 003:    162 / 169 loss=11.214, nll_loss=10.718, ppl=1684.06, wps=19157.3, ups=3, wpb=6383.1, bsz=266.6, num_updates=500, lr=2.50875e-05, gnorm=1.382, clip=0, train_wall=33, wall=170
2020-10-12 02:30:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.006786
2020-10-12 02:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017459
2020-10-12 02:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012990
2020-10-12 02:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037572
2020-10-12 02:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000430
2020-10-12 02:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017105
2020-10-12 02:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013194
2020-10-12 02:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031038
2020-10-12 02:30:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:30:43 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.748 | nll_loss 10.132 | ppl 1121.81 | wps 43316.5 | wpb 2341.5 | bsz 92.7 | num_updates 507 | best_loss 10.748
2020-10-12 02:30:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:30:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 3 @ 507 updates, score 10.748) (writing took 1.5061154480063124 seconds)
2020-10-12 02:30:44 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 02:30:44 | INFO | train | epoch 003 | loss 11.354 | nll_loss 10.883 | ppl 1888.14 | wps 18124.8 | ups 2.85 | wpb 6357.8 | bsz 263.3 | num_updates 507 | lr 2.54373e-05 | gnorm 1.397 | clip 0 | train_wall 55 | wall 176
2020-10-12 02:30:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 02:30:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 02:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:30:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000442
2020-10-12 02:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005735
2020-10-12 02:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 02:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109241
2020-10-12 02:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115609
2020-10-12 02:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004212
2020-10-12 02:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 02:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102933
2020-10-12 02:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107631
2020-10-12 02:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:30:44 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 02:31:15 | INFO | train_inner | epoch 004:     93 / 169 loss=10.886, nll_loss=10.317, ppl=1275.85, wps=17255.1, ups=2.77, wpb=6238.8, bsz=258.1, num_updates=600, lr=3.0085e-05, gnorm=1.379, clip=0, train_wall=33, wall=207
2020-10-12 02:31:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:31:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:31:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000580
2020-10-12 02:31:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:31:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017240
2020-10-12 02:31:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:31:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013235
2020-10-12 02:31:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031395
2020-10-12 02:31:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:31:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:31:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000425
2020-10-12 02:31:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:31:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016979
2020-10-12 02:31:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:31:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013368
2020-10-12 02:31:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031096
2020-10-12 02:31:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:31:42 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.406 | nll_loss 9.712 | ppl 838.67 | wps 42681.7 | wpb 2341.5 | bsz 92.7 | num_updates 676 | best_loss 10.406
2020-10-12 02:31:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:31:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 4 @ 676 updates, score 10.406) (writing took 1.5011470170138637 seconds)
2020-10-12 02:31:43 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 02:31:43 | INFO | train | epoch 004 | loss 10.815 | nll_loss 10.227 | ppl 1198.81 | wps 18086.6 | ups 2.84 | wpb 6357.8 | bsz 263.3 | num_updates 676 | lr 3.38831e-05 | gnorm 1.298 | clip 0 | train_wall 56 | wall 235
2020-10-12 02:31:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 02:31:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 02:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:31:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000452
2020-10-12 02:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005770
2020-10-12 02:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000254
2020-10-12 02:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.108316
2020-10-12 02:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114668
2020-10-12 02:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004242
2020-10-12 02:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 02:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102409
2020-10-12 02:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107159
2020-10-12 02:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:31:44 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 02:31:52 | INFO | train_inner | epoch 005:     24 / 169 loss=10.724, nll_loss=10.113, ppl=1107.62, wps=17726.4, ups=2.72, wpb=6517.5, bsz=261, num_updates=700, lr=3.50825e-05, gnorm=1.163, clip=0, train_wall=33, wall=243
2020-10-12 02:32:25 | INFO | train_inner | epoch 005:    124 / 169 loss=10.531, nll_loss=9.88, ppl=942.37, wps=19023.3, ups=3, wpb=6350.1, bsz=263.3, num_updates=800, lr=4.008e-05, gnorm=1.227, clip=0, train_wall=33, wall=277
2020-10-12 02:32:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000529
2020-10-12 02:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017436
2020-10-12 02:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013194
2020-10-12 02:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031487
2020-10-12 02:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000412
2020-10-12 02:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017323
2020-10-12 02:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013255
2020-10-12 02:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031308
2020-10-12 02:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:32:41 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.191 | nll_loss 9.479 | ppl 713.38 | wps 42984.9 | wpb 2341.5 | bsz 92.7 | num_updates 845 | best_loss 10.191
2020-10-12 02:32:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:32:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 5 @ 845 updates, score 10.191) (writing took 1.5041376619919902 seconds)
2020-10-12 02:32:43 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 02:32:43 | INFO | train | epoch 005 | loss 10.524 | nll_loss 9.872 | ppl 936.85 | wps 18087.2 | ups 2.84 | wpb 6357.8 | bsz 263.3 | num_updates 845 | lr 4.23289e-05 | gnorm 1.342 | clip 0 | train_wall 56 | wall 294
2020-10-12 02:32:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 02:32:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 02:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:32:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000553
2020-10-12 02:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005994
2020-10-12 02:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 02:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.107926
2020-10-12 02:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114519
2020-10-12 02:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004199
2020-10-12 02:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 02:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102519
2020-10-12 02:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107211
2020-10-12 02:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:32:43 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 02:33:01 | INFO | train_inner | epoch 006:     55 / 169 loss=10.387, nll_loss=9.71, ppl=837.43, wps=17296.6, ups=2.77, wpb=6249.2, bsz=262.9, num_updates=900, lr=4.50775e-05, gnorm=1.43, clip=0, train_wall=33, wall=313
2020-10-12 02:33:35 | INFO | train_inner | epoch 006:    155 / 169 loss=10.271, nll_loss=9.576, ppl=763.14, wps=19150.5, ups=3, wpb=6379.6, bsz=268.1, num_updates=1000, lr=5.0075e-05, gnorm=1.185, clip=0, train_wall=33, wall=346
2020-10-12 02:33:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:33:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:33:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000644
2020-10-12 02:33:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:33:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016960
2020-10-12 02:33:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:33:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013027
2020-10-12 02:33:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030962
2020-10-12 02:33:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:33:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:33:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000418
2020-10-12 02:33:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:33:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017114
2020-10-12 02:33:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:33:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012965
2020-10-12 02:33:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030810
2020-10-12 02:33:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:33:41 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.938 | nll_loss 9.164 | ppl 573.49 | wps 43906 | wpb 2341.5 | bsz 92.7 | num_updates 1014 | best_loss 9.938
2020-10-12 02:33:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:33:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 6 @ 1014 updates, score 9.938) (writing took 1.8457835699955467 seconds)
2020-10-12 02:33:43 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 02:33:43 | INFO | train | epoch 006 | loss 10.296 | nll_loss 9.605 | ppl 778.48 | wps 18010.2 | ups 2.83 | wpb 6357.8 | bsz 263.3 | num_updates 1014 | lr 5.07747e-05 | gnorm 1.238 | clip 0 | train_wall 55 | wall 354
2020-10-12 02:33:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 02:33:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 02:33:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:33:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000514
2020-10-12 02:33:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005359
2020-10-12 02:33:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:33:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000210
2020-10-12 02:33:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:33:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104258
2020-10-12 02:33:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110156
2020-10-12 02:33:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:33:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:33:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004200
2020-10-12 02:33:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:33:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 02:33:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:33:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103304
2020-10-12 02:33:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107996
2020-10-12 02:33:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:33:43 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 02:34:11 | INFO | train_inner | epoch 007:     86 / 169 loss=10.134, nll_loss=9.417, ppl=683.76, wps=17152, ups=2.72, wpb=6311.7, bsz=271.3, num_updates=1100, lr=5.50725e-05, gnorm=1.197, clip=0, train_wall=33, wall=383
2020-10-12 02:34:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000642
2020-10-12 02:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017296
2020-10-12 02:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013144
2020-10-12 02:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031423
2020-10-12 02:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000422
2020-10-12 02:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017157
2020-10-12 02:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012902
2020-10-12 02:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030805
2020-10-12 02:34:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:34:40 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.816 | nll_loss 9.023 | ppl 520.06 | wps 42928.8 | wpb 2341.5 | bsz 92.7 | num_updates 1183 | best_loss 9.816
2020-10-12 02:34:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:34:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 7 @ 1183 updates, score 9.816) (writing took 1.8255777589947684 seconds)
2020-10-12 02:34:42 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 02:34:42 | INFO | train | epoch 007 | loss 10.123 | nll_loss 9.402 | ppl 676.32 | wps 17980.7 | ups 2.83 | wpb 6357.8 | bsz 263.3 | num_updates 1183 | lr 5.92204e-05 | gnorm 1.205 | clip 0 | train_wall 56 | wall 414
2020-10-12 02:34:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 02:34:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 02:34:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:34:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000451
2020-10-12 02:34:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005379
2020-10-12 02:34:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:34:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 02:34:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:34:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106501
2020-10-12 02:34:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112388
2020-10-12 02:34:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:34:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:34:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004235
2020-10-12 02:34:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:34:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 02:34:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:34:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102726
2020-10-12 02:34:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107459
2020-10-12 02:34:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:34:43 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 02:34:48 | INFO | train_inner | epoch 008:     17 / 169 loss=10.115, nll_loss=9.39, ppl=671.05, wps=17512.7, ups=2.72, wpb=6436.3, bsz=252.4, num_updates=1200, lr=6.007e-05, gnorm=1.291, clip=0, train_wall=33, wall=420
2020-10-12 02:35:21 | INFO | train_inner | epoch 008:    117 / 169 loss=10.034, nll_loss=9.295, ppl=628.12, wps=19067.9, ups=3.02, wpb=6322.4, bsz=255.8, num_updates=1300, lr=6.50675e-05, gnorm=1.334, clip=0, train_wall=33, wall=453
2020-10-12 02:35:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:35:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:35:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000579
2020-10-12 02:35:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:35:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017447
2020-10-12 02:35:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:35:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013282
2020-10-12 02:35:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031642
2020-10-12 02:35:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:35:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:35:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000421
2020-10-12 02:35:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:35:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017245
2020-10-12 02:35:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:35:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013401
2020-10-12 02:35:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031385
2020-10-12 02:35:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:35:40 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.671 | nll_loss 8.854 | ppl 462.6 | wps 42722.7 | wpb 2341.5 | bsz 92.7 | num_updates 1352 | best_loss 9.671
2020-10-12 02:35:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:35:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 8 @ 1352 updates, score 9.671) (writing took 1.4956553759984672 seconds)
2020-10-12 02:35:42 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 02:35:42 | INFO | train | epoch 008 | loss 9.999 | nll_loss 9.255 | ppl 610.95 | wps 18038.5 | ups 2.84 | wpb 6357.8 | bsz 263.3 | num_updates 1352 | lr 6.76662e-05 | gnorm 1.291 | clip 0 | train_wall 56 | wall 473
2020-10-12 02:35:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 02:35:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 02:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:35:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000582
2020-10-12 02:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006006
2020-10-12 02:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-12 02:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.107128
2020-10-12 02:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113654
2020-10-12 02:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004233
2020-10-12 02:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 02:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101476
2020-10-12 02:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106215
2020-10-12 02:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:35:42 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 02:35:58 | INFO | train_inner | epoch 009:     48 / 169 loss=9.908, nll_loss=9.151, ppl=568.53, wps=17030.7, ups=2.74, wpb=6209.6, bsz=272.6, num_updates=1400, lr=7.0065e-05, gnorm=1.249, clip=0, train_wall=33, wall=489
2020-10-12 02:36:31 | INFO | train_inner | epoch 009:    148 / 169 loss=9.837, nll_loss=9.068, ppl=536.62, wps=19544.7, ups=2.98, wpb=6561.4, bsz=268.2, num_updates=1500, lr=7.50625e-05, gnorm=1.21, clip=0, train_wall=33, wall=523
2020-10-12 02:36:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:36:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:36:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000582
2020-10-12 02:36:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:36:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017671
2020-10-12 02:36:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:36:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013262
2020-10-12 02:36:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031853
2020-10-12 02:36:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:36:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:36:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000447
2020-10-12 02:36:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:36:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017327
2020-10-12 02:36:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:36:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013167
2020-10-12 02:36:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031257
2020-10-12 02:36:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:36:40 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 9.502 | nll_loss 8.653 | ppl 402.64 | wps 42302 | wpb 2341.5 | bsz 92.7 | num_updates 1521 | best_loss 9.502
2020-10-12 02:36:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:36:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 9 @ 1521 updates, score 9.502) (writing took 1.5462126589991385 seconds)
2020-10-12 02:36:41 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 02:36:41 | INFO | train | epoch 009 | loss 9.851 | nll_loss 9.084 | ppl 542.6 | wps 18080 | ups 2.84 | wpb 6357.8 | bsz 263.3 | num_updates 1521 | lr 7.6112e-05 | gnorm 1.223 | clip 0 | train_wall 56 | wall 533
2020-10-12 02:36:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 02:36:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 02:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:36:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000567
2020-10-12 02:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005831
2020-10-12 02:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 02:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.108087
2020-10-12 02:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114507
2020-10-12 02:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004308
2020-10-12 02:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 02:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103997
2020-10-12 02:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108809
2020-10-12 02:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:36:42 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 02:37:08 | INFO | train_inner | epoch 010:     79 / 169 loss=9.698, nll_loss=8.908, ppl=480.45, wps=17393.5, ups=2.75, wpb=6316.2, bsz=266.3, num_updates=1600, lr=8.006e-05, gnorm=1.291, clip=0, train_wall=33, wall=559
2020-10-12 02:37:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000576
2020-10-12 02:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017509
2020-10-12 02:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013304
2020-10-12 02:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031720
2020-10-12 02:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000442
2020-10-12 02:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016954
2020-10-12 02:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012984
2020-10-12 02:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030730
2020-10-12 02:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:37:39 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 9.34 | nll_loss 8.473 | ppl 355.28 | wps 43607.7 | wpb 2341.5 | bsz 92.7 | num_updates 1690 | best_loss 9.34
2020-10-12 02:37:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:37:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 10 @ 1690 updates, score 9.34) (writing took 1.4616424120031297 seconds)
2020-10-12 02:37:41 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 02:37:41 | INFO | train | epoch 010 | loss 9.666 | nll_loss 8.872 | ppl 468.41 | wps 18135.5 | ups 2.85 | wpb 6357.8 | bsz 263.3 | num_updates 1690 | lr 8.45578e-05 | gnorm 1.22 | clip 0 | train_wall 55 | wall 592
2020-10-12 02:37:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 02:37:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 02:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:37:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000503
2020-10-12 02:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005968
2020-10-12 02:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000262
2020-10-12 02:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.108909
2020-10-12 02:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115499
2020-10-12 02:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004341
2020-10-12 02:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-12 02:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102408
2020-10-12 02:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107266
2020-10-12 02:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:37:41 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 02:37:44 | INFO | train_inner | epoch 011:     10 / 169 loss=9.659, nll_loss=8.863, ppl=465.76, wps=17635.6, ups=2.75, wpb=6409.8, bsz=262.7, num_updates=1700, lr=8.50575e-05, gnorm=1.186, clip=0, train_wall=33, wall=596
2020-10-12 02:38:17 | INFO | train_inner | epoch 011:    110 / 169 loss=9.529, nll_loss=8.715, ppl=420.27, wps=19235.7, ups=3, wpb=6417.4, bsz=263.4, num_updates=1800, lr=9.0055e-05, gnorm=1.186, clip=0, train_wall=33, wall=629
2020-10-12 02:38:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000581
2020-10-12 02:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017152
2020-10-12 02:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013243
2020-10-12 02:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031299
2020-10-12 02:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000439
2020-10-12 02:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016895
2020-10-12 02:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013031
2020-10-12 02:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030676
2020-10-12 02:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:38:38 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 9.195 | nll_loss 8.307 | ppl 316.68 | wps 43759.4 | wpb 2341.5 | bsz 92.7 | num_updates 1859 | best_loss 9.195
2020-10-12 02:38:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:38:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 11 @ 1859 updates, score 9.195) (writing took 1.836431793999509 seconds)
2020-10-12 02:38:40 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 02:38:40 | INFO | train | epoch 011 | loss 9.487 | nll_loss 8.667 | ppl 406.38 | wps 18039.5 | ups 2.84 | wpb 6357.8 | bsz 263.3 | num_updates 1859 | lr 9.30035e-05 | gnorm 1.199 | clip 0 | train_wall 55 | wall 652
2020-10-12 02:38:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 02:38:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 02:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:38:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000488
2020-10-12 02:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005326
2020-10-12 02:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 02:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105091
2020-10-12 02:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110920
2020-10-12 02:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004200
2020-10-12 02:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 02:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103183
2020-10-12 02:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107878
2020-10-12 02:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:38:40 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 02:38:54 | INFO | train_inner | epoch 012:     41 / 169 loss=9.388, nll_loss=8.552, ppl=375.45, wps=17220.9, ups=2.74, wpb=6281, bsz=256.7, num_updates=1900, lr=9.50525e-05, gnorm=1.209, clip=0, train_wall=33, wall=665
2020-10-12 02:39:27 | INFO | train_inner | epoch 012:    141 / 169 loss=9.331, nll_loss=8.485, ppl=358.24, wps=19027.5, ups=3, wpb=6332, bsz=266.7, num_updates=2000, lr=0.00010005, gnorm=1.274, clip=0, train_wall=33, wall=699
2020-10-12 02:39:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000527
2020-10-12 02:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017099
2020-10-12 02:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012859
2020-10-12 02:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030811
2020-10-12 02:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000416
2020-10-12 02:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017314
2020-10-12 02:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012963
2020-10-12 02:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031006
2020-10-12 02:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:39:38 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 9.084 | nll_loss 8.175 | ppl 289.09 | wps 42779.7 | wpb 2341.5 | bsz 92.7 | num_updates 2028 | best_loss 9.084
2020-10-12 02:39:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:39:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 12 @ 2028 updates, score 9.084) (writing took 1.9341039349965286 seconds)
2020-10-12 02:39:40 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 02:39:40 | INFO | train | epoch 012 | loss 9.344 | nll_loss 8.5 | ppl 361.92 | wps 18000.1 | ups 2.83 | wpb 6357.8 | bsz 263.3 | num_updates 2028 | lr 0.000101449 | gnorm 1.279 | clip 0 | train_wall 55 | wall 711
2020-10-12 02:39:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 02:39:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 02:39:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:39:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000542
2020-10-12 02:39:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005380
2020-10-12 02:39:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:39:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 02:39:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:39:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105857
2020-10-12 02:39:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111722
2020-10-12 02:39:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:39:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:39:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004225
2020-10-12 02:39:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:39:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 02:39:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:39:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102956
2020-10-12 02:39:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107675
2020-10-12 02:39:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:39:40 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 02:40:04 | INFO | train_inner | epoch 013:     72 / 169 loss=9.276, nll_loss=8.42, ppl=342.59, wps=17482.9, ups=2.72, wpb=6437.6, bsz=264.2, num_updates=2100, lr=0.000105048, gnorm=1.204, clip=0, train_wall=33, wall=736
2020-10-12 02:40:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:40:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:40:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000645
2020-10-12 02:40:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:40:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017326
2020-10-12 02:40:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:40:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013406
2020-10-12 02:40:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031716
2020-10-12 02:40:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:40:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:40:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000449
2020-10-12 02:40:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:40:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017316
2020-10-12 02:40:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:40:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013129
2020-10-12 02:40:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031212
2020-10-12 02:40:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:40:37 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.976 | nll_loss 8.038 | ppl 262.76 | wps 43205.8 | wpb 2341.5 | bsz 92.7 | num_updates 2197 | best_loss 8.976
2020-10-12 02:40:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:40:39 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 13 @ 2197 updates, score 8.976) (writing took 1.4980886220000684 seconds)
2020-10-12 02:40:39 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 02:40:39 | INFO | train | epoch 013 | loss 9.206 | nll_loss 8.339 | ppl 323.82 | wps 18137 | ups 2.85 | wpb 6357.8 | bsz 263.3 | num_updates 2197 | lr 0.000109895 | gnorm 1.23 | clip 0 | train_wall 55 | wall 771
2020-10-12 02:40:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 02:40:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 02:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:40:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000622
2020-10-12 02:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006019
2020-10-12 02:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 02:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.108759
2020-10-12 02:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115314
2020-10-12 02:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004386
2020-10-12 02:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 02:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099563
2020-10-12 02:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104465
2020-10-12 02:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:40:39 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 02:40:40 | INFO | train_inner | epoch 014:      3 / 169 loss=9.169, nll_loss=8.295, ppl=314.16, wps=17332.9, ups=2.75, wpb=6292, bsz=261, num_updates=2200, lr=0.000110045, gnorm=1.25, clip=0, train_wall=33, wall=772
2020-10-12 02:41:13 | INFO | train_inner | epoch 014:    103 / 169 loss=9.043, nll_loss=8.152, ppl=284.44, wps=19167.6, ups=3.03, wpb=6333.4, bsz=275.8, num_updates=2300, lr=0.000115043, gnorm=1.193, clip=0, train_wall=33, wall=805
2020-10-12 02:41:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000642
2020-10-12 02:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017167
2020-10-12 02:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013443
2020-10-12 02:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031603
2020-10-12 02:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000448
2020-10-12 02:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017395
2020-10-12 02:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013207
2020-10-12 02:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031367
2020-10-12 02:41:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:41:37 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.855 | nll_loss 7.9 | ppl 238.91 | wps 43820.6 | wpb 2341.5 | bsz 92.7 | num_updates 2366 | best_loss 8.855
2020-10-12 02:41:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:41:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 14 @ 2366 updates, score 8.855) (writing took 1.4883807959995465 seconds)
2020-10-12 02:41:38 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 02:41:38 | INFO | train | epoch 014 | loss 9.056 | nll_loss 8.165 | ppl 287.1 | wps 18177.6 | ups 2.86 | wpb 6357.8 | bsz 263.3 | num_updates 2366 | lr 0.000118341 | gnorm 1.184 | clip 0 | train_wall 55 | wall 830
2020-10-12 02:41:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 02:41:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 02:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:41:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000547
2020-10-12 02:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005964
2020-10-12 02:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 02:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106863
2020-10-12 02:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113340
2020-10-12 02:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004235
2020-10-12 02:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 02:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102697
2020-10-12 02:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107424
2020-10-12 02:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:41:38 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 02:41:50 | INFO | train_inner | epoch 015:     34 / 169 loss=9.043, nll_loss=8.148, ppl=283.72, wps=17603.2, ups=2.76, wpb=6382.6, bsz=244.4, num_updates=2400, lr=0.00012004, gnorm=1.173, clip=0, train_wall=33, wall=841
2020-10-12 02:42:23 | INFO | train_inner | epoch 015:    134 / 169 loss=8.917, nll_loss=8.004, ppl=256.74, wps=19265.9, ups=3.01, wpb=6400.9, bsz=263.9, num_updates=2500, lr=0.000125037, gnorm=1.198, clip=0, train_wall=33, wall=874
2020-10-12 02:42:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:42:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:42:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000666
2020-10-12 02:42:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:42:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017115
2020-10-12 02:42:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:42:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013096
2020-10-12 02:42:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031208
2020-10-12 02:42:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:42:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:42:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000440
2020-10-12 02:42:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:42:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016701
2020-10-12 02:42:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:42:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012983
2020-10-12 02:42:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030439
2020-10-12 02:42:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:42:36 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.733 | nll_loss 7.754 | ppl 215.82 | wps 43262.8 | wpb 2341.5 | bsz 92.7 | num_updates 2535 | best_loss 8.733
2020-10-12 02:42:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:42:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 15 @ 2535 updates, score 8.733) (writing took 1.8450493309937883 seconds)
2020-10-12 02:42:38 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 02:42:38 | INFO | train | epoch 015 | loss 8.911 | nll_loss 7.997 | ppl 255.55 | wps 18064.7 | ups 2.84 | wpb 6357.8 | bsz 263.3 | num_updates 2535 | lr 0.000126787 | gnorm 1.205 | clip 0 | train_wall 55 | wall 889
2020-10-12 02:42:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 02:42:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 02:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:42:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000554
2020-10-12 02:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005413
2020-10-12 02:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 02:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.107118
2020-10-12 02:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113036
2020-10-12 02:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004367
2020-10-12 02:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 02:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103872
2020-10-12 02:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108748
2020-10-12 02:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:42:38 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 02:42:59 | INFO | train_inner | epoch 016:     65 / 169 loss=8.773, nll_loss=7.839, ppl=228.97, wps=17441.4, ups=2.73, wpb=6379.6, bsz=282.6, num_updates=2600, lr=0.000130035, gnorm=1.256, clip=0, train_wall=33, wall=911
2020-10-12 02:43:33 | INFO | train_inner | epoch 016:    165 / 169 loss=8.761, nll_loss=7.823, ppl=226.46, wps=19025.4, ups=3.01, wpb=6330.9, bsz=255.8, num_updates=2700, lr=0.000135032, gnorm=1.242, clip=0, train_wall=33, wall=944
2020-10-12 02:43:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000570
2020-10-12 02:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017179
2020-10-12 02:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013259
2020-10-12 02:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031337
2020-10-12 02:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000441
2020-10-12 02:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017257
2020-10-12 02:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013219
2020-10-12 02:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031230
2020-10-12 02:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:43:35 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.567 | nll_loss 7.557 | ppl 188.32 | wps 42947.2 | wpb 2341.5 | bsz 92.7 | num_updates 2704 | best_loss 8.567
2020-10-12 02:43:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:43:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 16 @ 2704 updates, score 8.567) (writing took 1.4247019040049054 seconds)
2020-10-12 02:43:37 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 02:43:37 | INFO | train | epoch 016 | loss 8.756 | nll_loss 7.818 | ppl 225.61 | wps 18184.2 | ups 2.86 | wpb 6357.8 | bsz 263.3 | num_updates 2704 | lr 0.000135232 | gnorm 1.242 | clip 0 | train_wall 55 | wall 948
2020-10-12 02:43:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 02:43:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 02:43:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:43:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000518
2020-10-12 02:43:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005354
2020-10-12 02:43:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:43:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 02:43:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:43:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104848
2020-10-12 02:43:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110699
2020-10-12 02:43:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:43:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:43:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004241
2020-10-12 02:43:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:43:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 02:43:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:43:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102572
2020-10-12 02:43:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107309
2020-10-12 02:43:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:43:37 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 02:44:09 | INFO | train_inner | epoch 017:     96 / 169 loss=8.597, nll_loss=7.636, ppl=198.89, wps=17756, ups=2.77, wpb=6413.4, bsz=272.5, num_updates=2800, lr=0.00014003, gnorm=1.258, clip=0, train_wall=33, wall=980
2020-10-12 02:44:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000591
2020-10-12 02:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017121
2020-10-12 02:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013170
2020-10-12 02:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031217
2020-10-12 02:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000449
2020-10-12 02:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017413
2020-10-12 02:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013343
2020-10-12 02:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031520
2020-10-12 02:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:44:34 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 8.415 | nll_loss 7.378 | ppl 166.34 | wps 43894.7 | wpb 2341.5 | bsz 92.7 | num_updates 2873 | best_loss 8.415
2020-10-12 02:44:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:44:36 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 17 @ 2873 updates, score 8.415) (writing took 1.4980345549993217 seconds)
2020-10-12 02:44:36 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 02:44:36 | INFO | train | epoch 017 | loss 8.588 | nll_loss 7.624 | ppl 197.28 | wps 18158.3 | ups 2.86 | wpb 6357.8 | bsz 263.3 | num_updates 2873 | lr 0.000143678 | gnorm 1.238 | clip 0 | train_wall 55 | wall 1007
2020-10-12 02:44:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 02:44:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 02:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:44:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000551
2020-10-12 02:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006005
2020-10-12 02:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 02:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106938
2020-10-12 02:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113483
2020-10-12 02:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004294
2020-10-12 02:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-12 02:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106165
2020-10-12 02:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110949
2020-10-12 02:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:44:36 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 02:44:45 | INFO | train_inner | epoch 018:     27 / 169 loss=8.551, nll_loss=7.58, ppl=191.37, wps=17266.1, ups=2.76, wpb=6246.5, bsz=242.8, num_updates=2900, lr=0.000145028, gnorm=1.196, clip=0, train_wall=33, wall=1017
2020-10-12 02:45:18 | INFO | train_inner | epoch 018:    127 / 169 loss=8.413, nll_loss=7.422, ppl=171.54, wps=19154.3, ups=3, wpb=6392.2, bsz=277.9, num_updates=3000, lr=0.000150025, gnorm=1.247, clip=0, train_wall=33, wall=1050
2020-10-12 02:45:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000585
2020-10-12 02:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017305
2020-10-12 02:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013374
2020-10-12 02:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031598
2020-10-12 02:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000445
2020-10-12 02:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017383
2020-10-12 02:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013031
2020-10-12 02:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031175
2020-10-12 02:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:45:34 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 8.308 | nll_loss 7.249 | ppl 152.12 | wps 43437.2 | wpb 2341.5 | bsz 92.7 | num_updates 3042 | best_loss 8.308
2020-10-12 02:45:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:45:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 18 @ 3042 updates, score 8.308) (writing took 1.4985214910120703 seconds)
2020-10-12 02:45:35 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 02:45:35 | INFO | train | epoch 018 | loss 8.414 | nll_loss 7.423 | ppl 171.6 | wps 18153.2 | ups 2.86 | wpb 6357.8 | bsz 263.3 | num_updates 3042 | lr 0.000152124 | gnorm 1.228 | clip 0 | train_wall 55 | wall 1067
2020-10-12 02:45:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 02:45:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 02:45:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:45:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000642
2020-10-12 02:45:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005939
2020-10-12 02:45:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:45:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 02:45:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:45:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109010
2020-10-12 02:45:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115454
2020-10-12 02:45:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:45:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:45:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004210
2020-10-12 02:45:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:45:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 02:45:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:45:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103495
2020-10-12 02:45:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108198
2020-10-12 02:45:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:45:35 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 02:45:55 | INFO | train_inner | epoch 019:     58 / 169 loss=8.308, nll_loss=7.302, ppl=157.77, wps=17467, ups=2.76, wpb=6318.2, bsz=255.5, num_updates=3100, lr=0.000155023, gnorm=1.254, clip=0, train_wall=33, wall=1086
2020-10-12 02:46:28 | INFO | train_inner | epoch 019:    158 / 169 loss=8.233, nll_loss=7.213, ppl=148.38, wps=19050.2, ups=3.01, wpb=6335.1, bsz=269.2, num_updates=3200, lr=0.00016002, gnorm=1.287, clip=0, train_wall=33, wall=1119
2020-10-12 02:46:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000523
2020-10-12 02:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:46:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017141
2020-10-12 02:46:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:46:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013011
2020-10-12 02:46:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031001
2020-10-12 02:46:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:46:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:46:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000420
2020-10-12 02:46:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:46:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017449
2020-10-12 02:46:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:46:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013113
2020-10-12 02:46:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031297
2020-10-12 02:46:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:46:33 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 8.164 | nll_loss 7.076 | ppl 134.89 | wps 43619.6 | wpb 2341.5 | bsz 92.7 | num_updates 3211 | best_loss 8.164
2020-10-12 02:46:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:46:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 19 @ 3211 updates, score 8.164) (writing took 1.4397031020052964 seconds)
2020-10-12 02:46:34 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 02:46:34 | INFO | train | epoch 019 | loss 8.255 | nll_loss 7.24 | ppl 151.14 | wps 18160 | ups 2.86 | wpb 6357.8 | bsz 263.3 | num_updates 3211 | lr 0.00016057 | gnorm 1.274 | clip 0 | train_wall 55 | wall 1126
2020-10-12 02:46:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 02:46:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 02:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:46:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000550
2020-10-12 02:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005816
2020-10-12 02:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000272
2020-10-12 02:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.112134
2020-10-12 02:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.118591
2020-10-12 02:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004263
2020-10-12 02:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 02:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102626
2020-10-12 02:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107398
2020-10-12 02:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:46:34 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 02:47:04 | INFO | train_inner | epoch 020:     89 / 169 loss=8.172, nll_loss=7.142, ppl=141.27, wps=17672.1, ups=2.75, wpb=6417.9, bsz=238.5, num_updates=3300, lr=0.000165018, gnorm=1.23, clip=0, train_wall=33, wall=1156
2020-10-12 02:47:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000674
2020-10-12 02:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017241
2020-10-12 02:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013033
2020-10-12 02:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031285
2020-10-12 02:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000443
2020-10-12 02:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016981
2020-10-12 02:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013163
2020-10-12 02:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030905
2020-10-12 02:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:47:32 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 8.028 | nll_loss 6.906 | ppl 119.96 | wps 42898.3 | wpb 2341.5 | bsz 92.7 | num_updates 3380 | best_loss 8.028
2020-10-12 02:47:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:47:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 20 @ 3380 updates, score 8.028) (writing took 1.8458444159914507 seconds)
2020-10-12 02:47:34 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 02:47:34 | INFO | train | epoch 020 | loss 8.077 | nll_loss 7.034 | ppl 131.08 | wps 18043.8 | ups 2.84 | wpb 6357.8 | bsz 263.3 | num_updates 3380 | lr 0.000169016 | gnorm 1.271 | clip 0 | train_wall 55 | wall 1185
2020-10-12 02:47:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 02:47:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 02:47:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:47:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000435
2020-10-12 02:47:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005220
2020-10-12 02:47:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:47:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-12 02:47:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:47:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106046
2020-10-12 02:47:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111782
2020-10-12 02:47:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:47:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:47:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004306
2020-10-12 02:47:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:47:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 02:47:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:47:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104922
2020-10-12 02:47:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109720
2020-10-12 02:47:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:47:34 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 02:47:41 | INFO | train_inner | epoch 021:     20 / 169 loss=7.98, nll_loss=6.923, ppl=121.33, wps=17309.7, ups=2.74, wpb=6312.1, bsz=280.6, num_updates=3400, lr=0.000170015, gnorm=1.285, clip=0, train_wall=33, wall=1192
2020-10-12 02:48:14 | INFO | train_inner | epoch 021:    120 / 169 loss=7.965, nll_loss=6.903, ppl=119.64, wps=19006.6, ups=3.02, wpb=6300.9, bsz=243.2, num_updates=3500, lr=0.000175013, gnorm=1.329, clip=0, train_wall=33, wall=1225
2020-10-12 02:48:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000636
2020-10-12 02:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017440
2020-10-12 02:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012966
2020-10-12 02:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031373
2020-10-12 02:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000436
2020-10-12 02:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017228
2020-10-12 02:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013010
2020-10-12 02:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030986
2020-10-12 02:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:48:31 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.916 | nll_loss 6.759 | ppl 108.33 | wps 43228.6 | wpb 2341.5 | bsz 92.7 | num_updates 3549 | best_loss 7.916
2020-10-12 02:48:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:48:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 21 @ 3549 updates, score 7.916) (writing took 1.6946927220124053 seconds)
2020-10-12 02:48:33 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 02:48:33 | INFO | train | epoch 021 | loss 7.905 | nll_loss 6.834 | ppl 114.13 | wps 18092.3 | ups 2.85 | wpb 6357.8 | bsz 263.3 | num_updates 3549 | lr 0.000177461 | gnorm 1.314 | clip 0 | train_wall 55 | wall 1245
2020-10-12 02:48:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 02:48:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 02:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:48:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000435
2020-10-12 02:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005210
2020-10-12 02:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 02:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105401
2020-10-12 02:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111102
2020-10-12 02:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004283
2020-10-12 02:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 02:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102726
2020-10-12 02:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107506
2020-10-12 02:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:48:33 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 02:48:50 | INFO | train_inner | epoch 022:     51 / 169 loss=7.796, nll_loss=6.708, ppl=104.57, wps=17817.1, ups=2.74, wpb=6512.6, bsz=277.1, num_updates=3600, lr=0.00018001, gnorm=1.298, clip=0, train_wall=33, wall=1262
2020-10-12 02:49:24 | INFO | train_inner | epoch 022:    151 / 169 loss=7.722, nll_loss=6.622, ppl=98.48, wps=18845.2, ups=3.01, wpb=6269.5, bsz=267.4, num_updates=3700, lr=0.000185008, gnorm=1.36, clip=0, train_wall=33, wall=1295
2020-10-12 02:49:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000575
2020-10-12 02:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017477
2020-10-12 02:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013359
2020-10-12 02:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031746
2020-10-12 02:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000444
2020-10-12 02:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017671
2020-10-12 02:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013256
2020-10-12 02:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031688
2020-10-12 02:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:49:31 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.747 | nll_loss 6.576 | ppl 95.37 | wps 43339.2 | wpb 2341.5 | bsz 92.7 | num_updates 3718 | best_loss 7.747
2020-10-12 02:49:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:49:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 22 @ 3718 updates, score 7.747) (writing took 1.8115062970027793 seconds)
2020-10-12 02:49:33 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 02:49:33 | INFO | train | epoch 022 | loss 7.732 | nll_loss 6.633 | ppl 99.28 | wps 18040.6 | ups 2.84 | wpb 6357.8 | bsz 263.3 | num_updates 3718 | lr 0.000185907 | gnorm 1.323 | clip 0 | train_wall 55 | wall 1304
2020-10-12 02:49:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 02:49:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 02:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:49:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000485
2020-10-12 02:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005287
2020-10-12 02:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 02:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102892
2020-10-12 02:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108690
2020-10-12 02:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004264
2020-10-12 02:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 02:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103055
2020-10-12 02:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107819
2020-10-12 02:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:49:33 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 02:50:00 | INFO | train_inner | epoch 023:     82 / 169 loss=7.535, nll_loss=6.409, ppl=84.96, wps=17519.7, ups=2.73, wpb=6415.3, bsz=276.7, num_updates=3800, lr=0.000190005, gnorm=1.296, clip=0, train_wall=33, wall=1332
2020-10-12 02:50:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000583
2020-10-12 02:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017064
2020-10-12 02:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013182
2020-10-12 02:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031163
2020-10-12 02:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000442
2020-10-12 02:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017085
2020-10-12 02:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013210
2020-10-12 02:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031052
2020-10-12 02:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:50:30 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.628 | nll_loss 6.43 | ppl 86.2 | wps 43066 | wpb 2341.5 | bsz 92.7 | num_updates 3887 | best_loss 7.628
2020-10-12 02:50:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:50:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 23 @ 3887 updates, score 7.628) (writing took 1.4988750539923785 seconds)
2020-10-12 02:50:32 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 02:50:32 | INFO | train | epoch 023 | loss 7.545 | nll_loss 6.417 | ppl 85.45 | wps 18143.9 | ups 2.85 | wpb 6357.8 | bsz 263.3 | num_updates 3887 | lr 0.000194353 | gnorm 1.305 | clip 0 | train_wall 55 | wall 1363
2020-10-12 02:50:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 02:50:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 02:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:50:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000437
2020-10-12 02:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005711
2020-10-12 02:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000242
2020-10-12 02:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.111251
2020-10-12 02:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.117542
2020-10-12 02:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004240
2020-10-12 02:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-12 02:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103034
2020-10-12 02:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107754
2020-10-12 02:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:50:32 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 02:50:36 | INFO | train_inner | epoch 024:     13 / 169 loss=7.556, nll_loss=6.428, ppl=86.1, wps=17317.2, ups=2.76, wpb=6277.3, bsz=247.1, num_updates=3900, lr=0.000195003, gnorm=1.3, clip=0, train_wall=33, wall=1368
2020-10-12 02:51:10 | INFO | train_inner | epoch 024:    113 / 169 loss=7.379, nll_loss=6.225, ppl=74.8, wps=19386.4, ups=2.99, wpb=6477, bsz=266.6, num_updates=4000, lr=0.0002, gnorm=1.287, clip=0, train_wall=33, wall=1401
2020-10-12 02:51:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000571
2020-10-12 02:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017292
2020-10-12 02:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012985
2020-10-12 02:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031173
2020-10-12 02:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000443
2020-10-12 02:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017226
2020-10-12 02:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013088
2020-10-12 02:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031064
2020-10-12 02:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:51:30 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.487 | nll_loss 6.252 | ppl 76.19 | wps 43026 | wpb 2341.5 | bsz 92.7 | num_updates 4056 | best_loss 7.487
2020-10-12 02:51:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:51:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 24 @ 4056 updates, score 7.487) (writing took 1.514345282004797 seconds)
2020-10-12 02:51:31 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 02:51:31 | INFO | train | epoch 024 | loss 7.361 | nll_loss 6.204 | ppl 73.73 | wps 18135.1 | ups 2.85 | wpb 6357.8 | bsz 263.3 | num_updates 4056 | lr 0.000198615 | gnorm 1.311 | clip 0 | train_wall 55 | wall 1423
2020-10-12 02:51:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 02:51:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 02:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:51:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000540
2020-10-12 02:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005931
2020-10-12 02:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 02:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106167
2020-10-12 02:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112653
2020-10-12 02:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004363
2020-10-12 02:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 02:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105188
2020-10-12 02:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110047
2020-10-12 02:51:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:51:31 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 02:51:46 | INFO | train_inner | epoch 025:     44 / 169 loss=7.24, nll_loss=6.066, ppl=66.98, wps=17250.7, ups=2.77, wpb=6235, bsz=271.7, num_updates=4100, lr=0.000197546, gnorm=1.337, clip=0, train_wall=33, wall=1438
2020-10-12 02:52:19 | INFO | train_inner | epoch 025:    144 / 169 loss=7.179, nll_loss=5.992, ppl=63.67, wps=19235.9, ups=3.01, wpb=6400.1, bsz=255.3, num_updates=4200, lr=0.00019518, gnorm=1.292, clip=0, train_wall=33, wall=1471
2020-10-12 02:52:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000579
2020-10-12 02:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017659
2020-10-12 02:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013418
2020-10-12 02:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031981
2020-10-12 02:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000419
2020-10-12 02:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017093
2020-10-12 02:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013060
2020-10-12 02:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030880
2020-10-12 02:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:52:29 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.508 | nll_loss 6.262 | ppl 76.75 | wps 43142.5 | wpb 2341.5 | bsz 92.7 | num_updates 4225 | best_loss 7.487
2020-10-12 02:52:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:52:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_last.pt (epoch 25 @ 4225 updates, score 7.508) (writing took 1.091867494993494 seconds)
2020-10-12 02:52:30 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 02:52:30 | INFO | train | epoch 025 | loss 7.167 | nll_loss 5.98 | ppl 63.1 | wps 18279.6 | ups 2.88 | wpb 6357.8 | bsz 263.3 | num_updates 4225 | lr 0.000194602 | gnorm 1.314 | clip 0 | train_wall 55 | wall 1482
2020-10-12 02:52:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 02:52:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 02:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:52:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000505
2020-10-12 02:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005335
2020-10-12 02:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 02:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106012
2020-10-12 02:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111839
2020-10-12 02:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004243
2020-10-12 02:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 02:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105471
2020-10-12 02:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110196
2020-10-12 02:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:52:30 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 02:52:55 | INFO | train_inner | epoch 026:     75 / 169 loss=7.076, nll_loss=5.873, ppl=58.62, wps=17758.5, ups=2.78, wpb=6388.3, bsz=261.6, num_updates=4300, lr=0.000192897, gnorm=1.367, clip=0, train_wall=33, wall=1507
2020-10-12 02:53:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000521
2020-10-12 02:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017103
2020-10-12 02:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013569
2020-10-12 02:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031513
2020-10-12 02:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000411
2020-10-12 02:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017387
2020-10-12 02:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013359
2020-10-12 02:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031463
2020-10-12 02:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:53:28 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.303 | nll_loss 6.019 | ppl 64.85 | wps 43004.9 | wpb 2341.5 | bsz 92.7 | num_updates 4394 | best_loss 7.303
2020-10-12 02:53:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:53:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 26 @ 4394 updates, score 7.303) (writing took 1.3343788359925384 seconds)
2020-10-12 02:53:29 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 02:53:29 | INFO | train | epoch 026 | loss 7.006 | nll_loss 5.791 | ppl 55.37 | wps 18201 | ups 2.86 | wpb 6357.8 | bsz 263.3 | num_updates 4394 | lr 0.000190823 | gnorm 1.359 | clip 0 | train_wall 55 | wall 1541
2020-10-12 02:53:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 02:53:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 02:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:53:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000566
2020-10-12 02:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005851
2020-10-12 02:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 02:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.107411
2020-10-12 02:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113844
2020-10-12 02:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004357
2020-10-12 02:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-12 02:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103748
2020-10-12 02:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108609
2020-10-12 02:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:53:29 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 02:53:31 | INFO | train_inner | epoch 027:      6 / 169 loss=6.972, nll_loss=5.751, ppl=53.86, wps=17560.3, ups=2.78, wpb=6317.3, bsz=262.2, num_updates=4400, lr=0.000190693, gnorm=1.356, clip=0, train_wall=33, wall=1543
2020-10-12 02:54:04 | INFO | train_inner | epoch 027:    106 / 169 loss=6.837, nll_loss=5.596, ppl=48.37, wps=19279, ups=3.01, wpb=6402.3, bsz=273.1, num_updates=4500, lr=0.000188562, gnorm=1.33, clip=0, train_wall=33, wall=1576
2020-10-12 02:54:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000641
2020-10-12 02:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016956
2020-10-12 02:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012880
2020-10-12 02:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030806
2020-10-12 02:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000441
2020-10-12 02:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017111
2020-10-12 02:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012936
2020-10-12 02:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030795
2020-10-12 02:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:54:27 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.226 | nll_loss 5.921 | ppl 60.61 | wps 43994.3 | wpb 2341.5 | bsz 92.7 | num_updates 4563 | best_loss 7.226
2020-10-12 02:54:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:54:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 27 @ 4563 updates, score 7.226) (writing took 1.5295025850064121 seconds)
2020-10-12 02:54:28 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 02:54:28 | INFO | train | epoch 027 | loss 6.837 | nll_loss 5.594 | ppl 48.31 | wps 18152.6 | ups 2.86 | wpb 6357.8 | bsz 263.3 | num_updates 4563 | lr 0.000187256 | gnorm 1.317 | clip 0 | train_wall 55 | wall 1600
2020-10-12 02:54:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 02:54:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 02:54:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:54:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000579
2020-10-12 02:54:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005956
2020-10-12 02:54:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:54:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000244
2020-10-12 02:54:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:54:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109221
2020-10-12 02:54:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115752
2020-10-12 02:54:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:54:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:54:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004242
2020-10-12 02:54:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:54:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 02:54:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:54:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102872
2020-10-12 02:54:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107598
2020-10-12 02:54:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:54:28 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 02:54:41 | INFO | train_inner | epoch 028:     37 / 169 loss=6.733, nll_loss=5.475, ppl=44.47, wps=17564.8, ups=2.75, wpb=6382.4, bsz=271.6, num_updates=4600, lr=0.000186501, gnorm=1.276, clip=0, train_wall=33, wall=1612
2020-10-12 02:55:14 | INFO | train_inner | epoch 028:    137 / 169 loss=6.694, nll_loss=5.427, ppl=43.02, wps=19106.4, ups=3.02, wpb=6336.5, bsz=255.5, num_updates=4700, lr=0.000184506, gnorm=1.365, clip=0, train_wall=33, wall=1645
2020-10-12 02:55:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.006817
2020-10-12 02:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017035
2020-10-12 02:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013077
2020-10-12 02:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037266
2020-10-12 02:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000437
2020-10-12 02:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017001
2020-10-12 02:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013184
2020-10-12 02:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030928
2020-10-12 02:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:55:26 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.154 | nll_loss 5.832 | ppl 56.95 | wps 43728.5 | wpb 2341.5 | bsz 92.7 | num_updates 4732 | best_loss 7.154
2020-10-12 02:55:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:55:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 28 @ 4732 updates, score 7.154) (writing took 1.5289892139990116 seconds)
2020-10-12 02:55:27 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 02:55:27 | INFO | train | epoch 028 | loss 6.682 | nll_loss 5.414 | ppl 42.62 | wps 18152.9 | ups 2.86 | wpb 6357.8 | bsz 263.3 | num_updates 4732 | lr 0.000183881 | gnorm 1.325 | clip 0 | train_wall 55 | wall 1659
2020-10-12 02:55:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 02:55:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 02:55:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:55:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000545
2020-10-12 02:55:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005648
2020-10-12 02:55:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:55:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 02:55:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.110622
2020-10-12 02:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.116877
2020-10-12 02:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004275
2020-10-12 02:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-12 02:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103458
2020-10-12 02:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108218
2020-10-12 02:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:55:28 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 02:55:50 | INFO | train_inner | epoch 029:     68 / 169 loss=6.583, nll_loss=5.298, ppl=39.35, wps=17396.3, ups=2.75, wpb=6335.4, bsz=260.5, num_updates=4800, lr=0.000182574, gnorm=1.288, clip=0, train_wall=33, wall=1682
2020-10-12 02:56:24 | INFO | train_inner | epoch 029:    168 / 169 loss=6.553, nll_loss=5.261, ppl=38.33, wps=19124.6, ups=3.01, wpb=6356.5, bsz=262.4, num_updates=4900, lr=0.000180702, gnorm=1.364, clip=0, train_wall=33, wall=1715
2020-10-12 02:56:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000580
2020-10-12 02:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017428
2020-10-12 02:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013047
2020-10-12 02:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031384
2020-10-12 02:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000442
2020-10-12 02:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017527
2020-10-12 02:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013296
2020-10-12 02:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031581
2020-10-12 02:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:56:25 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.086 | nll_loss 5.743 | ppl 53.55 | wps 43110 | wpb 2341.5 | bsz 92.7 | num_updates 4901 | best_loss 7.086
2020-10-12 02:56:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:56:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 29 @ 4901 updates, score 7.086) (writing took 1.4986933159962064 seconds)
2020-10-12 02:56:27 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 02:56:27 | INFO | train | epoch 029 | loss 6.535 | nll_loss 5.242 | ppl 37.84 | wps 18126.8 | ups 2.85 | wpb 6357.8 | bsz 263.3 | num_updates 4901 | lr 0.000180683 | gnorm 1.327 | clip 0 | train_wall 55 | wall 1718
2020-10-12 02:56:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 02:56:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 02:56:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:56:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000531
2020-10-12 02:56:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005706
2020-10-12 02:56:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:56:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 02:56:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:56:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109082
2020-10-12 02:56:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115312
2020-10-12 02:56:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:56:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:56:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004391
2020-10-12 02:56:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:56:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 02:56:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:56:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103375
2020-10-12 02:56:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108279
2020-10-12 02:56:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:56:27 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 02:56:59 | INFO | train_inner | epoch 030:     99 / 169 loss=6.434, nll_loss=5.123, ppl=34.86, wps=17293.3, ups=2.78, wpb=6212.2, bsz=251.1, num_updates=5000, lr=0.000178885, gnorm=1.366, clip=0, train_wall=32, wall=1751
2020-10-12 02:57:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000577
2020-10-12 02:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016951
2020-10-12 02:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013292
2020-10-12 02:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031143
2020-10-12 02:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000448
2020-10-12 02:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017114
2020-10-12 02:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013150
2020-10-12 02:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031023
2020-10-12 02:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:57:24 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.033 | nll_loss 5.665 | ppl 50.74 | wps 43541.1 | wpb 2341.5 | bsz 92.7 | num_updates 5070 | best_loss 7.033
2020-10-12 02:57:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:57:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 30 @ 5070 updates, score 7.033) (writing took 1.4985539870103821 seconds)
2020-10-12 02:57:26 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 02:57:26 | INFO | train | epoch 030 | loss 6.416 | nll_loss 5.102 | ppl 34.34 | wps 18195.6 | ups 2.86 | wpb 6357.8 | bsz 263.3 | num_updates 5070 | lr 0.000177646 | gnorm 1.359 | clip 0 | train_wall 55 | wall 1777
2020-10-12 02:57:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 02:57:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 02:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:57:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000587
2020-10-12 02:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005931
2020-10-12 02:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 02:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.110274
2020-10-12 02:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.116790
2020-10-12 02:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004252
2020-10-12 02:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-12 02:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104925
2020-10-12 02:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109664
2020-10-12 02:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:57:26 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 02:57:36 | INFO | train_inner | epoch 031:     30 / 169 loss=6.361, nll_loss=5.037, ppl=32.83, wps=17870.2, ups=2.75, wpb=6505.7, bsz=268, num_updates=5100, lr=0.000177123, gnorm=1.296, clip=0, train_wall=33, wall=1787
2020-10-12 02:58:09 | INFO | train_inner | epoch 031:    130 / 169 loss=6.303, nll_loss=4.969, ppl=31.32, wps=19257.6, ups=3, wpb=6419.8, bsz=259.6, num_updates=5200, lr=0.000175412, gnorm=1.38, clip=0, train_wall=33, wall=1821
2020-10-12 02:58:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000584
2020-10-12 02:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017317
2020-10-12 02:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013179
2020-10-12 02:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031415
2020-10-12 02:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000461
2020-10-12 02:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016805
2020-10-12 02:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013127
2020-10-12 02:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030701
2020-10-12 02:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:58:23 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.978 | nll_loss 5.605 | ppl 48.66 | wps 43352.7 | wpb 2341.5 | bsz 92.7 | num_updates 5239 | best_loss 6.978
2020-10-12 02:58:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:58:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 31 @ 5239 updates, score 6.978) (writing took 1.9123743209929671 seconds)
2020-10-12 02:58:25 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 02:58:25 | INFO | train | epoch 031 | loss 6.28 | nll_loss 4.943 | ppl 30.77 | wps 18038.2 | ups 2.84 | wpb 6357.8 | bsz 263.3 | num_updates 5239 | lr 0.000174757 | gnorm 1.305 | clip 0 | train_wall 55 | wall 1837
2020-10-12 02:58:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 02:58:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 02:58:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:58:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000462
2020-10-12 02:58:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005397
2020-10-12 02:58:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:58:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 02:58:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:58:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106043
2020-10-12 02:58:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111947
2020-10-12 02:58:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:58:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:58:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004251
2020-10-12 02:58:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:58:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 02:58:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:58:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102518
2020-10-12 02:58:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107259
2020-10-12 02:58:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:58:26 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 02:58:46 | INFO | train_inner | epoch 032:     61 / 169 loss=6.177, nll_loss=4.824, ppl=28.33, wps=17141.4, ups=2.74, wpb=6265.5, bsz=272.2, num_updates=5300, lr=0.000173749, gnorm=1.269, clip=0, train_wall=33, wall=1857
2020-10-12 02:59:19 | INFO | train_inner | epoch 032:    161 / 169 loss=6.18, nll_loss=4.824, ppl=28.32, wps=19218.1, ups=3.01, wpb=6378.2, bsz=261.6, num_updates=5400, lr=0.000172133, gnorm=1.329, clip=0, train_wall=33, wall=1891
2020-10-12 02:59:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.006766
2020-10-12 02:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017278
2020-10-12 02:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013293
2020-10-12 02:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037674
2020-10-12 02:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000476
2020-10-12 02:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017133
2020-10-12 02:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013280
2020-10-12 02:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031201
2020-10-12 02:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:59:23 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.89 | nll_loss 5.497 | ppl 45.15 | wps 43428.3 | wpb 2341.5 | bsz 92.7 | num_updates 5408 | best_loss 6.89
2020-10-12 02:59:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:59:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 32 @ 5408 updates, score 6.89) (writing took 1.5048264460056089 seconds)
2020-10-12 02:59:24 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 02:59:24 | INFO | train | epoch 032 | loss 6.165 | nll_loss 4.808 | ppl 28.01 | wps 18167.5 | ups 2.86 | wpb 6357.8 | bsz 263.3 | num_updates 5408 | lr 0.000172005 | gnorm 1.312 | clip 0 | train_wall 55 | wall 1896
2020-10-12 02:59:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 02:59:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 02:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:59:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000621
2020-10-12 02:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006142
2020-10-12 02:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 02:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.107411
2020-10-12 02:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114076
2020-10-12 02:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004299
2020-10-12 02:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000201
2020-10-12 02:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105358
2020-10-12 02:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110182
2020-10-12 02:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:59:25 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 02:59:55 | INFO | train_inner | epoch 033:     92 / 169 loss=6.069, nll_loss=4.696, ppl=25.93, wps=17614, ups=2.76, wpb=6372.9, bsz=259, num_updates=5500, lr=0.000170561, gnorm=1.334, clip=0, train_wall=33, wall=1927
2020-10-12 03:00:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000522
2020-10-12 03:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017240
2020-10-12 03:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013201
2020-10-12 03:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031287
2020-10-12 03:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000409
2020-10-12 03:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017408
2020-10-12 03:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013221
2020-10-12 03:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031348
2020-10-12 03:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:00:22 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.88 | nll_loss 5.484 | ppl 44.74 | wps 43059.9 | wpb 2341.5 | bsz 92.7 | num_updates 5577 | best_loss 6.88
2020-10-12 03:00:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:00:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 33 @ 5577 updates, score 6.88) (writing took 1.4985688190063229 seconds)
2020-10-12 03:00:24 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 03:00:24 | INFO | train | epoch 033 | loss 6.065 | nll_loss 4.691 | ppl 25.83 | wps 18152.9 | ups 2.86 | wpb 6357.8 | bsz 263.3 | num_updates 5577 | lr 0.000169379 | gnorm 1.356 | clip 0 | train_wall 55 | wall 1955
2020-10-12 03:00:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 03:00:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 03:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:00:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000490
2020-10-12 03:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005941
2020-10-12 03:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 03:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109996
2020-10-12 03:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.116550
2020-10-12 03:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004318
2020-10-12 03:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 03:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103835
2020-10-12 03:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108674
2020-10-12 03:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:00:24 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 03:00:31 | INFO | train_inner | epoch 034:     23 / 169 loss=6.024, nll_loss=4.643, ppl=24.98, wps=17513.9, ups=2.76, wpb=6354.8, bsz=274.8, num_updates=5600, lr=0.000169031, gnorm=1.335, clip=0, train_wall=33, wall=1963
2020-10-12 03:01:05 | INFO | train_inner | epoch 034:    123 / 169 loss=5.925, nll_loss=4.529, ppl=23.08, wps=19022.7, ups=3, wpb=6337.4, bsz=270.6, num_updates=5700, lr=0.000167542, gnorm=1.308, clip=0, train_wall=33, wall=1996
2020-10-12 03:01:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000646
2020-10-12 03:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017172
2020-10-12 03:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013297
2020-10-12 03:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031453
2020-10-12 03:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000419
2020-10-12 03:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017267
2020-10-12 03:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012992
2020-10-12 03:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030991
2020-10-12 03:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:01:21 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.839 | nll_loss 5.418 | ppl 42.75 | wps 42960.6 | wpb 2341.5 | bsz 92.7 | num_updates 5746 | best_loss 6.839
2020-10-12 03:01:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:01:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 34 @ 5746 updates, score 6.839) (writing took 2.8183549189998303 seconds)
2020-10-12 03:01:24 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 03:01:24 | INFO | train | epoch 034 | loss 5.944 | nll_loss 4.55 | ppl 23.43 | wps 17751.8 | ups 2.79 | wpb 6357.8 | bsz 263.3 | num_updates 5746 | lr 0.00016687 | gnorm 1.289 | clip 0 | train_wall 55 | wall 2016
2020-10-12 03:01:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 03:01:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 03:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:01:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000515
2020-10-12 03:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005408
2020-10-12 03:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 03:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.107035
2020-10-12 03:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112944
2020-10-12 03:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004278
2020-10-12 03:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 03:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103977
2020-10-12 03:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108753
2020-10-12 03:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:01:24 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 03:01:42 | INFO | train_inner | epoch 035:     54 / 169 loss=5.912, nll_loss=4.511, ppl=22.8, wps=16892.6, ups=2.69, wpb=6290.3, bsz=242, num_updates=5800, lr=0.000166091, gnorm=1.291, clip=0, train_wall=32, wall=2034
2020-10-12 03:02:15 | INFO | train_inner | epoch 035:    154 / 169 loss=5.864, nll_loss=4.454, ppl=21.91, wps=19164.2, ups=3, wpb=6390.6, bsz=271.6, num_updates=5900, lr=0.000164677, gnorm=1.344, clip=0, train_wall=33, wall=2067
2020-10-12 03:02:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000645
2020-10-12 03:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017304
2020-10-12 03:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013360
2020-10-12 03:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031646
2020-10-12 03:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000455
2020-10-12 03:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017037
2020-10-12 03:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013466
2020-10-12 03:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037724
2020-10-12 03:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:02:22 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.789 | nll_loss 5.359 | ppl 41.04 | wps 43493.1 | wpb 2341.5 | bsz 92.7 | num_updates 5915 | best_loss 6.789
2020-10-12 03:02:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:02:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 35 @ 5915 updates, score 6.789) (writing took 1.4991287470038515 seconds)
2020-10-12 03:02:23 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 03:02:23 | INFO | train | epoch 035 | loss 5.857 | nll_loss 4.447 | ppl 21.81 | wps 18227.2 | ups 2.87 | wpb 6357.8 | bsz 263.3 | num_updates 5915 | lr 0.000164468 | gnorm 1.331 | clip 0 | train_wall 55 | wall 2075
2020-10-12 03:02:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 03:02:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 03:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:02:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000509
2020-10-12 03:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005828
2020-10-12 03:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 03:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106091
2020-10-12 03:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112464
2020-10-12 03:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004360
2020-10-12 03:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-12 03:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102505
2020-10-12 03:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107374
2020-10-12 03:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:02:23 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 03:02:51 | INFO | train_inner | epoch 036:     85 / 169 loss=5.781, nll_loss=4.359, ppl=20.52, wps=17586.1, ups=2.77, wpb=6353.6, bsz=260.4, num_updates=6000, lr=0.000163299, gnorm=1.332, clip=0, train_wall=33, wall=2103
2020-10-12 03:03:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000662
2020-10-12 03:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:03:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016596
2020-10-12 03:03:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:03:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013305
2020-10-12 03:03:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030897
2020-10-12 03:03:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:03:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:03:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000452
2020-10-12 03:03:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:03:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016775
2020-10-12 03:03:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:03:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013125
2020-10-12 03:03:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030665
2020-10-12 03:03:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:03:21 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.779 | nll_loss 5.34 | ppl 40.52 | wps 43383.6 | wpb 2341.5 | bsz 92.7 | num_updates 6084 | best_loss 6.779
2020-10-12 03:03:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:03:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 36 @ 6084 updates, score 6.779) (writing took 1.5006019849970471 seconds)
2020-10-12 03:03:22 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 03:03:22 | INFO | train | epoch 036 | loss 5.758 | nll_loss 4.331 | ppl 20.13 | wps 18149.9 | ups 2.85 | wpb 6357.8 | bsz 263.3 | num_updates 6084 | lr 0.000162168 | gnorm 1.301 | clip 0 | train_wall 55 | wall 2134
2020-10-12 03:03:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 03:03:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 03:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:03:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000521
2020-10-12 03:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005940
2020-10-12 03:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000245
2020-10-12 03:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.108618
2020-10-12 03:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115193
2020-10-12 03:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004390
2020-10-12 03:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-12 03:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:03:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102720
2020-10-12 03:03:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107619
2020-10-12 03:03:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:03:23 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 03:03:28 | INFO | train_inner | epoch 037:     16 / 169 loss=5.726, nll_loss=4.293, ppl=19.6, wps=17583.3, ups=2.74, wpb=6406, bsz=275.4, num_updates=6100, lr=0.000161955, gnorm=1.274, clip=0, train_wall=33, wall=2139
2020-10-12 03:04:01 | INFO | train_inner | epoch 037:    116 / 169 loss=5.683, nll_loss=4.242, ppl=18.92, wps=19190.3, ups=3, wpb=6400.4, bsz=252.5, num_updates=6200, lr=0.000160644, gnorm=1.278, clip=0, train_wall=33, wall=2173
2020-10-12 03:04:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000586
2020-10-12 03:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017222
2020-10-12 03:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013227
2020-10-12 03:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031372
2020-10-12 03:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000454
2020-10-12 03:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016998
2020-10-12 03:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013243
2020-10-12 03:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031008
2020-10-12 03:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:04:20 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.749 | nll_loss 5.307 | ppl 39.6 | wps 42679.9 | wpb 2341.5 | bsz 92.7 | num_updates 6253 | best_loss 6.749
2020-10-12 03:04:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:04:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 37 @ 6253 updates, score 6.749) (writing took 1.4976562419906259 seconds)
2020-10-12 03:04:22 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 03:04:22 | INFO | train | epoch 037 | loss 5.666 | nll_loss 4.223 | ppl 18.68 | wps 18135.4 | ups 2.85 | wpb 6357.8 | bsz 263.3 | num_updates 6253 | lr 0.000159962 | gnorm 1.277 | clip 0 | train_wall 55 | wall 2193
2020-10-12 03:04:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 03:04:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 03:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:04:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000470
2020-10-12 03:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005769
2020-10-12 03:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 03:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105726
2020-10-12 03:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111999
2020-10-12 03:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004275
2020-10-12 03:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 03:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102974
2020-10-12 03:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107749
2020-10-12 03:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:04:22 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 03:04:37 | INFO | train_inner | epoch 038:     47 / 169 loss=5.605, nll_loss=4.152, ppl=17.78, wps=17603.1, ups=2.77, wpb=6360, bsz=273.3, num_updates=6300, lr=0.000159364, gnorm=1.286, clip=0, train_wall=33, wall=2209
2020-10-12 03:05:11 | INFO | train_inner | epoch 038:    147 / 169 loss=5.607, nll_loss=4.153, ppl=17.79, wps=19057.3, ups=3, wpb=6343.5, bsz=262.4, num_updates=6400, lr=0.000158114, gnorm=1.343, clip=0, train_wall=33, wall=2242
2020-10-12 03:05:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000588
2020-10-12 03:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017308
2020-10-12 03:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013293
2020-10-12 03:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031519
2020-10-12 03:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000460
2020-10-12 03:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017228
2020-10-12 03:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013201
2020-10-12 03:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031202
2020-10-12 03:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:05:19 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.718 | nll_loss 5.25 | ppl 38.06 | wps 43177.2 | wpb 2341.5 | bsz 92.7 | num_updates 6422 | best_loss 6.718
2020-10-12 03:05:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:05:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 38 @ 6422 updates, score 6.718) (writing took 1.8557233570027165 seconds)
2020-10-12 03:05:21 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 03:05:21 | INFO | train | epoch 038 | loss 5.591 | nll_loss 4.135 | ppl 17.57 | wps 18071 | ups 2.84 | wpb 6357.8 | bsz 263.3 | num_updates 6422 | lr 0.000157843 | gnorm 1.325 | clip 0 | train_wall 55 | wall 2253
2020-10-12 03:05:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 03:05:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 03:05:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:05:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000435
2020-10-12 03:05:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005307
2020-10-12 03:05:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:05:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 03:05:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:05:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105264
2020-10-12 03:05:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111084
2020-10-12 03:05:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:05:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:05:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004308
2020-10-12 03:05:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:05:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 03:05:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:05:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105233
2020-10-12 03:05:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110050
2020-10-12 03:05:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:05:21 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 03:05:47 | INFO | train_inner | epoch 039:     78 / 169 loss=5.535, nll_loss=4.069, ppl=16.79, wps=17467.7, ups=2.74, wpb=6372.3, bsz=250.9, num_updates=6500, lr=0.000156893, gnorm=1.254, clip=0, train_wall=33, wall=2279
2020-10-12 03:06:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000570
2020-10-12 03:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016992
2020-10-12 03:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013297
2020-10-12 03:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031187
2020-10-12 03:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000425
2020-10-12 03:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017165
2020-10-12 03:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013222
2020-10-12 03:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031125
2020-10-12 03:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:06:19 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.699 | nll_loss 5.243 | ppl 37.87 | wps 43171.1 | wpb 2341.5 | bsz 92.7 | num_updates 6591 | best_loss 6.699
2020-10-12 03:06:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:06:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 39 @ 6591 updates, score 6.699) (writing took 1.4955738179996843 seconds)
2020-10-12 03:06:20 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 03:06:20 | INFO | train | epoch 039 | loss 5.506 | nll_loss 4.036 | ppl 16.4 | wps 18174.3 | ups 2.86 | wpb 6357.8 | bsz 263.3 | num_updates 6591 | lr 0.000155806 | gnorm 1.292 | clip 0 | train_wall 55 | wall 2312
2020-10-12 03:06:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 03:06:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 03:06:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:06:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000515
2020-10-12 03:06:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005775
2020-10-12 03:06:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:06:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-12 03:06:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:06:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101608
2020-10-12 03:06:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107971
2020-10-12 03:06:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:06:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:06:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004294
2020-10-12 03:06:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:06:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 03:06:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:06:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102337
2020-10-12 03:06:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107140
2020-10-12 03:06:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:06:20 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 03:06:23 | INFO | train_inner | epoch 040:      9 / 169 loss=5.502, nll_loss=4.03, ppl=16.33, wps=17487.5, ups=2.76, wpb=6327.7, bsz=269.7, num_updates=6600, lr=0.0001557, gnorm=1.33, clip=0, train_wall=33, wall=2315
2020-10-12 03:06:57 | INFO | train_inner | epoch 040:    109 / 169 loss=5.441, nll_loss=3.958, ppl=15.54, wps=19409, ups=2.99, wpb=6481.8, bsz=271.5, num_updates=6700, lr=0.000154533, gnorm=1.393, clip=0, train_wall=33, wall=2348
2020-10-12 03:07:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000524
2020-10-12 03:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016751
2020-10-12 03:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013223
2020-10-12 03:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030823
2020-10-12 03:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000411
2020-10-12 03:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:07:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016917
2020-10-12 03:07:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:07:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013082
2020-10-12 03:07:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030716
2020-10-12 03:07:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:07:18 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.671 | nll_loss 5.192 | ppl 36.55 | wps 43832.3 | wpb 2341.5 | bsz 92.7 | num_updates 6760 | best_loss 6.671
2020-10-12 03:07:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:07:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/O2M/checkpoint_best.pt (epoch 40 @ 6760 updates, score 6.671) (writing took 1.4891512600006536 seconds)
2020-10-12 03:07:19 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 03:07:19 | INFO | train | epoch 040 | loss 5.445 | nll_loss 3.963 | ppl 15.6 | wps 18178.4 | ups 2.86 | wpb 6357.8 | bsz 263.3 | num_updates 6760 | lr 0.000153846 | gnorm 1.346 | clip 0 | train_wall 55 | wall 2371
2020-10-12 03:07:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 03:07:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 03:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:07:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000498
2020-10-12 03:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005862
2020-10-12 03:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000248
2020-10-12 03:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.110753
2020-10-12 03:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.117187
2020-10-12 03:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:07:19 | INFO | fairseq_cli.train | done training in 2370.8 seconds
