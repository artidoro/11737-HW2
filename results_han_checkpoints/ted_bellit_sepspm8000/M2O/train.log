2020-10-12 01:47:28 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_bellit_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='bel-eng,lit-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_bellit_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 01:47:28 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 01:47:28 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['bel', 'eng', 'lit']
2020-10-12 01:47:28 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bel] dictionary: 22083 types
2020-10-12 01:47:28 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 22083 types
2020-10-12 01:47:28 | INFO | fairseq.data.multilingual.multilingual_data_manager | [lit] dictionary: 22083 types
2020-10-12 01:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 01:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:47:28 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 01:47:28 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:bel-eng': 1, 'main:lit-eng': 1}
2020-10-12 01:47:28 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-12 01:47:28 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_bellit_sepspm8000/M2O/valid.bel-eng.bel
2020-10-12 01:47:28 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_bellit_sepspm8000/M2O/valid.bel-eng.eng
2020-10-12 01:47:28 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_bellit_sepspm8000/M2O/ valid bel-eng 248 examples
2020-10-12 01:47:28 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:lit-eng src_langtok: None; tgt_langtok: None
2020-10-12 01:47:28 | INFO | fairseq.data.data_utils | loaded 1791 examples from: fairseq/data-bin/ted_bellit_sepspm8000/M2O/valid.lit-eng.lit
2020-10-12 01:47:28 | INFO | fairseq.data.data_utils | loaded 1791 examples from: fairseq/data-bin/ted_bellit_sepspm8000/M2O/valid.lit-eng.eng
2020-10-12 01:47:28 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_bellit_sepspm8000/M2O/ valid lit-eng 1791 examples
2020-10-12 01:47:29 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22083, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22083, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=22083, bias=False)
  )
)
2020-10-12 01:47:29 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 01:47:29 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 01:47:29 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 01:47:29 | INFO | fairseq_cli.train | num. model params: 42849792 (num. trained: 42849792)
2020-10-12 01:47:31 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 01:47:31 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 01:47:31 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 01:47:31 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.732 GB ; name = GeForce RTX 2080 Ti                     
2020-10-12 01:47:31 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 01:47:31 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 01:47:31 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 01:47:31 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_last.pt
2020-10-12 01:47:31 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 01:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 01:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:47:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 01:47:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:bel-eng': 1, 'main:lit-eng': 1}
2020-10-12 01:47:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-12 01:47:31 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_bellit_sepspm8000/M2O/train.bel-eng.bel
2020-10-12 01:47:31 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_bellit_sepspm8000/M2O/train.bel-eng.eng
2020-10-12 01:47:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_bellit_sepspm8000/M2O/ train bel-eng 4509 examples
2020-10-12 01:47:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:lit-eng src_langtok: None; tgt_langtok: None
2020-10-12 01:47:31 | INFO | fairseq.data.data_utils | loaded 39992 examples from: fairseq/data-bin/ted_bellit_sepspm8000/M2O/train.lit-eng.lit
2020-10-12 01:47:31 | INFO | fairseq.data.data_utils | loaded 39992 examples from: fairseq/data-bin/ted_bellit_sepspm8000/M2O/train.lit-eng.eng
2020-10-12 01:47:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_bellit_sepspm8000/M2O/ train lit-eng 39992 examples
2020-10-12 01:47:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:bel-eng', 4509), ('main:lit-eng', 39992)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 01:47:31 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 01:47:31 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 44501
2020-10-12 01:47:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 44501; virtual dataset size 44501
2020-10-12 01:47:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 01:47:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:bel-eng': 4509, 'main:lit-eng': 39992}; raw total size: 44501
2020-10-12 01:47:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:bel-eng': 4509, 'main:lit-eng': 39992}; resampled total size: 44501
2020-10-12 01:47:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 01:47:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.003128
2020-10-12 01:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:47:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000442
2020-10-12 01:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005176
2020-10-12 01:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000188
2020-10-12 01:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097865
2020-10-12 01:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103603
2020-10-12 01:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:47:31 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 01:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004235
2020-10-12 01:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-12 01:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095204
2020-10-12 01:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099949
2020-10-12 01:47:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:47:31 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 01:48:03 | INFO | train_inner | epoch 001:    100 / 171 loss=14.045, nll_loss=13.92, ppl=15498.2, wps=21463.6, ups=3.16, wpb=6788.5, bsz=267.2, num_updates=100, lr=5.0975e-06, gnorm=4.931, clip=0, train_wall=31, wall=32
2020-10-12 01:48:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000543
2020-10-12 01:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016793
2020-10-12 01:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013022
2020-10-12 01:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030695
2020-10-12 01:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000443
2020-10-12 01:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016385
2020-10-12 01:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012894
2020-10-12 01:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030049
2020-10-12 01:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
/home/han/Documents/11737-hw/assign2/fairseq/fairseq/utils.py:340: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2020-10-12 01:48:27 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.786 | nll_loss 11.391 | ppl 2685.45 | wps 45198.6 | wpb 2298.7 | bsz 85 | num_updates 171
2020-10-12 01:48:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:48:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 1 @ 171 updates, score 11.786) (writing took 0.9516353319922928 seconds)
2020-10-12 01:48:28 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 01:48:28 | INFO | train | epoch 001 | loss 13.427 | nll_loss 13.231 | ppl 9611.98 | wps 20299.9 | ups 2.99 | wpb 6780.2 | bsz 260.2 | num_updates 171 | lr 8.64573e-06 | gnorm 3.833 | clip 0 | train_wall 54 | wall 58
2020-10-12 01:48:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 01:48:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 01:48:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:48:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000468
2020-10-12 01:48:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005323
2020-10-12 01:48:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:48:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 01:48:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:48:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094857
2020-10-12 01:48:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100686
2020-10-12 01:48:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004269
2020-10-12 01:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 01:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093281
2020-10-12 01:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098064
2020-10-12 01:48:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:48:29 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 01:48:38 | INFO | train_inner | epoch 002:     29 / 171 loss=12.404, nll_loss=12.089, ppl=4357.79, wps=19159.4, ups=2.84, wpb=6740.8, bsz=264.5, num_updates=200, lr=1.0095e-05, gnorm=2.179, clip=0, train_wall=32, wall=67
2020-10-12 01:49:11 | INFO | train_inner | epoch 002:    129 / 171 loss=11.541, nll_loss=11.128, ppl=2237.49, wps=20730.6, ups=3.02, wpb=6854.2, bsz=248.3, num_updates=300, lr=1.50925e-05, gnorm=1.735, clip=0, train_wall=33, wall=100
2020-10-12 01:49:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000554
2020-10-12 01:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016877
2020-10-12 01:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012588
2020-10-12 01:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030357
2020-10-12 01:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000454
2020-10-12 01:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016799
2020-10-12 01:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012559
2020-10-12 01:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030137
2020-10-12 01:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:49:26 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.187 | nll_loss 9.562 | ppl 755.83 | wps 44289.4 | wpb 2298.7 | bsz 85 | num_updates 342 | best_loss 10.187
2020-10-12 01:49:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:49:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 2 @ 342 updates, score 10.187) (writing took 1.4955572740000207 seconds)
2020-10-12 01:49:28 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 01:49:28 | INFO | train | epoch 002 | loss 11.444 | nll_loss 11.018 | ppl 2073.54 | wps 19552.4 | ups 2.88 | wpb 6780.2 | bsz 260.2 | num_updates 342 | lr 1.71914e-05 | gnorm 1.773 | clip 0 | train_wall 55 | wall 117
2020-10-12 01:49:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 01:49:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:49:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000508
2020-10-12 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005699
2020-10-12 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097088
2020-10-12 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103376
2020-10-12 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004210
2020-10-12 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094861
2020-10-12 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099558
2020-10-12 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:49:28 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 01:49:47 | INFO | train_inner | epoch 003:     58 / 171 loss=10.442, nll_loss=9.878, ppl=940.99, wps=18641.4, ups=2.79, wpb=6685.3, bsz=256.5, num_updates=400, lr=2.009e-05, gnorm=1.971, clip=0, train_wall=32, wall=136
2020-10-12 01:50:20 | INFO | train_inner | epoch 003:    158 / 171 loss=9.581, nll_loss=8.859, ppl=464.39, wps=20550.5, ups=3.02, wpb=6806.4, bsz=256.8, num_updates=500, lr=2.50875e-05, gnorm=1.395, clip=0, train_wall=33, wall=169
2020-10-12 01:50:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:50:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:50:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000560
2020-10-12 01:50:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:50:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016767
2020-10-12 01:50:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:50:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013074
2020-10-12 01:50:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030737
2020-10-12 01:50:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:50:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:50:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000449
2020-10-12 01:50:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:50:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016857
2020-10-12 01:50:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012743
2020-10-12 01:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030370
2020-10-12 01:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:50:26 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.047 | nll_loss 8.176 | ppl 289.24 | wps 44634.6 | wpb 2298.7 | bsz 85 | num_updates 513 | best_loss 9.047
2020-10-12 01:50:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:50:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 3 @ 513 updates, score 9.047) (writing took 1.5037569489941234 seconds)
2020-10-12 01:50:27 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 01:50:27 | INFO | train | epoch 003 | loss 9.774 | nll_loss 9.087 | ppl 543.99 | wps 19462.7 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 513 | lr 2.57372e-05 | gnorm 1.62 | clip 0 | train_wall 55 | wall 177
2020-10-12 01:50:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 01:50:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 01:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:50:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000623
2020-10-12 01:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007568
2020-10-12 01:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000203
2020-10-12 01:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100941
2020-10-12 01:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109098
2020-10-12 01:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004215
2020-10-12 01:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 01:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095644
2020-10-12 01:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100365
2020-10-12 01:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:50:27 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 01:50:56 | INFO | train_inner | epoch 004:     87 / 171 loss=9.264, nll_loss=8.456, ppl=351.11, wps=18815.2, ups=2.77, wpb=6784.3, bsz=263.1, num_updates=600, lr=3.0085e-05, gnorm=1.508, clip=0, train_wall=32, wall=206
2020-10-12 01:51:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000673
2020-10-12 01:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016980
2020-10-12 01:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012714
2020-10-12 01:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030704
2020-10-12 01:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000465
2020-10-12 01:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016627
2020-10-12 01:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012781
2020-10-12 01:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030202
2020-10-12 01:51:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:51:25 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.805 | nll_loss 7.846 | ppl 230.12 | wps 45458.1 | wpb 2298.7 | bsz 85 | num_updates 684 | best_loss 8.805
2020-10-12 01:51:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:51:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 4 @ 684 updates, score 8.805) (writing took 1.4996020359976683 seconds)
2020-10-12 01:51:27 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 01:51:27 | INFO | train | epoch 004 | loss 9.162 | nll_loss 8.326 | ppl 320.89 | wps 19474.6 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 684 | lr 3.42829e-05 | gnorm 1.49 | clip 0 | train_wall 55 | wall 236
2020-10-12 01:51:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 01:51:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 01:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:51:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000503
2020-10-12 01:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006746
2020-10-12 01:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 01:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096335
2020-10-12 01:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103606
2020-10-12 01:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004235
2020-10-12 01:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 01:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093461
2020-10-12 01:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098190
2020-10-12 01:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:51:27 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 01:51:32 | INFO | train_inner | epoch 005:     16 / 171 loss=9.089, nll_loss=8.231, ppl=300.45, wps=18782.5, ups=2.77, wpb=6791.5, bsz=262.5, num_updates=700, lr=3.50825e-05, gnorm=1.443, clip=0, train_wall=32, wall=242
2020-10-12 01:52:06 | INFO | train_inner | epoch 005:    116 / 171 loss=8.958, nll_loss=8.072, ppl=269.05, wps=20739.4, ups=2.99, wpb=6931.9, bsz=271, num_updates=800, lr=4.008e-05, gnorm=1.424, clip=0, train_wall=33, wall=275
2020-10-12 01:52:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000496
2020-10-12 01:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016931
2020-10-12 01:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012792
2020-10-12 01:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030555
2020-10-12 01:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000450
2020-10-12 01:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017004
2020-10-12 01:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012661
2020-10-12 01:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030440
2020-10-12 01:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:52:25 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.58 | nll_loss 7.568 | ppl 189.7 | wps 45210.3 | wpb 2298.7 | bsz 85 | num_updates 855 | best_loss 8.58
2020-10-12 01:52:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:52:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 5 @ 855 updates, score 8.58) (writing took 1.4933435790007934 seconds)
2020-10-12 01:52:26 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 01:52:26 | INFO | train | epoch 005 | loss 8.92 | nll_loss 8.027 | ppl 260.92 | wps 19458.8 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 855 | lr 4.28286e-05 | gnorm 1.585 | clip 0 | train_wall 56 | wall 296
2020-10-12 01:52:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 01:52:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 01:52:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:52:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000584
2020-10-12 01:52:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007401
2020-10-12 01:52:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:52:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000263
2020-10-12 01:52:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:52:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097797
2020-10-12 01:52:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105824
2020-10-12 01:52:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:52:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:52:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004202
2020-10-12 01:52:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:52:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 01:52:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:52:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092754
2020-10-12 01:52:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097457
2020-10-12 01:52:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:52:27 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 01:52:41 | INFO | train_inner | epoch 006:     45 / 171 loss=8.786, nll_loss=7.874, ppl=234.6, wps=18437.8, ups=2.81, wpb=6563.8, bsz=249.1, num_updates=900, lr=4.50775e-05, gnorm=1.67, clip=0, train_wall=32, wall=311
2020-10-12 01:53:15 | INFO | train_inner | epoch 006:    145 / 171 loss=8.66, nll_loss=7.731, ppl=212.52, wps=20518.8, ups=3.01, wpb=6813.4, bsz=253.5, num_updates=1000, lr=5.0075e-05, gnorm=1.286, clip=0, train_wall=33, wall=344
2020-10-12 01:53:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:53:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:53:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000564
2020-10-12 01:53:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:53:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017039
2020-10-12 01:53:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:53:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012915
2020-10-12 01:53:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030855
2020-10-12 01:53:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:53:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:53:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000446
2020-10-12 01:53:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:53:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016428
2020-10-12 01:53:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:53:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012573
2020-10-12 01:53:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.029770
2020-10-12 01:53:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:53:24 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.274 | nll_loss 7.233 | ppl 150.4 | wps 46443 | wpb 2298.7 | bsz 85 | num_updates 1026 | best_loss 8.274
2020-10-12 01:53:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:53:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 6 @ 1026 updates, score 8.274) (writing took 1.5012639970082091 seconds)
2020-10-12 01:53:26 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 01:53:26 | INFO | train | epoch 006 | loss 8.659 | nll_loss 7.73 | ppl 212.24 | wps 19457.2 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 1026 | lr 5.13744e-05 | gnorm 1.379 | clip 0 | train_wall 56 | wall 355
2020-10-12 01:53:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 01:53:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 01:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:53:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000662
2020-10-12 01:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007184
2020-10-12 01:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 01:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096150
2020-10-12 01:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103885
2020-10-12 01:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004180
2020-10-12 01:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 01:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093109
2020-10-12 01:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097772
2020-10-12 01:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:53:26 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 01:53:51 | INFO | train_inner | epoch 007:     74 / 171 loss=8.45, nll_loss=7.492, ppl=180.06, wps=18984, ups=2.76, wpb=6882, bsz=281.7, num_updates=1100, lr=5.50725e-05, gnorm=1.401, clip=0, train_wall=33, wall=380
2020-10-12 01:54:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000548
2020-10-12 01:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016869
2020-10-12 01:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012589
2020-10-12 01:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030332
2020-10-12 01:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000474
2020-10-12 01:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016649
2020-10-12 01:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012507
2020-10-12 01:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.029956
2020-10-12 01:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:54:24 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.022 | nll_loss 6.957 | ppl 124.23 | wps 44175.5 | wpb 2298.7 | bsz 85 | num_updates 1197 | best_loss 8.022
2020-10-12 01:54:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:54:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 7 @ 1197 updates, score 8.022) (writing took 1.5091814120096387 seconds)
2020-10-12 01:54:26 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 01:54:26 | INFO | train | epoch 007 | loss 8.413 | nll_loss 7.449 | ppl 174.7 | wps 19419.8 | ups 2.86 | wpb 6780.2 | bsz 260.2 | num_updates 1197 | lr 5.99201e-05 | gnorm 1.411 | clip 0 | train_wall 56 | wall 415
2020-10-12 01:54:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 01:54:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 01:54:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:54:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000723
2020-10-12 01:54:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007863
2020-10-12 01:54:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:54:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000210
2020-10-12 01:54:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:54:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099304
2020-10-12 01:54:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107871
2020-10-12 01:54:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:54:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:54:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004246
2020-10-12 01:54:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:54:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 01:54:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:54:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094017
2020-10-12 01:54:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098786
2020-10-12 01:54:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:54:26 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 01:54:27 | INFO | train_inner | epoch 008:      3 / 171 loss=8.387, nll_loss=7.418, ppl=170.99, wps=18665.4, ups=2.78, wpb=6726, bsz=246.4, num_updates=1200, lr=6.007e-05, gnorm=1.482, clip=0, train_wall=32, wall=416
2020-10-12 01:55:00 | INFO | train_inner | epoch 008:    103 / 171 loss=8.244, nll_loss=7.257, ppl=152.94, wps=20213.4, ups=3.02, wpb=6692.2, bsz=252.9, num_updates=1300, lr=6.50675e-05, gnorm=1.445, clip=0, train_wall=33, wall=449
2020-10-12 01:55:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:55:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:55:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000550
2020-10-12 01:55:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:55:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016978
2020-10-12 01:55:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:55:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012590
2020-10-12 01:55:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030443
2020-10-12 01:55:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:55:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:55:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000457
2020-10-12 01:55:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:55:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016786
2020-10-12 01:55:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:55:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012418
2020-10-12 01:55:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.029975
2020-10-12 01:55:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:55:24 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.866 | nll_loss 6.772 | ppl 109.3 | wps 44795.9 | wpb 2298.7 | bsz 85 | num_updates 1368 | best_loss 7.866
2020-10-12 01:55:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:55:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 8 @ 1368 updates, score 7.866) (writing took 1.5197840009932406 seconds)
2020-10-12 01:55:25 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 01:55:25 | INFO | train | epoch 008 | loss 8.199 | nll_loss 7.205 | ppl 147.52 | wps 19446.1 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 1368 | lr 6.84658e-05 | gnorm 1.431 | clip 0 | train_wall 56 | wall 475
2020-10-12 01:55:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 01:55:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 01:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:55:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000432
2020-10-12 01:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005520
2020-10-12 01:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 01:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102080
2020-10-12 01:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108123
2020-10-12 01:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004214
2020-10-12 01:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 01:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093889
2020-10-12 01:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098598
2020-10-12 01:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:55:26 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 01:55:36 | INFO | train_inner | epoch 009:     32 / 171 loss=8.123, nll_loss=7.118, ppl=138.91, wps=18794.5, ups=2.77, wpb=6778.4, bsz=262.1, num_updates=1400, lr=7.0065e-05, gnorm=1.353, clip=0, train_wall=32, wall=485
2020-10-12 01:56:09 | INFO | train_inner | epoch 009:    132 / 171 loss=7.988, nll_loss=6.963, ppl=124.76, wps=20692.1, ups=3.01, wpb=6876.3, bsz=265.1, num_updates=1500, lr=7.50625e-05, gnorm=1.389, clip=0, train_wall=33, wall=519
2020-10-12 01:56:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000558
2020-10-12 01:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017161
2020-10-12 01:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013128
2020-10-12 01:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031181
2020-10-12 01:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000466
2020-10-12 01:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016723
2020-10-12 01:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012879
2020-10-12 01:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030387
2020-10-12 01:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:56:23 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.689 | nll_loss 6.572 | ppl 95.12 | wps 44521.3 | wpb 2298.7 | bsz 85 | num_updates 1539 | best_loss 7.689
2020-10-12 01:56:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:56:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 9 @ 1539 updates, score 7.689) (writing took 1.4909770769882016 seconds)
2020-10-12 01:56:25 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 01:56:25 | INFO | train | epoch 009 | loss 8.018 | nll_loss 6.998 | ppl 127.78 | wps 19428.3 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 1539 | lr 7.70115e-05 | gnorm 1.376 | clip 0 | train_wall 56 | wall 534
2020-10-12 01:56:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 01:56:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 01:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:56:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000643
2020-10-12 01:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006648
2020-10-12 01:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 01:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098517
2020-10-12 01:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105788
2020-10-12 01:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004237
2020-10-12 01:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 01:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092371
2020-10-12 01:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097107
2020-10-12 01:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:56:25 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 01:56:45 | INFO | train_inner | epoch 010:     61 / 171 loss=7.938, nll_loss=6.906, ppl=119.91, wps=18460, ups=2.77, wpb=6660, bsz=269.1, num_updates=1600, lr=8.006e-05, gnorm=1.4, clip=0, train_wall=32, wall=555
2020-10-12 01:57:19 | INFO | train_inner | epoch 010:    161 / 171 loss=7.856, nll_loss=6.81, ppl=112.2, wps=20795.1, ups=3, wpb=6934.6, bsz=260.8, num_updates=1700, lr=8.50575e-05, gnorm=1.442, clip=0, train_wall=33, wall=588
2020-10-12 01:57:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000543
2020-10-12 01:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016947
2020-10-12 01:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012715
2020-10-12 01:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030536
2020-10-12 01:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000452
2020-10-12 01:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016657
2020-10-12 01:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012431
2020-10-12 01:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.029859
2020-10-12 01:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:57:23 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.54 | nll_loss 6.395 | ppl 84.14 | wps 44344.9 | wpb 2298.7 | bsz 85 | num_updates 1710 | best_loss 7.54
2020-10-12 01:57:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:57:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 10 @ 1710 updates, score 7.54) (writing took 1.5175797080009943 seconds)
2020-10-12 01:57:25 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 01:57:25 | INFO | train | epoch 010 | loss 7.867 | nll_loss 6.823 | ppl 113.25 | wps 19423.4 | ups 2.86 | wpb 6780.2 | bsz 260.2 | num_updates 1710 | lr 8.55573e-05 | gnorm 1.401 | clip 0 | train_wall 56 | wall 594
2020-10-12 01:57:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 01:57:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 01:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:57:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001114
2020-10-12 01:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.010691
2020-10-12 01:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000290
2020-10-12 01:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097853
2020-10-12 01:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109367
2020-10-12 01:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004210
2020-10-12 01:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 01:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094329
2020-10-12 01:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099036
2020-10-12 01:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:57:25 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 01:57:55 | INFO | train_inner | epoch 011:     90 / 171 loss=7.814, nll_loss=6.764, ppl=108.66, wps=18808.4, ups=2.75, wpb=6840.1, bsz=266.3, num_updates=1800, lr=9.0055e-05, gnorm=1.314, clip=0, train_wall=33, wall=624
2020-10-12 01:58:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000558
2020-10-12 01:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016404
2020-10-12 01:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012881
2020-10-12 01:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030171
2020-10-12 01:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000459
2020-10-12 01:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016628
2020-10-12 01:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012535
2020-10-12 01:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.029938
2020-10-12 01:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:58:23 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.452 | nll_loss 6.305 | ppl 79.09 | wps 44161.5 | wpb 2298.7 | bsz 85 | num_updates 1881 | best_loss 7.452
2020-10-12 01:58:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:58:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 11 @ 1881 updates, score 7.452) (writing took 1.4986186070018448 seconds)
2020-10-12 01:58:24 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 01:58:24 | INFO | train | epoch 011 | loss 7.731 | nll_loss 6.668 | ppl 101.66 | wps 19430.8 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 1881 | lr 9.4103e-05 | gnorm 1.327 | clip 0 | train_wall 56 | wall 654
2020-10-12 01:58:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 01:58:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 01:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:58:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000767
2020-10-12 01:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007674
2020-10-12 01:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000203
2020-10-12 01:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096308
2020-10-12 01:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104572
2020-10-12 01:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004181
2020-10-12 01:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 01:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:58:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094059
2020-10-12 01:58:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098730
2020-10-12 01:58:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:58:25 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 01:58:31 | INFO | train_inner | epoch 012:     19 / 171 loss=7.661, nll_loss=6.587, ppl=96.14, wps=18672.5, ups=2.79, wpb=6686.4, bsz=248.8, num_updates=1900, lr=9.50525e-05, gnorm=1.327, clip=0, train_wall=32, wall=660
2020-10-12 01:59:04 | INFO | train_inner | epoch 012:    119 / 171 loss=7.609, nll_loss=6.528, ppl=92.3, wps=20483.4, ups=3.02, wpb=6782.8, bsz=259.9, num_updates=2000, lr=0.00010005, gnorm=1.179, clip=0, train_wall=33, wall=693
2020-10-12 01:59:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 01:59:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:59:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000555
2020-10-12 01:59:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:59:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016531
2020-10-12 01:59:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:59:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012777
2020-10-12 01:59:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030188
2020-10-12 01:59:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:59:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:59:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000446
2020-10-12 01:59:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:59:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016818
2020-10-12 01:59:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:59:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012561
2020-10-12 01:59:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030136
2020-10-12 01:59:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:59:22 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.378 | nll_loss 6.214 | ppl 74.25 | wps 44655.4 | wpb 2298.7 | bsz 85 | num_updates 2052 | best_loss 7.378
2020-10-12 01:59:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 01:59:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 12 @ 2052 updates, score 7.378) (writing took 1.494999539994751 seconds)
2020-10-12 01:59:24 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 01:59:24 | INFO | train | epoch 012 | loss 7.608 | nll_loss 6.527 | ppl 92.2 | wps 19457.5 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 2052 | lr 0.000102649 | gnorm 1.27 | clip 0 | train_wall 55 | wall 713
2020-10-12 01:59:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 01:59:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 01:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:59:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000682
2020-10-12 01:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006263
2020-10-12 01:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 01:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099757
2020-10-12 01:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106596
2020-10-12 01:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 01:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004199
2020-10-12 01:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 01:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094619
2020-10-12 01:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099309
2020-10-12 01:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 01:59:24 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 01:59:40 | INFO | train_inner | epoch 013:     48 / 171 loss=7.551, nll_loss=6.461, ppl=88.11, wps=18848.6, ups=2.76, wpb=6833.7, bsz=255.8, num_updates=2100, lr=0.000105048, gnorm=1.316, clip=0, train_wall=33, wall=730
2020-10-12 02:00:14 | INFO | train_inner | epoch 013:    148 / 171 loss=7.5, nll_loss=6.403, ppl=84.63, wps=20440.6, ups=3, wpb=6814.2, bsz=267.4, num_updates=2200, lr=0.000110045, gnorm=1.201, clip=0, train_wall=33, wall=763
2020-10-12 02:00:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000557
2020-10-12 02:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016627
2020-10-12 02:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012503
2020-10-12 02:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030018
2020-10-12 02:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000503
2020-10-12 02:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016685
2020-10-12 02:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012464
2020-10-12 02:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.029968
2020-10-12 02:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:00:22 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.276 | nll_loss 6.099 | ppl 68.53 | wps 44439.9 | wpb 2298.7 | bsz 85 | num_updates 2223 | best_loss 7.276
2020-10-12 02:00:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:00:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 13 @ 2223 updates, score 7.276) (writing took 1.4965526570013026 seconds)
2020-10-12 02:00:24 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 02:00:24 | INFO | train | epoch 013 | loss 7.491 | nll_loss 6.392 | ppl 83.98 | wps 19418.9 | ups 2.86 | wpb 6780.2 | bsz 260.2 | num_updates 2223 | lr 0.000111194 | gnorm 1.213 | clip 0 | train_wall 56 | wall 773
2020-10-12 02:00:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 02:00:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 02:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:00:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000802
2020-10-12 02:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007655
2020-10-12 02:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000267
2020-10-12 02:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097762
2020-10-12 02:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106069
2020-10-12 02:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004258
2020-10-12 02:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 02:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094124
2020-10-12 02:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098874
2020-10-12 02:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:00:24 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 02:00:49 | INFO | train_inner | epoch 014:     77 / 171 loss=7.363, nll_loss=6.245, ppl=75.84, wps=18713.9, ups=2.81, wpb=6666.6, bsz=269.8, num_updates=2300, lr=0.000115043, gnorm=1.282, clip=0, train_wall=32, wall=799
2020-10-12 02:01:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000494
2020-10-12 02:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016593
2020-10-12 02:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012227
2020-10-12 02:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.029637
2020-10-12 02:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000416
2020-10-12 02:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016454
2020-10-12 02:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012324
2020-10-12 02:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.029509
2020-10-12 02:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:01:22 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.171 | nll_loss 5.968 | ppl 62.6 | wps 44890.4 | wpb 2298.7 | bsz 85 | num_updates 2394 | best_loss 7.171
2020-10-12 02:01:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:01:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 14 @ 2394 updates, score 7.171) (writing took 1.5386063540063333 seconds)
2020-10-12 02:01:23 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 02:01:23 | INFO | train | epoch 014 | loss 7.387 | nll_loss 6.272 | ppl 77.3 | wps 19490.8 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 2394 | lr 0.00011974 | gnorm 1.249 | clip 0 | train_wall 55 | wall 832
2020-10-12 02:01:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 02:01:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 02:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:01:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000543
2020-10-12 02:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005936
2020-10-12 02:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 02:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096384
2020-10-12 02:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102914
2020-10-12 02:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004239
2020-10-12 02:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 02:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091511
2020-10-12 02:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096238
2020-10-12 02:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:01:23 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 02:01:25 | INFO | train_inner | epoch 015:      6 / 171 loss=7.403, nll_loss=6.29, ppl=78.25, wps=18750.3, ups=2.77, wpb=6760.8, bsz=241.7, num_updates=2400, lr=0.00012004, gnorm=1.22, clip=0, train_wall=32, wall=835
2020-10-12 02:01:59 | INFO | train_inner | epoch 015:    106 / 171 loss=7.312, nll_loss=6.188, ppl=72.9, wps=20626.4, ups=3.01, wpb=6853.1, bsz=262.6, num_updates=2500, lr=0.000125037, gnorm=1.233, clip=0, train_wall=33, wall=868
2020-10-12 02:02:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000569
2020-10-12 02:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016906
2020-10-12 02:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012751
2020-10-12 02:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030558
2020-10-12 02:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000445
2020-10-12 02:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016238
2020-10-12 02:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012592
2020-10-12 02:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.029595
2020-10-12 02:02:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:02:21 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.109 | nll_loss 5.902 | ppl 59.8 | wps 44205 | wpb 2298.7 | bsz 85 | num_updates 2565 | best_loss 7.109
2020-10-12 02:02:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:02:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 15 @ 2565 updates, score 7.109) (writing took 1.4940833740110975 seconds)
2020-10-12 02:02:23 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 02:02:23 | INFO | train | epoch 015 | loss 7.284 | nll_loss 6.155 | ppl 71.24 | wps 19473 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 2565 | lr 0.000128286 | gnorm 1.245 | clip 0 | train_wall 55 | wall 892
2020-10-12 02:02:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 02:02:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 02:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:02:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000523
2020-10-12 02:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005899
2020-10-12 02:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000257
2020-10-12 02:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097246
2020-10-12 02:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103743
2020-10-12 02:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004237
2020-10-12 02:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 02:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091868
2020-10-12 02:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096603
2020-10-12 02:02:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:02:23 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 02:02:34 | INFO | train_inner | epoch 016:     35 / 171 loss=7.14, nll_loss=5.989, ppl=63.53, wps=18826.6, ups=2.8, wpb=6733.1, bsz=267.8, num_updates=2600, lr=0.000130035, gnorm=1.238, clip=0, train_wall=32, wall=904
2020-10-12 02:03:08 | INFO | train_inner | epoch 016:    135 / 171 loss=7.207, nll_loss=6.065, ppl=66.95, wps=20705.9, ups=2.98, wpb=6937.8, bsz=268.8, num_updates=2700, lr=0.000135032, gnorm=1.166, clip=0, train_wall=33, wall=937
2020-10-12 02:03:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000560
2020-10-12 02:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017048
2020-10-12 02:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012844
2020-10-12 02:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030783
2020-10-12 02:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000459
2020-10-12 02:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016846
2020-10-12 02:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012626
2020-10-12 02:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030249
2020-10-12 02:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:03:21 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.01 | nll_loss 5.776 | ppl 54.8 | wps 44457.5 | wpb 2298.7 | bsz 85 | num_updates 2736 | best_loss 7.01
2020-10-12 02:03:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:03:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 16 @ 2736 updates, score 7.01) (writing took 1.5082410140021238 seconds)
2020-10-12 02:03:22 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 02:03:22 | INFO | train | epoch 016 | loss 7.157 | nll_loss 6.008 | ppl 64.38 | wps 19451.3 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 2736 | lr 0.000136832 | gnorm 1.141 | clip 0 | train_wall 56 | wall 952
2020-10-12 02:03:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 02:03:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 02:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:03:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000569
2020-10-12 02:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006425
2020-10-12 02:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000249
2020-10-12 02:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100995
2020-10-12 02:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108016
2020-10-12 02:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004259
2020-10-12 02:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 02:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092528
2020-10-12 02:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097282
2020-10-12 02:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:03:22 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 02:03:44 | INFO | train_inner | epoch 017:     64 / 171 loss=7.104, nll_loss=5.948, ppl=61.73, wps=18410.5, ups=2.76, wpb=6664.3, bsz=261.5, num_updates=2800, lr=0.00014003, gnorm=1.179, clip=0, train_wall=33, wall=973
2020-10-12 02:04:17 | INFO | train_inner | epoch 017:    164 / 171 loss=7.045, nll_loss=5.879, ppl=58.83, wps=20638.5, ups=3.03, wpb=6807.4, bsz=248.9, num_updates=2900, lr=0.000145028, gnorm=1.169, clip=0, train_wall=32, wall=1006
2020-10-12 02:04:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000549
2020-10-12 02:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016847
2020-10-12 02:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012918
2020-10-12 02:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030657
2020-10-12 02:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000459
2020-10-12 02:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016800
2020-10-12 02:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012565
2020-10-12 02:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030143
2020-10-12 02:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:04:20 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.919 | nll_loss 5.665 | ppl 50.73 | wps 44403.9 | wpb 2298.7 | bsz 85 | num_updates 2907 | best_loss 6.919
2020-10-12 02:04:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:04:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 17 @ 2907 updates, score 6.919) (writing took 1.49593876900326 seconds)
2020-10-12 02:04:22 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 02:04:22 | INFO | train | epoch 017 | loss 7.046 | nll_loss 5.88 | ppl 58.9 | wps 19439.8 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 2907 | lr 0.000145377 | gnorm 1.19 | clip 0 | train_wall 56 | wall 1011
2020-10-12 02:04:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 02:04:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 02:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:04:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000564
2020-10-12 02:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006362
2020-10-12 02:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000247
2020-10-12 02:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097589
2020-10-12 02:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104542
2020-10-12 02:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007807
2020-10-12 02:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 02:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093500
2020-10-12 02:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101828
2020-10-12 02:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:04:22 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 02:04:53 | INFO | train_inner | epoch 018:     93 / 171 loss=6.905, nll_loss=5.72, ppl=52.72, wps=18728.3, ups=2.78, wpb=6740.2, bsz=265.4, num_updates=3000, lr=0.000150025, gnorm=1.138, clip=0, train_wall=32, wall=1042
2020-10-12 02:05:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000545
2020-10-12 02:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017049
2020-10-12 02:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012676
2020-10-12 02:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030600
2020-10-12 02:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000462
2020-10-12 02:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016452
2020-10-12 02:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012424
2020-10-12 02:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.029651
2020-10-12 02:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:05:20 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.867 | nll_loss 5.614 | ppl 48.98 | wps 44281.3 | wpb 2298.7 | bsz 85 | num_updates 3078 | best_loss 6.867
2020-10-12 02:05:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:05:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 18 @ 3078 updates, score 6.867) (writing took 1.5190465269988636 seconds)
2020-10-12 02:05:22 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 02:05:22 | INFO | train | epoch 018 | loss 6.921 | nll_loss 5.737 | ppl 53.34 | wps 19446.8 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 3078 | lr 0.000153923 | gnorm 1.187 | clip 0 | train_wall 56 | wall 1071
2020-10-12 02:05:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 02:05:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 02:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:05:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000712
2020-10-12 02:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007018
2020-10-12 02:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000212
2020-10-12 02:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095317
2020-10-12 02:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102948
2020-10-12 02:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004275
2020-10-12 02:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-12 02:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093357
2020-10-12 02:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098123
2020-10-12 02:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:05:22 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 02:05:29 | INFO | train_inner | epoch 019:     22 / 171 loss=6.903, nll_loss=5.715, ppl=52.54, wps=18858.4, ups=2.78, wpb=6788.1, bsz=253.2, num_updates=3100, lr=0.000155023, gnorm=1.209, clip=0, train_wall=32, wall=1078
2020-10-12 02:06:02 | INFO | train_inner | epoch 019:    122 / 171 loss=6.798, nll_loss=5.596, ppl=48.37, wps=20303.2, ups=3.02, wpb=6724.4, bsz=266.6, num_updates=3200, lr=0.00016002, gnorm=1.179, clip=0, train_wall=33, wall=1111
2020-10-12 02:06:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000561
2020-10-12 02:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016974
2020-10-12 02:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012854
2020-10-12 02:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030721
2020-10-12 02:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000461
2020-10-12 02:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016437
2020-10-12 02:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012975
2020-10-12 02:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030192
2020-10-12 02:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:06:20 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.785 | nll_loss 5.508 | ppl 45.52 | wps 44981.7 | wpb 2298.7 | bsz 85 | num_updates 3249 | best_loss 6.785
2020-10-12 02:06:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:06:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 19 @ 3249 updates, score 6.785) (writing took 1.8456919439922785 seconds)
2020-10-12 02:06:21 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 02:06:21 | INFO | train | epoch 019 | loss 6.805 | nll_loss 5.604 | ppl 48.63 | wps 19330.5 | ups 2.85 | wpb 6780.2 | bsz 260.2 | num_updates 3249 | lr 0.000162469 | gnorm 1.168 | clip 0 | train_wall 56 | wall 1131
2020-10-12 02:06:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 02:06:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 02:06:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:06:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000907
2020-10-12 02:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008719
2020-10-12 02:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000209
2020-10-12 02:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095533
2020-10-12 02:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104949
2020-10-12 02:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004302
2020-10-12 02:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 02:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094194
2020-10-12 02:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098999
2020-10-12 02:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:06:22 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 02:06:39 | INFO | train_inner | epoch 020:     51 / 171 loss=6.815, nll_loss=5.614, ppl=48.98, wps=18468.4, ups=2.74, wpb=6741.4, bsz=232, num_updates=3300, lr=0.000165018, gnorm=1.191, clip=0, train_wall=33, wall=1148
2020-10-12 02:07:12 | INFO | train_inner | epoch 020:    151 / 171 loss=6.647, nll_loss=5.422, ppl=42.88, wps=20558.3, ups=3.03, wpb=6790, bsz=270, num_updates=3400, lr=0.000170015, gnorm=1.238, clip=0, train_wall=32, wall=1181
2020-10-12 02:07:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000559
2020-10-12 02:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016782
2020-10-12 02:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013149
2020-10-12 02:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030830
2020-10-12 02:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000475
2020-10-12 02:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017079
2020-10-12 02:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013037
2020-10-12 02:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030918
2020-10-12 02:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:07:20 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.639 | nll_loss 5.342 | ppl 40.57 | wps 45027.5 | wpb 2298.7 | bsz 85 | num_updates 3420 | best_loss 6.639
2020-10-12 02:07:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:07:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 20 @ 3420 updates, score 6.639) (writing took 1.5019207430013921 seconds)
2020-10-12 02:07:21 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 02:07:21 | INFO | train | epoch 020 | loss 6.69 | nll_loss 5.471 | ppl 44.36 | wps 19460.9 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 3420 | lr 0.000171015 | gnorm 1.227 | clip 0 | train_wall 55 | wall 1190
2020-10-12 02:07:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 02:07:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 02:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:07:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000652
2020-10-12 02:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007706
2020-10-12 02:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000233
2020-10-12 02:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096693
2020-10-12 02:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105165
2020-10-12 02:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004328
2020-10-12 02:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 02:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095595
2020-10-12 02:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100431
2020-10-12 02:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:07:21 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 02:07:48 | INFO | train_inner | epoch 021:     80 / 171 loss=6.578, nll_loss=5.343, ppl=40.6, wps=18861.4, ups=2.76, wpb=6832.3, bsz=264.2, num_updates=3500, lr=0.000175013, gnorm=1.217, clip=0, train_wall=33, wall=1217
2020-10-12 02:08:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000558
2020-10-12 02:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016504
2020-10-12 02:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012582
2020-10-12 02:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.029971
2020-10-12 02:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000450
2020-10-12 02:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016613
2020-10-12 02:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012498
2020-10-12 02:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.029874
2020-10-12 02:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:08:19 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.588 | nll_loss 5.28 | ppl 38.85 | wps 44615.3 | wpb 2298.7 | bsz 85 | num_updates 3591 | best_loss 6.588
2020-10-12 02:08:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:08:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 21 @ 3591 updates, score 6.588) (writing took 1.4927039689937374 seconds)
2020-10-12 02:08:21 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 02:08:21 | INFO | train | epoch 021 | loss 6.563 | nll_loss 5.325 | ppl 40.09 | wps 19431.7 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 3591 | lr 0.00017956 | gnorm 1.223 | clip 0 | train_wall 56 | wall 1250
2020-10-12 02:08:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 02:08:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 02:08:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:08:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000624
2020-10-12 02:08:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006908
2020-10-12 02:08:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:08:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000190
2020-10-12 02:08:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:08:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100745
2020-10-12 02:08:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108314
2020-10-12 02:08:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:08:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:08:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004224
2020-10-12 02:08:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:08:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 02:08:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:08:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094295
2020-10-12 02:08:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099018
2020-10-12 02:08:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:08:21 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 02:08:24 | INFO | train_inner | epoch 022:      9 / 171 loss=6.564, nll_loss=5.325, ppl=40.09, wps=18768.2, ups=2.76, wpb=6793.2, bsz=265.3, num_updates=3600, lr=0.00018001, gnorm=1.229, clip=0, train_wall=33, wall=1253
2020-10-12 02:08:57 | INFO | train_inner | epoch 022:    109 / 171 loss=6.453, nll_loss=5.199, ppl=36.72, wps=20445.6, ups=2.99, wpb=6826.8, bsz=263.9, num_updates=3700, lr=0.000185008, gnorm=1.193, clip=0, train_wall=33, wall=1287
2020-10-12 02:09:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000581
2020-10-12 02:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016935
2020-10-12 02:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012804
2020-10-12 02:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030657
2020-10-12 02:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000471
2020-10-12 02:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016824
2020-10-12 02:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012589
2020-10-12 02:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030203
2020-10-12 02:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:09:19 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.514 | nll_loss 5.172 | ppl 36.06 | wps 45314.5 | wpb 2298.7 | bsz 85 | num_updates 3762 | best_loss 6.514
2020-10-12 02:09:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:09:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 22 @ 3762 updates, score 6.514) (writing took 1.7119220969907474 seconds)
2020-10-12 02:09:21 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 02:09:21 | INFO | train | epoch 022 | loss 6.441 | nll_loss 5.184 | ppl 36.35 | wps 19378 | ups 2.86 | wpb 6780.2 | bsz 260.2 | num_updates 3762 | lr 0.000188106 | gnorm 1.228 | clip 0 | train_wall 56 | wall 1310
2020-10-12 02:09:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 02:09:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 02:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:09:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000733
2020-10-12 02:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008286
2020-10-12 02:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000301
2020-10-12 02:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096903
2020-10-12 02:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105911
2020-10-12 02:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004204
2020-10-12 02:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000164
2020-10-12 02:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091074
2020-10-12 02:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095764
2020-10-12 02:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:09:21 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 02:09:33 | INFO | train_inner | epoch 023:     38 / 171 loss=6.386, nll_loss=5.121, ppl=34.8, wps=18780.1, ups=2.79, wpb=6739.5, bsz=253.2, num_updates=3800, lr=0.000190005, gnorm=1.278, clip=0, train_wall=32, wall=1323
2020-10-12 02:10:07 | INFO | train_inner | epoch 023:    138 / 171 loss=6.289, nll_loss=5.009, ppl=32.21, wps=20499, ups=2.99, wpb=6853.1, bsz=276.6, num_updates=3900, lr=0.000195003, gnorm=1.176, clip=0, train_wall=33, wall=1356
2020-10-12 02:10:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:10:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:10:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000499
2020-10-12 02:10:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:10:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016880
2020-10-12 02:10:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:10:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012767
2020-10-12 02:10:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030480
2020-10-12 02:10:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:10:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:10:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000425
2020-10-12 02:10:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:10:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016372
2020-10-12 02:10:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:10:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012899
2020-10-12 02:10:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030015
2020-10-12 02:10:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:10:19 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.393 | nll_loss 5.038 | ppl 32.85 | wps 44442.4 | wpb 2298.7 | bsz 85 | num_updates 3933 | best_loss 6.393
2020-10-12 02:10:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:10:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 23 @ 3933 updates, score 6.393) (writing took 1.4982455689896597 seconds)
2020-10-12 02:10:20 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 02:10:20 | INFO | train | epoch 023 | loss 6.308 | nll_loss 5.031 | ppl 32.69 | wps 19431.4 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 3933 | lr 0.000196652 | gnorm 1.201 | clip 0 | train_wall 56 | wall 1370
2020-10-12 02:10:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 02:10:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 02:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:10:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000703
2020-10-12 02:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007019
2020-10-12 02:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000268
2020-10-12 02:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098285
2020-10-12 02:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105926
2020-10-12 02:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004279
2020-10-12 02:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 02:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094549
2020-10-12 02:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099335
2020-10-12 02:10:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:10:20 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 02:10:43 | INFO | train_inner | epoch 024:     67 / 171 loss=6.238, nll_loss=4.95, ppl=30.92, wps=18785.6, ups=2.78, wpb=6755.9, bsz=243.3, num_updates=4000, lr=0.0002, gnorm=1.232, clip=0, train_wall=32, wall=1392
2020-10-12 02:11:16 | INFO | train_inner | epoch 024:    167 / 171 loss=6.137, nll_loss=4.833, ppl=28.51, wps=20341.9, ups=3.02, wpb=6728.8, bsz=269.1, num_updates=4100, lr=0.000197546, gnorm=1.203, clip=0, train_wall=33, wall=1425
2020-10-12 02:11:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:11:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:11:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000565
2020-10-12 02:11:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:11:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017090
2020-10-12 02:11:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:11:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012540
2020-10-12 02:11:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030521
2020-10-12 02:11:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:11:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:11:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000448
2020-10-12 02:11:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:11:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016441
2020-10-12 02:11:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:11:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012487
2020-10-12 02:11:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.029688
2020-10-12 02:11:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:11:18 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.352 | nll_loss 4.956 | ppl 31.04 | wps 44651.6 | wpb 2298.7 | bsz 85 | num_updates 4104 | best_loss 6.352
2020-10-12 02:11:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:11:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 24 @ 4104 updates, score 6.352) (writing took 1.822559963009553 seconds)
2020-10-12 02:11:20 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 02:11:20 | INFO | train | epoch 024 | loss 6.174 | nll_loss 4.875 | ppl 29.35 | wps 19359.7 | ups 2.86 | wpb 6780.2 | bsz 260.2 | num_updates 4104 | lr 0.00019745 | gnorm 1.225 | clip 0 | train_wall 56 | wall 1429
2020-10-12 02:11:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 02:11:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 02:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:11:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000697
2020-10-12 02:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006322
2020-10-12 02:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-12 02:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092807
2020-10-12 02:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099622
2020-10-12 02:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004235
2020-10-12 02:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 02:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093692
2020-10-12 02:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098410
2020-10-12 02:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:11:20 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 02:11:52 | INFO | train_inner | epoch 025:     96 / 171 loss=6.068, nll_loss=4.753, ppl=26.97, wps=18891.7, ups=2.75, wpb=6876.2, bsz=263.7, num_updates=4200, lr=0.00019518, gnorm=1.286, clip=0, train_wall=32, wall=1461
2020-10-12 02:12:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000571
2020-10-12 02:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016990
2020-10-12 02:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012628
2020-10-12 02:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030517
2020-10-12 02:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000484
2020-10-12 02:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016800
2020-10-12 02:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012526
2020-10-12 02:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030126
2020-10-12 02:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:12:18 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.231 | nll_loss 4.823 | ppl 28.31 | wps 45006 | wpb 2298.7 | bsz 85 | num_updates 4275 | best_loss 6.231
2020-10-12 02:12:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:12:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 25 @ 4275 updates, score 6.231) (writing took 1.6211219940014416 seconds)
2020-10-12 02:12:20 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 02:12:20 | INFO | train | epoch 025 | loss 6.046 | nll_loss 4.727 | ppl 26.49 | wps 19441.3 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 4275 | lr 0.00019346 | gnorm 1.244 | clip 0 | train_wall 55 | wall 1489
2020-10-12 02:12:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 02:12:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 02:12:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:12:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000655
2020-10-12 02:12:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006190
2020-10-12 02:12:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:12:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 02:12:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:12:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097135
2020-10-12 02:12:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103905
2020-10-12 02:12:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:12:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:12:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004246
2020-10-12 02:12:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:12:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 02:12:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:12:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093008
2020-10-12 02:12:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097741
2020-10-12 02:12:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:12:20 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 02:12:28 | INFO | train_inner | epoch 026:     25 / 171 loss=6.019, nll_loss=4.696, ppl=25.92, wps=18430.8, ups=2.78, wpb=6635.1, bsz=248.1, num_updates=4300, lr=0.000192897, gnorm=1.22, clip=0, train_wall=32, wall=1497
2020-10-12 02:13:01 | INFO | train_inner | epoch 026:    125 / 171 loss=5.845, nll_loss=4.497, ppl=22.58, wps=20243.6, ups=3.03, wpb=6685.6, bsz=276.6, num_updates=4400, lr=0.000190693, gnorm=1.17, clip=0, train_wall=32, wall=1530
2020-10-12 02:13:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000549
2020-10-12 02:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017243
2020-10-12 02:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013015
2020-10-12 02:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031142
2020-10-12 02:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000470
2020-10-12 02:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016959
2020-10-12 02:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013089
2020-10-12 02:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030840
2020-10-12 02:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:13:18 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.159 | nll_loss 4.735 | ppl 26.63 | wps 44500.7 | wpb 2298.7 | bsz 85 | num_updates 4446 | best_loss 6.159
2020-10-12 02:13:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:13:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 26 @ 4446 updates, score 6.159) (writing took 1.4931779560138239 seconds)
2020-10-12 02:13:19 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 02:13:19 | INFO | train | epoch 026 | loss 5.91 | nll_loss 4.571 | ppl 23.77 | wps 19439.6 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 4446 | lr 0.000189703 | gnorm 1.217 | clip 0 | train_wall 56 | wall 1549
2020-10-12 02:13:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 02:13:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 02:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:13:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000623
2020-10-12 02:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006179
2020-10-12 02:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000261
2020-10-12 02:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093961
2020-10-12 02:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100754
2020-10-12 02:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006705
2020-10-12 02:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 02:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093445
2020-10-12 02:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100663
2020-10-12 02:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:13:20 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 02:13:38 | INFO | train_inner | epoch 027:     54 / 171 loss=5.862, nll_loss=4.514, ppl=22.85, wps=19373.3, ups=2.74, wpb=7074.2, bsz=261.7, num_updates=4500, lr=0.000188562, gnorm=1.21, clip=0, train_wall=33, wall=1567
2020-10-12 02:14:11 | INFO | train_inner | epoch 027:    154 / 171 loss=5.833, nll_loss=4.479, ppl=22.3, wps=20225.9, ups=3.02, wpb=6705.5, bsz=251.6, num_updates=4600, lr=0.000186501, gnorm=1.253, clip=0, train_wall=33, wall=1600
2020-10-12 02:14:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000556
2020-10-12 02:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016837
2020-10-12 02:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012988
2020-10-12 02:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030718
2020-10-12 02:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000475
2020-10-12 02:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016568
2020-10-12 02:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012684
2020-10-12 02:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030051
2020-10-12 02:14:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:14:18 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.133 | nll_loss 4.714 | ppl 26.24 | wps 44421.3 | wpb 2298.7 | bsz 85 | num_updates 4617 | best_loss 6.133
2020-10-12 02:14:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:14:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 27 @ 4617 updates, score 6.133) (writing took 1.8300739870028337 seconds)
2020-10-12 02:14:19 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 02:14:19 | INFO | train | epoch 027 | loss 5.792 | nll_loss 4.434 | ppl 21.61 | wps 19333.3 | ups 2.85 | wpb 6780.2 | bsz 260.2 | num_updates 4617 | lr 0.000186157 | gnorm 1.22 | clip 0 | train_wall 56 | wall 1609
2020-10-12 02:14:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 02:14:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 02:14:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:14:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000830
2020-10-12 02:14:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007982
2020-10-12 02:14:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:14:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000217
2020-10-12 02:14:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:14:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097204
2020-10-12 02:14:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105802
2020-10-12 02:14:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:14:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:14:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004265
2020-10-12 02:14:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:14:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 02:14:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:14:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093293
2020-10-12 02:14:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098048
2020-10-12 02:14:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:14:20 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 02:14:47 | INFO | train_inner | epoch 028:     83 / 171 loss=5.691, nll_loss=4.316, ppl=19.92, wps=18873.5, ups=2.75, wpb=6865.4, bsz=267.8, num_updates=4700, lr=0.000184506, gnorm=1.185, clip=0, train_wall=32, wall=1637
2020-10-12 02:15:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000554
2020-10-12 02:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016989
2020-10-12 02:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012545
2020-10-12 02:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030415
2020-10-12 02:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000463
2020-10-12 02:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016665
2020-10-12 02:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012799
2020-10-12 02:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030240
2020-10-12 02:15:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:15:17 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.077 | nll_loss 4.628 | ppl 24.72 | wps 44387.3 | wpb 2298.7 | bsz 85 | num_updates 4788 | best_loss 6.077
2020-10-12 02:15:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:15:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 28 @ 4788 updates, score 6.077) (writing took 1.642737392001436 seconds)
2020-10-12 02:15:19 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 02:15:19 | INFO | train | epoch 028 | loss 5.674 | nll_loss 4.296 | ppl 19.64 | wps 19425 | ups 2.86 | wpb 6780.2 | bsz 260.2 | num_updates 4788 | lr 0.000182803 | gnorm 1.204 | clip 0 | train_wall 55 | wall 1668
2020-10-12 02:15:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 02:15:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 02:15:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:15:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000629
2020-10-12 02:15:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006780
2020-10-12 02:15:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:15:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 02:15:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:15:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097898
2020-10-12 02:15:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105305
2020-10-12 02:15:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:15:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:15:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004282
2020-10-12 02:15:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:15:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 02:15:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:15:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093076
2020-10-12 02:15:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097857
2020-10-12 02:15:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:15:19 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 02:15:23 | INFO | train_inner | epoch 029:     12 / 171 loss=5.653, nll_loss=4.271, ppl=19.31, wps=18423.3, ups=2.77, wpb=6654.4, bsz=252.5, num_updates=4800, lr=0.000182574, gnorm=1.227, clip=0, train_wall=32, wall=1673
2020-10-12 02:15:56 | INFO | train_inner | epoch 029:    112 / 171 loss=5.552, nll_loss=4.155, ppl=17.81, wps=20413.2, ups=3.03, wpb=6729.9, bsz=261.8, num_updates=4900, lr=0.000180702, gnorm=1.206, clip=0, train_wall=32, wall=1706
2020-10-12 02:16:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:16:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:16:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000554
2020-10-12 02:16:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:16:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016660
2020-10-12 02:16:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:16:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013058
2020-10-12 02:16:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030604
2020-10-12 02:16:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:16:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:16:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000473
2020-10-12 02:16:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:16:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016495
2020-10-12 02:16:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:16:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012912
2020-10-12 02:16:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030202
2020-10-12 02:16:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:16:17 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.02 | nll_loss 4.566 | ppl 23.68 | wps 44694.1 | wpb 2298.7 | bsz 85 | num_updates 4959 | best_loss 6.02
2020-10-12 02:16:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:16:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 29 @ 4959 updates, score 6.02) (writing took 1.8292400950012961 seconds)
2020-10-12 02:16:19 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 02:16:19 | INFO | train | epoch 029 | loss 5.564 | nll_loss 4.168 | ppl 17.98 | wps 19365 | ups 2.86 | wpb 6780.2 | bsz 260.2 | num_updates 4959 | lr 0.000179623 | gnorm 1.202 | clip 0 | train_wall 55 | wall 1728
2020-10-12 02:16:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 02:16:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 02:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:16:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000786
2020-10-12 02:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008603
2020-10-12 02:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000209
2020-10-12 02:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096104
2020-10-12 02:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105403
2020-10-12 02:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004267
2020-10-12 02:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 02:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094007
2020-10-12 02:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098772
2020-10-12 02:16:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:16:19 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 02:16:33 | INFO | train_inner | epoch 030:     41 / 171 loss=5.517, nll_loss=4.113, ppl=17.3, wps=18607.5, ups=2.76, wpb=6745.1, bsz=257.6, num_updates=5000, lr=0.000178885, gnorm=1.172, clip=0, train_wall=32, wall=1742
2020-10-12 02:17:06 | INFO | train_inner | epoch 030:    141 / 171 loss=5.472, nll_loss=4.06, ppl=16.68, wps=20507.3, ups=3.02, wpb=6789, bsz=256.2, num_updates=5100, lr=0.000177123, gnorm=1.228, clip=0, train_wall=33, wall=1775
2020-10-12 02:17:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000554
2020-10-12 02:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017113
2020-10-12 02:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012909
2020-10-12 02:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030907
2020-10-12 02:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000453
2020-10-12 02:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016647
2020-10-12 02:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012585
2020-10-12 02:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030000
2020-10-12 02:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:17:17 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.962 | nll_loss 4.472 | ppl 22.19 | wps 44863.7 | wpb 2298.7 | bsz 85 | num_updates 5130 | best_loss 5.962
2020-10-12 02:17:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:17:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 30 @ 5130 updates, score 5.962) (writing took 1.7003950140060624 seconds)
2020-10-12 02:17:19 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 02:17:19 | INFO | train | epoch 030 | loss 5.459 | nll_loss 4.045 | ppl 16.51 | wps 19379.8 | ups 2.86 | wpb 6780.2 | bsz 260.2 | num_updates 5130 | lr 0.000176604 | gnorm 1.19 | clip 0 | train_wall 56 | wall 1788
2020-10-12 02:17:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 02:17:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 02:17:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:17:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000715
2020-10-12 02:17:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008268
2020-10-12 02:17:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:17:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000214
2020-10-12 02:17:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:17:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094933
2020-10-12 02:17:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103904
2020-10-12 02:17:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:17:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:17:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004232
2020-10-12 02:17:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:17:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 02:17:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:17:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094999
2020-10-12 02:17:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099729
2020-10-12 02:17:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:17:19 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 02:17:42 | INFO | train_inner | epoch 031:     70 / 171 loss=5.403, nll_loss=3.98, ppl=15.78, wps=18521.8, ups=2.76, wpb=6714.2, bsz=248.1, num_updates=5200, lr=0.000175412, gnorm=1.174, clip=0, train_wall=32, wall=1811
2020-10-12 02:18:15 | INFO | train_inner | epoch 031:    170 / 171 loss=5.379, nll_loss=3.95, ppl=15.45, wps=20648.4, ups=2.98, wpb=6918.8, bsz=275.2, num_updates=5300, lr=0.000173749, gnorm=1.227, clip=0, train_wall=33, wall=1845
2020-10-12 02:18:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:18:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:18:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000548
2020-10-12 02:18:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:18:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016922
2020-10-12 02:18:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:18:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012852
2020-10-12 02:18:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030661
2020-10-12 02:18:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:18:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:18:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000469
2020-10-12 02:18:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:18:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016620
2020-10-12 02:18:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:18:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012407
2020-10-12 02:18:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.029812
2020-10-12 02:18:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:18:17 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 5.962 | nll_loss 4.478 | ppl 22.29 | wps 44969.7 | wpb 2298.7 | bsz 85 | num_updates 5301 | best_loss 5.962
2020-10-12 02:18:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:18:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 31 @ 5301 updates, score 5.962) (writing took 1.6764921139983926 seconds)
2020-10-12 02:18:19 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 02:18:19 | INFO | train | epoch 031 | loss 5.375 | nll_loss 3.947 | ppl 15.43 | wps 19370.1 | ups 2.86 | wpb 6780.2 | bsz 260.2 | num_updates 5301 | lr 0.000173733 | gnorm 1.212 | clip 0 | train_wall 56 | wall 1848
2020-10-12 02:18:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 02:18:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 02:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:18:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000648
2020-10-12 02:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006074
2020-10-12 02:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 02:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094612
2020-10-12 02:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101221
2020-10-12 02:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004221
2020-10-12 02:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 02:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094114
2020-10-12 02:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098837
2020-10-12 02:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:18:19 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 02:18:52 | INFO | train_inner | epoch 032:     99 / 171 loss=5.289, nll_loss=3.848, ppl=14.4, wps=18584.3, ups=2.76, wpb=6740.9, bsz=258.2, num_updates=5400, lr=0.000172133, gnorm=1.185, clip=0, train_wall=32, wall=1881
2020-10-12 02:19:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000490
2020-10-12 02:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016714
2020-10-12 02:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012707
2020-10-12 02:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030238
2020-10-12 02:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000417
2020-10-12 02:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016641
2020-10-12 02:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012754
2020-10-12 02:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030129
2020-10-12 02:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:19:17 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.911 | nll_loss 4.415 | ppl 21.33 | wps 44376.3 | wpb 2298.7 | bsz 85 | num_updates 5472 | best_loss 5.911
2020-10-12 02:19:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:19:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 32 @ 5472 updates, score 5.911) (writing took 1.5813222020078683 seconds)
2020-10-12 02:19:18 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 02:19:18 | INFO | train | epoch 032 | loss 5.277 | nll_loss 3.833 | ppl 14.26 | wps 19429.6 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 5472 | lr 0.000170996 | gnorm 1.212 | clip 0 | train_wall 55 | wall 1908
2020-10-12 02:19:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 02:19:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 02:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:19:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000679
2020-10-12 02:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007280
2020-10-12 02:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000252
2020-10-12 02:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100478
2020-10-12 02:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108542
2020-10-12 02:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004543
2020-10-12 02:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 02:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093047
2020-10-12 02:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098176
2020-10-12 02:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:19:19 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 02:19:28 | INFO | train_inner | epoch 033:     28 / 171 loss=5.214, nll_loss=3.759, ppl=13.54, wps=19008.9, ups=2.77, wpb=6863.5, bsz=270.7, num_updates=5500, lr=0.000170561, gnorm=1.243, clip=0, train_wall=32, wall=1917
2020-10-12 02:20:01 | INFO | train_inner | epoch 033:    128 / 171 loss=5.215, nll_loss=3.761, ppl=13.56, wps=20181.1, ups=3.05, wpb=6625.7, bsz=244.4, num_updates=5600, lr=0.000169031, gnorm=1.234, clip=0, train_wall=32, wall=1950
2020-10-12 02:20:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000575
2020-10-12 02:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017090
2020-10-12 02:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012898
2020-10-12 02:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030900
2020-10-12 02:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000446
2020-10-12 02:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016648
2020-10-12 02:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012629
2020-10-12 02:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030042
2020-10-12 02:20:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:20:16 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 5.887 | nll_loss 4.384 | ppl 20.87 | wps 44572.8 | wpb 2298.7 | bsz 85 | num_updates 5643 | best_loss 5.887
2020-10-12 02:20:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:20:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 33 @ 5643 updates, score 5.887) (writing took 1.9328954130032798 seconds)
2020-10-12 02:20:18 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 02:20:18 | INFO | train | epoch 033 | loss 5.202 | nll_loss 3.745 | ppl 13.41 | wps 19343.7 | ups 2.85 | wpb 6780.2 | bsz 260.2 | num_updates 5643 | lr 0.000168386 | gnorm 1.24 | clip 0 | train_wall 55 | wall 1968
2020-10-12 02:20:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 02:20:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:20:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000600
2020-10-12 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006092
2020-10-12 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102893
2020-10-12 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109603
2020-10-12 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004254
2020-10-12 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-12 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093218
2020-10-12 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097958
2020-10-12 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:20:18 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 02:20:37 | INFO | train_inner | epoch 034:     57 / 171 loss=5.146, nll_loss=3.68, ppl=12.81, wps=18663.2, ups=2.73, wpb=6841.5, bsz=282, num_updates=5700, lr=0.000167542, gnorm=1.221, clip=0, train_wall=33, wall=1987
2020-10-12 02:21:10 | INFO | train_inner | epoch 034:    157 / 171 loss=5.113, nll_loss=3.641, ppl=12.47, wps=20519.1, ups=3.03, wpb=6771.9, bsz=249.2, num_updates=5800, lr=0.000166091, gnorm=1.146, clip=0, train_wall=32, wall=2020
2020-10-12 02:21:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000568
2020-10-12 02:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016969
2020-10-12 02:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012944
2020-10-12 02:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030823
2020-10-12 02:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000463
2020-10-12 02:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016654
2020-10-12 02:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012750
2020-10-12 02:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030188
2020-10-12 02:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:21:16 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 5.859 | nll_loss 4.343 | ppl 20.3 | wps 44153.8 | wpb 2298.7 | bsz 85 | num_updates 5814 | best_loss 5.859
2020-10-12 02:21:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:21:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 34 @ 5814 updates, score 5.859) (writing took 2.980002776006586 seconds)
2020-10-12 02:21:19 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 02:21:19 | INFO | train | epoch 034 | loss 5.111 | nll_loss 3.639 | ppl 12.46 | wps 19003.6 | ups 2.8 | wpb 6780.2 | bsz 260.2 | num_updates 5814 | lr 0.000165891 | gnorm 1.162 | clip 0 | train_wall 55 | wall 2029
2020-10-12 02:21:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 02:21:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 02:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:21:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000655
2020-10-12 02:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006477
2020-10-12 02:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 02:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095712
2020-10-12 02:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102742
2020-10-12 02:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004279
2020-10-12 02:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 02:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096532
2020-10-12 02:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101324
2020-10-12 02:21:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:21:19 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 02:21:48 | INFO | train_inner | epoch 035:     86 / 171 loss=5.061, nll_loss=3.582, ppl=11.98, wps=17829.2, ups=2.68, wpb=6653.4, bsz=238.4, num_updates=5900, lr=0.000164677, gnorm=1.213, clip=0, train_wall=32, wall=2057
2020-10-12 02:22:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:22:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:22:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000552
2020-10-12 02:22:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:22:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016834
2020-10-12 02:22:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:22:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012760
2020-10-12 02:22:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030478
2020-10-12 02:22:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:22:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:22:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000452
2020-10-12 02:22:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:22:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016764
2020-10-12 02:22:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:22:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012706
2020-10-12 02:22:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030244
2020-10-12 02:22:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:22:17 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 5.841 | nll_loss 4.32 | ppl 19.97 | wps 44353.3 | wpb 2298.7 | bsz 85 | num_updates 5985 | best_loss 5.841
2020-10-12 02:22:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:22:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 35 @ 5985 updates, score 5.841) (writing took 1.4924847669899464 seconds)
2020-10-12 02:22:19 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 02:22:19 | INFO | train | epoch 035 | loss 5.037 | nll_loss 3.552 | ppl 11.73 | wps 19472.3 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 5985 | lr 0.000163504 | gnorm 1.194 | clip 0 | train_wall 55 | wall 2088
2020-10-12 02:22:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 02:22:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 02:22:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:22:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000648
2020-10-12 02:22:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006514
2020-10-12 02:22:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:22:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000246
2020-10-12 02:22:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:22:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096197
2020-10-12 02:22:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103299
2020-10-12 02:22:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:22:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:22:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008093
2020-10-12 02:22:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:22:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 02:22:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:22:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093747
2020-10-12 02:22:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102364
2020-10-12 02:22:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:22:19 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 02:22:24 | INFO | train_inner | epoch 036:     15 / 171 loss=5.035, nll_loss=3.549, ppl=11.7, wps=19218.1, ups=2.75, wpb=6987.8, bsz=276.1, num_updates=6000, lr=0.000163299, gnorm=1.184, clip=0, train_wall=33, wall=2093
2020-10-12 02:22:57 | INFO | train_inner | epoch 036:    115 / 171 loss=4.952, nll_loss=3.454, ppl=10.96, wps=20285.1, ups=3.02, wpb=6727.4, bsz=260.6, num_updates=6100, lr=0.000161955, gnorm=1.185, clip=0, train_wall=33, wall=2126
2020-10-12 02:23:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:23:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:23:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000547
2020-10-12 02:23:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:23:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016695
2020-10-12 02:23:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:23:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012801
2020-10-12 02:23:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030370
2020-10-12 02:23:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:23:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:23:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000463
2020-10-12 02:23:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:23:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016742
2020-10-12 02:23:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:23:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012732
2020-10-12 02:23:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030250
2020-10-12 02:23:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:23:17 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 5.815 | nll_loss 4.286 | ppl 19.51 | wps 44734.1 | wpb 2298.7 | bsz 85 | num_updates 6156 | best_loss 5.815
2020-10-12 02:23:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:23:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 36 @ 6156 updates, score 5.815) (writing took 1.4935505259927595 seconds)
2020-10-12 02:23:18 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 02:23:18 | INFO | train | epoch 036 | loss 4.956 | nll_loss 3.458 | ppl 10.99 | wps 19457 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 6156 | lr 0.000161217 | gnorm 1.167 | clip 0 | train_wall 56 | wall 2148
2020-10-12 02:23:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 02:23:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 02:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:23:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000806
2020-10-12 02:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008474
2020-10-12 02:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000306
2020-10-12 02:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096114
2020-10-12 02:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105309
2020-10-12 02:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004242
2020-10-12 02:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 02:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093725
2020-10-12 02:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098454
2020-10-12 02:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:23:19 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 02:23:33 | INFO | train_inner | epoch 037:     44 / 171 loss=4.871, nll_loss=3.362, ppl=10.28, wps=18623.4, ups=2.8, wpb=6662.1, bsz=269.3, num_updates=6200, lr=0.000160644, gnorm=1.132, clip=0, train_wall=32, wall=2162
2020-10-12 02:24:06 | INFO | train_inner | epoch 037:    144 / 171 loss=4.954, nll_loss=3.453, ppl=10.95, wps=20490.9, ups=3.02, wpb=6775.8, bsz=234.5, num_updates=6300, lr=0.000159364, gnorm=1.224, clip=0, train_wall=33, wall=2195
2020-10-12 02:24:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000556
2020-10-12 02:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016695
2020-10-12 02:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013087
2020-10-12 02:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030693
2020-10-12 02:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000465
2020-10-12 02:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016705
2020-10-12 02:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012845
2020-10-12 02:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030331
2020-10-12 02:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:24:16 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 5.808 | nll_loss 4.284 | ppl 19.48 | wps 44043.7 | wpb 2298.7 | bsz 85 | num_updates 6327 | best_loss 5.808
2020-10-12 02:24:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:24:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 37 @ 6327 updates, score 5.808) (writing took 2.6153941550001036 seconds)
2020-10-12 02:24:19 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 02:24:19 | INFO | train | epoch 037 | loss 4.893 | nll_loss 3.385 | ppl 10.44 | wps 19096.1 | ups 2.82 | wpb 6780.2 | bsz 260.2 | num_updates 6327 | lr 0.000159023 | gnorm 1.188 | clip 0 | train_wall 56 | wall 2208
2020-10-12 02:24:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 02:24:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 02:24:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:24:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002206
2020-10-12 02:24:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.017236
2020-10-12 02:24:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:24:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000366
2020-10-12 02:24:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:24:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100832
2020-10-12 02:24:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.119054
2020-10-12 02:24:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:24:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:24:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004286
2020-10-12 02:24:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:24:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 02:24:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:24:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093786
2020-10-12 02:24:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098586
2020-10-12 02:24:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:24:19 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 02:24:44 | INFO | train_inner | epoch 038:     73 / 171 loss=4.793, nll_loss=3.27, ppl=9.65, wps=18614.2, ups=2.66, wpb=7003.2, bsz=305.6, num_updates=6400, lr=0.000158114, gnorm=1.091, clip=0, train_wall=33, wall=2233
2020-10-12 02:25:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000550
2020-10-12 02:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016678
2020-10-12 02:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012847
2020-10-12 02:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030403
2020-10-12 02:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000473
2020-10-12 02:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016736
2020-10-12 02:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012648
2020-10-12 02:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030174
2020-10-12 02:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:25:17 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 5.814 | nll_loss 4.278 | ppl 19.41 | wps 44202.7 | wpb 2298.7 | bsz 85 | num_updates 6498 | best_loss 5.808
2020-10-12 02:25:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:25:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_last.pt (epoch 38 @ 6498 updates, score 5.814) (writing took 1.0351008249999722 seconds)
2020-10-12 02:25:18 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 02:25:18 | INFO | train | epoch 038 | loss 4.822 | nll_loss 3.302 | ppl 9.86 | wps 19602.3 | ups 2.89 | wpb 6780.2 | bsz 260.2 | num_updates 6498 | lr 0.000156917 | gnorm 1.158 | clip 0 | train_wall 55 | wall 2268
2020-10-12 02:25:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 02:25:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 02:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:25:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000737
2020-10-12 02:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006967
2020-10-12 02:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000190
2020-10-12 02:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093072
2020-10-12 02:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100599
2020-10-12 02:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004223
2020-10-12 02:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 02:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094416
2020-10-12 02:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099134
2020-10-12 02:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:25:18 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 02:25:19 | INFO | train_inner | epoch 039:      2 / 171 loss=4.869, nll_loss=3.354, ppl=10.23, wps=18815.6, ups=2.81, wpb=6687.4, bsz=231.8, num_updates=6500, lr=0.000156893, gnorm=1.213, clip=0, train_wall=32, wall=2268
2020-10-12 02:25:52 | INFO | train_inner | epoch 039:    102 / 171 loss=4.775, nll_loss=3.247, ppl=9.49, wps=20494.7, ups=3.02, wpb=6789.8, bsz=257.3, num_updates=6600, lr=0.0001557, gnorm=1.257, clip=0, train_wall=33, wall=2302
2020-10-12 02:26:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000557
2020-10-12 02:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016900
2020-10-12 02:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012762
2020-10-12 02:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030548
2020-10-12 02:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000456
2020-10-12 02:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016981
2020-10-12 02:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012813
2020-10-12 02:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.030567
2020-10-12 02:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:26:16 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 5.799 | nll_loss 4.267 | ppl 19.25 | wps 44464.6 | wpb 2298.7 | bsz 85 | num_updates 6669 | best_loss 5.799
2020-10-12 02:26:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:26:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 39 @ 6669 updates, score 5.799) (writing took 1.4922144199954346 seconds)
2020-10-12 02:26:18 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 02:26:18 | INFO | train | epoch 039 | loss 4.772 | nll_loss 3.243 | ppl 9.47 | wps 19444.1 | ups 2.87 | wpb 6780.2 | bsz 260.2 | num_updates 6669 | lr 0.000154892 | gnorm 1.216 | clip 0 | train_wall 56 | wall 2327
2020-10-12 02:26:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 02:26:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 02:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:26:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000737
2020-10-12 02:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008785
2020-10-12 02:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000222
2020-10-12 02:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098913
2020-10-12 02:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108432
2020-10-12 02:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004233
2020-10-12 02:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 02:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092120
2020-10-12 02:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096850
2020-10-12 02:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:26:18 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 02:26:28 | INFO | train_inner | epoch 040:     31 / 171 loss=4.735, nll_loss=3.199, ppl=9.19, wps=18646.5, ups=2.76, wpb=6747.8, bsz=269.4, num_updates=6700, lr=0.000154533, gnorm=1.141, clip=0, train_wall=33, wall=2338
2020-10-12 02:27:01 | INFO | train_inner | epoch 040:    131 / 171 loss=4.703, nll_loss=3.162, ppl=8.95, wps=20406.6, ups=3.05, wpb=6698.6, bsz=253, num_updates=6800, lr=0.000153393, gnorm=1.184, clip=0, train_wall=32, wall=2371
2020-10-12 02:27:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 02:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000549
2020-10-12 02:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016519
2020-10-12 02:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012595
2020-10-12 02:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.029996
2020-10-12 02:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000473
2020-10-12 02:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016441
2020-10-12 02:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012667
2020-10-12 02:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.029893
2020-10-12 02:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:27:16 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 5.778 | nll_loss 4.238 | ppl 18.87 | wps 44307 | wpb 2298.7 | bsz 85 | num_updates 6840 | best_loss 5.778
2020-10-12 02:27:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 02:27:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_bellit_sepspm8000/M2O/checkpoint_best.pt (epoch 40 @ 6840 updates, score 5.778) (writing took 1.9378302200057078 seconds)
2020-10-12 02:27:18 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 02:27:18 | INFO | train | epoch 040 | loss 4.708 | nll_loss 3.168 | ppl 8.99 | wps 19298.3 | ups 2.85 | wpb 6780.2 | bsz 260.2 | num_updates 6840 | lr 0.000152944 | gnorm 1.159 | clip 0 | train_wall 56 | wall 2387
2020-10-12 02:27:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 02:27:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 02:27:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 02:27:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000866
2020-10-12 02:27:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008783
2020-10-12 02:27:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:27:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000308
2020-10-12 02:27:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:27:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095280
2020-10-12 02:27:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104793
2020-10-12 02:27:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 02:27:18 | INFO | fairseq_cli.train | done training in 2387.2 seconds
