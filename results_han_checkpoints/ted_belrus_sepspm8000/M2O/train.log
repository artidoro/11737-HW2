2020-10-11 18:55:03 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_belrus_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='bel-eng,rus-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_belrus_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-11 18:55:03 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-11 18:55:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['bel', 'eng', 'rus']
2020-10-11 18:55:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bel] dictionary: 21771 types
2020-10-11 18:55:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 21771 types
2020-10-11 18:55:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | [rus] dictionary: 21771 types
2020-10-11 18:55:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-11 18:55:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:55:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-11 18:55:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:bel-eng': 1, 'main:rus-eng': 1}
2020-10-11 18:55:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-11 18:55:03 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belrus_sepspm8000/M2O/valid.bel-eng.bel
2020-10-11 18:55:03 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belrus_sepspm8000/M2O/valid.bel-eng.eng
2020-10-11 18:55:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrus_sepspm8000/M2O/ valid bel-eng 248 examples
2020-10-11 18:55:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:rus-eng src_langtok: None; tgt_langtok: None
2020-10-11 18:55:03 | INFO | fairseq.data.data_utils | loaded 4814 examples from: fairseq/data-bin/ted_belrus_sepspm8000/M2O/valid.rus-eng.rus
2020-10-11 18:55:03 | INFO | fairseq.data.data_utils | loaded 4814 examples from: fairseq/data-bin/ted_belrus_sepspm8000/M2O/valid.rus-eng.eng
2020-10-11 18:55:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrus_sepspm8000/M2O/ valid rus-eng 4814 examples
2020-10-11 18:55:03 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21771, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21771, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=21771, bias=False)
  )
)
2020-10-11 18:55:03 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-11 18:55:03 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-11 18:55:03 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-11 18:55:03 | INFO | fairseq_cli.train | num. model params: 42690048 (num. trained: 42690048)
2020-10-11 18:55:05 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-11 18:55:05 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-11 18:55:05 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-11 18:55:05 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.732 GB ; name = GeForce RTX 2080 Ti                     
2020-10-11 18:55:05 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-11 18:55:05 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-11 18:55:05 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-11 18:55:05 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_last.pt
2020-10-11 18:55:05 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-11 18:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-11 18:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:55:05 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-11 18:55:05 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:bel-eng': 1, 'main:rus-eng': 1}
2020-10-11 18:55:05 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-11 18:55:05 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belrus_sepspm8000/M2O/train.bel-eng.bel
2020-10-11 18:55:05 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belrus_sepspm8000/M2O/train.bel-eng.eng
2020-10-11 18:55:05 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrus_sepspm8000/M2O/ train bel-eng 4509 examples
2020-10-11 18:55:05 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:rus-eng src_langtok: None; tgt_langtok: None
2020-10-11 18:55:05 | INFO | fairseq.data.data_utils | loaded 39988 examples from: fairseq/data-bin/ted_belrus_sepspm8000/M2O/train.rus-eng.rus
2020-10-11 18:55:05 | INFO | fairseq.data.data_utils | loaded 39988 examples from: fairseq/data-bin/ted_belrus_sepspm8000/M2O/train.rus-eng.eng
2020-10-11 18:55:05 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrus_sepspm8000/M2O/ train rus-eng 39988 examples
2020-10-11 18:55:05 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:bel-eng', 4509), ('main:rus-eng', 39988)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-11 18:55:05 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-11 18:55:05 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 44497
2020-10-11 18:55:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 44497; virtual dataset size 44497
2020-10-11 18:55:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-11 18:55:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:bel-eng': 4509, 'main:rus-eng': 39988}; raw total size: 44497
2020-10-11 18:55:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:bel-eng': 4509, 'main:rus-eng': 39988}; resampled total size: 44497
2020-10-11 18:55:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-11 18:55:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.003058
2020-10-11 18:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 18:55:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000412
2020-10-11 18:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005160
2020-10-11 18:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-11 18:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101452
2020-10-11 18:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107138
2020-10-11 18:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:55:05 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-11 18:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 18:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004228
2020-10-11 18:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-11 18:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100197
2020-10-11 18:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104936
2020-10-11 18:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:55:05 | INFO | fairseq.trainer | begin training epoch 1
2020-10-11 18:55:35 | INFO | train_inner | epoch 001:    100 / 183 loss=14.193, nll_loss=14.088, ppl=17410.6, wps=22920.8, ups=3.41, wpb=6722, bsz=247.1, num_updates=100, lr=5.0975e-06, gnorm=4.421, clip=0, train_wall=29, wall=30
2020-10-11 18:55:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 18:55:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 18:55:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000777
2020-10-11 18:55:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:55:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042916
2020-10-11 18:55:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:55:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033626
2020-10-11 18:55:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077662
2020-10-11 18:55:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:55:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 18:55:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000669
2020-10-11 18:55:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:56:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043923
2020-10-11 18:56:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:56:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032966
2020-10-11 18:56:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077890
2020-10-11 18:56:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
/home/han/Documents/11737-hw/assign2/fairseq/fairseq/utils.py:340: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2020-10-11 18:56:02 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.892 | nll_loss 11.513 | ppl 2922.59 | wps 50903.1 | wpb 2270.2 | bsz 84.4 | num_updates 183
2020-10-11 18:56:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 18:56:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 1 @ 183 updates, score 11.892) (writing took 0.9439396870002383 seconds)
2020-10-11 18:56:03 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-11 18:56:03 | INFO | train | epoch 001 | loss 13.45 | nll_loss 13.259 | ppl 9803.89 | wps 21235.1 | ups 3.19 | wpb 6662 | bsz 243.2 | num_updates 183 | lr 9.24543e-06 | gnorm 3.461 | clip 0 | train_wall 53 | wall 58
2020-10-11 18:56:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-11 18:56:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-11 18:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 18:56:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000560
2020-10-11 18:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005516
2020-10-11 18:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-11 18:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099080
2020-10-11 18:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105086
2020-10-11 18:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 18:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004368
2020-10-11 18:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-11 18:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099310
2020-10-11 18:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104175
2020-10-11 18:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:56:03 | INFO | fairseq.trainer | begin training epoch 2
2020-10-11 18:56:08 | INFO | train_inner | epoch 002:     17 / 183 loss=12.469, nll_loss=12.165, ppl=4592.46, wps=19534.1, ups=3, wpb=6501.6, bsz=239.7, num_updates=200, lr=1.0095e-05, gnorm=2.27, clip=0, train_wall=29, wall=63
2020-10-11 18:56:38 | INFO | train_inner | epoch 002:    117 / 183 loss=11.627, nll_loss=11.225, ppl=2394.39, wps=22519.9, ups=3.36, wpb=6704.4, bsz=236.8, num_updates=300, lr=1.50925e-05, gnorm=1.815, clip=0, train_wall=29, wall=93
2020-10-11 18:56:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 18:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 18:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000806
2020-10-11 18:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043750
2020-10-11 18:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032821
2020-10-11 18:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077713
2020-10-11 18:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 18:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000682
2020-10-11 18:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043499
2020-10-11 18:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033196
2020-10-11 18:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077698
2020-10-11 18:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:57:01 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.1 | nll_loss 9.463 | ppl 705.98 | wps 50179.1 | wpb 2270.2 | bsz 84.4 | num_updates 366 | best_loss 10.1
2020-10-11 18:57:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 18:57:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 2 @ 366 updates, score 10.1) (writing took 1.94806381300441 seconds)
2020-10-11 18:57:03 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-11 18:57:03 | INFO | train | epoch 002 | loss 11.366 | nll_loss 10.93 | ppl 1951.54 | wps 20417.1 | ups 3.06 | wpb 6662 | bsz 243.2 | num_updates 366 | lr 1.83909e-05 | gnorm 1.894 | clip 0 | train_wall 54 | wall 118
2020-10-11 18:57:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-11 18:57:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-11 18:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 18:57:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000634
2020-10-11 18:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006030
2020-10-11 18:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-11 18:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.112439
2020-10-11 18:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.119091
2020-10-11 18:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 18:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004282
2020-10-11 18:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-11 18:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100055
2020-10-11 18:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104841
2020-10-11 18:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:57:03 | INFO | fairseq.trainer | begin training epoch 3
2020-10-11 18:57:13 | INFO | train_inner | epoch 003:     34 / 183 loss=10.598, nll_loss=10.058, ppl=1066, wps=18872.7, ups=2.86, wpb=6603.1, bsz=235.8, num_updates=400, lr=2.009e-05, gnorm=1.872, clip=0, train_wall=29, wall=128
2020-10-11 18:57:44 | INFO | train_inner | epoch 003:    134 / 183 loss=9.736, nll_loss=9.041, ppl=526.91, wps=22309.7, ups=3.27, wpb=6813.5, bsz=254, num_updates=500, lr=2.50875e-05, gnorm=1.411, clip=0, train_wall=30, wall=159
2020-10-11 18:57:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 18:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 18:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000794
2020-10-11 18:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042338
2020-10-11 18:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032901
2020-10-11 18:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076368
2020-10-11 18:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 18:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000702
2020-10-11 18:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042046
2020-10-11 18:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032736
2020-10-11 18:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075808
2020-10-11 18:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:58:02 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.155 | nll_loss 8.287 | ppl 312.4 | wps 49499.2 | wpb 2270.2 | bsz 84.4 | num_updates 549 | best_loss 9.155
2020-10-11 18:58:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 18:58:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 3 @ 549 updates, score 9.155) (writing took 1.4959989609924378 seconds)
2020-10-11 18:58:03 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-11 18:58:03 | INFO | train | epoch 003 | loss 9.735 | nll_loss 9.036 | ppl 525.02 | wps 20268.6 | ups 3.04 | wpb 6662 | bsz 243.2 | num_updates 549 | lr 2.75363e-05 | gnorm 1.449 | clip 0 | train_wall 54 | wall 178
2020-10-11 18:58:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-11 18:58:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-11 18:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 18:58:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000601
2020-10-11 18:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006046
2020-10-11 18:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-11 18:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104037
2020-10-11 18:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110600
2020-10-11 18:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 18:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004241
2020-10-11 18:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 18:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100849
2020-10-11 18:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105586
2020-10-11 18:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:58:03 | INFO | fairseq.trainer | begin training epoch 4
2020-10-11 18:58:19 | INFO | train_inner | epoch 004:     51 / 183 loss=9.344, nll_loss=8.552, ppl=375.37, wps=18878.7, ups=2.85, wpb=6633.2, bsz=239.4, num_updates=600, lr=3.0085e-05, gnorm=1.272, clip=0, train_wall=30, wall=194
2020-10-11 18:58:50 | INFO | train_inner | epoch 004:    151 / 183 loss=9.184, nll_loss=8.342, ppl=324.44, wps=21695.2, ups=3.25, wpb=6668, bsz=247.1, num_updates=700, lr=3.50825e-05, gnorm=1.623, clip=0, train_wall=30, wall=224
2020-10-11 18:58:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 18:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 18:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000799
2020-10-11 18:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042915
2020-10-11 18:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032783
2020-10-11 18:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076832
2020-10-11 18:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 18:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000700
2020-10-11 18:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042498
2020-10-11 18:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032599
2020-10-11 18:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076120
2020-10-11 18:59:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:59:02 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.872 | nll_loss 7.932 | ppl 244.16 | wps 48937.8 | wpb 2270.2 | bsz 84.4 | num_updates 732 | best_loss 8.872
2020-10-11 18:59:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 18:59:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 4 @ 732 updates, score 8.872) (writing took 1.5482368809898617 seconds)
2020-10-11 18:59:04 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-11 18:59:04 | INFO | train | epoch 004 | loss 9.19 | nll_loss 8.353 | ppl 326.96 | wps 20033.2 | ups 3.01 | wpb 6662 | bsz 243.2 | num_updates 732 | lr 3.66817e-05 | gnorm 1.475 | clip 0 | train_wall 55 | wall 239
2020-10-11 18:59:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-11 18:59:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-11 18:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 18:59:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000558
2020-10-11 18:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005781
2020-10-11 18:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 18:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.108155
2020-10-11 18:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114544
2020-10-11 18:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 18:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004325
2020-10-11 18:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-11 18:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099703
2020-10-11 18:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104532
2020-10-11 18:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 18:59:04 | INFO | fairseq.trainer | begin training epoch 5
2020-10-11 18:59:25 | INFO | train_inner | epoch 005:     68 / 183 loss=9.02, nll_loss=8.146, ppl=283.23, wps=18858.1, ups=2.82, wpb=6693.2, bsz=238.1, num_updates=800, lr=4.008e-05, gnorm=1.252, clip=0, train_wall=30, wall=260
2020-10-11 18:59:56 | INFO | train_inner | epoch 005:    168 / 183 loss=8.884, nll_loss=7.99, ppl=254.16, wps=21469.8, ups=3.21, wpb=6689.7, bsz=247.6, num_updates=900, lr=4.50775e-05, gnorm=1.415, clip=0, train_wall=31, wall=291
2020-10-11 19:00:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000804
2020-10-11 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.044035
2020-10-11 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032958
2020-10-11 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.078139
2020-10-11 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000671
2020-10-11 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042763
2020-10-11 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033000
2020-10-11 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076761
2020-10-11 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:00:04 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.593 | nll_loss 7.605 | ppl 194.67 | wps 47621.2 | wpb 2270.2 | bsz 84.4 | num_updates 915 | best_loss 8.593
2020-10-11 19:00:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:00:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 5 @ 915 updates, score 8.593) (writing took 1.4850372570072068 seconds)
2020-10-11 19:00:05 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-11 19:00:05 | INFO | train | epoch 005 | loss 8.913 | nll_loss 8.022 | ppl 260.02 | wps 19855 | ups 2.98 | wpb 6662 | bsz 243.2 | num_updates 915 | lr 4.58271e-05 | gnorm 1.331 | clip 0 | train_wall 56 | wall 300
2020-10-11 19:00:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-11 19:00:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-11 19:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:00:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000721
2020-10-11 19:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007426
2020-10-11 19:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-11 19:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103397
2020-10-11 19:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111478
2020-10-11 19:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004262
2020-10-11 19:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-11 19:00:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:00:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100598
2020-10-11 19:00:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105369
2020-10-11 19:00:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:00:06 | INFO | fairseq.trainer | begin training epoch 6
2020-10-11 19:00:32 | INFO | train_inner | epoch 006:     85 / 183 loss=8.694, nll_loss=7.773, ppl=218.68, wps=18564.3, ups=2.77, wpb=6703.8, bsz=246.9, num_updates=1000, lr=5.0075e-05, gnorm=1.365, clip=0, train_wall=31, wall=327
2020-10-11 19:01:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000816
2020-10-11 19:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043425
2020-10-11 19:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033142
2020-10-11 19:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077727
2020-10-11 19:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000663
2020-10-11 19:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042817
2020-10-11 19:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032649
2020-10-11 19:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076455
2020-10-11 19:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:01:06 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.321 | nll_loss 7.298 | ppl 157.4 | wps 47321.5 | wpb 2270.2 | bsz 84.4 | num_updates 1098 | best_loss 8.321
2020-10-11 19:01:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:01:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 6 @ 1098 updates, score 8.321) (writing took 1.9188827890029643 seconds)
2020-10-11 19:01:08 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-11 19:01:08 | INFO | train | epoch 006 | loss 8.632 | nll_loss 7.701 | ppl 208.1 | wps 19446.9 | ups 2.92 | wpb 6662 | bsz 243.2 | num_updates 1098 | lr 5.49726e-05 | gnorm 1.36 | clip 0 | train_wall 56 | wall 363
2020-10-11 19:01:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-11 19:01:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-11 19:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:01:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000603
2020-10-11 19:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005938
2020-10-11 19:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-11 19:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098484
2020-10-11 19:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104952
2020-10-11 19:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004350
2020-10-11 19:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 19:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100500
2020-10-11 19:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105349
2020-10-11 19:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:01:08 | INFO | fairseq.trainer | begin training epoch 7
2020-10-11 19:01:09 | INFO | train_inner | epoch 007:      2 / 183 loss=8.593, nll_loss=7.656, ppl=201.76, wps=17961.4, ups=2.73, wpb=6578.6, bsz=242.2, num_updates=1100, lr=5.50725e-05, gnorm=1.381, clip=0, train_wall=31, wall=364
2020-10-11 19:01:40 | INFO | train_inner | epoch 007:    102 / 183 loss=8.447, nll_loss=7.489, ppl=179.67, wps=21081.4, ups=3.18, wpb=6631.4, bsz=242.9, num_updates=1200, lr=6.007e-05, gnorm=1.4, clip=0, train_wall=31, wall=395
2020-10-11 19:02:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:02:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:02:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000794
2020-10-11 19:02:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:02:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043262
2020-10-11 19:02:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:02:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032817
2020-10-11 19:02:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077214
2020-10-11 19:02:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:02:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:02:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000695
2020-10-11 19:02:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:02:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042801
2020-10-11 19:02:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:02:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032715
2020-10-11 19:02:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076532
2020-10-11 19:02:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:02:09 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.082 | nll_loss 7.024 | ppl 130.18 | wps 46733.2 | wpb 2270.2 | bsz 84.4 | num_updates 1281 | best_loss 8.082
2020-10-11 19:02:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:02:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 7 @ 1281 updates, score 8.082) (writing took 1.6202927179983817 seconds)
2020-10-11 19:02:11 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-11 19:02:11 | INFO | train | epoch 007 | loss 8.398 | nll_loss 7.434 | ppl 172.88 | wps 19358.4 | ups 2.91 | wpb 6662 | bsz 243.2 | num_updates 1281 | lr 6.4118e-05 | gnorm 1.402 | clip 0 | train_wall 57 | wall 426
2020-10-11 19:02:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-11 19:02:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-11 19:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:02:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000801
2020-10-11 19:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008683
2020-10-11 19:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000254
2020-10-11 19:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.116226
2020-10-11 19:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.125631
2020-10-11 19:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004299
2020-10-11 19:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-11 19:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101641
2020-10-11 19:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106440
2020-10-11 19:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:02:11 | INFO | fairseq.trainer | begin training epoch 8
2020-10-11 19:02:17 | INFO | train_inner | epoch 008:     19 / 183 loss=8.328, nll_loss=7.354, ppl=163.55, wps=18034.9, ups=2.71, wpb=6651.6, bsz=240.6, num_updates=1300, lr=6.50675e-05, gnorm=1.394, clip=0, train_wall=31, wall=432
2020-10-11 19:02:49 | INFO | train_inner | epoch 008:    119 / 183 loss=8.219, nll_loss=7.228, ppl=149.95, wps=20795.1, ups=3.13, wpb=6646.3, bsz=247, num_updates=1400, lr=7.0065e-05, gnorm=1.589, clip=0, train_wall=31, wall=464
2020-10-11 19:03:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000799
2020-10-11 19:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043667
2020-10-11 19:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032928
2020-10-11 19:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077733
2020-10-11 19:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000693
2020-10-11 19:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043678
2020-10-11 19:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032528
2020-10-11 19:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077222
2020-10-11 19:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:03:13 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.92 | nll_loss 6.829 | ppl 113.7 | wps 46262.2 | wpb 2270.2 | bsz 84.4 | num_updates 1464 | best_loss 7.92
2020-10-11 19:03:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:03:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 8 @ 1464 updates, score 7.92) (writing took 1.5337789909972344 seconds)
2020-10-11 19:03:14 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-11 19:03:14 | INFO | train | epoch 008 | loss 8.207 | nll_loss 7.214 | ppl 148.51 | wps 19218.5 | ups 2.88 | wpb 6662 | bsz 243.2 | num_updates 1464 | lr 7.32634e-05 | gnorm 1.443 | clip 0 | train_wall 57 | wall 489
2020-10-11 19:03:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-11 19:03:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-11 19:03:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:03:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000534
2020-10-11 19:03:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006118
2020-10-11 19:03:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:03:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-11 19:03:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:03:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.107576
2020-10-11 19:03:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114297
2020-10-11 19:03:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:03:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:03:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004369
2020-10-11 19:03:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:03:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-11 19:03:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:03:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098980
2020-10-11 19:03:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103842
2020-10-11 19:03:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:03:15 | INFO | fairseq.trainer | begin training epoch 9
2020-10-11 19:03:26 | INFO | train_inner | epoch 009:     36 / 183 loss=8.116, nll_loss=7.111, ppl=138.19, wps=18226.4, ups=2.71, wpb=6723.9, bsz=239.1, num_updates=1500, lr=7.50625e-05, gnorm=1.27, clip=0, train_wall=31, wall=501
2020-10-11 19:03:58 | INFO | train_inner | epoch 009:    136 / 183 loss=8.046, nll_loss=7.03, ppl=130.66, wps=20674.9, ups=3.11, wpb=6651.7, bsz=250, num_updates=1600, lr=8.006e-05, gnorm=1.388, clip=0, train_wall=32, wall=533
2020-10-11 19:04:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000792
2020-10-11 19:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042900
2020-10-11 19:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032290
2020-10-11 19:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076315
2020-10-11 19:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000691
2020-10-11 19:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043060
2020-10-11 19:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032291
2020-10-11 19:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076358
2020-10-11 19:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:04:17 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.754 | nll_loss 6.635 | ppl 99.42 | wps 45998.2 | wpb 2270.2 | bsz 84.4 | num_updates 1647 | best_loss 7.754
2020-10-11 19:04:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:04:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 9 @ 1647 updates, score 7.754) (writing took 1.480721211002674 seconds)
2020-10-11 19:04:18 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-11 19:04:18 | INFO | train | epoch 009 | loss 8.028 | nll_loss 7.01 | ppl 128.88 | wps 19155.2 | ups 2.88 | wpb 6662 | bsz 243.2 | num_updates 1647 | lr 8.24088e-05 | gnorm 1.342 | clip 0 | train_wall 58 | wall 553
2020-10-11 19:04:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-11 19:04:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-11 19:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:04:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000666
2020-10-11 19:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007151
2020-10-11 19:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-11 19:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103917
2020-10-11 19:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111684
2020-10-11 19:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004240
2020-10-11 19:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-11 19:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098836
2020-10-11 19:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103577
2020-10-11 19:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:04:18 | INFO | fairseq.trainer | begin training epoch 10
2020-10-11 19:04:35 | INFO | train_inner | epoch 010:     53 / 183 loss=7.948, nll_loss=6.919, ppl=120.99, wps=17853.8, ups=2.71, wpb=6584.9, bsz=232.9, num_updates=1700, lr=8.50575e-05, gnorm=1.232, clip=0, train_wall=31, wall=570
2020-10-11 19:05:08 | INFO | train_inner | epoch 010:    153 / 183 loss=7.875, nll_loss=6.834, ppl=114.09, wps=20627.3, ups=3.08, wpb=6693.7, bsz=255.5, num_updates=1800, lr=9.0055e-05, gnorm=1.369, clip=0, train_wall=32, wall=602
2020-10-11 19:05:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000795
2020-10-11 19:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043212
2020-10-11 19:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032628
2020-10-11 19:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076981
2020-10-11 19:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000701
2020-10-11 19:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042322
2020-10-11 19:05:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032548
2020-10-11 19:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075893
2020-10-11 19:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:05:21 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.638 | nll_loss 6.513 | ppl 91.35 | wps 45925.7 | wpb 2270.2 | bsz 84.4 | num_updates 1830 | best_loss 7.638
2020-10-11 19:05:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:05:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 10 @ 1830 updates, score 7.638) (writing took 2.0332456909964094 seconds)
2020-10-11 19:05:23 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-11 19:05:23 | INFO | train | epoch 010 | loss 7.88 | nll_loss 6.84 | ppl 114.59 | wps 18914 | ups 2.84 | wpb 6662 | bsz 243.2 | num_updates 1830 | lr 9.15543e-05 | gnorm 1.302 | clip 0 | train_wall 58 | wall 617
2020-10-11 19:05:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-11 19:05:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-11 19:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:05:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000546
2020-10-11 19:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006096
2020-10-11 19:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-11 19:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100932
2020-10-11 19:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107532
2020-10-11 19:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004377
2020-10-11 19:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-11 19:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099223
2020-10-11 19:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104105
2020-10-11 19:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:05:23 | INFO | fairseq.trainer | begin training epoch 11
2020-10-11 19:05:45 | INFO | train_inner | epoch 011:     70 / 183 loss=7.767, nll_loss=6.712, ppl=104.82, wps=17535.1, ups=2.66, wpb=6602.8, bsz=238.7, num_updates=1900, lr=9.50525e-05, gnorm=1.275, clip=0, train_wall=32, wall=640
2020-10-11 19:06:18 | INFO | train_inner | epoch 011:    170 / 183 loss=7.75, nll_loss=6.691, ppl=103.34, wps=20927, ups=3.07, wpb=6824.6, bsz=245.8, num_updates=2000, lr=0.00010005, gnorm=1.25, clip=0, train_wall=32, wall=673
2020-10-11 19:06:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.007057
2020-10-11 19:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042842
2020-10-11 19:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032612
2020-10-11 19:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.082857
2020-10-11 19:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000690
2020-10-11 19:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043183
2020-10-11 19:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032521
2020-10-11 19:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076715
2020-10-11 19:06:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:06:25 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.562 | nll_loss 6.43 | ppl 86.24 | wps 45940.1 | wpb 2270.2 | bsz 84.4 | num_updates 2013 | best_loss 7.562
2020-10-11 19:06:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:06:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 11 @ 2013 updates, score 7.562) (writing took 1.7265112579916604 seconds)
2020-10-11 19:06:27 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-11 19:06:27 | INFO | train | epoch 011 | loss 7.75 | nll_loss 6.691 | ppl 103.32 | wps 18938.8 | ups 2.84 | wpb 6662 | bsz 243.2 | num_updates 2013 | lr 0.0001007 | gnorm 1.282 | clip 0 | train_wall 58 | wall 682
2020-10-11 19:06:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-11 19:06:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-11 19:06:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:06:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000795
2020-10-11 19:06:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007547
2020-10-11 19:06:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:06:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-11 19:06:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:06:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102889
2020-10-11 19:06:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111079
2020-10-11 19:06:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:06:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:06:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004363
2020-10-11 19:06:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:06:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 19:06:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:06:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099563
2020-10-11 19:06:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104426
2020-10-11 19:06:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:06:27 | INFO | fairseq.trainer | begin training epoch 12
2020-10-11 19:06:55 | INFO | train_inner | epoch 012:     87 / 183 loss=7.657, nll_loss=6.585, ppl=96.01, wps=17301.5, ups=2.7, wpb=6407.8, bsz=230.2, num_updates=2100, lr=0.000105048, gnorm=1.335, clip=0, train_wall=31, wall=710
2020-10-11 19:07:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:07:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:07:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000796
2020-10-11 19:07:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:07:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043124
2020-10-11 19:07:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:07:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033079
2020-10-11 19:07:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077345
2020-10-11 19:07:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:07:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:07:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000689
2020-10-11 19:07:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:07:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043123
2020-10-11 19:07:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:07:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032631
2020-10-11 19:07:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076764
2020-10-11 19:07:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:07:30 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.495 | nll_loss 6.348 | ppl 81.43 | wps 45813.7 | wpb 2270.2 | bsz 84.4 | num_updates 2196 | best_loss 7.495
2020-10-11 19:07:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:07:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 12 @ 2196 updates, score 7.495) (writing took 1.929385445007938 seconds)
2020-10-11 19:07:32 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-11 19:07:32 | INFO | train | epoch 012 | loss 7.645 | nll_loss 6.571 | ppl 95.09 | wps 18870.7 | ups 2.83 | wpb 6662 | bsz 243.2 | num_updates 2196 | lr 0.000109845 | gnorm 1.353 | clip 0 | train_wall 58 | wall 746
2020-10-11 19:07:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-11 19:07:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-11 19:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:07:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000541
2020-10-11 19:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006059
2020-10-11 19:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-11 19:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.110270
2020-10-11 19:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.116931
2020-10-11 19:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004258
2020-10-11 19:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-11 19:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100243
2020-10-11 19:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104993
2020-10-11 19:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:07:32 | INFO | fairseq.trainer | begin training epoch 13
2020-10-11 19:07:33 | INFO | train_inner | epoch 013:      4 / 183 loss=7.642, nll_loss=6.567, ppl=94.83, wps=17909.8, ups=2.63, wpb=6819.4, bsz=253.4, num_updates=2200, lr=0.000110045, gnorm=1.408, clip=0, train_wall=32, wall=748
2020-10-11 19:08:05 | INFO | train_inner | epoch 013:    104 / 183 loss=7.556, nll_loss=6.469, ppl=88.59, wps=20569.3, ups=3.09, wpb=6661.2, bsz=249.2, num_updates=2300, lr=0.000115043, gnorm=1.177, clip=0, train_wall=32, wall=780
2020-10-11 19:08:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000711
2020-10-11 19:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042585
2020-10-11 19:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032978
2020-10-11 19:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076612
2020-10-11 19:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000645
2020-10-11 19:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042580
2020-10-11 19:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033081
2020-10-11 19:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076624
2020-10-11 19:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:08:34 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.327 | nll_loss 6.158 | ppl 71.42 | wps 45394 | wpb 2270.2 | bsz 84.4 | num_updates 2379 | best_loss 7.327
2020-10-11 19:08:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:08:36 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 13 @ 2379 updates, score 7.327) (writing took 1.4888658239942743 seconds)
2020-10-11 19:08:36 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-11 19:08:36 | INFO | train | epoch 013 | loss 7.531 | nll_loss 6.44 | ppl 86.8 | wps 18972.5 | ups 2.85 | wpb 6662 | bsz 243.2 | num_updates 2379 | lr 0.000118991 | gnorm 1.196 | clip 0 | train_wall 58 | wall 811
2020-10-11 19:08:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-11 19:08:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-11 19:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:08:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000611
2020-10-11 19:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006182
2020-10-11 19:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-11 19:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101489
2020-10-11 19:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108184
2020-10-11 19:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004457
2020-10-11 19:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 19:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099740
2020-10-11 19:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104699
2020-10-11 19:08:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:08:36 | INFO | fairseq.trainer | begin training epoch 14
2020-10-11 19:08:43 | INFO | train_inner | epoch 014:     21 / 183 loss=7.509, nll_loss=6.413, ppl=85.24, wps=17760.1, ups=2.67, wpb=6647.3, bsz=234.7, num_updates=2400, lr=0.00012004, gnorm=1.194, clip=0, train_wall=32, wall=818
2020-10-11 19:09:15 | INFO | train_inner | epoch 014:    121 / 183 loss=7.408, nll_loss=6.298, ppl=78.66, wps=20529.8, ups=3.09, wpb=6652.8, bsz=247.2, num_updates=2500, lr=0.000125037, gnorm=1.193, clip=0, train_wall=32, wall=850
2020-10-11 19:09:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000829
2020-10-11 19:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043615
2020-10-11 19:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033425
2020-10-11 19:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.078210
2020-10-11 19:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000673
2020-10-11 19:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043268
2020-10-11 19:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033281
2020-10-11 19:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077549
2020-10-11 19:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:09:39 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.261 | nll_loss 6.058 | ppl 66.62 | wps 45612.8 | wpb 2270.2 | bsz 84.4 | num_updates 2562 | best_loss 7.261
2020-10-11 19:09:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:09:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 14 @ 2562 updates, score 7.261) (writing took 1.5168389850005042 seconds)
2020-10-11 19:09:40 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-11 19:09:40 | INFO | train | epoch 014 | loss 7.429 | nll_loss 6.322 | ppl 80.03 | wps 18918.8 | ups 2.84 | wpb 6662 | bsz 243.2 | num_updates 2562 | lr 0.000128136 | gnorm 1.193 | clip 0 | train_wall 58 | wall 875
2020-10-11 19:09:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-11 19:09:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-11 19:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:09:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000972
2020-10-11 19:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.010344
2020-10-11 19:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000278
2020-10-11 19:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104223
2020-10-11 19:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115500
2020-10-11 19:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004317
2020-10-11 19:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-11 19:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100695
2020-10-11 19:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105510
2020-10-11 19:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:09:40 | INFO | fairseq.trainer | begin training epoch 15
2020-10-11 19:09:53 | INFO | train_inner | epoch 015:     38 / 183 loss=7.429, nll_loss=6.321, ppl=79.96, wps=17825.5, ups=2.67, wpb=6685.6, bsz=227.4, num_updates=2600, lr=0.000130035, gnorm=1.164, clip=0, train_wall=32, wall=888
2020-10-11 19:10:26 | INFO | train_inner | epoch 015:    138 / 183 loss=7.34, nll_loss=6.22, ppl=74.52, wps=20547.9, ups=3.05, wpb=6743, bsz=245.4, num_updates=2700, lr=0.000135032, gnorm=1.217, clip=0, train_wall=32, wall=920
2020-10-11 19:10:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000821
2020-10-11 19:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043540
2020-10-11 19:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033541
2020-10-11 19:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.078247
2020-10-11 19:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000679
2020-10-11 19:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042819
2020-10-11 19:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033338
2020-10-11 19:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077162
2020-10-11 19:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:10:43 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.224 | nll_loss 6.03 | ppl 65.36 | wps 45465.6 | wpb 2270.2 | bsz 84.4 | num_updates 2745 | best_loss 7.224
2020-10-11 19:10:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:10:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 15 @ 2745 updates, score 7.224) (writing took 1.4925008440040983 seconds)
2020-10-11 19:10:45 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-11 19:10:45 | INFO | train | epoch 015 | loss 7.326 | nll_loss 6.203 | ppl 73.67 | wps 18885 | ups 2.83 | wpb 6662 | bsz 243.2 | num_updates 2745 | lr 0.000137281 | gnorm 1.187 | clip 0 | train_wall 59 | wall 940
2020-10-11 19:10:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-11 19:10:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-11 19:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:10:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000647
2020-10-11 19:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006589
2020-10-11 19:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-11 19:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106500
2020-10-11 19:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113689
2020-10-11 19:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004447
2020-10-11 19:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-11 19:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100575
2020-10-11 19:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105537
2020-10-11 19:10:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:10:45 | INFO | fairseq.trainer | begin training epoch 16
2020-10-11 19:11:03 | INFO | train_inner | epoch 016:     55 / 183 loss=7.2, nll_loss=6.06, ppl=66.71, wps=17833.6, ups=2.68, wpb=6654.3, bsz=257.7, num_updates=2800, lr=0.00014003, gnorm=1.188, clip=0, train_wall=32, wall=958
2020-10-11 19:11:36 | INFO | train_inner | epoch 016:    155 / 183 loss=7.211, nll_loss=6.071, ppl=67.21, wps=20238.3, ups=3.06, wpb=6617, bsz=242.8, num_updates=2900, lr=0.000145028, gnorm=1.158, clip=0, train_wall=32, wall=990
2020-10-11 19:11:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000827
2020-10-11 19:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043859
2020-10-11 19:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032812
2020-10-11 19:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077836
2020-10-11 19:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000672
2020-10-11 19:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043479
2020-10-11 19:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032959
2020-10-11 19:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077430
2020-10-11 19:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:11:48 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.116 | nll_loss 5.904 | ppl 59.86 | wps 45219.4 | wpb 2270.2 | bsz 84.4 | num_updates 2928 | best_loss 7.116
2020-10-11 19:11:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:11:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 16 @ 2928 updates, score 7.116) (writing took 2.0620606140000746 seconds)
2020-10-11 19:11:50 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-11 19:11:50 | INFO | train | epoch 016 | loss 7.216 | nll_loss 6.077 | ppl 67.51 | wps 18699.1 | ups 2.81 | wpb 6662 | bsz 243.2 | num_updates 2928 | lr 0.000146427 | gnorm 1.163 | clip 0 | train_wall 59 | wall 1005
2020-10-11 19:11:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-11 19:11:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-11 19:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:11:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000608
2020-10-11 19:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006159
2020-10-11 19:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000260
2020-10-11 19:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.113011
2020-10-11 19:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.119820
2020-10-11 19:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004824
2020-10-11 19:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-11 19:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101072
2020-10-11 19:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106426
2020-10-11 19:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:11:50 | INFO | fairseq.trainer | begin training epoch 17
2020-10-11 19:12:14 | INFO | train_inner | epoch 017:     72 / 183 loss=7.136, nll_loss=5.986, ppl=63.36, wps=17690.2, ups=2.62, wpb=6744, bsz=247.1, num_updates=3000, lr=0.000150025, gnorm=1.156, clip=0, train_wall=32, wall=1029
2020-10-11 19:12:47 | INFO | train_inner | epoch 017:    172 / 183 loss=7.135, nll_loss=5.983, ppl=63.26, wps=20083.9, ups=3.04, wpb=6601.4, bsz=240.8, num_updates=3100, lr=0.000155023, gnorm=1.183, clip=0, train_wall=32, wall=1061
2020-10-11 19:12:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000801
2020-10-11 19:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043469
2020-10-11 19:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033005
2020-10-11 19:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077608
2020-10-11 19:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000732
2020-10-11 19:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042897
2020-10-11 19:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032578
2020-10-11 19:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076533
2020-10-11 19:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:12:53 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.011 | nll_loss 5.792 | ppl 55.4 | wps 45281.6 | wpb 2270.2 | bsz 84.4 | num_updates 3111 | best_loss 7.011
2020-10-11 19:12:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:12:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 17 @ 3111 updates, score 7.011) (writing took 1.812059349991614 seconds)
2020-10-11 19:12:55 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-11 19:12:55 | INFO | train | epoch 017 | loss 7.112 | nll_loss 5.957 | ppl 62.12 | wps 18723.8 | ups 2.81 | wpb 6662 | bsz 243.2 | num_updates 3111 | lr 0.000155572 | gnorm 1.174 | clip 0 | train_wall 59 | wall 1070
2020-10-11 19:12:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-11 19:12:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-11 19:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:12:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000798
2020-10-11 19:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008762
2020-10-11 19:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000306
2020-10-11 19:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103744
2020-10-11 19:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113219
2020-10-11 19:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004317
2020-10-11 19:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 19:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101366
2020-10-11 19:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106185
2020-10-11 19:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:12:55 | INFO | fairseq.trainer | begin training epoch 18
2020-10-11 19:13:24 | INFO | train_inner | epoch 018:     89 / 183 loss=6.983, nll_loss=5.811, ppl=56.13, wps=17466.5, ups=2.65, wpb=6590.6, bsz=245.3, num_updates=3200, lr=0.00016002, gnorm=1.149, clip=0, train_wall=32, wall=1099
2020-10-11 19:13:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000875
2020-10-11 19:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043361
2020-10-11 19:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033892
2020-10-11 19:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.078467
2020-10-11 19:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000690
2020-10-11 19:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042223
2020-10-11 19:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033136
2020-10-11 19:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076374
2020-10-11 19:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:13:58 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.933 | nll_loss 5.689 | ppl 51.59 | wps 44795.6 | wpb 2270.2 | bsz 84.4 | num_updates 3294 | best_loss 6.933
2020-10-11 19:13:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:14:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 18 @ 3294 updates, score 6.933) (writing took 1.490119745998527 seconds)
2020-10-11 19:14:00 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-11 19:14:00 | INFO | train | epoch 018 | loss 6.991 | nll_loss 5.818 | ppl 56.43 | wps 18835.5 | ups 2.83 | wpb 6662 | bsz 243.2 | num_updates 3294 | lr 0.000164718 | gnorm 1.158 | clip 0 | train_wall 59 | wall 1135
2020-10-11 19:14:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-11 19:14:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-11 19:14:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:14:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000674
2020-10-11 19:14:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007546
2020-10-11 19:14:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:14:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000226
2020-10-11 19:14:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:14:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103387
2020-10-11 19:14:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111698
2020-10-11 19:14:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:14:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:14:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004333
2020-10-11 19:14:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:14:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 19:14:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:14:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099823
2020-10-11 19:14:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104701
2020-10-11 19:14:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:14:00 | INFO | fairseq.trainer | begin training epoch 19
2020-10-11 19:14:02 | INFO | train_inner | epoch 019:      6 / 183 loss=6.985, nll_loss=5.811, ppl=56.14, wps=17810, ups=2.65, wpb=6714.2, bsz=240.5, num_updates=3300, lr=0.000165018, gnorm=1.158, clip=0, train_wall=32, wall=1137
2020-10-11 19:14:35 | INFO | train_inner | epoch 019:    106 / 183 loss=6.882, nll_loss=5.694, ppl=51.77, wps=20892.6, ups=3.05, wpb=6855.6, bsz=249.8, num_updates=3400, lr=0.000170015, gnorm=1.167, clip=0, train_wall=32, wall=1170
2020-10-11 19:15:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000714
2020-10-11 19:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043563
2020-10-11 19:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032513
2020-10-11 19:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077126
2020-10-11 19:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000664
2020-10-11 19:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042909
2020-10-11 19:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032426
2020-10-11 19:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076365
2020-10-11 19:15:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:15:03 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.825 | nll_loss 5.553 | ppl 46.95 | wps 44929.2 | wpb 2270.2 | bsz 84.4 | num_updates 3477 | best_loss 6.825
2020-10-11 19:15:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:15:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 19 @ 3477 updates, score 6.825) (writing took 1.8139429059956456 seconds)
2020-10-11 19:15:05 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-11 19:15:05 | INFO | train | epoch 019 | loss 6.88 | nll_loss 5.691 | ppl 51.64 | wps 18737.3 | ups 2.81 | wpb 6662 | bsz 243.2 | num_updates 3477 | lr 0.000173863 | gnorm 1.16 | clip 0 | train_wall 59 | wall 1200
2020-10-11 19:15:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-11 19:15:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-11 19:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:15:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000954
2020-10-11 19:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.009019
2020-10-11 19:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000225
2020-10-11 19:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101930
2020-10-11 19:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111665
2020-10-11 19:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004314
2020-10-11 19:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-11 19:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100896
2020-10-11 19:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105725
2020-10-11 19:15:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:15:05 | INFO | fairseq.trainer | begin training epoch 20
2020-10-11 19:15:13 | INFO | train_inner | epoch 020:     23 / 183 loss=6.86, nll_loss=5.667, ppl=50.81, wps=17195.9, ups=2.66, wpb=6476.2, bsz=228.3, num_updates=3500, lr=0.000175013, gnorm=1.135, clip=0, train_wall=32, wall=1207
2020-10-11 19:15:45 | INFO | train_inner | epoch 020:    123 / 183 loss=6.811, nll_loss=5.611, ppl=48.86, wps=20249.9, ups=3.05, wpb=6641.8, bsz=243.8, num_updates=3600, lr=0.00018001, gnorm=1.164, clip=0, train_wall=32, wall=1240
2020-10-11 19:16:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:16:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:16:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000817
2020-10-11 19:16:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:16:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042147
2020-10-11 19:16:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:16:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033243
2020-10-11 19:16:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076545
2020-10-11 19:16:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:16:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:16:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000805
2020-10-11 19:16:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:16:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042690
2020-10-11 19:16:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:16:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033204
2020-10-11 19:16:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077020
2020-10-11 19:16:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:16:08 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.729 | nll_loss 5.454 | ppl 43.84 | wps 45237.1 | wpb 2270.2 | bsz 84.4 | num_updates 3660 | best_loss 6.729
2020-10-11 19:16:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:16:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 20 @ 3660 updates, score 6.729) (writing took 1.618772189001902 seconds)
2020-10-11 19:16:10 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-11 19:16:10 | INFO | train | epoch 020 | loss 6.757 | nll_loss 5.549 | ppl 46.83 | wps 18800.7 | ups 2.82 | wpb 6662 | bsz 243.2 | num_updates 3660 | lr 0.000183009 | gnorm 1.161 | clip 0 | train_wall 59 | wall 1265
2020-10-11 19:16:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-11 19:16:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-11 19:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:16:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000596
2020-10-11 19:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006085
2020-10-11 19:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002411
2020-10-11 19:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.126672
2020-10-11 19:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.163836
2020-10-11 19:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004351
2020-10-11 19:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-11 19:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099019
2020-10-11 19:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103865
2020-10-11 19:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:16:10 | INFO | fairseq.trainer | begin training epoch 21
2020-10-11 19:16:23 | INFO | train_inner | epoch 021:     40 / 183 loss=6.688, nll_loss=5.469, ppl=44.3, wps=17566.8, ups=2.65, wpb=6628.5, bsz=245.3, num_updates=3700, lr=0.000185008, gnorm=1.224, clip=0, train_wall=32, wall=1278
2020-10-11 19:16:56 | INFO | train_inner | epoch 021:    140 / 183 loss=6.67, nll_loss=5.447, ppl=43.62, wps=20632.2, ups=3.04, wpb=6789.3, bsz=231.5, num_updates=3800, lr=0.000190005, gnorm=1.128, clip=0, train_wall=32, wall=1311
2020-10-11 19:17:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:17:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:17:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000817
2020-10-11 19:17:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:17:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042841
2020-10-11 19:17:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:17:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033258
2020-10-11 19:17:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077255
2020-10-11 19:17:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:17:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:17:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000671
2020-10-11 19:17:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:17:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043428
2020-10-11 19:17:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:17:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033103
2020-10-11 19:17:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077536
2020-10-11 19:17:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:17:13 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.667 | nll_loss 5.361 | ppl 41.1 | wps 45021.5 | wpb 2270.2 | bsz 84.4 | num_updates 3843 | best_loss 6.667
2020-10-11 19:17:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:17:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 21 @ 3843 updates, score 6.667) (writing took 1.8149950299994089 seconds)
2020-10-11 19:17:15 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-11 19:17:15 | INFO | train | epoch 021 | loss 6.641 | nll_loss 5.415 | ppl 42.67 | wps 18696.6 | ups 2.81 | wpb 6662 | bsz 243.2 | num_updates 3843 | lr 0.000192154 | gnorm 1.176 | clip 0 | train_wall 59 | wall 1330
2020-10-11 19:17:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-11 19:17:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-11 19:17:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:17:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000959
2020-10-11 19:17:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008982
2020-10-11 19:17:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:17:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000217
2020-10-11 19:17:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:17:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103573
2020-10-11 19:17:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113268
2020-10-11 19:17:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:17:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:17:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004346
2020-10-11 19:17:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:17:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 19:17:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:17:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099934
2020-10-11 19:17:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104782
2020-10-11 19:17:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:17:15 | INFO | fairseq.trainer | begin training epoch 22
2020-10-11 19:17:34 | INFO | train_inner | epoch 022:     57 / 183 loss=6.486, nll_loss=5.237, ppl=37.71, wps=17335.9, ups=2.64, wpb=6576.7, bsz=265.7, num_updates=3900, lr=0.000195003, gnorm=1.193, clip=0, train_wall=32, wall=1349
2020-10-11 19:18:07 | INFO | train_inner | epoch 022:    157 / 183 loss=6.553, nll_loss=5.311, ppl=39.71, wps=20232.8, ups=3.04, wpb=6652.8, bsz=241, num_updates=4000, lr=0.0002, gnorm=1.167, clip=0, train_wall=32, wall=1382
2020-10-11 19:18:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000808
2020-10-11 19:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043907
2020-10-11 19:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033023
2020-10-11 19:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.078080
2020-10-11 19:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000672
2020-10-11 19:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043047
2020-10-11 19:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033292
2020-10-11 19:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077334
2020-10-11 19:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:18:18 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.559 | nll_loss 5.23 | ppl 37.52 | wps 45263.4 | wpb 2270.2 | bsz 84.4 | num_updates 4026 | best_loss 6.559
2020-10-11 19:18:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:18:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 22 @ 4026 updates, score 6.559) (writing took 1.9313774079928407 seconds)
2020-10-11 19:18:20 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-11 19:18:20 | INFO | train | epoch 022 | loss 6.511 | nll_loss 5.264 | ppl 38.42 | wps 18658.6 | ups 2.8 | wpb 6662 | bsz 243.2 | num_updates 4026 | lr 0.000199353 | gnorm 1.151 | clip 0 | train_wall 59 | wall 1395
2020-10-11 19:18:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-11 19:18:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-11 19:18:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:18:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000998
2020-10-11 19:18:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008962
2020-10-11 19:18:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:18:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000218
2020-10-11 19:18:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:18:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101314
2020-10-11 19:18:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110906
2020-10-11 19:18:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:18:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:18:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004318
2020-10-11 19:18:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:18:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 19:18:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:18:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099470
2020-10-11 19:18:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104288
2020-10-11 19:18:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:18:21 | INFO | fairseq.trainer | begin training epoch 23
2020-10-11 19:18:45 | INFO | train_inner | epoch 023:     74 / 183 loss=6.399, nll_loss=5.136, ppl=35.17, wps=17510.3, ups=2.64, wpb=6629.9, bsz=233, num_updates=4100, lr=0.000197546, gnorm=1.182, clip=0, train_wall=32, wall=1419
2020-10-11 19:19:18 | INFO | train_inner | epoch 023:    174 / 183 loss=6.373, nll_loss=5.104, ppl=34.4, wps=20446.8, ups=3.04, wpb=6728.6, bsz=253.2, num_updates=4200, lr=0.00019518, gnorm=1.136, clip=0, train_wall=32, wall=1452
2020-10-11 19:19:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000804
2020-10-11 19:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042522
2020-10-11 19:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033132
2020-10-11 19:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076794
2020-10-11 19:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000686
2020-10-11 19:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043315
2020-10-11 19:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:19:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033202
2020-10-11 19:19:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077525
2020-10-11 19:19:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:19:24 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.46 | nll_loss 5.121 | ppl 34.8 | wps 45205.3 | wpb 2270.2 | bsz 84.4 | num_updates 4209 | best_loss 6.46
2020-10-11 19:19:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:19:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 23 @ 4209 updates, score 6.46) (writing took 1.8495837800001027 seconds)
2020-10-11 19:19:25 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-11 19:19:25 | INFO | train | epoch 023 | loss 6.383 | nll_loss 5.116 | ppl 34.68 | wps 18724.9 | ups 2.81 | wpb 6662 | bsz 243.2 | num_updates 4209 | lr 0.000194971 | gnorm 1.185 | clip 0 | train_wall 59 | wall 1460
2020-10-11 19:19:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-11 19:19:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-11 19:19:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:19:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000608
2020-10-11 19:19:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006142
2020-10-11 19:19:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:19:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000255
2020-10-11 19:19:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.107788
2020-10-11 19:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114533
2020-10-11 19:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004338
2020-10-11 19:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-11 19:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099379
2020-10-11 19:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104227
2020-10-11 19:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:19:26 | INFO | fairseq.trainer | begin training epoch 24
2020-10-11 19:19:56 | INFO | train_inner | epoch 024:     91 / 183 loss=6.3, nll_loss=5.021, ppl=32.46, wps=17788.9, ups=2.63, wpb=6752.1, bsz=233.1, num_updates=4300, lr=0.000192897, gnorm=1.168, clip=0, train_wall=32, wall=1490
2020-10-11 19:20:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000808
2020-10-11 19:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043232
2020-10-11 19:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032621
2020-10-11 19:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076993
2020-10-11 19:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000684
2020-10-11 19:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042086
2020-10-11 19:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:20:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032443
2020-10-11 19:20:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075531
2020-10-11 19:20:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:20:29 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.422 | nll_loss 5.07 | ppl 33.59 | wps 45472.7 | wpb 2270.2 | bsz 84.4 | num_updates 4392 | best_loss 6.422
2020-10-11 19:20:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:20:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 24 @ 4392 updates, score 6.422) (writing took 1.4850722889968893 seconds)
2020-10-11 19:20:30 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-11 19:20:30 | INFO | train | epoch 024 | loss 6.247 | nll_loss 4.959 | ppl 31.1 | wps 18859.1 | ups 2.83 | wpb 6662 | bsz 243.2 | num_updates 4392 | lr 0.000190866 | gnorm 1.173 | clip 0 | train_wall 59 | wall 1525
2020-10-11 19:20:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-11 19:20:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-11 19:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:20:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000632
2020-10-11 19:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006836
2020-10-11 19:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000253
2020-10-11 19:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105448
2020-10-11 19:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112897
2020-10-11 19:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004319
2020-10-11 19:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-11 19:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100558
2020-10-11 19:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105501
2020-10-11 19:20:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:20:30 | INFO | fairseq.trainer | begin training epoch 25
2020-10-11 19:20:33 | INFO | train_inner | epoch 025:      8 / 183 loss=6.201, nll_loss=4.906, ppl=29.97, wps=17440.4, ups=2.68, wpb=6515.9, bsz=239.1, num_updates=4400, lr=0.000190693, gnorm=1.186, clip=0, train_wall=32, wall=1528
2020-10-11 19:21:06 | INFO | train_inner | epoch 025:    108 / 183 loss=6.128, nll_loss=4.822, ppl=28.28, wps=20333, ups=3.06, wpb=6644.9, bsz=247.4, num_updates=4500, lr=0.000188562, gnorm=1.188, clip=0, train_wall=32, wall=1560
2020-10-11 19:21:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000721
2020-10-11 19:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043649
2020-10-11 19:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033334
2020-10-11 19:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.078042
2020-10-11 19:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000679
2020-10-11 19:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042785
2020-10-11 19:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033006
2020-10-11 19:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076789
2020-10-11 19:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:21:33 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.316 | nll_loss 4.93 | ppl 30.49 | wps 45039.8 | wpb 2270.2 | bsz 84.4 | num_updates 4575 | best_loss 6.316
2020-10-11 19:21:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:21:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 25 @ 4575 updates, score 6.316) (writing took 1.4933529809932224 seconds)
2020-10-11 19:21:35 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-11 19:21:35 | INFO | train | epoch 025 | loss 6.121 | nll_loss 4.813 | ppl 28.11 | wps 18786.5 | ups 2.82 | wpb 6662 | bsz 243.2 | num_updates 4575 | lr 0.00018701 | gnorm 1.168 | clip 0 | train_wall 59 | wall 1590
2020-10-11 19:21:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-11 19:21:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-11 19:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:21:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000606
2020-10-11 19:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007013
2020-10-11 19:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000261
2020-10-11 19:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104289
2020-10-11 19:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111913
2020-10-11 19:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004326
2020-10-11 19:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 19:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100671
2020-10-11 19:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105496
2020-10-11 19:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:21:35 | INFO | fairseq.trainer | begin training epoch 26
2020-10-11 19:21:43 | INFO | train_inner | epoch 026:     25 / 183 loss=6.093, nll_loss=4.78, ppl=27.48, wps=17795.9, ups=2.64, wpb=6730.2, bsz=242.2, num_updates=4600, lr=0.000186501, gnorm=1.161, clip=0, train_wall=32, wall=1598
2020-10-11 19:22:16 | INFO | train_inner | epoch 026:    125 / 183 loss=5.99, nll_loss=4.661, ppl=25.31, wps=20393.1, ups=3.05, wpb=6675.5, bsz=257.1, num_updates=4700, lr=0.000184506, gnorm=1.125, clip=0, train_wall=32, wall=1631
2020-10-11 19:22:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:22:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:22:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000836
2020-10-11 19:22:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:22:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042863
2020-10-11 19:22:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:22:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032968
2020-10-11 19:22:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077005
2020-10-11 19:22:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:22:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:22:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000692
2020-10-11 19:22:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:22:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042945
2020-10-11 19:22:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:22:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032976
2020-10-11 19:22:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076934
2020-10-11 19:22:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:22:38 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.242 | nll_loss 4.833 | ppl 28.5 | wps 44941.5 | wpb 2270.2 | bsz 84.4 | num_updates 4758 | best_loss 6.242
2020-10-11 19:22:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:22:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 26 @ 4758 updates, score 6.242) (writing took 1.4952546330023324 seconds)
2020-10-11 19:22:40 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-11 19:22:40 | INFO | train | epoch 026 | loss 6.004 | nll_loss 4.676 | ppl 25.57 | wps 18838.7 | ups 2.83 | wpb 6662 | bsz 243.2 | num_updates 4758 | lr 0.000183378 | gnorm 1.163 | clip 0 | train_wall 59 | wall 1654
2020-10-11 19:22:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-11 19:22:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-11 19:22:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:22:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000697
2020-10-11 19:22:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006857
2020-10-11 19:22:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:22:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-11 19:22:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:22:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103750
2020-10-11 19:22:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111145
2020-10-11 19:22:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:22:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:22:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004314
2020-10-11 19:22:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:22:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-11 19:22:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:22:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099199
2020-10-11 19:22:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104001
2020-10-11 19:22:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:22:40 | INFO | fairseq.trainer | begin training epoch 27
2020-10-11 19:22:54 | INFO | train_inner | epoch 027:     42 / 183 loss=5.926, nll_loss=4.586, ppl=24.02, wps=17809.8, ups=2.66, wpb=6703.9, bsz=248.7, num_updates=4800, lr=0.000182574, gnorm=1.195, clip=0, train_wall=32, wall=1669
2020-10-11 19:23:27 | INFO | train_inner | epoch 027:    142 / 183 loss=5.923, nll_loss=4.582, ppl=23.95, wps=20244.1, ups=3.05, wpb=6633.2, bsz=228.6, num_updates=4900, lr=0.000180702, gnorm=1.16, clip=0, train_wall=32, wall=1701
2020-10-11 19:23:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000819
2020-10-11 19:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042749
2020-10-11 19:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032600
2020-10-11 19:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076501
2020-10-11 19:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000669
2020-10-11 19:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041232
2020-10-11 19:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032745
2020-10-11 19:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074964
2020-10-11 19:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:23:43 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.225 | nll_loss 4.821 | ppl 28.27 | wps 45146.3 | wpb 2270.2 | bsz 84.4 | num_updates 4941 | best_loss 6.225
2020-10-11 19:23:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:23:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 27 @ 4941 updates, score 6.225) (writing took 1.4971654600085458 seconds)
2020-10-11 19:23:45 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-11 19:23:45 | INFO | train | epoch 027 | loss 5.888 | nll_loss 4.542 | ppl 23.3 | wps 18789.2 | ups 2.82 | wpb 6662 | bsz 243.2 | num_updates 4941 | lr 0.00017995 | gnorm 1.163 | clip 0 | train_wall 59 | wall 1719
2020-10-11 19:23:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-11 19:23:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-11 19:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:23:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000702
2020-10-11 19:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006749
2020-10-11 19:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000271
2020-10-11 19:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105061
2020-10-11 19:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112438
2020-10-11 19:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004334
2020-10-11 19:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-11 19:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100175
2020-10-11 19:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104997
2020-10-11 19:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:23:45 | INFO | fairseq.trainer | begin training epoch 28
2020-10-11 19:24:04 | INFO | train_inner | epoch 028:     59 / 183 loss=5.794, nll_loss=4.434, ppl=21.62, wps=17625.4, ups=2.66, wpb=6614.9, bsz=244.6, num_updates=5000, lr=0.000178885, gnorm=1.166, clip=0, train_wall=32, wall=1739
2020-10-11 19:24:37 | INFO | train_inner | epoch 028:    159 / 183 loss=5.814, nll_loss=4.455, ppl=21.93, wps=20232.9, ups=3.03, wpb=6677.5, bsz=247.2, num_updates=5100, lr=0.000177123, gnorm=1.191, clip=0, train_wall=32, wall=1772
2020-10-11 19:24:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:24:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:24:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000807
2020-10-11 19:24:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:24:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042947
2020-10-11 19:24:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:24:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032509
2020-10-11 19:24:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076589
2020-10-11 19:24:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:24:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:24:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000669
2020-10-11 19:24:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:24:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042018
2020-10-11 19:24:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:24:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032385
2020-10-11 19:24:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075402
2020-10-11 19:24:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:24:48 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.144 | nll_loss 4.722 | ppl 26.4 | wps 45528.1 | wpb 2270.2 | bsz 84.4 | num_updates 5124 | best_loss 6.144
2020-10-11 19:24:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:24:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 28 @ 5124 updates, score 6.144) (writing took 1.8512930610013427 seconds)
2020-10-11 19:24:50 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-11 19:24:50 | INFO | train | epoch 028 | loss 5.794 | nll_loss 4.433 | ppl 21.59 | wps 18699.2 | ups 2.81 | wpb 6662 | bsz 243.2 | num_updates 5124 | lr 0.000176708 | gnorm 1.187 | clip 0 | train_wall 59 | wall 1785
2020-10-11 19:24:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-11 19:24:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-11 19:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:24:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000991
2020-10-11 19:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.009165
2020-10-11 19:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000253
2020-10-11 19:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103045
2020-10-11 19:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112916
2020-10-11 19:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004320
2020-10-11 19:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 19:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100723
2020-10-11 19:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105536
2020-10-11 19:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:24:50 | INFO | fairseq.trainer | begin training epoch 29
2020-10-11 19:25:15 | INFO | train_inner | epoch 029:     76 / 183 loss=5.707, nll_loss=4.332, ppl=20.14, wps=17744.8, ups=2.66, wpb=6677, bsz=235.1, num_updates=5200, lr=0.000175412, gnorm=1.148, clip=0, train_wall=32, wall=1809
2020-10-11 19:25:48 | INFO | train_inner | epoch 029:    176 / 183 loss=5.719, nll_loss=4.344, ppl=20.31, wps=20181.5, ups=3.03, wpb=6663.1, bsz=245.4, num_updates=5300, lr=0.000173749, gnorm=1.182, clip=0, train_wall=32, wall=1842
2020-10-11 19:25:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:25:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:25:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000837
2020-10-11 19:25:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:25:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043171
2020-10-11 19:25:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:25:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033154
2020-10-11 19:25:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077497
2020-10-11 19:25:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:25:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:25:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000673
2020-10-11 19:25:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:25:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042660
2020-10-11 19:25:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:25:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033004
2020-10-11 19:25:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076654
2020-10-11 19:25:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:25:53 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.12 | nll_loss 4.673 | ppl 25.5 | wps 45050.9 | wpb 2270.2 | bsz 84.4 | num_updates 5307 | best_loss 6.12
2020-10-11 19:25:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:25:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 29 @ 5307 updates, score 6.12) (writing took 1.792620154999895 seconds)
2020-10-11 19:25:55 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-11 19:25:55 | INFO | train | epoch 029 | loss 5.69 | nll_loss 4.312 | ppl 19.86 | wps 18726.3 | ups 2.81 | wpb 6662 | bsz 243.2 | num_updates 5307 | lr 0.000173634 | gnorm 1.162 | clip 0 | train_wall 59 | wall 1850
2020-10-11 19:25:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-11 19:25:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-11 19:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:25:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000968
2020-10-11 19:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008754
2020-10-11 19:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000216
2020-10-11 19:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103153
2020-10-11 19:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112611
2020-10-11 19:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004353
2020-10-11 19:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 19:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100462
2020-10-11 19:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105313
2020-10-11 19:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:25:55 | INFO | fairseq.trainer | begin training epoch 30
2020-10-11 19:26:25 | INFO | train_inner | epoch 030:     93 / 183 loss=5.575, nll_loss=4.18, ppl=18.12, wps=17569.6, ups=2.66, wpb=6612.2, bsz=236.8, num_updates=5400, lr=0.000172133, gnorm=1.137, clip=0, train_wall=32, wall=1880
2020-10-11 19:26:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000807
2020-10-11 19:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043064
2020-10-11 19:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032697
2020-10-11 19:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076902
2020-10-11 19:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000724
2020-10-11 19:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043649
2020-10-11 19:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033144
2020-10-11 19:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077843
2020-10-11 19:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:26:58 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.097 | nll_loss 4.641 | ppl 24.95 | wps 45283.5 | wpb 2270.2 | bsz 84.4 | num_updates 5490 | best_loss 6.097
2020-10-11 19:26:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:27:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 30 @ 5490 updates, score 6.097) (writing took 2.1012221120035974 seconds)
2020-10-11 19:27:00 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-11 19:27:00 | INFO | train | epoch 030 | loss 5.601 | nll_loss 4.208 | ppl 18.48 | wps 18648.2 | ups 2.8 | wpb 6662 | bsz 243.2 | num_updates 5490 | lr 0.000170716 | gnorm 1.178 | clip 0 | train_wall 59 | wall 1915
2020-10-11 19:27:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-11 19:27:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-11 19:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:27:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000461
2020-10-11 19:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005904
2020-10-11 19:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000241
2020-10-11 19:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.108520
2020-10-11 19:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115007
2020-10-11 19:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004320
2020-10-11 19:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-11 19:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098805
2020-10-11 19:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103615
2020-10-11 19:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:27:00 | INFO | fairseq.trainer | begin training epoch 31
2020-10-11 19:27:04 | INFO | train_inner | epoch 031:     10 / 183 loss=5.641, nll_loss=4.252, ppl=19.05, wps=17332.6, ups=2.61, wpb=6641.4, bsz=240.6, num_updates=5500, lr=0.000170561, gnorm=1.22, clip=0, train_wall=32, wall=1918
2020-10-11 19:27:36 | INFO | train_inner | epoch 031:    110 / 183 loss=5.54, nll_loss=4.137, ppl=17.59, wps=20337.5, ups=3.07, wpb=6620.4, bsz=228.5, num_updates=5600, lr=0.000169031, gnorm=1.217, clip=0, train_wall=32, wall=1951
2020-10-11 19:28:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000798
2020-10-11 19:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042759
2020-10-11 19:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032473
2020-10-11 19:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076360
2020-10-11 19:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000693
2020-10-11 19:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043346
2020-10-11 19:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033594
2020-10-11 19:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077949
2020-10-11 19:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:28:03 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.054 | nll_loss 4.59 | ppl 24.09 | wps 45252.8 | wpb 2270.2 | bsz 84.4 | num_updates 5673 | best_loss 6.054
2020-10-11 19:28:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:28:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 31 @ 5673 updates, score 6.054) (writing took 1.4850023859908106 seconds)
2020-10-11 19:28:05 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-11 19:28:05 | INFO | train | epoch 031 | loss 5.52 | nll_loss 4.113 | ppl 17.3 | wps 18830 | ups 2.83 | wpb 6662 | bsz 243.2 | num_updates 5673 | lr 0.00016794 | gnorm 1.186 | clip 0 | train_wall 59 | wall 1980
2020-10-11 19:28:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-11 19:28:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-11 19:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:28:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000739
2020-10-11 19:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007295
2020-10-11 19:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000198
2020-10-11 19:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106050
2020-10-11 19:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114019
2020-10-11 19:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004353
2020-10-11 19:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-11 19:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100599
2020-10-11 19:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105462
2020-10-11 19:28:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:28:05 | INFO | fairseq.trainer | begin training epoch 32
2020-10-11 19:28:14 | INFO | train_inner | epoch 032:     27 / 183 loss=5.442, nll_loss=4.023, ppl=16.26, wps=17854.9, ups=2.65, wpb=6741.6, bsz=262.2, num_updates=5700, lr=0.000167542, gnorm=1.121, clip=0, train_wall=32, wall=1989
2020-10-11 19:28:47 | INFO | train_inner | epoch 032:    127 / 183 loss=5.435, nll_loss=4.014, ppl=16.15, wps=20616.2, ups=3.06, wpb=6734.7, bsz=239.5, num_updates=5800, lr=0.000166091, gnorm=1.209, clip=0, train_wall=32, wall=2021
2020-10-11 19:29:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000821
2020-10-11 19:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042548
2020-10-11 19:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032665
2020-10-11 19:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076373
2020-10-11 19:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000690
2020-10-11 19:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043198
2020-10-11 19:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032424
2020-10-11 19:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076634
2020-10-11 19:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:29:08 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.053 | nll_loss 4.585 | ppl 24 | wps 45058.7 | wpb 2270.2 | bsz 84.4 | num_updates 5856 | best_loss 6.053
2020-10-11 19:29:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:29:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 32 @ 5856 updates, score 6.053) (writing took 1.9013525310001569 seconds)
2020-10-11 19:29:10 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-11 19:29:10 | INFO | train | epoch 032 | loss 5.443 | nll_loss 4.022 | ppl 16.25 | wps 18678.7 | ups 2.8 | wpb 6662 | bsz 243.2 | num_updates 5856 | lr 0.000165295 | gnorm 1.214 | clip 0 | train_wall 59 | wall 2045
2020-10-11 19:29:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-11 19:29:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-11 19:29:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:29:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000827
2020-10-11 19:29:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008885
2020-10-11 19:29:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:29:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000212
2020-10-11 19:29:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:29:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102250
2020-10-11 19:29:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111831
2020-10-11 19:29:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:29:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:29:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004323
2020-10-11 19:29:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:29:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-11 19:29:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:29:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099813
2020-10-11 19:29:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104625
2020-10-11 19:29:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:29:10 | INFO | fairseq.trainer | begin training epoch 33
2020-10-11 19:29:25 | INFO | train_inner | epoch 033:     44 / 183 loss=5.389, nll_loss=3.961, ppl=15.57, wps=17239.9, ups=2.62, wpb=6579.7, bsz=258.9, num_updates=5900, lr=0.000164677, gnorm=1.202, clip=0, train_wall=32, wall=2060
2020-10-11 19:29:58 | INFO | train_inner | epoch 033:    144 / 183 loss=5.379, nll_loss=3.947, ppl=15.42, wps=20460.7, ups=3.03, wpb=6753.4, bsz=238.9, num_updates=6000, lr=0.000163299, gnorm=1.163, clip=0, train_wall=32, wall=2093
2020-10-11 19:30:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000752
2020-10-11 19:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042146
2020-10-11 19:30:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032251
2020-10-11 19:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075477
2020-10-11 19:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000645
2020-10-11 19:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042143
2020-10-11 19:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032201
2020-10-11 19:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075308
2020-10-11 19:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:30:14 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.033 | nll_loss 4.564 | ppl 23.66 | wps 45060.3 | wpb 2270.2 | bsz 84.4 | num_updates 6039 | best_loss 6.033
2020-10-11 19:30:14 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:30:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 33 @ 6039 updates, score 6.033) (writing took 1.839160614006687 seconds)
2020-10-11 19:30:15 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-11 19:30:15 | INFO | train | epoch 033 | loss 5.357 | nll_loss 3.923 | ppl 15.17 | wps 18682.9 | ups 2.8 | wpb 6662 | bsz 243.2 | num_updates 6039 | lr 0.000162771 | gnorm 1.149 | clip 0 | train_wall 59 | wall 2110
2020-10-11 19:30:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-11 19:30:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-11 19:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:30:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000501
2020-10-11 19:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006330
2020-10-11 19:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-11 19:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:30:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099260
2020-10-11 19:30:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106096
2020-10-11 19:30:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:30:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:30:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004309
2020-10-11 19:30:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:30:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-11 19:30:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:30:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100014
2020-10-11 19:30:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104808
2020-10-11 19:30:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:30:16 | INFO | fairseq.trainer | begin training epoch 34
2020-10-11 19:30:36 | INFO | train_inner | epoch 034:     61 / 183 loss=5.272, nll_loss=3.825, ppl=14.17, wps=17586.3, ups=2.64, wpb=6666.2, bsz=252.5, num_updates=6100, lr=0.000161955, gnorm=1.19, clip=0, train_wall=32, wall=2130
2020-10-11 19:31:08 | INFO | train_inner | epoch 034:    161 / 183 loss=5.316, nll_loss=3.873, ppl=14.66, wps=20171.9, ups=3.07, wpb=6568, bsz=231.1, num_updates=6200, lr=0.000160644, gnorm=1.146, clip=0, train_wall=32, wall=2163
2020-10-11 19:31:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:31:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:31:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000817
2020-10-11 19:31:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:31:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042646
2020-10-11 19:31:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:31:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032746
2020-10-11 19:31:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076538
2020-10-11 19:31:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:31:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:31:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000676
2020-10-11 19:31:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:31:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042165
2020-10-11 19:31:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:31:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032527
2020-10-11 19:31:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075684
2020-10-11 19:31:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:31:19 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 5.991 | nll_loss 4.51 | ppl 22.78 | wps 45016.1 | wpb 2270.2 | bsz 84.4 | num_updates 6222 | best_loss 5.991
2020-10-11 19:31:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:31:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 34 @ 6222 updates, score 5.991) (writing took 1.4732797420001589 seconds)
2020-10-11 19:31:20 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-11 19:31:20 | INFO | train | epoch 034 | loss 5.28 | nll_loss 3.833 | ppl 14.25 | wps 18830.9 | ups 2.83 | wpb 6662 | bsz 243.2 | num_updates 6222 | lr 0.00016036 | gnorm 1.167 | clip 0 | train_wall 59 | wall 2175
2020-10-11 19:31:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-11 19:31:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-11 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:31:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000674
2020-10-11 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006729
2020-10-11 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000190
2020-10-11 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103577
2020-10-11 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110927
2020-10-11 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004324
2020-10-11 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099900
2020-10-11 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104722
2020-10-11 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:31:20 | INFO | fairseq.trainer | begin training epoch 35
2020-10-11 19:31:46 | INFO | train_inner | epoch 035:     78 / 183 loss=5.219, nll_loss=3.76, ppl=13.55, wps=17924.6, ups=2.64, wpb=6781.4, bsz=243.1, num_updates=6300, lr=0.000159364, gnorm=1.115, clip=0, train_wall=32, wall=2201
2020-10-11 19:32:19 | INFO | train_inner | epoch 035:    178 / 183 loss=5.23, nll_loss=3.774, ppl=13.68, wps=20106.8, ups=3.04, wpb=6619.3, bsz=244.3, num_updates=6400, lr=0.000158114, gnorm=1.165, clip=0, train_wall=32, wall=2234
2020-10-11 19:32:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:32:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:32:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000823
2020-10-11 19:32:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:32:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042617
2020-10-11 19:32:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:32:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032591
2020-10-11 19:32:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076367
2020-10-11 19:32:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:32:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:32:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000672
2020-10-11 19:32:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:32:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042653
2020-10-11 19:32:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:32:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032668
2020-10-11 19:32:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076314
2020-10-11 19:32:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:32:24 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 5.949 | nll_loss 4.446 | ppl 21.79 | wps 45222 | wpb 2270.2 | bsz 84.4 | num_updates 6405 | best_loss 5.949
2020-10-11 19:32:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:32:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 35 @ 6405 updates, score 5.949) (writing took 1.7965455079975072 seconds)
2020-10-11 19:32:25 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-11 19:32:25 | INFO | train | epoch 035 | loss 5.207 | nll_loss 3.747 | ppl 13.43 | wps 18688.8 | ups 2.81 | wpb 6662 | bsz 243.2 | num_updates 6405 | lr 0.000158052 | gnorm 1.145 | clip 0 | train_wall 59 | wall 2240
2020-10-11 19:32:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-11 19:32:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-11 19:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:32:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000551
2020-10-11 19:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006092
2020-10-11 19:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-11 19:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106019
2020-10-11 19:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112717
2020-10-11 19:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004322
2020-10-11 19:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-11 19:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100442
2020-10-11 19:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105260
2020-10-11 19:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:32:26 | INFO | fairseq.trainer | begin training epoch 36
2020-10-11 19:32:57 | INFO | train_inner | epoch 036:     95 / 183 loss=5.137, nll_loss=3.666, ppl=12.69, wps=17776.2, ups=2.65, wpb=6707.6, bsz=242.1, num_updates=6500, lr=0.000156893, gnorm=1.199, clip=0, train_wall=32, wall=2272
2020-10-11 19:33:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000817
2020-10-11 19:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042793
2020-10-11 19:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033037
2020-10-11 19:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076981
2020-10-11 19:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000671
2020-10-11 19:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042272
2020-10-11 19:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032907
2020-10-11 19:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076175
2020-10-11 19:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:33:29 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 5.932 | nll_loss 4.427 | ppl 21.51 | wps 45062.6 | wpb 2270.2 | bsz 84.4 | num_updates 6588 | best_loss 5.932
2020-10-11 19:33:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:33:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 36 @ 6588 updates, score 5.932) (writing took 1.8478986859990982 seconds)
2020-10-11 19:33:31 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-11 19:33:31 | INFO | train | epoch 036 | loss 5.143 | nll_loss 3.673 | ppl 12.75 | wps 18697.8 | ups 2.81 | wpb 6662 | bsz 243.2 | num_updates 6588 | lr 0.000155842 | gnorm 1.164 | clip 0 | train_wall 59 | wall 2305
2020-10-11 19:33:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-11 19:33:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-11 19:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:33:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000614
2020-10-11 19:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006848
2020-10-11 19:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000223
2020-10-11 19:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102636
2020-10-11 19:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110074
2020-10-11 19:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004362
2020-10-11 19:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-11 19:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100860
2020-10-11 19:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105737
2020-10-11 19:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:33:31 | INFO | fairseq.trainer | begin training epoch 37
2020-10-11 19:33:35 | INFO | train_inner | epoch 037:     12 / 183 loss=5.157, nll_loss=3.687, ppl=12.88, wps=17264.3, ups=2.62, wpb=6599.5, bsz=240.5, num_updates=6600, lr=0.0001557, gnorm=1.158, clip=0, train_wall=32, wall=2310
2020-10-11 19:34:08 | INFO | train_inner | epoch 037:    112 / 183 loss=5.088, nll_loss=3.608, ppl=12.2, wps=20206.1, ups=3.06, wpb=6596, bsz=234.8, num_updates=6700, lr=0.000154533, gnorm=1.238, clip=0, train_wall=32, wall=2342
2020-10-11 19:34:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000800
2020-10-11 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042052
2020-10-11 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032622
2020-10-11 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075803
2020-10-11 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000684
2020-10-11 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042991
2020-10-11 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032701
2020-10-11 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076693
2020-10-11 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:34:34 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 5.923 | nll_loss 4.421 | ppl 21.43 | wps 45466.2 | wpb 2270.2 | bsz 84.4 | num_updates 6771 | best_loss 5.923
2020-10-11 19:34:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:34:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 37 @ 6771 updates, score 5.923) (writing took 1.4944218340096995 seconds)
2020-10-11 19:34:35 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-11 19:34:35 | INFO | train | epoch 037 | loss 5.085 | nll_loss 3.604 | ppl 12.16 | wps 18813.8 | ups 2.82 | wpb 6662 | bsz 243.2 | num_updates 6771 | lr 0.000153721 | gnorm 1.214 | clip 0 | train_wall 59 | wall 2370
2020-10-11 19:34:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-11 19:34:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-11 19:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:34:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000696
2020-10-11 19:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007878
2020-10-11 19:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000240
2020-10-11 19:34:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100366
2020-10-11 19:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108923
2020-10-11 19:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004440
2020-10-11 19:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-11 19:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099379
2020-10-11 19:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104311
2020-10-11 19:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:34:36 | INFO | fairseq.trainer | begin training epoch 38
2020-10-11 19:34:45 | INFO | train_inner | epoch 038:     29 / 183 loss=5.033, nll_loss=3.544, ppl=11.67, wps=17470.8, ups=2.68, wpb=6509.6, bsz=247.2, num_updates=6800, lr=0.000153393, gnorm=1.195, clip=0, train_wall=32, wall=2380
2020-10-11 19:35:18 | INFO | train_inner | epoch 038:    129 / 183 loss=5.022, nll_loss=3.53, ppl=11.55, wps=20583.4, ups=3.03, wpb=6793.6, bsz=248.6, num_updates=6900, lr=0.000152277, gnorm=1.157, clip=0, train_wall=32, wall=2413
2020-10-11 19:35:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:35:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:35:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000800
2020-10-11 19:35:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:35:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042990
2020-10-11 19:35:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:35:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032546
2020-10-11 19:35:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076669
2020-10-11 19:35:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:35:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:35:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000683
2020-10-11 19:35:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:35:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042345
2020-10-11 19:35:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:35:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031969
2020-10-11 19:35:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075312
2020-10-11 19:35:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:35:39 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 5.908 | nll_loss 4.391 | ppl 20.98 | wps 45357.7 | wpb 2270.2 | bsz 84.4 | num_updates 6954 | best_loss 5.908
2020-10-11 19:35:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:35:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 38 @ 6954 updates, score 5.908) (writing took 1.8074497409979813 seconds)
2020-10-11 19:35:41 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-11 19:35:41 | INFO | train | epoch 038 | loss 5.022 | nll_loss 3.53 | ppl 11.55 | wps 18705.4 | ups 2.81 | wpb 6662 | bsz 243.2 | num_updates 6954 | lr 0.000151685 | gnorm 1.164 | clip 0 | train_wall 59 | wall 2435
2020-10-11 19:35:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-11 19:35:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-11 19:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:35:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000586
2020-10-11 19:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006335
2020-10-11 19:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-11 19:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104924
2020-10-11 19:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111936
2020-10-11 19:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004335
2020-10-11 19:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 19:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100795
2020-10-11 19:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105623
2020-10-11 19:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:35:41 | INFO | fairseq.trainer | begin training epoch 39
2020-10-11 19:35:56 | INFO | train_inner | epoch 039:     46 / 183 loss=5.028, nll_loss=3.536, ppl=11.6, wps=17602.8, ups=2.61, wpb=6738.1, bsz=240.7, num_updates=7000, lr=0.000151186, gnorm=1.165, clip=0, train_wall=32, wall=2451
2020-10-11 19:36:29 | INFO | train_inner | epoch 039:    146 / 183 loss=4.942, nll_loss=3.438, ppl=10.84, wps=20129.9, ups=3.05, wpb=6596.5, bsz=246, num_updates=7100, lr=0.000150117, gnorm=1.145, clip=0, train_wall=32, wall=2484
2020-10-11 19:36:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000725
2020-10-11 19:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043292
2020-10-11 19:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031893
2020-10-11 19:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076238
2020-10-11 19:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000666
2020-10-11 19:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042848
2020-10-11 19:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032127
2020-10-11 19:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075953
2020-10-11 19:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:36:44 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 5.896 | nll_loss 4.373 | ppl 20.73 | wps 45507.9 | wpb 2270.2 | bsz 84.4 | num_updates 7137 | best_loss 5.896
2020-10-11 19:36:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:36:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 39 @ 7137 updates, score 5.896) (writing took 1.48467290699773 seconds)
2020-10-11 19:36:46 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-11 19:36:46 | INFO | train | epoch 039 | loss 4.956 | nll_loss 3.453 | ppl 10.95 | wps 18791.6 | ups 2.82 | wpb 6662 | bsz 243.2 | num_updates 7137 | lr 0.000149728 | gnorm 1.161 | clip 0 | train_wall 59 | wall 2500
2020-10-11 19:36:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-11 19:36:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-11 19:36:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:36:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000666
2020-10-11 19:36:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007339
2020-10-11 19:36:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:36:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000207
2020-10-11 19:36:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:36:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103237
2020-10-11 19:36:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111231
2020-10-11 19:36:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:36:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:36:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004260
2020-10-11 19:36:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:36:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-11 19:36:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:36:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101782
2020-10-11 19:36:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106559
2020-10-11 19:36:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:36:46 | INFO | fairseq.trainer | begin training epoch 40
2020-10-11 19:37:06 | INFO | train_inner | epoch 040:     63 / 183 loss=4.894, nll_loss=3.383, ppl=10.43, wps=17663.6, ups=2.68, wpb=6600.6, bsz=248.3, num_updates=7200, lr=0.000149071, gnorm=1.194, clip=0, train_wall=32, wall=2521
2020-10-11 19:37:39 | INFO | train_inner | epoch 040:    163 / 183 loss=4.92, nll_loss=3.41, ppl=10.63, wps=20375.8, ups=3.04, wpb=6705.2, bsz=238.4, num_updates=7300, lr=0.000148047, gnorm=1.203, clip=0, train_wall=32, wall=2554
2020-10-11 19:37:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 19:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000830
2020-10-11 19:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043330
2020-10-11 19:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032274
2020-10-11 19:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076762
2020-10-11 19:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000702
2020-10-11 19:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041710
2020-10-11 19:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032317
2020-10-11 19:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075049
2020-10-11 19:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:37:49 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 5.881 | nll_loss 4.361 | ppl 20.54 | wps 45170.4 | wpb 2270.2 | bsz 84.4 | num_updates 7320 | best_loss 5.881
2020-10-11 19:37:49 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 19:37:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 40 @ 7320 updates, score 5.881) (writing took 1.8381813460000558 seconds)
2020-10-11 19:37:51 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-11 19:37:51 | INFO | train | epoch 040 | loss 4.91 | nll_loss 3.399 | ppl 10.55 | wps 18674.2 | ups 2.8 | wpb 6662 | bsz 243.2 | num_updates 7320 | lr 0.000147844 | gnorm 1.202 | clip 0 | train_wall 59 | wall 2566
2020-10-11 19:37:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-11 19:37:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-11 19:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 19:37:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000628
2020-10-11 19:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006822
2020-10-11 19:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000196
2020-10-11 19:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103632
2020-10-11 19:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111008
2020-10-11 19:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 19:37:51 | INFO | fairseq_cli.train | done training in 2565.6 seconds
