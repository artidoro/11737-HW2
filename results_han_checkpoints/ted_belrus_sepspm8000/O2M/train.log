2020-10-11 21:23:26 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_belrus_sepspm8000/O2M/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok='tgt', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='eng-bel,eng-rus', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_belrus_sepspm8000/O2M/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-11 21:23:26 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-11 21:23:26 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['bel', 'eng', 'rus']
2020-10-11 21:23:26 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bel] dictionary: 21771 types
2020-10-11 21:23:26 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 21771 types
2020-10-11 21:23:26 | INFO | fairseq.data.multilingual.multilingual_data_manager | [rus] dictionary: 21771 types
2020-10-11 21:23:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-11 21:23:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:23:26 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-11 21:23:26 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:eng-bel': 1, 'main:eng-rus': 1}
2020-10-11 21:23:26 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-bel src_langtok: 21768; tgt_langtok: None
2020-10-11 21:23:26 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belrus_sepspm8000/O2M/valid.eng-bel.eng
2020-10-11 21:23:26 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belrus_sepspm8000/O2M/valid.eng-bel.bel
2020-10-11 21:23:26 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrus_sepspm8000/O2M/ valid eng-bel 248 examples
2020-10-11 21:23:26 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-rus src_langtok: 21770; tgt_langtok: None
2020-10-11 21:23:26 | INFO | fairseq.data.data_utils | loaded 4814 examples from: fairseq/data-bin/ted_belrus_sepspm8000/O2M/valid.eng-rus.eng
2020-10-11 21:23:26 | INFO | fairseq.data.data_utils | loaded 4814 examples from: fairseq/data-bin/ted_belrus_sepspm8000/O2M/valid.eng-rus.rus
2020-10-11 21:23:26 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrus_sepspm8000/O2M/ valid eng-rus 4814 examples
2020-10-11 21:23:27 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21771, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21771, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=21771, bias=False)
  )
)
2020-10-11 21:23:27 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-11 21:23:27 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-11 21:23:27 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-11 21:23:27 | INFO | fairseq_cli.train | num. model params: 42690048 (num. trained: 42690048)
2020-10-11 21:23:29 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-11 21:23:29 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-11 21:23:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-11 21:23:29 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.732 GB ; name = GeForce RTX 2080 Ti                     
2020-10-11 21:23:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-11 21:23:29 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-11 21:23:29 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-11 21:23:29 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_last.pt
2020-10-11 21:23:29 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-11 21:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-11 21:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:23:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-11 21:23:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:eng-bel': 1, 'main:eng-rus': 1}
2020-10-11 21:23:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-bel src_langtok: 21768; tgt_langtok: None
2020-10-11 21:23:29 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belrus_sepspm8000/O2M/train.eng-bel.eng
2020-10-11 21:23:29 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belrus_sepspm8000/O2M/train.eng-bel.bel
2020-10-11 21:23:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrus_sepspm8000/O2M/ train eng-bel 4509 examples
2020-10-11 21:23:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-rus src_langtok: 21770; tgt_langtok: None
2020-10-11 21:23:29 | INFO | fairseq.data.data_utils | loaded 39988 examples from: fairseq/data-bin/ted_belrus_sepspm8000/O2M/train.eng-rus.eng
2020-10-11 21:23:29 | INFO | fairseq.data.data_utils | loaded 39988 examples from: fairseq/data-bin/ted_belrus_sepspm8000/O2M/train.eng-rus.rus
2020-10-11 21:23:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrus_sepspm8000/O2M/ train eng-rus 39988 examples
2020-10-11 21:23:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:eng-bel', 4509), ('main:eng-rus', 39988)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-11 21:23:29 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-11 21:23:29 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 44497
2020-10-11 21:23:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 44497; virtual dataset size 44497
2020-10-11 21:23:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-11 21:23:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:eng-bel': 4509, 'main:eng-rus': 39988}; raw total size: 44497
2020-10-11 21:23:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:eng-bel': 4509, 'main:eng-rus': 39988}; resampled total size: 44497
2020-10-11 21:23:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-11 21:23:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.003018
2020-10-11 21:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:23:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000412
2020-10-11 21:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005220
2020-10-11 21:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000190
2020-10-11 21:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096120
2020-10-11 21:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101868
2020-10-11 21:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:23:29 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-11 21:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004243
2020-10-11 21:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000190
2020-10-11 21:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096561
2020-10-11 21:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101319
2020-10-11 21:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:23:29 | INFO | fairseq.trainer | begin training epoch 1
2020-10-11 21:23:59 | INFO | train_inner | epoch 001:    100 / 181 loss=14.279, nll_loss=14.18, ppl=18560.8, wps=22140.3, ups=3.39, wpb=6520.4, bsz=247.1, num_updates=100, lr=5.0975e-06, gnorm=3.725, clip=0, train_wall=29, wall=30
2020-10-11 21:24:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000751
2020-10-11 21:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041878
2020-10-11 21:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031906
2020-10-11 21:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074872
2020-10-11 21:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000695
2020-10-11 21:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040415
2020-10-11 21:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031704
2020-10-11 21:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073140
2020-10-11 21:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
/home/han/Documents/11737-hw/assign2/fairseq/fairseq/utils.py:340: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2020-10-11 21:24:26 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.713 | nll_loss 12.421 | ppl 5482.6 | wps 51133.5 | wpb 2256.5 | bsz 85.8 | num_updates 181
2020-10-11 21:24:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:24:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 1 @ 181 updates, score 12.713) (writing took 0.946817308999016 seconds)
2020-10-11 21:24:26 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-11 21:24:26 | INFO | train | epoch 001 | loss 13.78 | nll_loss 13.623 | ppl 12614.4 | wps 20473.1 | ups 3.17 | wpb 6452.8 | bsz 245.8 | num_updates 181 | lr 9.14548e-06 | gnorm 2.689 | clip 0 | train_wall 53 | wall 58
2020-10-11 21:24:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-11 21:24:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-11 21:24:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:24:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000587
2020-10-11 21:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005645
2020-10-11 21:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 21:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096953
2020-10-11 21:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103099
2020-10-11 21:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004292
2020-10-11 21:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-11 21:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095901
2020-10-11 21:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100692
2020-10-11 21:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:24:27 | INFO | fairseq.trainer | begin training epoch 2
2020-10-11 21:24:32 | INFO | train_inner | epoch 002:     19 / 181 loss=13.086, nll_loss=12.847, ppl=7367.33, wps=19168.9, ups=2.99, wpb=6418.8, bsz=253.2, num_updates=200, lr=1.0095e-05, gnorm=1.349, clip=0, train_wall=29, wall=64
2020-10-11 21:25:03 | INFO | train_inner | epoch 002:    119 / 181 loss=12.479, nll_loss=12.175, ppl=4624.43, wps=21466.5, ups=3.31, wpb=6480, bsz=238.8, num_updates=300, lr=1.50925e-05, gnorm=1.23, clip=0, train_wall=30, wall=94
2020-10-11 21:25:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000796
2020-10-11 21:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041983
2020-10-11 21:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032406
2020-10-11 21:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075517
2020-10-11 21:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000701
2020-10-11 21:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042296
2020-10-11 21:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032141
2020-10-11 21:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075456
2020-10-11 21:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:25:24 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.467 | nll_loss 11.012 | ppl 2065.01 | wps 50266.9 | wpb 2256.5 | bsz 85.8 | num_updates 362 | best_loss 11.467
2020-10-11 21:25:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:25:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 2 @ 362 updates, score 11.467) (writing took 1.8241160659963498 seconds)
2020-10-11 21:25:26 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-11 21:25:26 | INFO | train | epoch 002 | loss 12.326 | nll_loss 12.002 | ppl 4102.53 | wps 19706.7 | ups 3.05 | wpb 6452.8 | bsz 245.8 | num_updates 362 | lr 1.81909e-05 | gnorm 1.374 | clip 0 | train_wall 54 | wall 117
2020-10-11 21:25:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-11 21:25:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-11 21:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:25:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000801
2020-10-11 21:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008540
2020-10-11 21:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000216
2020-10-11 21:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098826
2020-10-11 21:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108073
2020-10-11 21:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004223
2020-10-11 21:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-11 21:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098173
2020-10-11 21:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102910
2020-10-11 21:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:25:26 | INFO | fairseq.trainer | begin training epoch 3
2020-10-11 21:25:38 | INFO | train_inner | epoch 003:     38 / 181 loss=11.787, nll_loss=11.393, ppl=2689.02, wps=18576.7, ups=2.84, wpb=6547.5, bsz=247.5, num_updates=400, lr=2.009e-05, gnorm=1.448, clip=0, train_wall=30, wall=129
2020-10-11 21:26:08 | INFO | train_inner | epoch 003:    138 / 181 loss=11.237, nll_loss=10.749, ppl=1720.82, wps=21055.2, ups=3.3, wpb=6385.2, bsz=248.9, num_updates=500, lr=2.50875e-05, gnorm=1.334, clip=0, train_wall=30, wall=159
2020-10-11 21:26:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:26:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:26:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000784
2020-10-11 21:26:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:26:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041961
2020-10-11 21:26:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:26:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032125
2020-10-11 21:26:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075198
2020-10-11 21:26:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:26:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:26:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000700
2020-10-11 21:26:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:26:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041545
2020-10-11 21:26:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:26:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031861
2020-10-11 21:26:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074424
2020-10-11 21:26:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:26:24 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.691 | nll_loss 10.076 | ppl 1079.59 | wps 49759 | wpb 2256.5 | bsz 85.8 | num_updates 543 | best_loss 10.691
2020-10-11 21:26:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:26:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 3 @ 543 updates, score 10.691) (writing took 1.4865449669887312 seconds)
2020-10-11 21:26:25 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-11 21:26:25 | INFO | train | epoch 003 | loss 11.268 | nll_loss 10.782 | ppl 1760.71 | wps 19593 | ups 3.04 | wpb 6452.8 | bsz 245.8 | num_updates 543 | lr 2.72364e-05 | gnorm 1.239 | clip 0 | train_wall 54 | wall 177
2020-10-11 21:26:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-11 21:26:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-11 21:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:26:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000684
2020-10-11 21:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007255
2020-10-11 21:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-11 21:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100478
2020-10-11 21:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108400
2020-10-11 21:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:26:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:26:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004200
2020-10-11 21:26:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:26:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000164
2020-10-11 21:26:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:26:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096895
2020-10-11 21:26:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101599
2020-10-11 21:26:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:26:26 | INFO | fairseq.trainer | begin training epoch 4
2020-10-11 21:26:43 | INFO | train_inner | epoch 004:     57 / 181 loss=10.954, nll_loss=10.396, ppl=1347.18, wps=18355.4, ups=2.85, wpb=6440.7, bsz=237.8, num_updates=600, lr=3.0085e-05, gnorm=1.119, clip=0, train_wall=30, wall=194
2020-10-11 21:27:14 | INFO | train_inner | epoch 004:    157 / 181 loss=10.744, nll_loss=10.132, ppl=1122.16, wps=20847.6, ups=3.25, wpb=6420, bsz=252.3, num_updates=700, lr=3.50825e-05, gnorm=1.204, clip=0, train_wall=30, wall=225
2020-10-11 21:27:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000797
2020-10-11 21:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042467
2020-10-11 21:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031730
2020-10-11 21:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075324
2020-10-11 21:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000698
2020-10-11 21:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041801
2020-10-11 21:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031548
2020-10-11 21:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074361
2020-10-11 21:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:27:24 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.442 | nll_loss 9.753 | ppl 862.59 | wps 49086.6 | wpb 2256.5 | bsz 85.8 | num_updates 724 | best_loss 10.442
2020-10-11 21:27:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:27:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 4 @ 724 updates, score 10.442) (writing took 1.669648014998529 seconds)
2020-10-11 21:27:26 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-11 21:27:26 | INFO | train | epoch 004 | loss 10.795 | nll_loss 10.195 | ppl 1171.88 | wps 19354.5 | ups 3 | wpb 6452.8 | bsz 245.8 | num_updates 724 | lr 3.62819e-05 | gnorm 1.184 | clip 0 | train_wall 55 | wall 237
2020-10-11 21:27:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-11 21:27:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-11 21:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:27:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000714
2020-10-11 21:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008391
2020-10-11 21:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000219
2020-10-11 21:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097945
2020-10-11 21:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106958
2020-10-11 21:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004321
2020-10-11 21:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 21:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094292
2020-10-11 21:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099128
2020-10-11 21:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:27:26 | INFO | fairseq.trainer | begin training epoch 5
2020-10-11 21:27:50 | INFO | train_inner | epoch 005:     76 / 181 loss=10.665, nll_loss=10.03, ppl=1045.68, wps=18053.2, ups=2.8, wpb=6439.6, bsz=233, num_updates=800, lr=4.008e-05, gnorm=1.146, clip=0, train_wall=30, wall=261
2020-10-11 21:28:21 | INFO | train_inner | epoch 005:    176 / 181 loss=10.502, nll_loss=9.837, ppl=914.9, wps=20879.6, ups=3.24, wpb=6438, bsz=252.4, num_updates=900, lr=4.50775e-05, gnorm=1.206, clip=0, train_wall=30, wall=292
2020-10-11 21:28:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000804
2020-10-11 21:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042317
2020-10-11 21:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031720
2020-10-11 21:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075177
2020-10-11 21:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000694
2020-10-11 21:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041671
2020-10-11 21:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031501
2020-10-11 21:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074184
2020-10-11 21:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:28:25 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.234 | nll_loss 9.512 | ppl 729.9 | wps 48933.2 | wpb 2256.5 | bsz 85.8 | num_updates 905 | best_loss 10.234
2020-10-11 21:28:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:28:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 5 @ 905 updates, score 10.234) (writing took 1.4590190710005118 seconds)
2020-10-11 21:28:26 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-11 21:28:26 | INFO | train | epoch 005 | loss 10.56 | nll_loss 9.906 | ppl 959.44 | wps 19241.4 | ups 2.98 | wpb 6452.8 | bsz 245.8 | num_updates 905 | lr 4.53274e-05 | gnorm 1.148 | clip 0 | train_wall 55 | wall 298
2020-10-11 21:28:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-11 21:28:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-11 21:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:28:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000907
2020-10-11 21:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008700
2020-10-11 21:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000302
2020-10-11 21:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098588
2020-10-11 21:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108003
2020-10-11 21:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004229
2020-10-11 21:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 21:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095368
2020-10-11 21:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100095
2020-10-11 21:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:28:27 | INFO | fairseq.trainer | begin training epoch 6
2020-10-11 21:28:56 | INFO | train_inner | epoch 006:     95 / 181 loss=10.422, nll_loss=9.745, ppl=858.11, wps=18006.4, ups=2.79, wpb=6443.2, bsz=246.1, num_updates=1000, lr=5.0075e-05, gnorm=1.168, clip=0, train_wall=31, wall=328
2020-10-11 21:29:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000813
2020-10-11 21:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041843
2020-10-11 21:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032163
2020-10-11 21:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075153
2020-10-11 21:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000732
2020-10-11 21:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041775
2020-10-11 21:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032122
2020-10-11 21:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074954
2020-10-11 21:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:29:26 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 10.052 | nll_loss 9.298 | ppl 629.68 | wps 48167.9 | wpb 2256.5 | bsz 85.8 | num_updates 1086 | best_loss 10.052
2020-10-11 21:29:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:29:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 6 @ 1086 updates, score 10.052) (writing took 1.8103070339857368 seconds)
2020-10-11 21:29:28 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-11 21:29:28 | INFO | train | epoch 006 | loss 10.374 | nll_loss 9.688 | ppl 825.03 | wps 18964.5 | ups 2.94 | wpb 6452.8 | bsz 245.8 | num_updates 1086 | lr 5.43729e-05 | gnorm 1.2 | clip 0 | train_wall 56 | wall 359
2020-10-11 21:29:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-11 21:29:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-11 21:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:29:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000815
2020-10-11 21:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008441
2020-10-11 21:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000307
2020-10-11 21:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097517
2020-10-11 21:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106668
2020-10-11 21:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004229
2020-10-11 21:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-11 21:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096557
2020-10-11 21:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101283
2020-10-11 21:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:29:28 | INFO | fairseq.trainer | begin training epoch 7
2020-10-11 21:29:33 | INFO | train_inner | epoch 007:     14 / 181 loss=10.311, nll_loss=9.616, ppl=784.75, wps=17742.2, ups=2.75, wpb=6440.1, bsz=250.6, num_updates=1100, lr=5.50725e-05, gnorm=1.241, clip=0, train_wall=31, wall=364
2020-10-11 21:30:04 | INFO | train_inner | epoch 007:    114 / 181 loss=10.219, nll_loss=9.509, ppl=728.66, wps=20492.9, ups=3.15, wpb=6497.1, bsz=256.7, num_updates=1200, lr=6.007e-05, gnorm=1.146, clip=0, train_wall=31, wall=396
2020-10-11 21:30:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:30:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:30:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000787
2020-10-11 21:30:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:30:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041681
2020-10-11 21:30:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:30:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031831
2020-10-11 21:30:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074625
2020-10-11 21:30:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:30:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:30:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000671
2020-10-11 21:30:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:30:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041933
2020-10-11 21:30:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:30:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031413
2020-10-11 21:30:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074332
2020-10-11 21:30:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:30:28 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.902 | nll_loss 9.113 | ppl 553.68 | wps 47426.8 | wpb 2256.5 | bsz 85.8 | num_updates 1267 | best_loss 9.902
2020-10-11 21:30:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:30:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 7 @ 1267 updates, score 9.902) (writing took 1.4785566950013163 seconds)
2020-10-11 21:30:30 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-11 21:30:30 | INFO | train | epoch 007 | loss 10.219 | nll_loss 9.508 | ppl 727.87 | wps 18911.5 | ups 2.93 | wpb 6452.8 | bsz 245.8 | num_updates 1267 | lr 6.34183e-05 | gnorm 1.181 | clip 0 | train_wall 56 | wall 421
2020-10-11 21:30:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-11 21:30:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-11 21:30:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:30:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000666
2020-10-11 21:30:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007499
2020-10-11 21:30:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:30:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000214
2020-10-11 21:30:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:30:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102136
2020-10-11 21:30:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110363
2020-10-11 21:30:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:30:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:30:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004260
2020-10-11 21:30:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:30:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 21:30:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:30:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097501
2020-10-11 21:30:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102254
2020-10-11 21:30:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:30:30 | INFO | fairseq.trainer | begin training epoch 8
2020-10-11 21:30:40 | INFO | train_inner | epoch 008:     33 / 181 loss=10.165, nll_loss=9.444, ppl=696.67, wps=17789.8, ups=2.77, wpb=6429, bsz=234.6, num_updates=1300, lr=6.50675e-05, gnorm=1.2, clip=0, train_wall=31, wall=432
2020-10-11 21:31:12 | INFO | train_inner | epoch 008:    133 / 181 loss=10.105, nll_loss=9.373, ppl=663.28, wps=20361.4, ups=3.15, wpb=6467.2, bsz=238.1, num_updates=1400, lr=7.0065e-05, gnorm=1.225, clip=0, train_wall=31, wall=463
2020-10-11 21:31:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:31:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:31:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000793
2020-10-11 21:31:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:31:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043084
2020-10-11 21:31:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:31:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032548
2020-10-11 21:31:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076757
2020-10-11 21:31:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:31:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:31:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000699
2020-10-11 21:31:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:31:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041153
2020-10-11 21:31:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:31:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032573
2020-10-11 21:31:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074744
2020-10-11 21:31:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:31:31 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.713 | nll_loss 8.903 | ppl 478.71 | wps 47145.3 | wpb 2256.5 | bsz 85.8 | num_updates 1448 | best_loss 9.713
2020-10-11 21:31:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:31:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 8 @ 1448 updates, score 9.713) (writing took 1.4764051060046768 seconds)
2020-10-11 21:31:32 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-11 21:31:32 | INFO | train | epoch 008 | loss 10.065 | nll_loss 9.329 | ppl 643 | wps 18749.4 | ups 2.91 | wpb 6452.8 | bsz 245.8 | num_updates 1448 | lr 7.24638e-05 | gnorm 1.197 | clip 0 | train_wall 57 | wall 483
2020-10-11 21:31:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-11 21:31:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-11 21:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:31:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000669
2020-10-11 21:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007087
2020-10-11 21:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000257
2020-10-11 21:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099547
2020-10-11 21:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107265
2020-10-11 21:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004301
2020-10-11 21:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 21:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097682
2020-10-11 21:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102480
2020-10-11 21:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:31:32 | INFO | fairseq.trainer | begin training epoch 9
2020-10-11 21:31:49 | INFO | train_inner | epoch 009:     52 / 181 loss=9.949, nll_loss=9.197, ppl=586.88, wps=17648.2, ups=2.71, wpb=6523, bsz=254, num_updates=1500, lr=7.50625e-05, gnorm=1.118, clip=0, train_wall=32, wall=500
2020-10-11 21:32:21 | INFO | train_inner | epoch 009:    152 / 181 loss=9.854, nll_loss=9.086, ppl=543.31, wps=19979.3, ups=3.16, wpb=6320.5, bsz=245.1, num_updates=1600, lr=8.006e-05, gnorm=1.321, clip=0, train_wall=31, wall=532
2020-10-11 21:32:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000786
2020-10-11 21:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041883
2020-10-11 21:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031953
2020-10-11 21:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074955
2020-10-11 21:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000706
2020-10-11 21:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041813
2020-10-11 21:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032009
2020-10-11 21:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074843
2020-10-11 21:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:32:33 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 9.501 | nll_loss 8.652 | ppl 402.15 | wps 46918.2 | wpb 2256.5 | bsz 85.8 | num_updates 1629 | best_loss 9.501
2020-10-11 21:32:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:32:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 9 @ 1629 updates, score 9.501) (writing took 1.4818285689980257 seconds)
2020-10-11 21:32:35 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-11 21:32:35 | INFO | train | epoch 009 | loss 9.871 | nll_loss 9.106 | ppl 551.04 | wps 18667.7 | ups 2.89 | wpb 6452.8 | bsz 245.8 | num_updates 1629 | lr 8.15093e-05 | gnorm 1.262 | clip 0 | train_wall 57 | wall 546
2020-10-11 21:32:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-11 21:32:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-11 21:32:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:32:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000626
2020-10-11 21:32:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006681
2020-10-11 21:32:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:32:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-11 21:32:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:32:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100294
2020-10-11 21:32:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107515
2020-10-11 21:32:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:32:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:32:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004223
2020-10-11 21:32:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:32:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-11 21:32:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:32:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096237
2020-10-11 21:32:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100950
2020-10-11 21:32:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:32:35 | INFO | fairseq.trainer | begin training epoch 10
2020-10-11 21:32:58 | INFO | train_inner | epoch 010:     71 / 181 loss=9.753, nll_loss=8.971, ppl=501.73, wps=17626.9, ups=2.71, wpb=6498, bsz=242.6, num_updates=1700, lr=8.50575e-05, gnorm=1.301, clip=0, train_wall=32, wall=569
2020-10-11 21:33:30 | INFO | train_inner | epoch 010:    171 / 181 loss=9.647, nll_loss=8.849, ppl=461.15, wps=20047.4, ups=3.1, wpb=6461.7, bsz=249.3, num_updates=1800, lr=9.0055e-05, gnorm=1.269, clip=0, train_wall=32, wall=601
2020-10-11 21:33:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000783
2020-10-11 21:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042067
2020-10-11 21:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031866
2020-10-11 21:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075050
2020-10-11 21:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000700
2020-10-11 21:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041317
2020-10-11 21:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031707
2020-10-11 21:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074036
2020-10-11 21:33:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:33:36 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 9.285 | nll_loss 8.413 | ppl 340.81 | wps 46617.9 | wpb 2256.5 | bsz 85.8 | num_updates 1810 | best_loss 9.285
2020-10-11 21:33:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:33:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 10 @ 1810 updates, score 9.285) (writing took 1.4723770620039431 seconds)
2020-10-11 21:33:37 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-11 21:33:37 | INFO | train | epoch 010 | loss 9.676 | nll_loss 8.883 | ppl 472.01 | wps 18576.6 | ups 2.88 | wpb 6452.8 | bsz 245.8 | num_updates 1810 | lr 9.05548e-05 | gnorm 1.272 | clip 0 | train_wall 57 | wall 609
2020-10-11 21:33:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-11 21:33:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-11 21:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:33:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000568
2020-10-11 21:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006024
2020-10-11 21:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 21:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099626
2020-10-11 21:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106149
2020-10-11 21:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004244
2020-10-11 21:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000164
2020-10-11 21:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097903
2020-10-11 21:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102638
2020-10-11 21:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:33:38 | INFO | fairseq.trainer | begin training epoch 11
2020-10-11 21:34:07 | INFO | train_inner | epoch 011:     90 / 181 loss=9.511, nll_loss=8.693, ppl=413.91, wps=17658.2, ups=2.7, wpb=6543.5, bsz=258.6, num_updates=1900, lr=9.50525e-05, gnorm=1.257, clip=0, train_wall=32, wall=638
2020-10-11 21:34:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000724
2020-10-11 21:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042103
2020-10-11 21:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.034066
2020-10-11 21:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.077227
2020-10-11 21:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000645
2020-10-11 21:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041930
2020-10-11 21:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032389
2020-10-11 21:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075288
2020-10-11 21:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:34:39 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 9.151 | nll_loss 8.243 | ppl 303.01 | wps 46301 | wpb 2256.5 | bsz 85.8 | num_updates 1991 | best_loss 9.151
2020-10-11 21:34:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:34:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 11 @ 1991 updates, score 9.151) (writing took 1.5128782090032473 seconds)
2020-10-11 21:34:41 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-11 21:34:41 | INFO | train | epoch 011 | loss 9.479 | nll_loss 8.655 | ppl 403.14 | wps 18459.2 | ups 2.86 | wpb 6452.8 | bsz 245.8 | num_updates 1991 | lr 9.96002e-05 | gnorm 1.223 | clip 0 | train_wall 58 | wall 672
2020-10-11 21:34:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-11 21:34:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-11 21:34:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:34:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000543
2020-10-11 21:34:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006412
2020-10-11 21:34:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:34:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-11 21:34:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:34:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097125
2020-10-11 21:34:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104083
2020-10-11 21:34:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:34:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:34:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004270
2020-10-11 21:34:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:34:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-11 21:34:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:34:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095400
2020-10-11 21:34:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100158
2020-10-11 21:34:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:34:41 | INFO | fairseq.trainer | begin training epoch 12
2020-10-11 21:34:44 | INFO | train_inner | epoch 012:      9 / 181 loss=9.456, nll_loss=8.628, ppl=395.51, wps=17074.6, ups=2.72, wpb=6277.5, bsz=223.8, num_updates=2000, lr=0.00010005, gnorm=1.22, clip=0, train_wall=31, wall=675
2020-10-11 21:35:16 | INFO | train_inner | epoch 012:    109 / 181 loss=9.344, nll_loss=8.498, ppl=361.55, wps=20078.7, ups=3.08, wpb=6516.3, bsz=246.3, num_updates=2100, lr=0.000105048, gnorm=1.282, clip=0, train_wall=32, wall=707
2020-10-11 21:35:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:35:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:35:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000821
2020-10-11 21:35:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:35:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042207
2020-10-11 21:35:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:35:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032097
2020-10-11 21:35:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075459
2020-10-11 21:35:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:35:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:35:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000685
2020-10-11 21:35:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:35:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040940
2020-10-11 21:35:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:35:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031956
2020-10-11 21:35:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073904
2020-10-11 21:35:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:35:43 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 9.039 | nll_loss 8.11 | ppl 276.2 | wps 46153.7 | wpb 2256.5 | bsz 85.8 | num_updates 2172 | best_loss 9.039
2020-10-11 21:35:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:35:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 12 @ 2172 updates, score 9.039) (writing took 1.490168743999675 seconds)
2020-10-11 21:35:44 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-11 21:35:44 | INFO | train | epoch 012 | loss 9.311 | nll_loss 8.46 | ppl 352.05 | wps 18411.8 | ups 2.85 | wpb 6452.8 | bsz 245.8 | num_updates 2172 | lr 0.000108646 | gnorm 1.241 | clip 0 | train_wall 58 | wall 735
2020-10-11 21:35:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-11 21:35:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-11 21:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:35:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000624
2020-10-11 21:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007442
2020-10-11 21:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000233
2020-10-11 21:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097381
2020-10-11 21:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105453
2020-10-11 21:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004271
2020-10-11 21:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 21:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095039
2020-10-11 21:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099802
2020-10-11 21:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:35:44 | INFO | fairseq.trainer | begin training epoch 13
2020-10-11 21:35:53 | INFO | train_inner | epoch 013:     28 / 181 loss=9.228, nll_loss=8.363, ppl=329.2, wps=17073, ups=2.68, wpb=6364.6, bsz=251.4, num_updates=2200, lr=0.000110045, gnorm=1.241, clip=0, train_wall=32, wall=745
2020-10-11 21:36:26 | INFO | train_inner | epoch 013:    128 / 181 loss=9.176, nll_loss=8.302, ppl=315.61, wps=20014.4, ups=3.07, wpb=6521.6, bsz=237.9, num_updates=2300, lr=0.000115043, gnorm=1.141, clip=0, train_wall=32, wall=777
2020-10-11 21:36:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000822
2020-10-11 21:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040895
2020-10-11 21:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032102
2020-10-11 21:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074153
2020-10-11 21:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000689
2020-10-11 21:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041503
2020-10-11 21:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031978
2020-10-11 21:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074488
2020-10-11 21:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:36:46 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.821 | nll_loss 7.86 | ppl 232.39 | wps 46074 | wpb 2256.5 | bsz 85.8 | num_updates 2353 | best_loss 8.821
2020-10-11 21:36:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:36:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 13 @ 2353 updates, score 8.821) (writing took 1.4184158479911275 seconds)
2020-10-11 21:36:48 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-11 21:36:48 | INFO | train | epoch 013 | loss 9.144 | nll_loss 8.265 | ppl 307.64 | wps 18385.6 | ups 2.85 | wpb 6452.8 | bsz 245.8 | num_updates 2353 | lr 0.000117691 | gnorm 1.216 | clip 0 | train_wall 58 | wall 799
2020-10-11 21:36:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-11 21:36:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-11 21:36:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:36:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000620
2020-10-11 21:36:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006006
2020-10-11 21:36:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:36:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-11 21:36:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:36:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103001
2020-10-11 21:36:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109530
2020-10-11 21:36:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:36:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:36:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004278
2020-10-11 21:36:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:36:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 21:36:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:36:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096177
2020-10-11 21:36:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100953
2020-10-11 21:36:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:36:48 | INFO | fairseq.trainer | begin training epoch 14
2020-10-11 21:37:03 | INFO | train_inner | epoch 014:     47 / 181 loss=9.044, nll_loss=8.15, ppl=284.13, wps=17388.5, ups=2.7, wpb=6435.1, bsz=251.2, num_updates=2400, lr=0.00012004, gnorm=1.262, clip=0, train_wall=32, wall=814
2020-10-11 21:37:36 | INFO | train_inner | epoch 014:    147 / 181 loss=8.984, nll_loss=8.081, ppl=270.7, wps=19990.3, ups=3.05, wpb=6550.5, bsz=259.3, num_updates=2500, lr=0.000125037, gnorm=1.238, clip=0, train_wall=32, wall=847
2020-10-11 21:37:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000821
2020-10-11 21:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041280
2020-10-11 21:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031855
2020-10-11 21:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074284
2020-10-11 21:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000681
2020-10-11 21:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040826
2020-10-11 21:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031516
2020-10-11 21:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073340
2020-10-11 21:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:37:50 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.692 | nll_loss 7.708 | ppl 209.07 | wps 45843.3 | wpb 2256.5 | bsz 85.8 | num_updates 2534 | best_loss 8.692
2020-10-11 21:37:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:37:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 14 @ 2534 updates, score 8.692) (writing took 1.4543688989942893 seconds)
2020-10-11 21:37:51 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-11 21:37:51 | INFO | train | epoch 014 | loss 8.989 | nll_loss 8.086 | ppl 271.67 | wps 18341.8 | ups 2.84 | wpb 6452.8 | bsz 245.8 | num_updates 2534 | lr 0.000126737 | gnorm 1.258 | clip 0 | train_wall 58 | wall 863
2020-10-11 21:37:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-11 21:37:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-11 21:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:37:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000561
2020-10-11 21:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006437
2020-10-11 21:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000244
2020-10-11 21:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:37:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100416
2020-10-11 21:37:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107448
2020-10-11 21:37:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:37:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:37:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004285
2020-10-11 21:37:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:37:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-11 21:37:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:37:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094000
2020-10-11 21:37:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098777
2020-10-11 21:37:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:37:52 | INFO | fairseq.trainer | begin training epoch 15
2020-10-11 21:38:13 | INFO | train_inner | epoch 015:     66 / 181 loss=8.933, nll_loss=8.02, ppl=259.65, wps=17303.1, ups=2.68, wpb=6453.4, bsz=226.5, num_updates=2600, lr=0.000130035, gnorm=1.219, clip=0, train_wall=32, wall=884
2020-10-11 21:38:46 | INFO | train_inner | epoch 015:    166 / 181 loss=8.805, nll_loss=7.872, ppl=234.34, wps=19727.6, ups=3.04, wpb=6490.9, bsz=256.3, num_updates=2700, lr=0.000135032, gnorm=1.252, clip=0, train_wall=32, wall=917
2020-10-11 21:38:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000806
2020-10-11 21:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042900
2020-10-11 21:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031652
2020-10-11 21:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075719
2020-10-11 21:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000682
2020-10-11 21:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041684
2020-10-11 21:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031990
2020-10-11 21:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074673
2020-10-11 21:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:38:54 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.577 | nll_loss 7.575 | ppl 190.63 | wps 45547.7 | wpb 2256.5 | bsz 85.8 | num_updates 2715 | best_loss 8.577
2020-10-11 21:38:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:38:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 15 @ 2715 updates, score 8.577) (writing took 1.4809191940003075 seconds)
2020-10-11 21:38:55 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-11 21:38:55 | INFO | train | epoch 015 | loss 8.839 | nll_loss 7.912 | ppl 240.86 | wps 18276.1 | ups 2.83 | wpb 6452.8 | bsz 245.8 | num_updates 2715 | lr 0.000135782 | gnorm 1.229 | clip 0 | train_wall 58 | wall 926
2020-10-11 21:38:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-11 21:38:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-11 21:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:38:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000740
2020-10-11 21:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008490
2020-10-11 21:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000235
2020-10-11 21:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098138
2020-10-11 21:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107279
2020-10-11 21:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004372
2020-10-11 21:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 21:38:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:38:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096114
2020-10-11 21:38:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100980
2020-10-11 21:38:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:38:56 | INFO | fairseq.trainer | begin training epoch 16
2020-10-11 21:39:23 | INFO | train_inner | epoch 016:     85 / 181 loss=8.677, nll_loss=7.727, ppl=211.81, wps=16938.4, ups=2.68, wpb=6313.1, bsz=252.9, num_updates=2800, lr=0.00014003, gnorm=1.237, clip=0, train_wall=32, wall=954
2020-10-11 21:39:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:39:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:39:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000808
2020-10-11 21:39:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:39:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041557
2020-10-11 21:39:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:39:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032330
2020-10-11 21:39:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075032
2020-10-11 21:39:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:39:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:39:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000691
2020-10-11 21:39:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:39:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042113
2020-10-11 21:39:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:39:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032338
2020-10-11 21:39:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075456
2020-10-11 21:39:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:39:58 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.459 | nll_loss 7.433 | ppl 172.82 | wps 45484.4 | wpb 2256.5 | bsz 85.8 | num_updates 2896 | best_loss 8.459
2020-10-11 21:39:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:39:59 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 16 @ 2896 updates, score 8.459) (writing took 1.4890428569924552 seconds)
2020-10-11 21:39:59 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-11 21:39:59 | INFO | train | epoch 016 | loss 8.677 | nll_loss 7.725 | ppl 211.6 | wps 18217.4 | ups 2.82 | wpb 6452.8 | bsz 245.8 | num_updates 2896 | lr 0.000144828 | gnorm 1.217 | clip 0 | train_wall 58 | wall 991
2020-10-11 21:39:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-11 21:39:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-11 21:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:39:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001084
2020-10-11 21:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.011720
2020-10-11 21:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000244
2020-10-11 21:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:40:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100742
2020-10-11 21:40:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113274
2020-10-11 21:40:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:40:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:40:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004300
2020-10-11 21:40:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:40:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-11 21:40:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:40:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097870
2020-10-11 21:40:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102680
2020-10-11 21:40:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:40:00 | INFO | fairseq.trainer | begin training epoch 17
2020-10-11 21:40:01 | INFO | train_inner | epoch 017:      4 / 181 loss=8.686, nll_loss=7.735, ppl=212.99, wps=17256.3, ups=2.65, wpb=6512.6, bsz=236, num_updates=2900, lr=0.000145028, gnorm=1.199, clip=0, train_wall=32, wall=992
2020-10-11 21:40:34 | INFO | train_inner | epoch 017:    104 / 181 loss=8.513, nll_loss=7.537, ppl=185.67, wps=19521.6, ups=3.07, wpb=6364.8, bsz=250.7, num_updates=3000, lr=0.000150025, gnorm=1.209, clip=0, train_wall=32, wall=1025
2020-10-11 21:40:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:40:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:40:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000789
2020-10-11 21:40:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:40:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042102
2020-10-11 21:40:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:40:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032546
2020-10-11 21:40:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075770
2020-10-11 21:40:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:40:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:40:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000712
2020-10-11 21:40:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:40:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041328
2020-10-11 21:40:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:40:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032346
2020-10-11 21:40:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074707
2020-10-11 21:40:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:41:02 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 8.265 | nll_loss 7.189 | ppl 145.87 | wps 45418.7 | wpb 2256.5 | bsz 85.8 | num_updates 3077 | best_loss 8.265
2020-10-11 21:41:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:41:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 17 @ 3077 updates, score 8.265) (writing took 1.5068628570006695 seconds)
2020-10-11 21:41:04 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-11 21:41:04 | INFO | train | epoch 017 | loss 8.511 | nll_loss 7.534 | ppl 185.39 | wps 18208.5 | ups 2.82 | wpb 6452.8 | bsz 245.8 | num_updates 3077 | lr 0.000153873 | gnorm 1.186 | clip 0 | train_wall 58 | wall 1055
2020-10-11 21:41:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-11 21:41:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-11 21:41:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:41:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000801
2020-10-11 21:41:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.009279
2020-10-11 21:41:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:41:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000234
2020-10-11 21:41:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:41:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102308
2020-10-11 21:41:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112363
2020-10-11 21:41:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:41:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:41:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004310
2020-10-11 21:41:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:41:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 21:41:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:41:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098915
2020-10-11 21:41:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103724
2020-10-11 21:41:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:41:04 | INFO | fairseq.trainer | begin training epoch 18
2020-10-11 21:41:11 | INFO | train_inner | epoch 018:     23 / 181 loss=8.479, nll_loss=7.496, ppl=180.56, wps=17429, ups=2.66, wpb=6556.6, bsz=240.5, num_updates=3100, lr=0.000155023, gnorm=1.184, clip=0, train_wall=32, wall=1062
2020-10-11 21:41:44 | INFO | train_inner | epoch 018:    123 / 181 loss=8.302, nll_loss=7.295, ppl=156.99, wps=19513.8, ups=3.04, wpb=6419.4, bsz=264.3, num_updates=3200, lr=0.00016002, gnorm=1.241, clip=0, train_wall=32, wall=1095
2020-10-11 21:42:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000810
2020-10-11 21:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042177
2020-10-11 21:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032214
2020-10-11 21:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075530
2020-10-11 21:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000700
2020-10-11 21:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040934
2020-10-11 21:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032184
2020-10-11 21:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074138
2020-10-11 21:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:42:06 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 8.141 | nll_loss 7.051 | ppl 132.65 | wps 45605.6 | wpb 2256.5 | bsz 85.8 | num_updates 3258 | best_loss 8.141
2020-10-11 21:42:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:42:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 18 @ 3258 updates, score 8.141) (writing took 1.5858130840060767 seconds)
2020-10-11 21:42:08 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-11 21:42:08 | INFO | train | epoch 018 | loss 8.337 | nll_loss 7.334 | ppl 161.3 | wps 18163.4 | ups 2.81 | wpb 6452.8 | bsz 245.8 | num_updates 3258 | lr 0.000162919 | gnorm 1.263 | clip 0 | train_wall 58 | wall 1119
2020-10-11 21:42:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-11 21:42:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-11 21:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:42:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000485
2020-10-11 21:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006372
2020-10-11 21:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-11 21:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102886
2020-10-11 21:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109899
2020-10-11 21:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008159
2020-10-11 21:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-11 21:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096007
2020-10-11 21:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104691
2020-10-11 21:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:42:08 | INFO | fairseq.trainer | begin training epoch 19
2020-10-11 21:42:22 | INFO | train_inner | epoch 019:     42 / 181 loss=8.276, nll_loss=7.262, ppl=153.48, wps=17237.2, ups=2.64, wpb=6520.4, bsz=242.2, num_updates=3300, lr=0.000165018, gnorm=1.287, clip=0, train_wall=32, wall=1133
2020-10-11 21:42:55 | INFO | train_inner | epoch 019:    142 / 181 loss=8.142, nll_loss=7.108, ppl=137.98, wps=19429.7, ups=3.04, wpb=6387.9, bsz=247, num_updates=3400, lr=0.000170015, gnorm=1.235, clip=0, train_wall=32, wall=1166
2020-10-11 21:43:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000822
2020-10-11 21:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042330
2020-10-11 21:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031790
2020-10-11 21:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075282
2020-10-11 21:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000682
2020-10-11 21:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041853
2020-10-11 21:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031451
2020-10-11 21:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074313
2020-10-11 21:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:43:11 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.956 | nll_loss 6.83 | ppl 113.76 | wps 45270 | wpb 2256.5 | bsz 85.8 | num_updates 3439 | best_loss 7.956
2020-10-11 21:43:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:43:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 19 @ 3439 updates, score 7.956) (writing took 1.4926628350076498 seconds)
2020-10-11 21:43:12 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-11 21:43:12 | INFO | train | epoch 019 | loss 8.155 | nll_loss 7.124 | ppl 139.44 | wps 18153.4 | ups 2.81 | wpb 6452.8 | bsz 245.8 | num_updates 3439 | lr 0.000171964 | gnorm 1.255 | clip 0 | train_wall 59 | wall 1183
2020-10-11 21:43:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-11 21:43:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-11 21:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:43:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000850
2020-10-11 21:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008029
2020-10-11 21:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000233
2020-10-11 21:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097766
2020-10-11 21:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106452
2020-10-11 21:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004278
2020-10-11 21:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000163
2020-10-11 21:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094710
2020-10-11 21:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099486
2020-10-11 21:43:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:43:12 | INFO | fairseq.trainer | begin training epoch 20
2020-10-11 21:43:32 | INFO | train_inner | epoch 020:     61 / 181 loss=8.101, nll_loss=7.06, ppl=133.4, wps=17050.4, ups=2.67, wpb=6391.6, bsz=216.3, num_updates=3500, lr=0.000175013, gnorm=1.259, clip=0, train_wall=32, wall=1204
2020-10-11 21:44:05 | INFO | train_inner | epoch 020:    161 / 181 loss=7.949, nll_loss=6.884, ppl=118.12, wps=19797.8, ups=3.02, wpb=6545.4, bsz=255.8, num_updates=3600, lr=0.00018001, gnorm=1.322, clip=0, train_wall=33, wall=1237
2020-10-11 21:44:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000820
2020-10-11 21:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042442
2020-10-11 21:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032781
2020-10-11 21:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076379
2020-10-11 21:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000700
2020-10-11 21:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041909
2020-10-11 21:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032221
2020-10-11 21:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075152
2020-10-11 21:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:44:15 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.796 | nll_loss 6.636 | ppl 99.46 | wps 45520.7 | wpb 2256.5 | bsz 85.8 | num_updates 3620 | best_loss 7.796
2020-10-11 21:44:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:44:17 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 20 @ 3620 updates, score 7.796) (writing took 1.4803995279944502 seconds)
2020-10-11 21:44:17 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-11 21:44:17 | INFO | train | epoch 020 | loss 7.971 | nll_loss 6.911 | ppl 120.33 | wps 18166 | ups 2.82 | wpb 6452.8 | bsz 245.8 | num_updates 3620 | lr 0.00018101 | gnorm 1.302 | clip 0 | train_wall 59 | wall 1248
2020-10-11 21:44:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-11 21:44:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-11 21:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:44:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000622
2020-10-11 21:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005976
2020-10-11 21:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 21:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100649
2020-10-11 21:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107201
2020-10-11 21:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004322
2020-10-11 21:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 21:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094669
2020-10-11 21:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099507
2020-10-11 21:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:44:17 | INFO | fairseq.trainer | begin training epoch 21
2020-10-11 21:44:43 | INFO | train_inner | epoch 021:     80 / 181 loss=7.844, nll_loss=6.765, ppl=108.77, wps=16992.2, ups=2.67, wpb=6365.8, bsz=246.8, num_updates=3700, lr=0.000185008, gnorm=1.362, clip=0, train_wall=32, wall=1274
2020-10-11 21:45:16 | INFO | train_inner | epoch 021:    180 / 181 loss=7.78, nll_loss=6.686, ppl=102.93, wps=19727.5, ups=3.02, wpb=6530.4, bsz=250.2, num_updates=3800, lr=0.000190005, gnorm=1.278, clip=0, train_wall=33, wall=1307
2020-10-11 21:45:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000807
2020-10-11 21:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042397
2020-10-11 21:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032054
2020-10-11 21:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075590
2020-10-11 21:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000696
2020-10-11 21:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041037
2020-10-11 21:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032214
2020-10-11 21:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074271
2020-10-11 21:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:45:19 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.679 | nll_loss 6.498 | ppl 90.39 | wps 45469.9 | wpb 2256.5 | bsz 85.8 | num_updates 3801 | best_loss 7.679
2020-10-11 21:45:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:45:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 21 @ 3801 updates, score 7.679) (writing took 1.4812288389948662 seconds)
2020-10-11 21:45:21 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-11 21:45:21 | INFO | train | epoch 021 | loss 7.804 | nll_loss 6.716 | ppl 105.09 | wps 18161.9 | ups 2.81 | wpb 6452.8 | bsz 245.8 | num_updates 3801 | lr 0.000190055 | gnorm 1.317 | clip 0 | train_wall 59 | wall 1312
2020-10-11 21:45:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-11 21:45:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-11 21:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:45:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000833
2020-10-11 21:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008430
2020-10-11 21:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000218
2020-10-11 21:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099482
2020-10-11 21:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108536
2020-10-11 21:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004299
2020-10-11 21:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 21:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097314
2020-10-11 21:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102114
2020-10-11 21:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:45:21 | INFO | fairseq.trainer | begin training epoch 22
2020-10-11 21:45:53 | INFO | train_inner | epoch 022:     99 / 181 loss=7.611, nll_loss=6.495, ppl=90.18, wps=16748.1, ups=2.67, wpb=6274.4, bsz=244.2, num_updates=3900, lr=0.000195003, gnorm=1.284, clip=0, train_wall=32, wall=1345
2020-10-11 21:46:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000801
2020-10-11 21:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041739
2020-10-11 21:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032106
2020-10-11 21:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074988
2020-10-11 21:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000703
2020-10-11 21:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040823
2020-10-11 21:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032169
2020-10-11 21:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074011
2020-10-11 21:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:46:24 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.559 | nll_loss 6.353 | ppl 81.72 | wps 45401.4 | wpb 2256.5 | bsz 85.8 | num_updates 3982 | best_loss 7.559
2020-10-11 21:46:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:46:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 22 @ 3982 updates, score 7.559) (writing took 1.4882801170024322 seconds)
2020-10-11 21:46:25 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-11 21:46:25 | INFO | train | epoch 022 | loss 7.604 | nll_loss 6.485 | ppl 89.56 | wps 18146.4 | ups 2.81 | wpb 6452.8 | bsz 245.8 | num_updates 3982 | lr 0.0001991 | gnorm 1.275 | clip 0 | train_wall 59 | wall 1376
2020-10-11 21:46:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-11 21:46:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-11 21:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:46:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000711
2020-10-11 21:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007196
2020-10-11 21:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-11 21:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099196
2020-10-11 21:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107006
2020-10-11 21:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004303
2020-10-11 21:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-11 21:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095425
2020-10-11 21:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100247
2020-10-11 21:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:46:25 | INFO | fairseq.trainer | begin training epoch 23
2020-10-11 21:46:31 | INFO | train_inner | epoch 023:     18 / 181 loss=7.581, nll_loss=6.456, ppl=87.77, wps=17635.5, ups=2.63, wpb=6701.1, bsz=247.8, num_updates=4000, lr=0.0002, gnorm=1.273, clip=0, train_wall=33, wall=1383
2020-10-11 21:47:04 | INFO | train_inner | epoch 023:    118 / 181 loss=7.414, nll_loss=6.264, ppl=76.86, wps=19425.3, ups=3.04, wpb=6379.7, bsz=255.1, num_updates=4100, lr=0.000197546, gnorm=1.361, clip=0, train_wall=32, wall=1415
2020-10-11 21:47:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000786
2020-10-11 21:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042899
2020-10-11 21:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032179
2020-10-11 21:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076189
2020-10-11 21:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000729
2020-10-11 21:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042128
2020-10-11 21:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032414
2020-10-11 21:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075590
2020-10-11 21:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:47:28 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.472 | nll_loss 6.231 | ppl 75.1 | wps 44985.5 | wpb 2256.5 | bsz 85.8 | num_updates 4163 | best_loss 7.472
2020-10-11 21:47:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:47:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 23 @ 4163 updates, score 7.472) (writing took 1.6094345210003667 seconds)
2020-10-11 21:47:30 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-11 21:47:30 | INFO | train | epoch 023 | loss 7.434 | nll_loss 6.286 | ppl 78.03 | wps 18077.7 | ups 2.8 | wpb 6452.8 | bsz 245.8 | num_updates 4163 | lr 0.000196045 | gnorm 1.336 | clip 0 | train_wall 59 | wall 1441
2020-10-11 21:47:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-11 21:47:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-11 21:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:47:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000796
2020-10-11 21:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008608
2020-10-11 21:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000218
2020-10-11 21:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096386
2020-10-11 21:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105702
2020-10-11 21:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004307
2020-10-11 21:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-11 21:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096400
2020-10-11 21:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101212
2020-10-11 21:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:47:30 | INFO | fairseq.trainer | begin training epoch 24
2020-10-11 21:47:42 | INFO | train_inner | epoch 024:     37 / 181 loss=7.4, nll_loss=6.245, ppl=75.84, wps=16925.9, ups=2.66, wpb=6362.5, bsz=225.7, num_updates=4200, lr=0.00019518, gnorm=1.285, clip=0, train_wall=32, wall=1453
2020-10-11 21:48:15 | INFO | train_inner | epoch 024:    137 / 181 loss=7.234, nll_loss=6.055, ppl=66.5, wps=19447.9, ups=3.02, wpb=6436.8, bsz=258.4, num_updates=4300, lr=0.000192897, gnorm=1.271, clip=0, train_wall=33, wall=1486
2020-10-11 21:48:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000794
2020-10-11 21:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042098
2020-10-11 21:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032101
2020-10-11 21:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075321
2020-10-11 21:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000722
2020-10-11 21:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041808
2020-10-11 21:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031822
2020-10-11 21:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074677
2020-10-11 21:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:48:33 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.302 | nll_loss 6.03 | ppl 65.35 | wps 45131.7 | wpb 2256.5 | bsz 85.8 | num_updates 4344 | best_loss 7.302
2020-10-11 21:48:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:48:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 24 @ 4344 updates, score 7.302) (writing took 1.632456965991878 seconds)
2020-10-11 21:48:34 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-11 21:48:34 | INFO | train | epoch 024 | loss 7.259 | nll_loss 6.082 | ppl 67.75 | wps 18071.8 | ups 2.8 | wpb 6452.8 | bsz 245.8 | num_updates 4344 | lr 0.000191918 | gnorm 1.25 | clip 0 | train_wall 59 | wall 1506
2020-10-11 21:48:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-11 21:48:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-11 21:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:48:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000643
2020-10-11 21:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006047
2020-10-11 21:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-11 21:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:48:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105013
2020-10-11 21:48:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111589
2020-10-11 21:48:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:48:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:48:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004277
2020-10-11 21:48:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:48:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-11 21:48:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:48:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096256
2020-10-11 21:48:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101036
2020-10-11 21:48:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:48:35 | INFO | fairseq.trainer | begin training epoch 25
2020-10-11 21:48:53 | INFO | train_inner | epoch 025:     56 / 181 loss=7.144, nll_loss=5.95, ppl=61.82, wps=17051, ups=2.62, wpb=6497.6, bsz=254.3, num_updates=4400, lr=0.000190693, gnorm=1.251, clip=0, train_wall=33, wall=1524
2020-10-11 21:49:26 | INFO | train_inner | epoch 025:    156 / 181 loss=7.112, nll_loss=5.91, ppl=60.13, wps=19693.5, ups=3.02, wpb=6525.7, bsz=237.4, num_updates=4500, lr=0.000188562, gnorm=1.26, clip=0, train_wall=33, wall=1557
2020-10-11 21:49:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000796
2020-10-11 21:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041983
2020-10-11 21:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032406
2020-10-11 21:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075511
2020-10-11 21:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000714
2020-10-11 21:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041601
2020-10-11 21:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032375
2020-10-11 21:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075006
2020-10-11 21:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:49:37 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.237 | nll_loss 5.941 | ppl 61.44 | wps 45269.2 | wpb 2256.5 | bsz 85.8 | num_updates 4525 | best_loss 7.237
2020-10-11 21:49:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:49:39 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 25 @ 4525 updates, score 7.237) (writing took 1.7956634959991788 seconds)
2020-10-11 21:49:39 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-11 21:49:39 | INFO | train | epoch 025 | loss 7.091 | nll_loss 5.887 | ppl 59.18 | wps 18012.3 | ups 2.79 | wpb 6452.8 | bsz 245.8 | num_updates 4525 | lr 0.00018804 | gnorm 1.273 | clip 0 | train_wall 59 | wall 1570
2020-10-11 21:49:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-11 21:49:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-11 21:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:49:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001036
2020-10-11 21:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008052
2020-10-11 21:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000214
2020-10-11 21:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097707
2020-10-11 21:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106371
2020-10-11 21:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004330
2020-10-11 21:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-11 21:49:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:49:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095873
2020-10-11 21:49:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100719
2020-10-11 21:49:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:49:40 | INFO | fairseq.trainer | begin training epoch 26
2020-10-11 21:50:04 | INFO | train_inner | epoch 026:     75 / 181 loss=6.988, nll_loss=5.768, ppl=54.5, wps=16904.4, ups=2.62, wpb=6449.2, bsz=240.5, num_updates=4600, lr=0.000186501, gnorm=1.249, clip=0, train_wall=32, wall=1596
2020-10-11 21:50:37 | INFO | train_inner | epoch 026:    175 / 181 loss=6.937, nll_loss=5.706, ppl=52.2, wps=19686.3, ups=3.03, wpb=6491.7, bsz=251.2, num_updates=4700, lr=0.000184506, gnorm=1.301, clip=0, train_wall=33, wall=1629
2020-10-11 21:50:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000796
2020-10-11 21:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042010
2020-10-11 21:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032130
2020-10-11 21:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075267
2020-10-11 21:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000668
2020-10-11 21:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040877
2020-10-11 21:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032108
2020-10-11 21:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073971
2020-10-11 21:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:50:42 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.135 | nll_loss 5.823 | ppl 56.6 | wps 45320.8 | wpb 2256.5 | bsz 85.8 | num_updates 4706 | best_loss 7.135
2020-10-11 21:50:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:50:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 26 @ 4706 updates, score 7.135) (writing took 1.486057909001829 seconds)
2020-10-11 21:50:44 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-11 21:50:44 | INFO | train | epoch 026 | loss 6.944 | nll_loss 5.716 | ppl 52.55 | wps 18098.1 | ups 2.8 | wpb 6452.8 | bsz 245.8 | num_updates 4706 | lr 0.000184389 | gnorm 1.273 | clip 0 | train_wall 59 | wall 1635
2020-10-11 21:50:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-11 21:50:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-11 21:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:50:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000645
2020-10-11 21:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007411
2020-10-11 21:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000241
2020-10-11 21:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096229
2020-10-11 21:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104278
2020-10-11 21:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004339
2020-10-11 21:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 21:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096191
2020-10-11 21:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101030
2020-10-11 21:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:50:44 | INFO | fairseq.trainer | begin training epoch 27
2020-10-11 21:51:15 | INFO | train_inner | epoch 027:     94 / 181 loss=6.822, nll_loss=5.574, ppl=47.64, wps=17224.6, ups=2.66, wpb=6483.8, bsz=244.8, num_updates=4800, lr=0.000182574, gnorm=1.284, clip=0, train_wall=32, wall=1666
2020-10-11 21:51:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:51:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:51:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000828
2020-10-11 21:51:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:51:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041763
2020-10-11 21:51:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:51:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032459
2020-10-11 21:51:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075384
2020-10-11 21:51:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:51:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:51:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000697
2020-10-11 21:51:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:51:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042122
2020-10-11 21:51:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:51:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031968
2020-10-11 21:51:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075117
2020-10-11 21:51:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:51:47 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.065 | nll_loss 5.733 | ppl 53.19 | wps 45521.2 | wpb 2256.5 | bsz 85.8 | num_updates 4887 | best_loss 7.065
2020-10-11 21:51:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:51:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 27 @ 4887 updates, score 7.065) (writing took 1.4924976150068687 seconds)
2020-10-11 21:51:48 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-11 21:51:48 | INFO | train | epoch 027 | loss 6.815 | nll_loss 5.565 | ppl 47.33 | wps 18089.7 | ups 2.8 | wpb 6452.8 | bsz 245.8 | num_updates 4887 | lr 0.000180942 | gnorm 1.283 | clip 0 | train_wall 59 | wall 1700
2020-10-11 21:51:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-11 21:51:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-11 21:51:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:51:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000837
2020-10-11 21:51:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008415
2020-10-11 21:51:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:51:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000234
2020-10-11 21:51:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:51:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098026
2020-10-11 21:51:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107165
2020-10-11 21:51:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:51:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:51:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004304
2020-10-11 21:51:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:51:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-11 21:51:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:51:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097409
2020-10-11 21:51:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102215
2020-10-11 21:51:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:51:49 | INFO | fairseq.trainer | begin training epoch 28
2020-10-11 21:51:53 | INFO | train_inner | epoch 028:     13 / 181 loss=6.799, nll_loss=5.545, ppl=46.7, wps=16726.4, ups=2.63, wpb=6352.9, bsz=242.4, num_updates=4900, lr=0.000180702, gnorm=1.281, clip=0, train_wall=33, wall=1704
2020-10-11 21:52:26 | INFO | train_inner | epoch 028:    113 / 181 loss=6.666, nll_loss=5.392, ppl=42.01, wps=19800.3, ups=3.01, wpb=6580.4, bsz=254.4, num_updates=5000, lr=0.000178885, gnorm=1.288, clip=0, train_wall=33, wall=1737
2020-10-11 21:52:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000833
2020-10-11 21:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041577
2020-10-11 21:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032265
2020-10-11 21:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075007
2020-10-11 21:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000686
2020-10-11 21:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040878
2020-10-11 21:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032441
2020-10-11 21:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074322
2020-10-11 21:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:52:52 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.047 | nll_loss 5.7 | ppl 52 | wps 45268 | wpb 2256.5 | bsz 85.8 | num_updates 5068 | best_loss 7.047
2020-10-11 21:52:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:52:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 28 @ 5068 updates, score 7.047) (writing took 1.4787908510043053 seconds)
2020-10-11 21:52:53 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-11 21:52:53 | INFO | train | epoch 028 | loss 6.69 | nll_loss 5.418 | ppl 42.76 | wps 18033.7 | ups 2.79 | wpb 6452.8 | bsz 245.8 | num_updates 5068 | lr 0.000177681 | gnorm 1.303 | clip 0 | train_wall 59 | wall 1764
2020-10-11 21:52:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-11 21:52:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-11 21:52:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:52:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000620
2020-10-11 21:52:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006504
2020-10-11 21:52:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:52:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000261
2020-10-11 21:52:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:52:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097249
2020-10-11 21:52:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104345
2020-10-11 21:52:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:52:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:52:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004324
2020-10-11 21:52:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:52:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-11 21:52:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:52:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097585
2020-10-11 21:52:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102414
2020-10-11 21:52:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:52:53 | INFO | fairseq.trainer | begin training epoch 29
2020-10-11 21:53:04 | INFO | train_inner | epoch 029:     32 / 181 loss=6.662, nll_loss=5.385, ppl=41.79, wps=16717.8, ups=2.64, wpb=6324.9, bsz=242.6, num_updates=5100, lr=0.000177123, gnorm=1.33, clip=0, train_wall=32, wall=1775
2020-10-11 21:53:37 | INFO | train_inner | epoch 029:    132 / 181 loss=6.559, nll_loss=5.265, ppl=38.44, wps=19717.8, ups=3.04, wpb=6481.5, bsz=250.3, num_updates=5200, lr=0.000175412, gnorm=1.277, clip=0, train_wall=32, wall=1808
2020-10-11 21:53:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000807
2020-10-11 21:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042106
2020-10-11 21:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032291
2020-10-11 21:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075533
2020-10-11 21:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000676
2020-10-11 21:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041224
2020-10-11 21:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032391
2020-10-11 21:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074608
2020-10-11 21:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:53:56 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.976 | nll_loss 5.62 | ppl 49.18 | wps 45052.9 | wpb 2256.5 | bsz 85.8 | num_updates 5249 | best_loss 6.976
2020-10-11 21:53:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:53:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 29 @ 5249 updates, score 6.976) (writing took 1.5228058670036262 seconds)
2020-10-11 21:53:58 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-11 21:53:58 | INFO | train | epoch 029 | loss 6.557 | nll_loss 5.263 | ppl 38.4 | wps 18089.6 | ups 2.8 | wpb 6452.8 | bsz 245.8 | num_updates 5249 | lr 0.000174591 | gnorm 1.271 | clip 0 | train_wall 59 | wall 1829
2020-10-11 21:53:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-11 21:53:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-11 21:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:53:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000528
2020-10-11 21:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005764
2020-10-11 21:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000247
2020-10-11 21:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102418
2020-10-11 21:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108760
2020-10-11 21:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004450
2020-10-11 21:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-11 21:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097125
2020-10-11 21:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102072
2020-10-11 21:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:53:58 | INFO | fairseq.trainer | begin training epoch 30
2020-10-11 21:54:15 | INFO | train_inner | epoch 030:     51 / 181 loss=6.519, nll_loss=5.219, ppl=37.23, wps=16752.9, ups=2.64, wpb=6335.5, bsz=235.4, num_updates=5300, lr=0.000173749, gnorm=1.33, clip=0, train_wall=32, wall=1846
2020-10-11 21:54:48 | INFO | train_inner | epoch 030:    151 / 181 loss=6.457, nll_loss=5.144, ppl=35.37, wps=19552.9, ups=3.03, wpb=6462.2, bsz=241.6, num_updates=5400, lr=0.000172133, gnorm=1.28, clip=0, train_wall=33, wall=1879
2020-10-11 21:54:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000809
2020-10-11 21:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041994
2020-10-11 21:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032165
2020-10-11 21:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075300
2020-10-11 21:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000698
2020-10-11 21:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040955
2020-10-11 21:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032746
2020-10-11 21:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074724
2020-10-11 21:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:55:01 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.884 | nll_loss 5.5 | ppl 45.27 | wps 44891 | wpb 2256.5 | bsz 85.8 | num_updates 5430 | best_loss 6.884
2020-10-11 21:55:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:55:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 30 @ 5430 updates, score 6.884) (writing took 1.485806169002899 seconds)
2020-10-11 21:55:02 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-11 21:55:02 | INFO | train | epoch 030 | loss 6.455 | nll_loss 5.143 | ppl 35.34 | wps 18050.4 | ups 2.8 | wpb 6452.8 | bsz 245.8 | num_updates 5430 | lr 0.000171656 | gnorm 1.313 | clip 0 | train_wall 59 | wall 1894
2020-10-11 21:55:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-11 21:55:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-11 21:55:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:55:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000608
2020-10-11 21:55:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007152
2020-10-11 21:55:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:55:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000256
2020-10-11 21:55:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:55:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097299
2020-10-11 21:55:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105054
2020-10-11 21:55:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:55:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:55:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004293
2020-10-11 21:55:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:55:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 21:55:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:55:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095421
2020-10-11 21:55:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100210
2020-10-11 21:55:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:55:03 | INFO | fairseq.trainer | begin training epoch 31
2020-10-11 21:55:26 | INFO | train_inner | epoch 031:     70 / 181 loss=6.377, nll_loss=5.053, ppl=33.19, wps=17177.6, ups=2.65, wpb=6494.2, bsz=239, num_updates=5500, lr=0.000170561, gnorm=1.233, clip=0, train_wall=32, wall=1917
2020-10-11 21:55:59 | INFO | train_inner | epoch 031:    170 / 181 loss=6.321, nll_loss=4.986, ppl=31.69, wps=19618.5, ups=2.99, wpb=6563.3, bsz=261, num_updates=5600, lr=0.000169031, gnorm=1.296, clip=0, train_wall=33, wall=1950
2020-10-11 21:56:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000801
2020-10-11 21:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041789
2020-10-11 21:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032167
2020-10-11 21:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075085
2020-10-11 21:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000686
2020-10-11 21:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041596
2020-10-11 21:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031888
2020-10-11 21:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074486
2020-10-11 21:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:56:06 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.875 | nll_loss 5.482 | ppl 44.7 | wps 44905.9 | wpb 2256.5 | bsz 85.8 | num_updates 5611 | best_loss 6.875
2020-10-11 21:56:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:56:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 31 @ 5611 updates, score 6.875) (writing took 1.8277559200068936 seconds)
2020-10-11 21:56:07 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-11 21:56:07 | INFO | train | epoch 031 | loss 6.337 | nll_loss 5.006 | ppl 32.12 | wps 17946.8 | ups 2.78 | wpb 6452.8 | bsz 245.8 | num_updates 5611 | lr 0.000168865 | gnorm 1.281 | clip 0 | train_wall 59 | wall 1959
2020-10-11 21:56:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-11 21:56:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-11 21:56:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:56:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000860
2020-10-11 21:56:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008372
2020-10-11 21:56:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:56:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000214
2020-10-11 21:56:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:56:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096732
2020-10-11 21:56:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105807
2020-10-11 21:56:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:56:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:56:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004335
2020-10-11 21:56:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:56:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-11 21:56:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:56:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096220
2020-10-11 21:56:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101051
2020-10-11 21:56:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:56:08 | INFO | fairseq.trainer | begin training epoch 32
2020-10-11 21:56:37 | INFO | train_inner | epoch 032:     89 / 181 loss=6.235, nll_loss=4.888, ppl=29.61, wps=17153.3, ups=2.6, wpb=6585.1, bsz=253.1, num_updates=5700, lr=0.000167542, gnorm=1.318, clip=0, train_wall=33, wall=1989
2020-10-11 21:57:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000721
2020-10-11 21:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041812
2020-10-11 21:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032072
2020-10-11 21:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074937
2020-10-11 21:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000648
2020-10-11 21:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040692
2020-10-11 21:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032610
2020-10-11 21:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074265
2020-10-11 21:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:57:11 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.851 | nll_loss 5.447 | ppl 43.62 | wps 45138.1 | wpb 2256.5 | bsz 85.8 | num_updates 5792 | best_loss 6.851
2020-10-11 21:57:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:57:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 32 @ 5792 updates, score 6.851) (writing took 1.6972496100061107 seconds)
2020-10-11 21:57:12 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-11 21:57:12 | INFO | train | epoch 032 | loss 6.239 | nll_loss 4.89 | ppl 29.66 | wps 17973.9 | ups 2.79 | wpb 6452.8 | bsz 245.8 | num_updates 5792 | lr 0.000166206 | gnorm 1.28 | clip 0 | train_wall 59 | wall 2024
2020-10-11 21:57:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-11 21:57:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-11 21:57:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:57:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000649
2020-10-11 21:57:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006132
2020-10-11 21:57:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:57:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-11 21:57:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:57:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098101
2020-10-11 21:57:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104745
2020-10-11 21:57:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:57:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:57:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004299
2020-10-11 21:57:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:57:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 21:57:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:57:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096148
2020-10-11 21:57:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100938
2020-10-11 21:57:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:57:13 | INFO | fairseq.trainer | begin training epoch 33
2020-10-11 21:57:15 | INFO | train_inner | epoch 033:      8 / 181 loss=6.242, nll_loss=4.893, ppl=29.71, wps=16314, ups=2.64, wpb=6168.6, bsz=235.1, num_updates=5800, lr=0.000166091, gnorm=1.269, clip=0, train_wall=32, wall=2026
2020-10-11 21:57:48 | INFO | train_inner | epoch 033:    108 / 181 loss=6.174, nll_loss=4.814, ppl=28.13, wps=20000.3, ups=3.01, wpb=6648.7, bsz=240.4, num_updates=5900, lr=0.000164677, gnorm=1.327, clip=0, train_wall=33, wall=2060
2020-10-11 21:58:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:58:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:58:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000827
2020-10-11 21:58:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:58:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042312
2020-10-11 21:58:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:58:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031873
2020-10-11 21:58:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075333
2020-10-11 21:58:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:58:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:58:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000668
2020-10-11 21:58:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:58:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041167
2020-10-11 21:58:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:58:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031989
2020-10-11 21:58:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074142
2020-10-11 21:58:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:58:16 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.795 | nll_loss 5.388 | ppl 41.88 | wps 44665.9 | wpb 2256.5 | bsz 85.8 | num_updates 5973 | best_loss 6.795
2020-10-11 21:58:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:58:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 33 @ 5973 updates, score 6.795) (writing took 1.8612473890098045 seconds)
2020-10-11 21:58:18 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-11 21:58:18 | INFO | train | epoch 033 | loss 6.154 | nll_loss 4.791 | ppl 27.68 | wps 17920.3 | ups 2.78 | wpb 6452.8 | bsz 245.8 | num_updates 5973 | lr 0.000163668 | gnorm 1.332 | clip 0 | train_wall 59 | wall 2089
2020-10-11 21:58:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-11 21:58:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-11 21:58:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:58:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000537
2020-10-11 21:58:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006840
2020-10-11 21:58:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:58:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-11 21:58:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:58:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097977
2020-10-11 21:58:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105483
2020-10-11 21:58:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:58:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:58:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004317
2020-10-11 21:58:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:58:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-11 21:58:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:58:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097157
2020-10-11 21:58:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101961
2020-10-11 21:58:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:58:18 | INFO | fairseq.trainer | begin training epoch 34
2020-10-11 21:58:27 | INFO | train_inner | epoch 034:     27 / 181 loss=6.088, nll_loss=4.714, ppl=26.25, wps=16390.5, ups=2.62, wpb=6252.5, bsz=253, num_updates=6000, lr=0.000163299, gnorm=1.322, clip=0, train_wall=32, wall=2098
2020-10-11 21:59:00 | INFO | train_inner | epoch 034:    127 / 181 loss=6.054, nll_loss=4.674, ppl=25.53, wps=19683.1, ups=3.01, wpb=6541.4, bsz=251.2, num_updates=6100, lr=0.000161955, gnorm=1.271, clip=0, train_wall=33, wall=2131
2020-10-11 21:59:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 21:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000807
2020-10-11 21:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041165
2020-10-11 21:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032657
2020-10-11 21:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074961
2020-10-11 21:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000697
2020-10-11 21:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041614
2020-10-11 21:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032715
2020-10-11 21:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075350
2020-10-11 21:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:59:21 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.757 | nll_loss 5.329 | ppl 40.18 | wps 45605.1 | wpb 2256.5 | bsz 85.8 | num_updates 6154 | best_loss 6.757
2020-10-11 21:59:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 21:59:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 34 @ 6154 updates, score 6.757) (writing took 1.6292742999939946 seconds)
2020-10-11 21:59:22 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-11 21:59:22 | INFO | train | epoch 034 | loss 6.057 | nll_loss 4.677 | ppl 25.57 | wps 18048.8 | ups 2.8 | wpb 6452.8 | bsz 245.8 | num_updates 6154 | lr 0.000161243 | gnorm 1.311 | clip 0 | train_wall 59 | wall 2154
2020-10-11 21:59:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-11 21:59:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-11 21:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:59:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000661
2020-10-11 21:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006374
2020-10-11 21:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-11 21:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096611
2020-10-11 21:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103480
2020-10-11 21:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 21:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004264
2020-10-11 21:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-11 21:59:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:59:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097184
2020-10-11 21:59:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101935
2020-10-11 21:59:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 21:59:23 | INFO | fairseq.trainer | begin training epoch 35
2020-10-11 21:59:38 | INFO | train_inner | epoch 035:     46 / 181 loss=6.04, nll_loss=4.655, ppl=25.19, wps=17061.2, ups=2.65, wpb=6440.9, bsz=234.2, num_updates=6200, lr=0.000160644, gnorm=1.315, clip=0, train_wall=32, wall=2169
2020-10-11 22:00:11 | INFO | train_inner | epoch 035:    146 / 181 loss=5.946, nll_loss=4.548, ppl=23.39, wps=19597.3, ups=3, wpb=6538, bsz=260.6, num_updates=6300, lr=0.000159364, gnorm=1.293, clip=0, train_wall=33, wall=2202
2020-10-11 22:00:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000803
2020-10-11 22:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041650
2020-10-11 22:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032685
2020-10-11 22:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075469
2020-10-11 22:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000684
2020-10-11 22:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:00:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040786
2020-10-11 22:00:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:00:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032479
2020-10-11 22:00:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074268
2020-10-11 22:00:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:00:26 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.723 | nll_loss 5.287 | ppl 39.03 | wps 44738.1 | wpb 2256.5 | bsz 85.8 | num_updates 6335 | best_loss 6.723
2020-10-11 22:00:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:00:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 35 @ 6335 updates, score 6.723) (writing took 1.4930221700051334 seconds)
2020-10-11 22:00:27 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-11 22:00:27 | INFO | train | epoch 035 | loss 5.959 | nll_loss 4.562 | ppl 23.62 | wps 18053.5 | ups 2.8 | wpb 6452.8 | bsz 245.8 | num_updates 6335 | lr 0.000158923 | gnorm 1.268 | clip 0 | train_wall 59 | wall 2218
2020-10-11 22:00:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-11 22:00:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-11 22:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:00:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000709
2020-10-11 22:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006813
2020-10-11 22:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-11 22:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101280
2020-10-11 22:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108731
2020-10-11 22:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004320
2020-10-11 22:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-11 22:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096726
2020-10-11 22:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101556
2020-10-11 22:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:00:27 | INFO | fairseq.trainer | begin training epoch 36
2020-10-11 22:00:49 | INFO | train_inner | epoch 036:     65 / 181 loss=5.892, nll_loss=4.485, ppl=22.39, wps=16862.6, ups=2.63, wpb=6409, bsz=248.7, num_updates=6400, lr=0.000158114, gnorm=1.322, clip=0, train_wall=33, wall=2240
2020-10-11 22:01:22 | INFO | train_inner | epoch 036:    165 / 181 loss=5.897, nll_loss=4.489, ppl=22.45, wps=19335.3, ups=3.04, wpb=6370.2, bsz=239.6, num_updates=6500, lr=0.000156893, gnorm=1.333, clip=0, train_wall=33, wall=2273
2020-10-11 22:01:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000795
2020-10-11 22:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042054
2020-10-11 22:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032497
2020-10-11 22:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075672
2020-10-11 22:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000685
2020-10-11 22:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041125
2020-10-11 22:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032053
2020-10-11 22:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074176
2020-10-11 22:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:01:30 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.702 | nll_loss 5.257 | ppl 38.23 | wps 45388.4 | wpb 2256.5 | bsz 85.8 | num_updates 6516 | best_loss 6.702
2020-10-11 22:01:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:01:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 36 @ 6516 updates, score 6.702) (writing took 1.6861252570088254 seconds)
2020-10-11 22:01:32 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-11 22:01:32 | INFO | train | epoch 036 | loss 5.888 | nll_loss 4.479 | ppl 22.3 | wps 17969.1 | ups 2.78 | wpb 6452.8 | bsz 245.8 | num_updates 6516 | lr 0.0001567 | gnorm 1.338 | clip 0 | train_wall 59 | wall 2283
2020-10-11 22:01:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-11 22:01:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-11 22:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:01:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001231
2020-10-11 22:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.010912
2020-10-11 22:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000286
2020-10-11 22:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101542
2020-10-11 22:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113259
2020-10-11 22:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007750
2020-10-11 22:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-11 22:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096782
2020-10-11 22:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105057
2020-10-11 22:01:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:01:32 | INFO | fairseq.trainer | begin training epoch 37
2020-10-11 22:02:00 | INFO | train_inner | epoch 037:     84 / 181 loss=5.822, nll_loss=4.4, ppl=21.11, wps=17136.5, ups=2.62, wpb=6529.4, bsz=237, num_updates=6600, lr=0.0001557, gnorm=1.268, clip=0, train_wall=33, wall=2311
2020-10-11 22:02:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000796
2020-10-11 22:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041639
2020-10-11 22:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032540
2020-10-11 22:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.075304
2020-10-11 22:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000734
2020-10-11 22:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041524
2020-10-11 22:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032138
2020-10-11 22:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074711
2020-10-11 22:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:02:35 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.685 | nll_loss 5.231 | ppl 37.55 | wps 44868.5 | wpb 2256.5 | bsz 85.8 | num_updates 6697 | best_loss 6.685
2020-10-11 22:02:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:02:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 37 @ 6697 updates, score 6.685) (writing took 1.910595456007286 seconds)
2020-10-11 22:02:37 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-11 22:02:37 | INFO | train | epoch 037 | loss 5.808 | nll_loss 4.384 | ppl 20.88 | wps 17934.4 | ups 2.78 | wpb 6452.8 | bsz 245.8 | num_updates 6697 | lr 0.000154568 | gnorm 1.32 | clip 0 | train_wall 59 | wall 2348
2020-10-11 22:02:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-11 22:02:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-11 22:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:02:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000782
2020-10-11 22:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008607
2020-10-11 22:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000303
2020-10-11 22:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097923
2020-10-11 22:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107239
2020-10-11 22:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004277
2020-10-11 22:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-11 22:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096274
2020-10-11 22:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101047
2020-10-11 22:02:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:02:37 | INFO | fairseq.trainer | begin training epoch 38
2020-10-11 22:02:38 | INFO | train_inner | epoch 038:      3 / 181 loss=5.826, nll_loss=4.404, ppl=21.17, wps=16760.6, ups=2.61, wpb=6426.1, bsz=252.4, num_updates=6700, lr=0.000154533, gnorm=1.37, clip=0, train_wall=33, wall=2350
2020-10-11 22:03:11 | INFO | train_inner | epoch 038:    103 / 181 loss=5.698, nll_loss=4.258, ppl=19.13, wps=19663.3, ups=3.03, wpb=6480.3, bsz=255.4, num_updates=6800, lr=0.000153393, gnorm=1.258, clip=0, train_wall=33, wall=2383
2020-10-11 22:03:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:03:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:03:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000785
2020-10-11 22:03:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:03:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041233
2020-10-11 22:03:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:03:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031963
2020-10-11 22:03:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074305
2020-10-11 22:03:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:03:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:03:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000722
2020-10-11 22:03:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:03:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041858
2020-10-11 22:03:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:03:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031680
2020-10-11 22:03:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074571
2020-10-11 22:03:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:03:40 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.676 | nll_loss 5.214 | ppl 37.13 | wps 44884 | wpb 2256.5 | bsz 85.8 | num_updates 6878 | best_loss 6.676
2020-10-11 22:03:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:03:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 38 @ 6878 updates, score 6.676) (writing took 1.8003442539920798 seconds)
2020-10-11 22:03:42 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-11 22:03:42 | INFO | train | epoch 038 | loss 5.726 | nll_loss 4.288 | ppl 19.54 | wps 17980.7 | ups 2.79 | wpb 6452.8 | bsz 245.8 | num_updates 6878 | lr 0.000152521 | gnorm 1.271 | clip 0 | train_wall 59 | wall 2413
2020-10-11 22:03:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-11 22:03:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-11 22:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:03:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000608
2020-10-11 22:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008063
2020-10-11 22:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000207
2020-10-11 22:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096025
2020-10-11 22:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104865
2020-10-11 22:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004277
2020-10-11 22:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-11 22:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094112
2020-10-11 22:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098875
2020-10-11 22:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:03:42 | INFO | fairseq.trainer | begin training epoch 39
2020-10-11 22:03:50 | INFO | train_inner | epoch 039:     22 / 181 loss=5.732, nll_loss=4.294, ppl=19.62, wps=16726.3, ups=2.62, wpb=6395.9, bsz=233.1, num_updates=6900, lr=0.000152277, gnorm=1.31, clip=0, train_wall=33, wall=2421
2020-10-11 22:04:23 | INFO | train_inner | epoch 039:    122 / 181 loss=5.66, nll_loss=4.211, ppl=18.52, wps=19531.1, ups=3.02, wpb=6464.1, bsz=245.3, num_updates=7000, lr=0.000151186, gnorm=1.3, clip=0, train_wall=33, wall=2454
2020-10-11 22:04:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000807
2020-10-11 22:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042697
2020-10-11 22:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033065
2020-10-11 22:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076901
2020-10-11 22:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000677
2020-10-11 22:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.042179
2020-10-11 22:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.032838
2020-10-11 22:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.076013
2020-10-11 22:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:04:45 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.637 | nll_loss 5.169 | ppl 35.97 | wps 44891.8 | wpb 2256.5 | bsz 85.8 | num_updates 7059 | best_loss 6.637
2020-10-11 22:04:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:04:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 39 @ 7059 updates, score 6.637) (writing took 1.698667418997502 seconds)
2020-10-11 22:04:47 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-11 22:04:47 | INFO | train | epoch 039 | loss 5.658 | nll_loss 4.208 | ppl 18.48 | wps 17979.7 | ups 2.79 | wpb 6452.8 | bsz 245.8 | num_updates 7059 | lr 0.000150553 | gnorm 1.32 | clip 0 | train_wall 59 | wall 2478
2020-10-11 22:04:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-11 22:04:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-11 22:04:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:04:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000761
2020-10-11 22:04:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008527
2020-10-11 22:04:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:04:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000347
2020-10-11 22:04:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:04:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098735
2020-10-11 22:04:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107983
2020-10-11 22:04:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:04:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:04:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004323
2020-10-11 22:04:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:04:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-11 22:04:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:04:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096706
2020-10-11 22:04:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101536
2020-10-11 22:04:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:04:47 | INFO | fairseq.trainer | begin training epoch 40
2020-10-11 22:05:01 | INFO | train_inner | epoch 040:     41 / 181 loss=5.621, nll_loss=4.164, ppl=17.93, wps=16965.6, ups=2.61, wpb=6502, bsz=252.1, num_updates=7100, lr=0.000150117, gnorm=1.281, clip=0, train_wall=33, wall=2492
2020-10-11 22:05:34 | INFO | train_inner | epoch 040:    141 / 181 loss=5.605, nll_loss=4.144, ppl=17.68, wps=19666.3, ups=3.02, wpb=6513, bsz=251.1, num_updates=7200, lr=0.000149071, gnorm=1.41, clip=0, train_wall=33, wall=2525
2020-10-11 22:05:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 22:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000808
2020-10-11 22:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041478
2020-10-11 22:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031947
2020-10-11 22:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074558
2020-10-11 22:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000658
2020-10-11 22:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040390
2020-10-11 22:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031672
2020-10-11 22:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073032
2020-10-11 22:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:05:50 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.62 | nll_loss 5.138 | ppl 35.2 | wps 45298.2 | wpb 2256.5 | bsz 85.8 | num_updates 7240 | best_loss 6.62
2020-10-11 22:05:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 22:05:52 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 40 @ 7240 updates, score 6.62) (writing took 1.7851867099961964 seconds)
2020-10-11 22:05:52 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-11 22:05:52 | INFO | train | epoch 040 | loss 5.6 | nll_loss 4.139 | ppl 17.62 | wps 17985.7 | ups 2.79 | wpb 6452.8 | bsz 245.8 | num_updates 7240 | lr 0.000148659 | gnorm 1.345 | clip 0 | train_wall 59 | wall 2543
2020-10-11 22:05:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-11 22:05:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-11 22:05:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-11 22:05:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001608
2020-10-11 22:05:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.015981
2020-10-11 22:05:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:05:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000349
2020-10-11 22:05:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:05:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100907
2020-10-11 22:05:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.117868
2020-10-11 22:05:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-11 22:05:52 | INFO | fairseq_cli.train | done training in 2543.2 seconds
