2020-10-12 19:48:17 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_azehye_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='aze-eng,hye-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_azehye_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 19:48:17 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 19:48:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['aze', 'eng', 'hye']
2020-10-12 19:48:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | [aze] dictionary: 21875 types
2020-10-12 19:48:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 21875 types
2020-10-12 19:48:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | [hye] dictionary: 21875 types
2020-10-12 19:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 19:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=8532.5859375Mb; avail=480307.0Mb
2020-10-12 19:48:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 19:48:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:aze-eng': 1, 'main:hye-eng': 1}
2020-10-12 19:48:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:17 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azehye_sepspm8000/M2O/valid.aze-eng.aze
2020-10-12 19:48:17 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azehye_sepspm8000/M2O/valid.aze-eng.eng
2020-10-12 19:48:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azehye_sepspm8000/M2O/ valid aze-eng 671 examples
2020-10-12 19:48:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:hye-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:17 | INFO | fairseq.data.data_utils | loaded 739 examples from: fairseq/data-bin/ted_azehye_sepspm8000/M2O/valid.hye-eng.hye
2020-10-12 19:48:17 | INFO | fairseq.data.data_utils | loaded 739 examples from: fairseq/data-bin/ted_azehye_sepspm8000/M2O/valid.hye-eng.eng
2020-10-12 19:48:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azehye_sepspm8000/M2O/ valid hye-eng 739 examples
2020-10-12 19:48:18 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21875, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21875, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=21875, bias=False)
  )
)
2020-10-12 19:48:18 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 19:48:18 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 19:48:18 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 19:48:18 | INFO | fairseq_cli.train | num. model params: 42743296 (num. trained: 42743296)
2020-10-12 19:48:22 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 19:48:22 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 19:48:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 19:48:22 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 19:48:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 19:48:22 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 19:48:22 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 19:48:22 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_last.pt
2020-10-12 19:48:22 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 19:48:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 19:48:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10666.46875Mb; avail=478146.94140625Mb
2020-10-12 19:48:22 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 19:48:22 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:aze-eng': 1, 'main:hye-eng': 1}
2020-10-12 19:48:22 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:22 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azehye_sepspm8000/M2O/train.aze-eng.aze
2020-10-12 19:48:22 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azehye_sepspm8000/M2O/train.aze-eng.eng
2020-10-12 19:48:22 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azehye_sepspm8000/M2O/ train aze-eng 5946 examples
2020-10-12 19:48:22 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:hye-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:22 | INFO | fairseq.data.data_utils | loaded 19995 examples from: fairseq/data-bin/ted_azehye_sepspm8000/M2O/train.hye-eng.hye
2020-10-12 19:48:22 | INFO | fairseq.data.data_utils | loaded 19995 examples from: fairseq/data-bin/ted_azehye_sepspm8000/M2O/train.hye-eng.eng
2020-10-12 19:48:22 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azehye_sepspm8000/M2O/ train hye-eng 19995 examples
2020-10-12 19:48:22 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:aze-eng', 5946), ('main:hye-eng', 19995)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 19:48:22 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 19:48:22 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 25941
2020-10-12 19:48:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 25941; virtual dataset size 25941
2020-10-12 19:48:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 19:48:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:aze-eng': 5946, 'main:hye-eng': 19995}; raw total size: 25941
2020-10-12 19:48:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:aze-eng': 5946, 'main:hye-eng': 19995}; resampled total size: 25941
2020-10-12 19:48:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 19:48:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.003646
2020-10-12 19:48:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10667.34375Mb; avail=478146.58203125Mb
2020-10-12 19:48:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000552
2020-10-12 19:48:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004811
2020-10-12 19:48:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10666.65625Mb; avail=478147.01953125Mb
2020-10-12 19:48:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000197
2020-10-12 19:48:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10666.52734375Mb; avail=478146.90625Mb
2020-10-12 19:48:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100594
2020-10-12 19:48:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106411
2020-10-12 19:48:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10586.94140625Mb; avail=478225.71875Mb
2020-10-12 19:48:22 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 19:48:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10563.4296875Mb; avail=478249.23046875Mb
2020-10-12 19:48:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003631
2020-10-12 19:48:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10558.5078125Mb; avail=478254.15234375Mb
2020-10-12 19:48:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 19:48:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10558.5078125Mb; avail=478254.15234375Mb
2020-10-12 19:48:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097472
2020-10-12 19:48:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102149
2020-10-12 19:48:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10559.15625Mb; avail=478253.30859375Mb
2020-10-12 19:48:22 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 19:48:51 | INFO | train_inner | epoch 001:    100 / 107 loss=14.054, nll_loss=13.932, ppl=15632.4, wps=22300.5, ups=3.57, wpb=6242.7, bsz=245.9, num_updates=100, lr=5.0975e-06, gnorm=4.78, clip=0, train_wall=28, wall=29
2020-10-12 19:48:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:48:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15501.4609375Mb; avail=473249.2265625Mb
2020-10-12 19:48:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001497
2020-10-12 19:48:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15501.4609375Mb; avail=473249.2265625Mb
2020-10-12 19:48:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021221
2020-10-12 19:48:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15501.4609375Mb; avail=473249.2265625Mb
2020-10-12 19:48:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016542
2020-10-12 19:48:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.040090
2020-10-12 19:48:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15501.44921875Mb; avail=473249.2265625Mb
2020-10-12 19:48:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15501.44921875Mb; avail=473249.23828125Mb
2020-10-12 19:48:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000733
2020-10-12 19:48:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15501.44921875Mb; avail=473249.23828125Mb
2020-10-12 19:48:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021087
2020-10-12 19:48:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15501.44921875Mb; avail=473249.23828125Mb
2020-10-12 19:48:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016498
2020-10-12 19:48:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039067
2020-10-12 19:48:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15501.48046875Mb; avail=473249.20703125Mb
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2020-10-12 19:48:53 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.363 | nll_loss 12.043 | ppl 4219.67 | wps 49137.4 | wpb 2186.6 | bsz 82.9 | num_updates 107
2020-10-12 19:48:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:48:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 1 @ 107 updates, score 12.363) (writing took 1.5555556079998496 seconds)
2020-10-12 19:48:55 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 19:48:55 | INFO | train | epoch 001 | loss 13.987 | nll_loss 13.858 | ppl 14848 | wps 20436.9 | ups 3.31 | wpb 6172 | bsz 242.4 | num_updates 107 | lr 5.44733e-06 | gnorm 4.655 | clip 0 | train_wall 29 | wall 33
2020-10-12 19:48:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 19:48:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 19:48:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16033.65625Mb; avail=472689.85546875Mb
2020-10-12 19:48:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000859
2020-10-12 19:48:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005474
2020-10-12 19:48:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16039.2265625Mb; avail=472684.6484375Mb
2020-10-12 19:48:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000222
2020-10-12 19:48:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16039.83203125Mb; avail=472684.04296875Mb
2020-10-12 19:48:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098104
2020-10-12 19:48:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104630
2020-10-12 19:48:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16046.98046875Mb; avail=472676.8984375Mb
2020-10-12 19:48:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16046.98046875Mb; avail=472676.8984375Mb
2020-10-12 19:48:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003867
2020-10-12 19:48:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16048.43359375Mb; avail=472675.4453125Mb
2020-10-12 19:48:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 19:48:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16049.0390625Mb; avail=472674.83984375Mb
2020-10-12 19:48:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101476
2020-10-12 19:48:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106390
2020-10-12 19:48:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16114.59375Mb; avail=472609.30859375Mb
2020-10-12 19:48:55 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 19:49:21 | INFO | train_inner | epoch 002:     93 / 107 loss=12.306, nll_loss=11.984, ppl=4051.12, wps=20182, ups=3.29, wpb=6135.6, bsz=245.2, num_updates=200, lr=1.0095e-05, gnorm=2.411, clip=0, train_wall=27, wall=59
2020-10-12 19:49:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18403.3359375Mb; avail=470286.95703125Mb
2020-10-12 19:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001210
2020-10-12 19:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18403.3359375Mb; avail=470286.95703125Mb
2020-10-12 19:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021017
2020-10-12 19:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18402.73046875Mb; avail=470287.5625Mb
2020-10-12 19:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.017174
2020-10-12 19:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.040242
2020-10-12 19:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18402.66796875Mb; avail=470287.625Mb
2020-10-12 19:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18402.66796875Mb; avail=470287.625Mb
2020-10-12 19:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000738
2020-10-12 19:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18402.66796875Mb; avail=470287.625Mb
2020-10-12 19:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021141
2020-10-12 19:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18402.66796875Mb; avail=470287.625Mb
2020-10-12 19:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016935
2020-10-12 19:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039594
2020-10-12 19:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18402.66796875Mb; avail=470287.625Mb
2020-10-12 19:49:26 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.475 | nll_loss 11.048 | ppl 2117.94 | wps 53483.1 | wpb 2186.6 | bsz 82.9 | num_updates 214 | best_loss 11.475
2020-10-12 19:49:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:49:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 2 @ 214 updates, score 11.475) (writing took 23.00768744800007 seconds)
2020-10-12 19:49:49 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 19:49:49 | INFO | train | epoch 002 | loss 12.212 | nll_loss 11.88 | ppl 3769.04 | wps 12270.7 | ups 1.99 | wpb 6172 | bsz 242.4 | num_updates 214 | lr 1.07947e-05 | gnorm 2.335 | clip 0 | train_wall 29 | wall 87
2020-10-12 19:49:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 19:49:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 19:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18366.51171875Mb; avail=470306.9609375Mb
2020-10-12 19:49:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000833
2020-10-12 19:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005473
2020-10-12 19:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18366.58203125Mb; avail=470306.9609375Mb
2020-10-12 19:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 19:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18366.58203125Mb; avail=470306.9609375Mb
2020-10-12 19:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099042
2020-10-12 19:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105464
2020-10-12 19:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18366.87890625Mb; avail=470307.35546875Mb
2020-10-12 19:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18367.109375Mb; avail=470307.13671875Mb
2020-10-12 19:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004315
2020-10-12 19:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18367.109375Mb; avail=470307.13671875Mb
2020-10-12 19:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000205
2020-10-12 19:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18367.109375Mb; avail=470307.13671875Mb
2020-10-12 19:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098907
2020-10-12 19:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104211
2020-10-12 19:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18367.15625Mb; avail=470307.0Mb
2020-10-12 19:49:49 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 19:50:13 | INFO | train_inner | epoch 003:     86 / 107 loss=11.41, nll_loss=10.984, ppl=2026.05, wps=11805.6, ups=1.91, wpb=6165.6, bsz=237.9, num_updates=300, lr=1.50925e-05, gnorm=1.815, clip=0, train_wall=27, wall=111
2020-10-12 19:50:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18375.96875Mb; avail=470297.36328125Mb
2020-10-12 19:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001255
2020-10-12 19:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18375.96875Mb; avail=470297.36328125Mb
2020-10-12 19:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021509
2020-10-12 19:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18375.9453125Mb; avail=470297.60546875Mb
2020-10-12 19:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.017326
2020-10-12 19:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.040911
2020-10-12 19:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18370.23046875Mb; avail=470303.51171875Mb
2020-10-12 19:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18370.234375Mb; avail=470303.91796875Mb
2020-10-12 19:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000735
2020-10-12 19:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18370.234375Mb; avail=470303.91796875Mb
2020-10-12 19:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021142
2020-10-12 19:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18370.1875Mb; avail=470304.16015625Mb
2020-10-12 19:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016944
2020-10-12 19:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039594
2020-10-12 19:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18370.1875Mb; avail=470304.16015625Mb
2020-10-12 19:50:20 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.478 | nll_loss 9.909 | ppl 961.24 | wps 52902.2 | wpb 2186.6 | bsz 82.9 | num_updates 321 | best_loss 10.478
2020-10-12 19:50:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:50:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 3 @ 321 updates, score 10.478) (writing took 7.039786833000107 seconds)
2020-10-12 19:50:27 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 19:50:27 | INFO | train | epoch 003 | loss 11.271 | nll_loss 10.827 | ppl 1816.83 | wps 17256.2 | ups 2.8 | wpb 6172 | bsz 242.4 | num_updates 321 | lr 1.6142e-05 | gnorm 1.82 | clip 0 | train_wall 29 | wall 125
2020-10-12 19:50:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 19:50:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 19:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17849.1484375Mb; avail=470825.03515625Mb
2020-10-12 19:50:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000897
2020-10-12 19:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005795
2020-10-12 19:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.1484375Mb; avail=470825.03515625Mb
2020-10-12 19:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000198
2020-10-12 19:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.1484375Mb; avail=470825.03515625Mb
2020-10-12 19:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105634
2020-10-12 19:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112599
2020-10-12 19:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.19140625Mb; avail=470825.21875Mb
2020-10-12 19:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17849.11328125Mb; avail=470825.21875Mb
2020-10-12 19:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003738
2020-10-12 19:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.11328125Mb; avail=470825.21875Mb
2020-10-12 19:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 19:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.11328125Mb; avail=470825.21875Mb
2020-10-12 19:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098542
2020-10-12 19:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103230
2020-10-12 19:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.3125Mb; avail=470825.21875Mb
2020-10-12 19:50:27 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 19:50:50 | INFO | train_inner | epoch 004:     79 / 107 loss=10.377, nll_loss=9.807, ppl=896.06, wps=17059.2, ups=2.76, wpb=6169.8, bsz=247.2, num_updates=400, lr=2.009e-05, gnorm=1.772, clip=0, train_wall=27, wall=148
2020-10-12 19:50:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:50:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17834.93359375Mb; avail=470838.58203125Mb
2020-10-12 19:50:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001200
2020-10-12 19:50:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.875Mb; avail=470838.82421875Mb
2020-10-12 19:50:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021126
2020-10-12 19:50:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.8046875Mb; avail=470838.703125Mb
2020-10-12 19:50:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016825
2020-10-12 19:50:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039967
2020-10-12 19:50:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.7578125Mb; avail=470838.9453125Mb
2020-10-12 19:50:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17834.80078125Mb; avail=470838.82421875Mb
2020-10-12 19:50:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000763
2020-10-12 19:50:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.80078125Mb; avail=470838.82421875Mb
2020-10-12 19:50:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021167
2020-10-12 19:50:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.81640625Mb; avail=470838.9453125Mb
2020-10-12 19:50:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016648
2020-10-12 19:50:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039344
2020-10-12 19:50:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.6953125Mb; avail=470839.06640625Mb
2020-10-12 19:50:58 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.44 | nll_loss 8.654 | ppl 402.87 | wps 53101.3 | wpb 2186.6 | bsz 82.9 | num_updates 428 | best_loss 9.44
2020-10-12 19:50:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:51:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 4 @ 428 updates, score 9.44) (writing took 5.4719119210003555 seconds)
2020-10-12 19:51:04 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 19:51:04 | INFO | train | epoch 004 | loss 10.093 | nll_loss 9.477 | ppl 712.79 | wps 18095.5 | ups 2.93 | wpb 6172 | bsz 242.4 | num_updates 428 | lr 2.14893e-05 | gnorm 1.578 | clip 0 | train_wall 29 | wall 162
2020-10-12 19:51:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 19:51:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 19:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18390.82421875Mb; avail=470283.453125Mb
2020-10-12 19:51:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000670
2020-10-12 19:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005064
2020-10-12 19:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18390.796875Mb; avail=470283.57421875Mb
2020-10-12 19:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 19:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18390.796875Mb; avail=470283.57421875Mb
2020-10-12 19:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098447
2020-10-12 19:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104432
2020-10-12 19:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18390.69921875Mb; avail=470283.6953125Mb
2020-10-12 19:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18390.61328125Mb; avail=470283.57421875Mb
2020-10-12 19:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003678
2020-10-12 19:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18390.640625Mb; avail=470283.453125Mb
2020-10-12 19:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 19:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18390.640625Mb; avail=470283.453125Mb
2020-10-12 19:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098468
2020-10-12 19:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103067
2020-10-12 19:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18390.90234375Mb; avail=470283.57421875Mb
2020-10-12 19:51:04 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 19:51:24 | INFO | train_inner | epoch 005:     72 / 107 loss=9.532, nll_loss=8.806, ppl=447.45, wps=18124.9, ups=2.88, wpb=6289.5, bsz=241.2, num_updates=500, lr=2.50875e-05, gnorm=1.426, clip=0, train_wall=27, wall=182
2020-10-12 19:51:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:51:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17745.234375Mb; avail=470926.66796875Mb
2020-10-12 19:51:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001170
2020-10-12 19:51:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17745.234375Mb; avail=470926.66796875Mb
2020-10-12 19:51:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021095
2020-10-12 19:51:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17745.29296875Mb; avail=470926.7890625Mb
2020-10-12 19:51:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016566
2020-10-12 19:51:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039661
2020-10-12 19:51:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17745.29296875Mb; avail=470926.7890625Mb
2020-10-12 19:51:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17745.29296875Mb; avail=470926.7890625Mb
2020-10-12 19:51:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000708
2020-10-12 19:51:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17745.29296875Mb; avail=470926.7890625Mb
2020-10-12 19:51:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020927
2020-10-12 19:51:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17745.29296875Mb; avail=470926.7890625Mb
2020-10-12 19:51:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016511
2020-10-12 19:51:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038913
2020-10-12 19:51:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17745.29296875Mb; avail=470926.7890625Mb
2020-10-12 19:51:35 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.035 | nll_loss 8.146 | ppl 283.23 | wps 53438.5 | wpb 2186.6 | bsz 82.9 | num_updates 535 | best_loss 9.035
2020-10-12 19:51:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:51:39 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 5 @ 535 updates, score 9.035) (writing took 4.431585628999528 seconds)
2020-10-12 19:51:39 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 19:51:39 | INFO | train | epoch 005 | loss 9.375 | nll_loss 8.614 | ppl 391.89 | wps 18511.6 | ups 3 | wpb 6172 | bsz 242.4 | num_updates 535 | lr 2.68366e-05 | gnorm 1.547 | clip 0 | train_wall 29 | wall 197
2020-10-12 19:51:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 19:51:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 19:51:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17745.9609375Mb; avail=470927.6015625Mb
2020-10-12 19:51:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000681
2020-10-12 19:51:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005329
2020-10-12 19:51:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17745.9609375Mb; avail=470927.6015625Mb
2020-10-12 19:51:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000205
2020-10-12 19:51:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17745.9609375Mb; avail=470927.6015625Mb
2020-10-12 19:51:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099546
2020-10-12 19:51:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105848
2020-10-12 19:51:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17746.14453125Mb; avail=470926.99609375Mb
2020-10-12 19:51:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17746.19140625Mb; avail=470926.76171875Mb
2020-10-12 19:51:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003744
2020-10-12 19:51:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17746.19140625Mb; avail=470926.76171875Mb
2020-10-12 19:51:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000199
2020-10-12 19:51:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17746.19140625Mb; avail=470926.76171875Mb
2020-10-12 19:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098740
2020-10-12 19:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103443
2020-10-12 19:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17746.0703125Mb; avail=470926.8828125Mb
2020-10-12 19:51:40 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 19:51:58 | INFO | train_inner | epoch 006:     65 / 107 loss=9.125, nll_loss=8.3, ppl=315.21, wps=18064.2, ups=2.97, wpb=6075.1, bsz=240.2, num_updates=600, lr=3.0085e-05, gnorm=1.478, clip=0, train_wall=27, wall=216
2020-10-12 19:52:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:52:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18306.73828125Mb; avail=470367.38671875Mb
2020-10-12 19:52:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001168
2020-10-12 19:52:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18306.73828125Mb; avail=470367.38671875Mb
2020-10-12 19:52:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020917
2020-10-12 19:52:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18306.75390625Mb; avail=470367.265625Mb
2020-10-12 19:52:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016672
2020-10-12 19:52:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039559
2020-10-12 19:52:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18306.9375Mb; avail=470367.265625Mb
2020-10-12 19:52:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18307.05078125Mb; avail=470366.66015625Mb
2020-10-12 19:52:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000701
2020-10-12 19:52:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18307.05078125Mb; avail=470366.66015625Mb
2020-10-12 19:52:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021118
2020-10-12 19:52:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18306.97265625Mb; avail=470366.93359375Mb
2020-10-12 19:52:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016693
2020-10-12 19:52:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039298
2020-10-12 19:52:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18306.97265625Mb; avail=470366.93359375Mb
2020-10-12 19:52:11 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.854 | nll_loss 7.921 | ppl 242.28 | wps 53097.2 | wpb 2186.6 | bsz 82.9 | num_updates 642 | best_loss 8.854
2020-10-12 19:52:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:52:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 6 @ 642 updates, score 8.854) (writing took 8.531045077999806 seconds)
2020-10-12 19:52:19 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 19:52:19 | INFO | train | epoch 006 | loss 9.079 | nll_loss 8.236 | ppl 301.58 | wps 16606.3 | ups 2.69 | wpb 6172 | bsz 242.4 | num_updates 642 | lr 3.2184e-05 | gnorm 1.391 | clip 0 | train_wall 29 | wall 237
2020-10-12 19:52:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 19:52:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 19:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18441.94921875Mb; avail=470231.94921875Mb
2020-10-12 19:52:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000813
2020-10-12 19:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005235
2020-10-12 19:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18441.95703125Mb; avail=470231.828125Mb
2020-10-12 19:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000313
2020-10-12 19:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18441.95703125Mb; avail=470231.828125Mb
2020-10-12 19:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.153762
2020-10-12 19:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.160079
2020-10-12 19:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18442.234375Mb; avail=470231.67578125Mb
2020-10-12 19:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18442.234375Mb; avail=470231.67578125Mb
2020-10-12 19:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003731
2020-10-12 19:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18442.234375Mb; avail=470231.67578125Mb
2020-10-12 19:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000204
2020-10-12 19:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18442.234375Mb; avail=470231.67578125Mb
2020-10-12 19:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098254
2020-10-12 19:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102926
2020-10-12 19:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18442.16796875Mb; avail=470231.75390625Mb
2020-10-12 19:52:19 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 19:52:36 | INFO | train_inner | epoch 007:     58 / 107 loss=8.986, nll_loss=8.117, ppl=277.55, wps=16086.1, ups=2.63, wpb=6105.4, bsz=250.4, num_updates=700, lr=3.50825e-05, gnorm=1.489, clip=0, train_wall=27, wall=254
2020-10-12 19:52:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17976.859375Mb; avail=470696.3671875Mb
2020-10-12 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001239
2020-10-12 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17976.8671875Mb; avail=470696.3671875Mb
2020-10-12 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.028332
2020-10-12 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17976.8984375Mb; avail=470695.76953125Mb
2020-10-12 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.017769
2020-10-12 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.048447
2020-10-12 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17976.94921875Mb; avail=470695.6484375Mb
2020-10-12 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17976.89453125Mb; avail=470695.76953125Mb
2020-10-12 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000769
2020-10-12 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17976.82421875Mb; avail=470695.76953125Mb
2020-10-12 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020832
2020-10-12 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17976.74609375Mb; avail=470696.1328125Mb
2020-10-12 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.017159
2020-10-12 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039555
2020-10-12 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17976.6640625Mb; avail=470696.1328125Mb
2020-10-12 19:52:50 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.732 | nll_loss 7.766 | ppl 217.7 | wps 52763 | wpb 2186.6 | bsz 82.9 | num_updates 749 | best_loss 8.732
2020-10-12 19:52:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:53:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 7 @ 749 updates, score 8.732) (writing took 9.153201783999975 seconds)
2020-10-12 19:53:00 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 19:53:00 | INFO | train | epoch 007 | loss 8.927 | nll_loss 8.044 | ppl 263.99 | wps 16280.8 | ups 2.64 | wpb 6172 | bsz 242.4 | num_updates 749 | lr 3.75313e-05 | gnorm 1.495 | clip 0 | train_wall 29 | wall 278
2020-10-12 19:53:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 19:53:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 19:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17943.9765625Mb; avail=470735.04296875Mb
2020-10-12 19:53:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000844
2020-10-12 19:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005798
2020-10-12 19:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17944.15625Mb; avail=470735.515625Mb
2020-10-12 19:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000232
2020-10-12 19:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17944.6328125Mb; avail=470734.796875Mb
2020-10-12 19:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100760
2020-10-12 19:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107760
2020-10-12 19:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17944.46484375Mb; avail=470732.65234375Mb
2020-10-12 19:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17944.8125Mb; avail=470731.90234375Mb
2020-10-12 19:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003905
2020-10-12 19:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17945.64453125Mb; avail=470731.39453125Mb
2020-10-12 19:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000217
2020-10-12 19:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17944.78125Mb; avail=470731.7734375Mb
2020-10-12 19:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099931
2020-10-12 19:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104916
2020-10-12 19:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17944.609375Mb; avail=470729.61328125Mb
2020-10-12 19:53:00 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 19:53:14 | INFO | train_inner | epoch 008:     51 / 107 loss=8.844, nll_loss=7.946, ppl=246.63, wps=16158.2, ups=2.61, wpb=6191.9, bsz=230.1, num_updates=800, lr=4.008e-05, gnorm=1.457, clip=0, train_wall=27, wall=292
2020-10-12 19:53:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18452.75390625Mb; avail=470220.328125Mb
2020-10-12 19:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001197
2020-10-12 19:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18452.75390625Mb; avail=470220.328125Mb
2020-10-12 19:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021320
2020-10-12 19:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18452.75390625Mb; avail=470220.328125Mb
2020-10-12 19:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.018020
2020-10-12 19:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.041358
2020-10-12 19:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18452.76953125Mb; avail=470220.20703125Mb
2020-10-12 19:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18452.78515625Mb; avail=470220.0859375Mb
2020-10-12 19:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000776
2020-10-12 19:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18452.78515625Mb; avail=470220.0859375Mb
2020-10-12 19:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021102
2020-10-12 19:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18452.73046875Mb; avail=470220.328125Mb
2020-10-12 19:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016970
2020-10-12 19:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039629
2020-10-12 19:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18452.73046875Mb; avail=470220.328125Mb
2020-10-12 19:53:31 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.572 | nll_loss 7.592 | ppl 192.92 | wps 53056.3 | wpb 2186.6 | bsz 82.9 | num_updates 856 | best_loss 8.572
2020-10-12 19:53:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:53:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 8 @ 856 updates, score 8.572) (writing took 7.314188845999524 seconds)
2020-10-12 19:53:38 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 19:53:38 | INFO | train | epoch 008 | loss 8.769 | nll_loss 7.86 | ppl 232.37 | wps 17235.8 | ups 2.79 | wpb 6172 | bsz 242.4 | num_updates 856 | lr 4.28786e-05 | gnorm 1.458 | clip 0 | train_wall 29 | wall 316
2020-10-12 19:53:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 19:53:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 19:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17755.98046875Mb; avail=470917.234375Mb
2020-10-12 19:53:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001242
2020-10-12 19:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005921
2020-10-12 19:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17755.98046875Mb; avail=470917.234375Mb
2020-10-12 19:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000236
2020-10-12 19:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17755.48828125Mb; avail=470917.7265625Mb
2020-10-12 19:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.123453
2020-10-12 19:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.130555
2020-10-12 19:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17755.55078125Mb; avail=470917.48046875Mb
2020-10-12 19:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17755.8046875Mb; avail=470917.48046875Mb
2020-10-12 19:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003816
2020-10-12 19:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17755.8046875Mb; avail=470917.48046875Mb
2020-10-12 19:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 19:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17755.8046875Mb; avail=470917.48046875Mb
2020-10-12 19:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097113
2020-10-12 19:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101920
2020-10-12 19:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17755.96484375Mb; avail=470916.99609375Mb
2020-10-12 19:53:38 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 19:53:51 | INFO | train_inner | epoch 009:     44 / 107 loss=8.708, nll_loss=7.791, ppl=221.42, wps=16990.5, ups=2.75, wpb=6185, bsz=247.4, num_updates=900, lr=4.50775e-05, gnorm=1.44, clip=0, train_wall=27, wall=329
2020-10-12 19:54:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:54:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18419.5390625Mb; avail=470259.09765625Mb
2020-10-12 19:54:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001164
2020-10-12 19:54:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18419.28125Mb; avail=470258.87109375Mb
2020-10-12 19:54:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.043844
2020-10-12 19:54:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18419.046875Mb; avail=470258.4453125Mb
2020-10-12 19:54:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016713
2020-10-12 19:54:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.062534
2020-10-12 19:54:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18418.9140625Mb; avail=470258.5234375Mb
2020-10-12 19:54:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18419.0390625Mb; avail=470258.71484375Mb
2020-10-12 19:54:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000734
2020-10-12 19:54:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18418.91015625Mb; avail=470257.99609375Mb
2020-10-12 19:54:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020983
2020-10-12 19:54:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18418.8671875Mb; avail=470257.88671875Mb
2020-10-12 19:54:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016784
2020-10-12 19:54:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039295
2020-10-12 19:54:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18419.2265625Mb; avail=470257.60546875Mb
2020-10-12 19:54:09 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.392 | nll_loss 7.372 | ppl 165.61 | wps 52925.3 | wpb 2186.6 | bsz 82.9 | num_updates 963 | best_loss 8.392
2020-10-12 19:54:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:54:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 9 @ 963 updates, score 8.392) (writing took 9.790534410999499 seconds)
2020-10-12 19:54:19 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 19:54:19 | INFO | train | epoch 009 | loss 8.602 | nll_loss 7.67 | ppl 203.61 | wps 16136.3 | ups 2.61 | wpb 6172 | bsz 242.4 | num_updates 963 | lr 4.82259e-05 | gnorm 1.43 | clip 0 | train_wall 29 | wall 357
2020-10-12 19:54:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 19:54:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 19:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17767.9296875Mb; avail=470905.2109375Mb
2020-10-12 19:54:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000860
2020-10-12 19:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005585
2020-10-12 19:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17767.9296875Mb; avail=470905.2109375Mb
2020-10-12 19:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-12 19:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17767.9296875Mb; avail=470905.2109375Mb
2020-10-12 19:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099243
2020-10-12 19:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105829
2020-10-12 19:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17768.6640625Mb; avail=470904.31640625Mb
2020-10-12 19:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17768.6328125Mb; avail=470904.31640625Mb
2020-10-12 19:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003747
2020-10-12 19:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17768.6328125Mb; avail=470904.31640625Mb
2020-10-12 19:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-12 19:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17768.6328125Mb; avail=470904.31640625Mb
2020-10-12 19:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098760
2020-10-12 19:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103732
2020-10-12 19:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17768.71875Mb; avail=470903.953125Mb
2020-10-12 19:54:19 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 19:54:30 | INFO | train_inner | epoch 010:     37 / 107 loss=8.513, nll_loss=7.568, ppl=189.7, wps=15683.9, ups=2.56, wpb=6116.3, bsz=239, num_updates=1000, lr=5.0075e-05, gnorm=1.402, clip=0, train_wall=27, wall=368
2020-10-12 19:54:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17808.8125Mb; avail=470863.71484375Mb
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001150
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.8125Mb; avail=470863.71484375Mb
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021163
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.9765625Mb; avail=470863.71484375Mb
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.017161
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.040319
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.83984375Mb; avail=470863.95703125Mb
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17808.7421875Mb; avail=470863.95703125Mb
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000757
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.7421875Mb; avail=470863.95703125Mb
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021003
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.78515625Mb; avail=470863.71484375Mb
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.017026
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039607
2020-10-12 19:54:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.78515625Mb; avail=470863.71484375Mb
2020-10-12 19:54:50 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.224 | nll_loss 7.19 | ppl 146.05 | wps 52505.5 | wpb 2186.6 | bsz 82.9 | num_updates 1070 | best_loss 8.224
2020-10-12 19:54:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:54:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 10 @ 1070 updates, score 8.224) (writing took 7.688771789999919 seconds)
2020-10-12 19:54:58 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 19:54:58 | INFO | train | epoch 010 | loss 8.438 | nll_loss 7.481 | ppl 178.63 | wps 16978.3 | ups 2.75 | wpb 6172 | bsz 242.4 | num_updates 1070 | lr 5.35733e-05 | gnorm 1.435 | clip 0 | train_wall 29 | wall 396
2020-10-12 19:54:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 19:54:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17811.984375Mb; avail=470861.88671875Mb
2020-10-12 19:54:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000984
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005603
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.9296875Mb; avail=470861.94140625Mb
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000199
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.9296875Mb; avail=470861.94140625Mb
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098055
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104632
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.984375Mb; avail=470861.09375Mb
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17812.58203125Mb; avail=470860.609375Mb
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003623
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17812.58203125Mb; avail=470860.609375Mb
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17812.58203125Mb; avail=470860.609375Mb
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097817
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102374
2020-10-12 19:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17812.953125Mb; avail=470860.1171875Mb
2020-10-12 19:54:58 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 19:55:06 | INFO | train_inner | epoch 011:     30 / 107 loss=8.432, nll_loss=7.473, ppl=177.72, wps=16764.1, ups=2.71, wpb=6176.9, bsz=231.4, num_updates=1100, lr=5.50725e-05, gnorm=1.476, clip=0, train_wall=27, wall=405
2020-10-12 19:55:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17837.04296875Mb; avail=470835.98828125Mb
2020-10-12 19:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001198
2020-10-12 19:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.04296875Mb; avail=470835.98828125Mb
2020-10-12 19:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.023717
2020-10-12 19:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.96875Mb; avail=470835.8671875Mb
2020-10-12 19:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.018131
2020-10-12 19:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.043851
2020-10-12 19:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.046875Mb; avail=470835.98828125Mb
2020-10-12 19:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17837.046875Mb; avail=470835.98828125Mb
2020-10-12 19:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000685
2020-10-12 19:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.046875Mb; avail=470835.98828125Mb
2020-10-12 19:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.024227
2020-10-12 19:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.046875Mb; avail=470835.98828125Mb
2020-10-12 19:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.018650
2020-10-12 19:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.044364
2020-10-12 19:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.046875Mb; avail=470835.98828125Mb
2020-10-12 19:55:29 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.107 | nll_loss 7.041 | ppl 131.66 | wps 53005.6 | wpb 2186.6 | bsz 82.9 | num_updates 1177 | best_loss 8.107
2020-10-12 19:55:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:55:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 11 @ 1177 updates, score 8.107) (writing took 4.446114925000074 seconds)
2020-10-12 19:55:33 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 19:55:33 | INFO | train | epoch 011 | loss 8.301 | nll_loss 7.324 | ppl 160.25 | wps 18697.4 | ups 3.03 | wpb 6172 | bsz 242.4 | num_updates 1177 | lr 5.89206e-05 | gnorm 1.548 | clip 0 | train_wall 29 | wall 431
2020-10-12 19:55:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 19:55:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 19:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17856.42578125Mb; avail=470816.6328125Mb
2020-10-12 19:55:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001103
2020-10-12 19:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006026
2020-10-12 19:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17856.45703125Mb; avail=470816.51171875Mb
2020-10-12 19:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 19:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17856.40625Mb; avail=470816.390625Mb
2020-10-12 19:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104747
2020-10-12 19:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111876
2020-10-12 19:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.04296875Mb; avail=470816.1484375Mb
2020-10-12 19:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17857.140625Mb; avail=470815.90625Mb
2020-10-12 19:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003954
2020-10-12 19:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17856.87890625Mb; avail=470816.390625Mb
2020-10-12 19:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 19:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17856.87890625Mb; avail=470816.390625Mb
2020-10-12 19:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097226
2020-10-12 19:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102119
2020-10-12 19:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17856.69140625Mb; avail=470816.26953125Mb
2020-10-12 19:55:33 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 19:55:40 | INFO | train_inner | epoch 012:     23 / 107 loss=8.233, nll_loss=7.247, ppl=151.93, wps=18708.5, ups=2.98, wpb=6272.4, bsz=261.5, num_updates=1200, lr=6.007e-05, gnorm=1.499, clip=0, train_wall=27, wall=438
2020-10-12 19:56:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17847.7578125Mb; avail=470825.11328125Mb
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001118
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.7578125Mb; avail=470825.11328125Mb
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021236
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.7578125Mb; avail=470825.11328125Mb
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016228
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039371
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.7578125Mb; avail=470825.11328125Mb
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17847.6640625Mb; avail=470825.11328125Mb
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000725
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.6640625Mb; avail=470825.11328125Mb
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021084
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.6953125Mb; avail=470824.9921875Mb
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016395
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038974
2020-10-12 19:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17847.53515625Mb; avail=470824.9921875Mb
2020-10-12 19:56:04 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.011 | nll_loss 6.928 | ppl 121.78 | wps 53277.7 | wpb 2186.6 | bsz 82.9 | num_updates 1284 | best_loss 8.011
2020-10-12 19:56:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:56:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 12 @ 1284 updates, score 8.011) (writing took 4.411768281999684 seconds)
2020-10-12 19:56:08 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 19:56:08 | INFO | train | epoch 012 | loss 8.165 | nll_loss 7.169 | ppl 143.9 | wps 18705.7 | ups 3.03 | wpb 6172 | bsz 242.4 | num_updates 1284 | lr 6.42679e-05 | gnorm 1.404 | clip 0 | train_wall 29 | wall 467
2020-10-12 19:56:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 19:56:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 19:56:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17871.26171875Mb; avail=470801.47265625Mb
2020-10-12 19:56:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000844
2020-10-12 19:56:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005339
2020-10-12 19:56:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.26171875Mb; avail=470801.47265625Mb
2020-10-12 19:56:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 19:56:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.26171875Mb; avail=470801.47265625Mb
2020-10-12 19:56:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096858
2020-10-12 19:56:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103172
2020-10-12 19:56:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.26171875Mb; avail=470801.47265625Mb
2020-10-12 19:56:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17871.26171875Mb; avail=470801.47265625Mb
2020-10-12 19:56:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003625
2020-10-12 19:56:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.26171875Mb; avail=470801.47265625Mb
2020-10-12 19:56:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 19:56:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.26171875Mb; avail=470801.47265625Mb
2020-10-12 19:56:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097778
2020-10-12 19:56:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102392
2020-10-12 19:56:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.27734375Mb; avail=470801.48046875Mb
2020-10-12 19:56:09 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 19:56:13 | INFO | train_inner | epoch 013:     16 / 107 loss=8.168, nll_loss=7.172, ppl=144.21, wps=18541.2, ups=3, wpb=6180.5, bsz=243.5, num_updates=1300, lr=6.50675e-05, gnorm=1.401, clip=0, train_wall=27, wall=471
2020-10-12 19:56:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18238.2578125Mb; avail=470434.7734375Mb
2020-10-12 19:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001121
2020-10-12 19:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18238.2578125Mb; avail=470434.7734375Mb
2020-10-12 19:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021381
2020-10-12 19:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18238.2578125Mb; avail=470434.7734375Mb
2020-10-12 19:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016999
2020-10-12 19:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.040294
2020-10-12 19:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18238.2578125Mb; avail=470434.7734375Mb
2020-10-12 19:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18238.2578125Mb; avail=470434.7734375Mb
2020-10-12 19:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000719
2020-10-12 19:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18238.2578125Mb; avail=470434.7734375Mb
2020-10-12 19:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020746
2020-10-12 19:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18238.18359375Mb; avail=470434.65234375Mb
2020-10-12 19:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016287
2020-10-12 19:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038505
2020-10-12 19:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18238.19921875Mb; avail=470434.63671875Mb
2020-10-12 19:56:39 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.892 | nll_loss 6.791 | ppl 110.71 | wps 53276.7 | wpb 2186.6 | bsz 82.9 | num_updates 1391 | best_loss 7.892
2020-10-12 19:56:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:56:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 13 @ 1391 updates, score 7.892) (writing took 5.033946689999539 seconds)
2020-10-12 19:56:44 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 19:56:44 | INFO | train | epoch 013 | loss 8.05 | nll_loss 7.037 | ppl 131.31 | wps 18506.1 | ups 3 | wpb 6172 | bsz 242.4 | num_updates 1391 | lr 6.96152e-05 | gnorm 1.44 | clip 0 | train_wall 28 | wall 502
2020-10-12 19:56:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 19:56:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 19:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17857.078125Mb; avail=470815.953125Mb
2020-10-12 19:56:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000789
2020-10-12 19:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005321
2020-10-12 19:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.08984375Mb; avail=470815.83203125Mb
2020-10-12 19:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 19:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.08984375Mb; avail=470815.83203125Mb
2020-10-12 19:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100387
2020-10-12 19:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106656
2020-10-12 19:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.21484375Mb; avail=470815.83203125Mb
2020-10-12 19:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17857.2734375Mb; avail=470815.58984375Mb
2020-10-12 19:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003631
2020-10-12 19:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.2734375Mb; avail=470815.58984375Mb
2020-10-12 19:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000199
2020-10-12 19:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.2734375Mb; avail=470815.58984375Mb
2020-10-12 19:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099057
2020-10-12 19:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103730
2020-10-12 19:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.27734375Mb; avail=470816.0859375Mb
2020-10-12 19:56:44 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 19:56:47 | INFO | train_inner | epoch 014:      9 / 107 loss=8.003, nll_loss=6.983, ppl=126.47, wps=18056, ups=2.98, wpb=6061.2, bsz=232.8, num_updates=1400, lr=7.0065e-05, gnorm=1.464, clip=0, train_wall=26, wall=505
2020-10-12 19:57:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18927.57421875Mb; avail=469746.21875Mb
2020-10-12 19:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001116
2020-10-12 19:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18927.57421875Mb; avail=469746.21875Mb
2020-10-12 19:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021192
2020-10-12 19:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18927.57421875Mb; avail=469746.21875Mb
2020-10-12 19:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016440
2020-10-12 19:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039593
2020-10-12 19:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18888.19921875Mb; avail=469789.0390625Mb
2020-10-12 19:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18531.88671875Mb; avail=470145.6328125Mb
2020-10-12 19:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000718
2020-10-12 19:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18523.51953125Mb; avail=470154.0Mb
2020-10-12 19:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021140
2020-10-12 19:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18353.3125Mb; avail=470321.22265625Mb
2020-10-12 19:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016326
2020-10-12 19:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038938
2020-10-12 19:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18352.8203125Mb; avail=470322.453125Mb
2020-10-12 19:57:15 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.78 | nll_loss 6.679 | ppl 102.47 | wps 52955.7 | wpb 2186.6 | bsz 82.9 | num_updates 1498 | best_loss 7.78
2020-10-12 19:57:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:57:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 14 @ 1498 updates, score 7.78) (writing took 9.64861617599945 seconds)
2020-10-12 19:57:25 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 19:57:25 | INFO | train | epoch 014 | loss 7.936 | nll_loss 6.907 | ppl 120.03 | wps 16306.6 | ups 2.64 | wpb 6172 | bsz 242.4 | num_updates 1498 | lr 7.49626e-05 | gnorm 1.369 | clip 0 | train_wall 29 | wall 543
2020-10-12 19:57:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 19:57:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 19:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17841.21484375Mb; avail=470832.421875Mb
2020-10-12 19:57:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000812
2020-10-12 19:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005451
2020-10-12 19:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.21484375Mb; avail=470832.421875Mb
2020-10-12 19:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 19:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.21484375Mb; avail=470832.421875Mb
2020-10-12 19:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098910
2020-10-12 19:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105316
2020-10-12 19:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.7421875Mb; avail=470831.57421875Mb
2020-10-12 19:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17841.7421875Mb; avail=470831.57421875Mb
2020-10-12 19:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003674
2020-10-12 19:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.7421875Mb; avail=470831.57421875Mb
2020-10-12 19:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 19:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.25Mb; avail=470831.57421875Mb
2020-10-12 19:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098346
2020-10-12 19:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102954
2020-10-12 19:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.83984375Mb; avail=470831.453125Mb
2020-10-12 19:57:25 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 19:57:25 | INFO | train_inner | epoch 015:      2 / 107 loss=7.947, nll_loss=6.92, ppl=121.05, wps=16016.9, ups=2.59, wpb=6180.1, bsz=240.7, num_updates=1500, lr=7.50625e-05, gnorm=1.372, clip=0, train_wall=27, wall=544
2020-10-12 19:57:53 | INFO | train_inner | epoch 015:    102 / 107 loss=7.856, nll_loss=6.814, ppl=112.53, wps=22454.6, ups=3.6, wpb=6245.3, bsz=246.8, num_updates=1600, lr=8.006e-05, gnorm=1.362, clip=0, train_wall=27, wall=571
2020-10-12 19:57:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17835.1640625Mb; avail=470838.11328125Mb
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001239
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.1640625Mb; avail=470838.11328125Mb
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021257
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.24609375Mb; avail=470838.234375Mb
2020-10-12 19:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016373
2020-10-12 19:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039660
2020-10-12 19:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.75390625Mb; avail=470838.234375Mb
2020-10-12 19:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17834.7421875Mb; avail=470838.234375Mb
2020-10-12 19:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000718
2020-10-12 19:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.7421875Mb; avail=470838.234375Mb
2020-10-12 19:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020742
2020-10-12 19:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.7421875Mb; avail=470838.234375Mb
2020-10-12 19:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016327
2020-10-12 19:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038533
2020-10-12 19:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.625Mb; avail=470838.11328125Mb
2020-10-12 19:57:55 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.747 | nll_loss 6.641 | ppl 99.77 | wps 53388 | wpb 2186.6 | bsz 82.9 | num_updates 1605 | best_loss 7.747
2020-10-12 19:57:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:58:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 15 @ 1605 updates, score 7.747) (writing took 12.089240508999865 seconds)
2020-10-12 19:58:07 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 19:58:07 | INFO | train | epoch 015 | loss 7.839 | nll_loss 6.794 | ppl 111 | wps 15403.1 | ups 2.5 | wpb 6172 | bsz 242.4 | num_updates 1605 | lr 8.03099e-05 | gnorm 1.364 | clip 0 | train_wall 29 | wall 586
2020-10-12 19:58:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 19:58:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 19:58:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17813.5859375Mb; avail=470859.85546875Mb
2020-10-12 19:58:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000864
2020-10-12 19:58:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005678
2020-10-12 19:58:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.5859375Mb; avail=470859.85546875Mb
2020-10-12 19:58:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000234
2020-10-12 19:58:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.5859375Mb; avail=470859.85546875Mb
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099882
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106634
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.48828125Mb; avail=470859.859375Mb
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17813.4453125Mb; avail=470860.1015625Mb
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003774
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.4453125Mb; avail=470860.1015625Mb
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000203
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.4453125Mb; avail=470860.1015625Mb
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097669
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102461
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.4453125Mb; avail=470860.22265625Mb
2020-10-12 19:58:08 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 19:58:34 | INFO | train_inner | epoch 016:     95 / 107 loss=7.706, nll_loss=6.643, ppl=99.93, wps=15086.6, ups=2.43, wpb=6198.5, bsz=248.1, num_updates=1700, lr=8.50575e-05, gnorm=1.331, clip=0, train_wall=27, wall=612
2020-10-12 19:58:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:58:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18733.58203125Mb; avail=469945.89453125Mb
2020-10-12 19:58:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001080
2020-10-12 19:58:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18733.9296875Mb; avail=469946.16015625Mb
2020-10-12 19:58:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021190
2020-10-12 19:58:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18734.46484375Mb; avail=469945.77734375Mb
2020-10-12 19:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016509
2020-10-12 19:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039576
2020-10-12 19:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18734.34375Mb; avail=469945.1640625Mb
2020-10-12 19:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18734.4921875Mb; avail=469944.27734375Mb
2020-10-12 19:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000690
2020-10-12 19:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18734.2578125Mb; avail=469944.53515625Mb
2020-10-12 19:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021109
2020-10-12 19:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18734.30078125Mb; avail=469944.3984375Mb
2020-10-12 19:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016319
2020-10-12 19:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038887
2020-10-12 19:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18734.1796875Mb; avail=469943.8984375Mb
2020-10-12 19:58:38 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.624 | nll_loss 6.5 | ppl 90.52 | wps 53207.9 | wpb 2186.6 | bsz 82.9 | num_updates 1712 | best_loss 7.624
2020-10-12 19:58:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:58:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 16 @ 1712 updates, score 7.624) (writing took 10.065084670999568 seconds)
2020-10-12 19:58:48 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 19:58:48 | INFO | train | epoch 016 | loss 7.729 | nll_loss 6.669 | ppl 101.74 | wps 16092.9 | ups 2.61 | wpb 6172 | bsz 242.4 | num_updates 1712 | lr 8.56572e-05 | gnorm 1.333 | clip 0 | train_wall 29 | wall 627
2020-10-12 19:58:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 19:58:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 19:58:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18370.17578125Mb; avail=470303.28125Mb
2020-10-12 19:58:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000916
2020-10-12 19:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006731
2020-10-12 19:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18370.1328125Mb; avail=470303.5234375Mb
2020-10-12 19:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000260
2020-10-12 19:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18370.1328125Mb; avail=470303.5234375Mb
2020-10-12 19:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098040
2020-10-12 19:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106136
2020-10-12 19:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18369.33203125Mb; avail=470304.19921875Mb
2020-10-12 19:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18369.51171875Mb; avail=470304.0625Mb
2020-10-12 19:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003764
2020-10-12 19:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18369.43359375Mb; avail=470304.0625Mb
2020-10-12 19:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000210
2020-10-12 19:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18369.43359375Mb; avail=470304.0625Mb
2020-10-12 19:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098766
2020-10-12 19:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103504
2020-10-12 19:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18369.5078125Mb; avail=470303.94140625Mb
2020-10-12 19:58:49 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 19:59:13 | INFO | train_inner | epoch 017:     88 / 107 loss=7.657, nll_loss=6.587, ppl=96.12, wps=15581.7, ups=2.58, wpb=6028.9, bsz=223.5, num_updates=1800, lr=9.0055e-05, gnorm=1.304, clip=0, train_wall=26, wall=651
2020-10-12 19:59:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18417.95703125Mb; avail=470259.71875Mb
2020-10-12 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001161
2020-10-12 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18417.95703125Mb; avail=470259.96484375Mb
2020-10-12 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021115
2020-10-12 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18417.46484375Mb; avail=470262.1796875Mb
2020-10-12 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016674
2020-10-12 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039770
2020-10-12 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18417.46484375Mb; avail=470262.1796875Mb
2020-10-12 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18418.4609375Mb; avail=470261.10546875Mb
2020-10-12 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000735
2020-10-12 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18418.203125Mb; avail=470260.87890625Mb
2020-10-12 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020981
2020-10-12 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18419.58203125Mb; avail=470260.3515625Mb
2020-10-12 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016512
2020-10-12 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039056
2020-10-12 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18420.078125Mb; avail=470259.6171875Mb
2020-10-12 19:59:19 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.543 | nll_loss 6.41 | ppl 85.06 | wps 52142.3 | wpb 2186.6 | bsz 82.9 | num_updates 1819 | best_loss 7.543
2020-10-12 19:59:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:59:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 17 @ 1819 updates, score 7.543) (writing took 12.001131734999944 seconds)
2020-10-12 19:59:31 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 19:59:31 | INFO | train | epoch 017 | loss 7.635 | nll_loss 6.562 | ppl 94.45 | wps 15358.2 | ups 2.49 | wpb 6172 | bsz 242.4 | num_updates 1819 | lr 9.10045e-05 | gnorm 1.326 | clip 0 | train_wall 29 | wall 670
2020-10-12 19:59:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 19:59:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 19:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18373.1328125Mb; avail=470300.21484375Mb
2020-10-12 19:59:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000883
2020-10-12 19:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005523
2020-10-12 19:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18373.15625Mb; avail=470300.09375Mb
2020-10-12 19:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-12 19:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18373.15625Mb; avail=470300.09375Mb
2020-10-12 19:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099329
2020-10-12 19:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105806
2020-10-12 19:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18371.99609375Mb; avail=470301.44140625Mb
2020-10-12 19:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18371.9765625Mb; avail=470301.68359375Mb
2020-10-12 19:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003599
2020-10-12 19:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18371.9765625Mb; avail=470301.68359375Mb
2020-10-12 19:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 19:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18371.9765625Mb; avail=470301.68359375Mb
2020-10-12 19:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098664
2020-10-12 19:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103177
2020-10-12 19:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18372.0234375Mb; avail=470301.44140625Mb
2020-10-12 19:59:32 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 19:59:55 | INFO | train_inner | epoch 018:     81 / 107 loss=7.582, nll_loss=6.5, ppl=90.52, wps=15165.4, ups=2.41, wpb=6297.1, bsz=251.5, num_updates=1900, lr=9.50525e-05, gnorm=1.375, clip=0, train_wall=27, wall=693
2020-10-12 20:00:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18348.5625Mb; avail=470324.60546875Mb
2020-10-12 20:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001233
2020-10-12 20:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18348.5625Mb; avail=470324.60546875Mb
2020-10-12 20:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020845
2020-10-12 20:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18348.578125Mb; avail=470324.58984375Mb
2020-10-12 20:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.017044
2020-10-12 20:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039956
2020-10-12 20:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18348.578125Mb; avail=470324.58984375Mb
2020-10-12 20:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18348.5625Mb; avail=470324.83203125Mb
2020-10-12 20:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000817
2020-10-12 20:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18348.5625Mb; avail=470324.83203125Mb
2020-10-12 20:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020960
2020-10-12 20:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18348.33203125Mb; avail=470325.0625Mb
2020-10-12 20:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016541
2020-10-12 20:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039136
2020-10-12 20:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18348.33203125Mb; avail=470325.0625Mb
2020-10-12 20:00:03 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.509 | nll_loss 6.357 | ppl 81.96 | wps 52442 | wpb 2186.6 | bsz 82.9 | num_updates 1926 | best_loss 7.509
2020-10-12 20:00:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:00:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 18 @ 1926 updates, score 7.509) (writing took 5.937137664000147 seconds)
2020-10-12 20:00:08 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 20:00:08 | INFO | train | epoch 018 | loss 7.549 | nll_loss 6.463 | ppl 88.22 | wps 17845.5 | ups 2.89 | wpb 6172 | bsz 242.4 | num_updates 1926 | lr 9.63519e-05 | gnorm 1.348 | clip 0 | train_wall 29 | wall 707
2020-10-12 20:00:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 20:00:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 20:00:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17790.1640625Mb; avail=470883.1328125Mb
2020-10-12 20:00:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001166
2020-10-12 20:00:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005943
2020-10-12 20:00:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17790.171875Mb; avail=470883.125Mb
2020-10-12 20:00:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000200
2020-10-12 20:00:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17790.171875Mb; avail=470883.125Mb
2020-10-12 20:00:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099301
2020-10-12 20:00:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106301
2020-10-12 20:00:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17789.8984375Mb; avail=470883.6015625Mb
2020-10-12 20:00:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17789.9453125Mb; avail=470883.72265625Mb
2020-10-12 20:00:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003778
2020-10-12 20:00:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17789.9453125Mb; avail=470883.72265625Mb
2020-10-12 20:00:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 20:00:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17789.9453125Mb; avail=470883.72265625Mb
2020-10-12 20:00:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099061
2020-10-12 20:00:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103788
2020-10-12 20:00:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17789.9375Mb; avail=470883.72265625Mb
2020-10-12 20:00:09 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 20:00:29 | INFO | train_inner | epoch 019:     74 / 107 loss=7.465, nll_loss=6.366, ppl=82.48, wps=17635, ups=2.86, wpb=6166.3, bsz=248.8, num_updates=2000, lr=0.00010005, gnorm=1.344, clip=0, train_wall=27, wall=728
2020-10-12 20:00:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:00:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19086.29296875Mb; avail=469586.734375Mb
2020-10-12 20:00:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001408
2020-10-12 20:00:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19086.1953125Mb; avail=469586.734375Mb
2020-10-12 20:00:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021760
2020-10-12 20:00:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19086.171875Mb; avail=469586.9765625Mb
2020-10-12 20:00:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016807
2020-10-12 20:00:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.040855
2020-10-12 20:00:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19086.05078125Mb; avail=469587.09765625Mb
2020-10-12 20:00:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19086.05078125Mb; avail=469587.09765625Mb
2020-10-12 20:00:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000772
2020-10-12 20:00:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19086.05078125Mb; avail=469587.09765625Mb
2020-10-12 20:00:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020938
2020-10-12 20:00:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19086.05078125Mb; avail=469587.09765625Mb
2020-10-12 20:00:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016545
2020-10-12 20:00:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039072
2020-10-12 20:00:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19086.05078125Mb; avail=469587.09765625Mb
2020-10-12 20:00:40 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.441 | nll_loss 6.283 | ppl 77.86 | wps 52906.4 | wpb 2186.6 | bsz 82.9 | num_updates 2033 | best_loss 7.441
2020-10-12 20:00:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:00:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 19 @ 2033 updates, score 7.441) (writing took 14.507635402999767 seconds)
2020-10-12 20:00:54 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 20:00:54 | INFO | train | epoch 019 | loss 7.47 | nll_loss 6.371 | ppl 82.78 | wps 14497.8 | ups 2.35 | wpb 6172 | bsz 242.4 | num_updates 2033 | lr 0.000101699 | gnorm 1.348 | clip 0 | train_wall 29 | wall 752
2020-10-12 20:00:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 20:00:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 20:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18375.60546875Mb; avail=470298.10546875Mb
2020-10-12 20:00:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001283
2020-10-12 20:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006266
2020-10-12 20:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18375.71875Mb; avail=470297.9921875Mb
2020-10-12 20:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000233
2020-10-12 20:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18375.71875Mb; avail=470297.9921875Mb
2020-10-12 20:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100444
2020-10-12 20:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107811
2020-10-12 20:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18376.109375Mb; avail=470297.5078125Mb
2020-10-12 20:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18376.859375Mb; avail=470296.66015625Mb
2020-10-12 20:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003736
2020-10-12 20:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18376.859375Mb; avail=470296.66015625Mb
2020-10-12 20:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 20:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18376.859375Mb; avail=470296.66015625Mb
2020-10-12 20:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097727
2020-10-12 20:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102420
2020-10-12 20:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18376.8359375Mb; avail=470296.90234375Mb
2020-10-12 20:00:54 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 20:01:13 | INFO | train_inner | epoch 020:     67 / 107 loss=7.399, nll_loss=6.291, ppl=78.31, wps=14312, ups=2.3, wpb=6221.7, bsz=244.4, num_updates=2100, lr=0.000105048, gnorm=1.326, clip=0, train_wall=27, wall=771
2020-10-12 20:01:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17866.91796875Mb; avail=470806.5859375Mb
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001140
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17866.91796875Mb; avail=470806.5859375Mb
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.023589
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17866.91796875Mb; avail=470806.5859375Mb
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016384
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.041919
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17866.91796875Mb; avail=470806.5859375Mb
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17866.91796875Mb; avail=470806.5859375Mb
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000737
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17866.91796875Mb; avail=470806.5859375Mb
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.023073
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17866.91796875Mb; avail=470806.5859375Mb
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016276
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.040865
2020-10-12 20:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17866.93359375Mb; avail=470806.5703125Mb
2020-10-12 20:01:25 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.375 | nll_loss 6.208 | ppl 73.93 | wps 52896.4 | wpb 2186.6 | bsz 82.9 | num_updates 2140 | best_loss 7.375
2020-10-12 20:01:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:01:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 20 @ 2140 updates, score 7.375) (writing took 4.491097903000082 seconds)
2020-10-12 20:01:29 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 20:01:29 | INFO | train | epoch 020 | loss 7.382 | nll_loss 6.27 | ppl 77.19 | wps 18700.2 | ups 3.03 | wpb 6172 | bsz 242.4 | num_updates 2140 | lr 0.000107047 | gnorm 1.32 | clip 0 | train_wall 29 | wall 788
2020-10-12 20:01:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 20:01:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 20:01:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17862.48828125Mb; avail=470810.96875Mb
2020-10-12 20:01:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000885
2020-10-12 20:01:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005506
2020-10-12 20:01:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.48828125Mb; avail=470810.96875Mb
2020-10-12 20:01:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-12 20:01:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.48828125Mb; avail=470810.96875Mb
2020-10-12 20:01:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.117385
2020-10-12 20:01:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.123887
2020-10-12 20:01:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.53515625Mb; avail=470810.7265625Mb
2020-10-12 20:01:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17861.828125Mb; avail=470810.7265625Mb
2020-10-12 20:01:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003718
2020-10-12 20:01:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.828125Mb; avail=470810.7265625Mb
2020-10-12 20:01:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 20:01:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.828125Mb; avail=470810.7265625Mb
2020-10-12 20:01:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097890
2020-10-12 20:01:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102547
2020-10-12 20:01:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.734375Mb; avail=470810.84765625Mb
2020-10-12 20:01:30 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 20:01:46 | INFO | train_inner | epoch 021:     60 / 107 loss=7.401, nll_loss=6.293, ppl=78.39, wps=18228.9, ups=2.99, wpb=6093.3, bsz=230.5, num_updates=2200, lr=0.000110045, gnorm=1.305, clip=0, train_wall=27, wall=805
2020-10-12 20:01:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19698.48046875Mb; avail=468975.1796875Mb
2020-10-12 20:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001133
2020-10-12 20:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19698.48046875Mb; avail=468975.1796875Mb
2020-10-12 20:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020927
2020-10-12 20:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19698.49609375Mb; avail=468975.05859375Mb
2020-10-12 20:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016633
2020-10-12 20:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039487
2020-10-12 20:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19698.45703125Mb; avail=468975.1796875Mb
2020-10-12 20:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19698.45703125Mb; avail=468975.1796875Mb
2020-10-12 20:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000751
2020-10-12 20:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19698.45703125Mb; avail=468975.1796875Mb
2020-10-12 20:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020640
2020-10-12 20:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19698.48046875Mb; avail=468975.05859375Mb
2020-10-12 20:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016552
2020-10-12 20:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038695
2020-10-12 20:01:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19698.50390625Mb; avail=468974.9375Mb
2020-10-12 20:02:00 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.361 | nll_loss 6.176 | ppl 72.33 | wps 53272.2 | wpb 2186.6 | bsz 82.9 | num_updates 2247 | best_loss 7.361
2020-10-12 20:02:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:02:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 21 @ 2247 updates, score 7.361) (writing took 15.103639719000057 seconds)
2020-10-12 20:02:15 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 20:02:15 | INFO | train | epoch 021 | loss 7.308 | nll_loss 6.185 | ppl 72.78 | wps 14360.4 | ups 2.33 | wpb 6172 | bsz 242.4 | num_updates 2247 | lr 0.000112394 | gnorm 1.35 | clip 0 | train_wall 29 | wall 833
2020-10-12 20:02:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 20:02:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17799.0859375Mb; avail=470874.26953125Mb
2020-10-12 20:02:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000833
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005367
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17799.20703125Mb; avail=470873.54296875Mb
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000213
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17799.8125Mb; avail=470873.54296875Mb
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098210
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104588
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.66015625Mb; avail=470864.4609375Mb
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17808.7109375Mb; avail=470864.82421875Mb
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003737
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.7109375Mb; avail=470864.8203125Mb
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.7109375Mb; avail=470864.8203125Mb
2020-10-12 20:02:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102278
2020-10-12 20:02:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107207
2020-10-12 20:02:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.86328125Mb; avail=470864.33984375Mb
2020-10-12 20:02:16 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 20:02:30 | INFO | train_inner | epoch 022:     53 / 107 loss=7.231, nll_loss=6.097, ppl=68.43, wps=13885.9, ups=2.28, wpb=6093.9, bsz=240.3, num_updates=2300, lr=0.000115043, gnorm=1.328, clip=0, train_wall=27, wall=848
2020-10-12 20:02:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18202.92578125Mb; avail=470469.94921875Mb
2020-10-12 20:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001122
2020-10-12 20:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18204.13671875Mb; avail=470468.73828125Mb
2020-10-12 20:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021319
2020-10-12 20:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18229.5859375Mb; avail=470442.58203125Mb
2020-10-12 20:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016625
2020-10-12 20:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039888
2020-10-12 20:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18250.18359375Mb; avail=470422.48046875Mb
2020-10-12 20:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18268.4375Mb; avail=470403.58984375Mb
2020-10-12 20:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000725
2020-10-12 20:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18269.6484375Mb; avail=470402.984375Mb
2020-10-12 20:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020976
2020-10-12 20:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18294.37109375Mb; avail=470378.28125Mb
2020-10-12 20:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016911
2020-10-12 20:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039361
2020-10-12 20:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18314.3515625Mb; avail=470358.30078125Mb
2020-10-12 20:02:46 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.318 | nll_loss 6.139 | ppl 70.49 | wps 53316.4 | wpb 2186.6 | bsz 82.9 | num_updates 2354 | best_loss 7.318
2020-10-12 20:02:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:02:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 22 @ 2354 updates, score 7.318) (writing took 7.523824130000321 seconds)
2020-10-12 20:02:54 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 20:02:54 | INFO | train | epoch 022 | loss 7.223 | nll_loss 6.088 | ppl 68.03 | wps 17231.1 | ups 2.79 | wpb 6172 | bsz 242.4 | num_updates 2354 | lr 0.000117741 | gnorm 1.313 | clip 0 | train_wall 29 | wall 872
2020-10-12 20:02:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 20:02:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 20:02:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17816.49609375Mb; avail=470856.19921875Mb
2020-10-12 20:02:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001439
2020-10-12 20:02:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.009216
2020-10-12 20:02:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.72265625Mb; avail=470855.97265625Mb
2020-10-12 20:02:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000378
2020-10-12 20:02:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.72265625Mb; avail=470855.97265625Mb
2020-10-12 20:02:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.131437
2020-10-12 20:02:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.142447
2020-10-12 20:02:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.9765625Mb; avail=470856.13671875Mb
2020-10-12 20:02:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17817.05859375Mb; avail=470856.13671875Mb
2020-10-12 20:02:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003744
2020-10-12 20:02:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.05859375Mb; avail=470856.13671875Mb
2020-10-12 20:02:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 20:02:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.05859375Mb; avail=470856.13671875Mb
2020-10-12 20:02:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097253
2020-10-12 20:02:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101956
2020-10-12 20:02:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.046875Mb; avail=470855.7734375Mb
2020-10-12 20:02:54 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 20:03:07 | INFO | train_inner | epoch 023:     46 / 107 loss=7.195, nll_loss=6.056, ppl=66.53, wps=16938.8, ups=2.73, wpb=6213.7, bsz=249.9, num_updates=2400, lr=0.00012004, gnorm=1.382, clip=0, train_wall=27, wall=885
2020-10-12 20:03:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:03:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17818.578125Mb; avail=470854.59765625Mb
2020-10-12 20:03:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001540
2020-10-12 20:03:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.578125Mb; avail=470854.59765625Mb
2020-10-12 20:03:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021367
2020-10-12 20:03:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.609375Mb; avail=470854.56640625Mb
2020-10-12 20:03:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016726
2020-10-12 20:03:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.040452
2020-10-12 20:03:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.4453125Mb; avail=470854.62890625Mb
2020-10-12 20:03:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17818.4609375Mb; avail=470854.61328125Mb
2020-10-12 20:03:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000758
2020-10-12 20:03:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.4609375Mb; avail=470854.61328125Mb
2020-10-12 20:03:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021008
2020-10-12 20:03:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.515625Mb; avail=470854.4765625Mb
2020-10-12 20:03:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016726
2020-10-12 20:03:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039245
2020-10-12 20:03:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.68359375Mb; avail=470854.11328125Mb
2020-10-12 20:03:25 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.263 | nll_loss 6.067 | ppl 67.04 | wps 52919.9 | wpb 2186.6 | bsz 82.9 | num_updates 2461 | best_loss 7.263
2020-10-12 20:03:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:03:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 23 @ 2461 updates, score 7.263) (writing took 8.29222044799917 seconds)
2020-10-12 20:03:33 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 20:03:33 | INFO | train | epoch 023 | loss 7.142 | nll_loss 5.995 | ppl 63.79 | wps 16775.9 | ups 2.72 | wpb 6172 | bsz 242.4 | num_updates 2461 | lr 0.000123088 | gnorm 1.264 | clip 0 | train_wall 29 | wall 911
2020-10-12 20:03:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 20:03:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 20:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17794.4921875Mb; avail=470878.1640625Mb
2020-10-12 20:03:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001170
2020-10-12 20:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006194
2020-10-12 20:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17794.0Mb; avail=470878.65625Mb
2020-10-12 20:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000237
2020-10-12 20:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17793.94921875Mb; avail=470878.8984375Mb
2020-10-12 20:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.118834
2020-10-12 20:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.126047
2020-10-12 20:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17793.7109375Mb; avail=470879.140625Mb
2020-10-12 20:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17793.53515625Mb; avail=470879.140625Mb
2020-10-12 20:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003710
2020-10-12 20:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17793.53515625Mb; avail=470879.140625Mb
2020-10-12 20:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000210
2020-10-12 20:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17793.53515625Mb; avail=470879.140625Mb
2020-10-12 20:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.134357
2020-10-12 20:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.139079
2020-10-12 20:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17794.41015625Mb; avail=470878.29296875Mb
2020-10-12 20:03:33 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 20:03:44 | INFO | train_inner | epoch 024:     39 / 107 loss=7.06, nll_loss=5.901, ppl=59.74, wps=16624.5, ups=2.68, wpb=6207.8, bsz=246.8, num_updates=2500, lr=0.000125037, gnorm=1.214, clip=0, train_wall=27, wall=922
2020-10-12 20:04:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17839.02734375Mb; avail=470833.33203125Mb
2020-10-12 20:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001269
2020-10-12 20:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.02734375Mb; avail=470833.33203125Mb
2020-10-12 20:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021359
2020-10-12 20:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.9296875Mb; avail=470833.81640625Mb
2020-10-12 20:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016091
2020-10-12 20:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039512
2020-10-12 20:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.9296875Mb; avail=470833.81640625Mb
2020-10-12 20:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17838.9296875Mb; avail=470833.81640625Mb
2020-10-12 20:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000741
2020-10-12 20:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.9296875Mb; avail=470833.81640625Mb
2020-10-12 20:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021615
2020-10-12 20:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.953125Mb; avail=470833.6953125Mb
2020-10-12 20:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016111
2020-10-12 20:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039230
2020-10-12 20:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.015625Mb; avail=470833.81640625Mb
2020-10-12 20:04:04 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.213 | nll_loss 6.002 | ppl 64.07 | wps 53085 | wpb 2186.6 | bsz 82.9 | num_updates 2568 | best_loss 7.213
2020-10-12 20:04:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:04:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 24 @ 2568 updates, score 7.213) (writing took 8.515647815000193 seconds)
2020-10-12 20:04:13 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 20:04:13 | INFO | train | epoch 024 | loss 7.065 | nll_loss 5.906 | ppl 59.98 | wps 16660.1 | ups 2.7 | wpb 6172 | bsz 242.4 | num_updates 2568 | lr 0.000128436 | gnorm 1.3 | clip 0 | train_wall 29 | wall 951
2020-10-12 20:04:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 20:04:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 20:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17817.13671875Mb; avail=470855.796875Mb
2020-10-12 20:04:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000635
2020-10-12 20:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005173
2020-10-12 20:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.16015625Mb; avail=470855.67578125Mb
2020-10-12 20:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 20:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.16015625Mb; avail=470855.67578125Mb
2020-10-12 20:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098102
2020-10-12 20:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104214
2020-10-12 20:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.0390625Mb; avail=470855.48046875Mb
2020-10-12 20:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17816.9921875Mb; avail=470855.72265625Mb
2020-10-12 20:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003624
2020-10-12 20:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.9921875Mb; avail=470855.72265625Mb
2020-10-12 20:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 20:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.9921875Mb; avail=470855.72265625Mb
2020-10-12 20:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098358
2020-10-12 20:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102954
2020-10-12 20:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.890625Mb; avail=470856.0859375Mb
2020-10-12 20:04:13 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 20:04:22 | INFO | train_inner | epoch 025:     32 / 107 loss=7.117, nll_loss=5.965, ppl=62.46, wps=16334.5, ups=2.66, wpb=6134.8, bsz=230.6, num_updates=2600, lr=0.000130035, gnorm=1.285, clip=0, train_wall=27, wall=960
2020-10-12 20:04:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18061.0078125Mb; avail=470611.91796875Mb
2020-10-12 20:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001224
2020-10-12 20:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18061.0078125Mb; avail=470611.91796875Mb
2020-10-12 20:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021012
2020-10-12 20:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18061.09765625Mb; avail=470611.796875Mb
2020-10-12 20:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016115
2020-10-12 20:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039151
2020-10-12 20:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18061.1328125Mb; avail=470611.67578125Mb
2020-10-12 20:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18061.10546875Mb; avail=470611.91796875Mb
2020-10-12 20:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000704
2020-10-12 20:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18061.10546875Mb; avail=470611.91796875Mb
2020-10-12 20:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020980
2020-10-12 20:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18061.15234375Mb; avail=470612.0703125Mb
2020-10-12 20:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016144
2020-10-12 20:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038627
2020-10-12 20:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18063.4609375Mb; avail=470609.76171875Mb
2020-10-12 20:04:44 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.185 | nll_loss 5.978 | ppl 63.02 | wps 53041.2 | wpb 2186.6 | bsz 82.9 | num_updates 2675 | best_loss 7.185
2020-10-12 20:04:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:04:52 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 25 @ 2675 updates, score 7.185) (writing took 8.115029132000018 seconds)
2020-10-12 20:04:52 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 20:04:52 | INFO | train | epoch 025 | loss 6.985 | nll_loss 5.814 | ppl 56.24 | wps 16883.3 | ups 2.74 | wpb 6172 | bsz 242.4 | num_updates 2675 | lr 0.000133783 | gnorm 1.283 | clip 0 | train_wall 29 | wall 990
2020-10-12 20:04:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 20:04:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 20:04:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18094.6875Mb; avail=470578.1484375Mb
2020-10-12 20:04:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000830
2020-10-12 20:04:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005671
2020-10-12 20:04:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18094.6875Mb; avail=470578.1484375Mb
2020-10-12 20:04:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000215
2020-10-12 20:04:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18094.6875Mb; avail=470578.1484375Mb
2020-10-12 20:04:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098741
2020-10-12 20:04:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105391
2020-10-12 20:04:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18094.2890625Mb; avail=470578.953125Mb
2020-10-12 20:04:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18094.359375Mb; avail=470578.6640625Mb
2020-10-12 20:04:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003707
2020-10-12 20:04:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18094.359375Mb; avail=470578.6640625Mb
2020-10-12 20:04:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:04:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18094.359375Mb; avail=470578.6640625Mb
2020-10-12 20:04:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098393
2020-10-12 20:04:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103035
2020-10-12 20:04:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18094.33984375Mb; avail=470579.13671875Mb
2020-10-12 20:04:52 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 20:04:59 | INFO | train_inner | epoch 026:     25 / 107 loss=6.946, nll_loss=5.769, ppl=54.52, wps=16603.5, ups=2.69, wpb=6171.5, bsz=245.7, num_updates=2700, lr=0.000135032, gnorm=1.299, clip=0, train_wall=27, wall=997
2020-10-12 20:05:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18082.82421875Mb; avail=470591.21875Mb
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001189
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18082.82421875Mb; avail=470591.21875Mb
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021185
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18082.85546875Mb; avail=470591.09765625Mb
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016826
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.040027
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18082.875Mb; avail=470590.9765625Mb
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18082.92578125Mb; avail=470591.09765625Mb
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000770
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18082.92578125Mb; avail=470591.09765625Mb
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020875
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18082.92578125Mb; avail=470591.09765625Mb
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016835
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039263
2020-10-12 20:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18082.92578125Mb; avail=470591.09765625Mb
2020-10-12 20:05:23 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.231 | nll_loss 6.027 | ppl 65.22 | wps 52680.1 | wpb 2186.6 | bsz 82.9 | num_updates 2782 | best_loss 7.185
2020-10-12 20:05:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:05:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_last.pt (epoch 26 @ 2782 updates, score 7.231) (writing took 2.214070877999802 seconds)
2020-10-12 20:05:25 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 20:05:25 | INFO | train | epoch 026 | loss 6.911 | nll_loss 5.728 | ppl 53 | wps 19917.3 | ups 3.23 | wpb 6172 | bsz 242.4 | num_updates 2782 | lr 0.00013913 | gnorm 1.29 | clip 0 | train_wall 29 | wall 1023
2020-10-12 20:05:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 20:05:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 20:05:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17905.23828125Mb; avail=470767.35546875Mb
2020-10-12 20:05:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000861
2020-10-12 20:05:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005341
2020-10-12 20:05:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17910.12890625Mb; avail=470763.24609375Mb
2020-10-12 20:05:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000196
2020-10-12 20:05:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17910.734375Mb; avail=470762.03515625Mb
2020-10-12 20:05:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098669
2020-10-12 20:05:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104955
2020-10-12 20:05:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18027.86328125Mb; avail=470645.31640625Mb
2020-10-12 20:05:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18062.98046875Mb; avail=470610.19921875Mb
2020-10-12 20:05:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003634
2020-10-12 20:05:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18067.82421875Mb; avail=470605.35546875Mb
2020-10-12 20:05:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 20:05:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18068.4296875Mb; avail=470604.14453125Mb
2020-10-12 20:05:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096691
2020-10-12 20:05:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101269
2020-10-12 20:05:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18195.28125Mb; avail=470477.96875Mb
2020-10-12 20:05:25 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 20:05:30 | INFO | train_inner | epoch 027:     18 / 107 loss=6.921, nll_loss=5.739, ppl=53.39, wps=19921.3, ups=3.21, wpb=6211.7, bsz=238.5, num_updates=2800, lr=0.00014003, gnorm=1.354, clip=0, train_wall=27, wall=1028
2020-10-12 20:05:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18377.59765625Mb; avail=470295.82421875Mb
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001147
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18377.59765625Mb; avail=470295.82421875Mb
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021238
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18377.59765625Mb; avail=470295.82421875Mb
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016616
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039834
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18381.7578125Mb; avail=470291.3359375Mb
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18388.7109375Mb; avail=470284.2734375Mb
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000749
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18388.7109375Mb; avail=470284.2734375Mb
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021157
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18390.41796875Mb; avail=470282.9453125Mb
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016373
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039073
2020-10-12 20:05:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18390.85546875Mb; avail=470282.4609375Mb
2020-10-12 20:05:56 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.114 | nll_loss 5.881 | ppl 58.93 | wps 52408.7 | wpb 2186.6 | bsz 82.9 | num_updates 2889 | best_loss 7.114
2020-10-12 20:05:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:06:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 27 @ 2889 updates, score 7.114) (writing took 10.364990594000119 seconds)
2020-10-12 20:06:06 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 20:06:06 | INFO | train | epoch 027 | loss 6.843 | nll_loss 5.65 | ppl 50.22 | wps 15997.8 | ups 2.59 | wpb 6172 | bsz 242.4 | num_updates 2889 | lr 0.000144478 | gnorm 1.335 | clip 0 | train_wall 29 | wall 1064
2020-10-12 20:06:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 20:06:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 20:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17841.93359375Mb; avail=470836.66796875Mb
2020-10-12 20:06:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000865
2020-10-12 20:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005519
2020-10-12 20:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.765625Mb; avail=470836.859375Mb
2020-10-12 20:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 20:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.63671875Mb; avail=470836.74609375Mb
2020-10-12 20:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098686
2020-10-12 20:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105166
2020-10-12 20:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.6953125Mb; avail=470835.47265625Mb
2020-10-12 20:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17841.31640625Mb; avail=470835.2109375Mb
2020-10-12 20:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003686
2020-10-12 20:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.51953125Mb; avail=470835.26953125Mb
2020-10-12 20:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 20:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.390625Mb; avail=470835.15625Mb
2020-10-12 20:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099671
2020-10-12 20:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104403
2020-10-12 20:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17840.80078125Mb; avail=470834.125Mb
2020-10-12 20:06:06 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 20:06:10 | INFO | train_inner | epoch 028:     11 / 107 loss=6.817, nll_loss=5.62, ppl=49.18, wps=15797.3, ups=2.53, wpb=6245.6, bsz=256.9, num_updates=2900, lr=0.000145028, gnorm=1.274, clip=0, train_wall=27, wall=1068
2020-10-12 20:06:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17842.109375Mb; avail=470830.93359375Mb
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001114
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.109375Mb; avail=470830.93359375Mb
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021355
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.13671875Mb; avail=470830.8125Mb
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016469
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039729
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.33203125Mb; avail=470830.69140625Mb
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17842.33203125Mb; avail=470830.69140625Mb
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000720
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.234375Mb; avail=470830.69140625Mb
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021250
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.234375Mb; avail=470830.69140625Mb
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016422
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039137
2020-10-12 20:06:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.234375Mb; avail=470830.69140625Mb
2020-10-12 20:06:37 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.087 | nll_loss 5.85 | ppl 57.67 | wps 52690.3 | wpb 2186.6 | bsz 82.9 | num_updates 2996 | best_loss 7.087
2020-10-12 20:06:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:06:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 28 @ 2996 updates, score 7.087) (writing took 11.684845529000086 seconds)
2020-10-12 20:06:49 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 20:06:49 | INFO | train | epoch 028 | loss 6.739 | nll_loss 5.529 | ppl 46.18 | wps 15566.8 | ups 2.52 | wpb 6172 | bsz 242.4 | num_updates 2996 | lr 0.000149825 | gnorm 1.274 | clip 0 | train_wall 29 | wall 1107
2020-10-12 20:06:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 20:06:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 20:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18403.765625Mb; avail=470270.05078125Mb
2020-10-12 20:06:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001297
2020-10-12 20:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007118
2020-10-12 20:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18403.5Mb; avail=470270.31640625Mb
2020-10-12 20:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000282
2020-10-12 20:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18403.5Mb; avail=470270.31640625Mb
2020-10-12 20:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101176
2020-10-12 20:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109478
2020-10-12 20:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18403.984375Mb; avail=470269.7421875Mb
2020-10-12 20:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18403.984375Mb; avail=470269.7421875Mb
2020-10-12 20:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003776
2020-10-12 20:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18403.984375Mb; avail=470269.7421875Mb
2020-10-12 20:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18403.984375Mb; avail=470269.7421875Mb
2020-10-12 20:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099450
2020-10-12 20:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104157
2020-10-12 20:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18403.953125Mb; avail=470269.984375Mb
2020-10-12 20:06:49 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 20:06:50 | INFO | train_inner | epoch 029:      4 / 107 loss=6.702, nll_loss=5.488, ppl=44.89, wps=15099.2, ups=2.47, wpb=6107.6, bsz=242.7, num_updates=3000, lr=0.000150025, gnorm=1.267, clip=0, train_wall=27, wall=1108
2020-10-12 20:07:18 | INFO | train_inner | epoch 029:    104 / 107 loss=6.661, nll_loss=5.44, ppl=43.4, wps=21869.7, ups=3.55, wpb=6157.6, bsz=235.8, num_updates=3100, lr=0.000155023, gnorm=1.269, clip=0, train_wall=27, wall=1136
2020-10-12 20:07:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17875.0234375Mb; avail=470798.02734375Mb
2020-10-12 20:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001519
2020-10-12 20:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17875.10546875Mb; avail=470797.90625Mb
2020-10-12 20:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021145
2020-10-12 20:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17875.09375Mb; avail=470797.78515625Mb
2020-10-12 20:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016662
2020-10-12 20:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.041460
2020-10-12 20:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17875.05859375Mb; avail=470798.02734375Mb
2020-10-12 20:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17875.05859375Mb; avail=470798.02734375Mb
2020-10-12 20:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000727
2020-10-12 20:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17875.05859375Mb; avail=470798.02734375Mb
2020-10-12 20:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021067
2020-10-12 20:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17875.05859375Mb; avail=470798.02734375Mb
2020-10-12 20:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016504
2020-10-12 20:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039055
2020-10-12 20:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17875.05859375Mb; avail=470798.02734375Mb
2020-10-12 20:07:20 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.057 | nll_loss 5.821 | ppl 56.52 | wps 52890.3 | wpb 2186.6 | bsz 82.9 | num_updates 3103 | best_loss 7.057
2020-10-12 20:07:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:07:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 29 @ 3103 updates, score 7.057) (writing took 4.425575342000229 seconds)
2020-10-12 20:07:24 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 20:07:24 | INFO | train | epoch 029 | loss 6.655 | nll_loss 5.433 | ppl 43.2 | wps 18443.9 | ups 2.99 | wpb 6172 | bsz 242.4 | num_updates 3103 | lr 0.000155172 | gnorm 1.263 | clip 0 | train_wall 29 | wall 1143
2020-10-12 20:07:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 20:07:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18429.48046875Mb; avail=470247.5703125Mb
2020-10-12 20:07:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001119
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005906
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18428.41796875Mb; avail=470248.3125Mb
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000188
2020-10-12 20:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18428.89453125Mb; avail=470248.19921875Mb
2020-10-12 20:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099659
2020-10-12 20:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106514
2020-10-12 20:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18429.44921875Mb; avail=470246.34765625Mb
2020-10-12 20:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18428.6328125Mb; avail=470246.0703125Mb
2020-10-12 20:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003791
2020-10-12 20:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18429.0703125Mb; avail=470246.75390625Mb
2020-10-12 20:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18429.546875Mb; avail=470246.03515625Mb
2020-10-12 20:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098402
2020-10-12 20:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103183
2020-10-12 20:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18429.3984375Mb; avail=470244.6171875Mb
2020-10-12 20:07:25 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 20:07:52 | INFO | train_inner | epoch 030:     97 / 107 loss=6.587, nll_loss=5.354, ppl=40.91, wps=18373.6, ups=2.98, wpb=6158, bsz=235.6, num_updates=3200, lr=0.00016002, gnorm=1.345, clip=0, train_wall=27, wall=1170
2020-10-12 20:07:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18797.35546875Mb; avail=469876.078125Mb
2020-10-12 20:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001155
2020-10-12 20:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18797.35546875Mb; avail=469876.078125Mb
2020-10-12 20:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021223
2020-10-12 20:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18797.35546875Mb; avail=469876.078125Mb
2020-10-12 20:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016842
2020-10-12 20:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.040010
2020-10-12 20:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18797.35546875Mb; avail=469876.078125Mb
2020-10-12 20:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18797.35546875Mb; avail=469876.078125Mb
2020-10-12 20:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000741
2020-10-12 20:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18797.35546875Mb; avail=469876.078125Mb
2020-10-12 20:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021117
2020-10-12 20:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18797.35546875Mb; avail=469876.078125Mb
2020-10-12 20:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016486
2020-10-12 20:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039093
2020-10-12 20:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18797.35546875Mb; avail=469876.078125Mb
2020-10-12 20:07:56 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.04 | nll_loss 5.812 | ppl 56.19 | wps 52652.8 | wpb 2186.6 | bsz 82.9 | num_updates 3210 | best_loss 7.04
2020-10-12 20:07:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:08:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 30 @ 3210 updates, score 7.04) (writing took 9.908860063999782 seconds)
2020-10-12 20:08:05 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 20:08:05 | INFO | train | epoch 030 | loss 6.584 | nll_loss 5.351 | ppl 40.81 | wps 16121.8 | ups 2.61 | wpb 6172 | bsz 242.4 | num_updates 3210 | lr 0.00016052 | gnorm 1.371 | clip 0 | train_wall 29 | wall 1184
2020-10-12 20:08:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 20:08:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 20:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18535.40625Mb; avail=470137.04296875Mb
2020-10-12 20:08:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000652
2020-10-12 20:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005245
2020-10-12 20:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18535.4921875Mb; avail=470137.04296875Mb
2020-10-12 20:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000200
2020-10-12 20:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18535.4921875Mb; avail=470137.04296875Mb
2020-10-12 20:08:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097927
2020-10-12 20:08:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104137
2020-10-12 20:08:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18535.75390625Mb; avail=470137.02734375Mb
2020-10-12 20:08:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18535.62109375Mb; avail=470137.1484375Mb
2020-10-12 20:08:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003748
2020-10-12 20:08:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18535.62109375Mb; avail=470137.1484375Mb
2020-10-12 20:08:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 20:08:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18535.62109375Mb; avail=470137.1484375Mb
2020-10-12 20:08:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097068
2020-10-12 20:08:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101742
2020-10-12 20:08:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18535.8125Mb; avail=470136.90625Mb
2020-10-12 20:08:06 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 20:08:31 | INFO | train_inner | epoch 031:     90 / 107 loss=6.492, nll_loss=5.245, ppl=37.91, wps=15867.5, ups=2.58, wpb=6151.9, bsz=247.9, num_updates=3300, lr=0.000165018, gnorm=1.321, clip=0, train_wall=27, wall=1209
2020-10-12 20:08:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17826.40234375Mb; avail=470846.5625Mb
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001197
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.40234375Mb; avail=470846.5625Mb
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021504
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.40234375Mb; avail=470846.5625Mb
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.017330
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.040890
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.33203125Mb; avail=470846.44140625Mb
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17826.33203125Mb; avail=470846.44140625Mb
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000788
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.33203125Mb; avail=470846.44140625Mb
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021464
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.27734375Mb; avail=470846.68359375Mb
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.017177
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.040219
2020-10-12 20:08:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.23828125Mb; avail=470846.92578125Mb
2020-10-12 20:08:36 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.962 | nll_loss 5.687 | ppl 51.52 | wps 52531.6 | wpb 2186.6 | bsz 82.9 | num_updates 3317 | best_loss 6.962
2020-10-12 20:08:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:08:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 31 @ 3317 updates, score 6.962) (writing took 5.7727012949999335 seconds)
2020-10-12 20:08:42 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 20:08:42 | INFO | train | epoch 031 | loss 6.491 | nll_loss 5.243 | ppl 37.87 | wps 18062.1 | ups 2.93 | wpb 6172 | bsz 242.4 | num_updates 3317 | lr 0.000165867 | gnorm 1.277 | clip 0 | train_wall 29 | wall 1220
2020-10-12 20:08:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 20:08:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 20:08:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18406.01953125Mb; avail=470267.1171875Mb
2020-10-12 20:08:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000712
2020-10-12 20:08:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005383
2020-10-12 20:08:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18406.01953125Mb; avail=470267.1171875Mb
2020-10-12 20:08:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:08:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18406.01953125Mb; avail=470267.1171875Mb
2020-10-12 20:08:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099290
2020-10-12 20:08:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105611
2020-10-12 20:08:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18405.9765625Mb; avail=470267.359375Mb
2020-10-12 20:08:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18405.89453125Mb; avail=470267.359375Mb
2020-10-12 20:08:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003729
2020-10-12 20:08:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18405.90625Mb; avail=470267.23828125Mb
2020-10-12 20:08:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 20:08:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18405.90625Mb; avail=470267.23828125Mb
2020-10-12 20:08:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097999
2020-10-12 20:08:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102660
2020-10-12 20:08:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18406.05078125Mb; avail=470267.1171875Mb
2020-10-12 20:08:42 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 20:09:05 | INFO | train_inner | epoch 032:     83 / 107 loss=6.427, nll_loss=5.169, ppl=35.97, wps=17683.9, ups=2.87, wpb=6156.6, bsz=242.4, num_updates=3400, lr=0.000170015, gnorm=1.307, clip=0, train_wall=27, wall=1244
2020-10-12 20:09:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17807.03515625Mb; avail=470865.78515625Mb
2020-10-12 20:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001156
2020-10-12 20:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17807.03515625Mb; avail=470865.78515625Mb
2020-10-12 20:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.024349
2020-10-12 20:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17807.06640625Mb; avail=470865.6640625Mb
2020-10-12 20:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.033466
2020-10-12 20:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.059995
2020-10-12 20:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17807.19140625Mb; avail=470865.421875Mb
2020-10-12 20:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17807.15234375Mb; avail=470865.6640625Mb
2020-10-12 20:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000729
2020-10-12 20:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17807.15234375Mb; avail=470865.6640625Mb
2020-10-12 20:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021272
2020-10-12 20:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17807.15234375Mb; avail=470865.6640625Mb
2020-10-12 20:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016874
2020-10-12 20:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039634
2020-10-12 20:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17807.02734375Mb; avail=470865.6640625Mb
2020-10-12 20:09:13 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.933 | nll_loss 5.653 | ppl 50.31 | wps 52834 | wpb 2186.6 | bsz 82.9 | num_updates 3424 | best_loss 6.933
2020-10-12 20:09:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:09:17 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 32 @ 3424 updates, score 6.933) (writing took 4.486831266000081 seconds)
2020-10-12 20:09:17 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 20:09:17 | INFO | train | epoch 032 | loss 6.403 | nll_loss 5.142 | ppl 35.31 | wps 18624.2 | ups 3.02 | wpb 6172 | bsz 242.4 | num_updates 3424 | lr 0.000171214 | gnorm 1.313 | clip 0 | train_wall 29 | wall 1256
2020-10-12 20:09:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 20:09:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 20:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17827.18359375Mb; avail=470845.62890625Mb
2020-10-12 20:09:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000836
2020-10-12 20:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006400
2020-10-12 20:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.1484375Mb; avail=470845.75Mb
2020-10-12 20:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000357
2020-10-12 20:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.109375Mb; avail=470845.9921875Mb
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098899
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107027
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.20703125Mb; avail=470845.91796875Mb
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17827.1796875Mb; avail=470845.91796875Mb
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003701
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.1796875Mb; avail=470845.91796875Mb
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000228
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.1796875Mb; avail=470845.91796875Mb
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097216
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101901
2020-10-12 20:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.80859375Mb; avail=470846.28125Mb
2020-10-12 20:09:18 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 20:09:39 | INFO | train_inner | epoch 033:     76 / 107 loss=6.306, nll_loss=5.03, ppl=32.68, wps=18693.6, ups=2.99, wpb=6261.1, bsz=257.4, num_updates=3500, lr=0.000175013, gnorm=1.264, clip=0, train_wall=27, wall=1277
2020-10-12 20:09:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17783.56640625Mb; avail=470889.0Mb
2020-10-12 20:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001649
2020-10-12 20:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17783.46875Mb; avail=470889.0Mb
2020-10-12 20:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.022579
2020-10-12 20:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17783.4296875Mb; avail=470889.12109375Mb
2020-10-12 20:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.017478
2020-10-12 20:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.042657
2020-10-12 20:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17783.4296875Mb; avail=470889.12109375Mb
2020-10-12 20:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17783.4375Mb; avail=470889.0Mb
2020-10-12 20:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000758
2020-10-12 20:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17783.4375Mb; avail=470889.0Mb
2020-10-12 20:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.023942
2020-10-12 20:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17783.40234375Mb; avail=470889.12109375Mb
2020-10-12 20:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.017278
2020-10-12 20:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.042820
2020-10-12 20:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17783.40234375Mb; avail=470889.12109375Mb
2020-10-12 20:09:48 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.909 | nll_loss 5.63 | ppl 49.52 | wps 52618.2 | wpb 2186.6 | bsz 82.9 | num_updates 3531 | best_loss 6.909
2020-10-12 20:09:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:09:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 33 @ 3531 updates, score 6.909) (writing took 4.866253088999656 seconds)
2020-10-12 20:09:53 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 20:09:53 | INFO | train | epoch 033 | loss 6.308 | nll_loss 5.032 | ppl 32.71 | wps 18482.6 | ups 2.99 | wpb 6172 | bsz 242.4 | num_updates 3531 | lr 0.000176562 | gnorm 1.276 | clip 0 | train_wall 29 | wall 1291
2020-10-12 20:09:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 20:09:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 20:09:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17804.34375Mb; avail=470868.03125Mb
2020-10-12 20:09:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000921
2020-10-12 20:09:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005645
2020-10-12 20:09:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17804.44921875Mb; avail=470867.92578125Mb
2020-10-12 20:09:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000196
2020-10-12 20:09:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17804.44921875Mb; avail=470867.92578125Mb
2020-10-12 20:09:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098368
2020-10-12 20:09:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105008
2020-10-12 20:09:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.33984375Mb; avail=470867.1953125Mb
2020-10-12 20:09:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17804.84765625Mb; avail=470867.6875Mb
2020-10-12 20:09:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003773
2020-10-12 20:09:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17804.84765625Mb; avail=470867.6875Mb
2020-10-12 20:09:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 20:09:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17804.84765625Mb; avail=470867.6875Mb
2020-10-12 20:09:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098984
2020-10-12 20:09:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103876
2020-10-12 20:09:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.0Mb; avail=470867.4296875Mb
2020-10-12 20:09:53 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 20:10:13 | INFO | train_inner | epoch 034:     69 / 107 loss=6.258, nll_loss=4.973, ppl=31.4, wps=17996.3, ups=2.96, wpb=6077.2, bsz=237.7, num_updates=3600, lr=0.00018001, gnorm=1.356, clip=0, train_wall=27, wall=1311
2020-10-12 20:10:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18077.32421875Mb; avail=470594.99609375Mb
2020-10-12 20:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000839
2020-10-12 20:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18077.32421875Mb; avail=470594.99609375Mb
2020-10-12 20:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021395
2020-10-12 20:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18077.2734375Mb; avail=470595.1171875Mb
2020-10-12 20:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016659
2020-10-12 20:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039658
2020-10-12 20:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18077.3359375Mb; avail=470594.875Mb
2020-10-12 20:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18077.3046875Mb; avail=470594.99609375Mb
2020-10-12 20:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000723
2020-10-12 20:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18077.3046875Mb; avail=470594.99609375Mb
2020-10-12 20:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021041
2020-10-12 20:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18077.15625Mb; avail=470595.23828125Mb
2020-10-12 20:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016313
2020-10-12 20:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038816
2020-10-12 20:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18077.12890625Mb; avail=470594.875Mb
2020-10-12 20:10:24 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.945 | nll_loss 5.67 | ppl 50.92 | wps 53442.3 | wpb 2186.6 | bsz 82.9 | num_updates 3638 | best_loss 6.909
2020-10-12 20:10:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:10:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_last.pt (epoch 34 @ 3638 updates, score 6.945) (writing took 3.6630224700002145 seconds)
2020-10-12 20:10:28 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 20:10:28 | INFO | train | epoch 034 | loss 6.245 | nll_loss 4.957 | ppl 31.05 | wps 19052 | ups 3.09 | wpb 6172 | bsz 242.4 | num_updates 3638 | lr 0.000181909 | gnorm 1.383 | clip 0 | train_wall 29 | wall 1326
2020-10-12 20:10:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 20:10:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 20:10:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19309.828125Mb; avail=469363.171875Mb
2020-10-12 20:10:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000851
2020-10-12 20:10:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005680
2020-10-12 20:10:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19309.828125Mb; avail=469363.171875Mb
2020-10-12 20:10:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000206
2020-10-12 20:10:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19309.828125Mb; avail=469363.171875Mb
2020-10-12 20:10:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098197
2020-10-12 20:10:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104843
2020-10-12 20:10:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19308.9609375Mb; avail=469363.9296875Mb
2020-10-12 20:10:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19308.9609375Mb; avail=469363.9296875Mb
2020-10-12 20:10:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003595
2020-10-12 20:10:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19308.9609375Mb; avail=469363.9296875Mb
2020-10-12 20:10:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 20:10:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19308.9609375Mb; avail=469363.9296875Mb
2020-10-12 20:10:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099934
2020-10-12 20:10:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104526
2020-10-12 20:10:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19308.9296875Mb; avail=469363.9296875Mb
2020-10-12 20:10:28 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 20:10:46 | INFO | train_inner | epoch 035:     62 / 107 loss=6.199, nll_loss=4.904, ppl=29.94, wps=18853.6, ups=3.03, wpb=6221.3, bsz=241, num_updates=3700, lr=0.000185008, gnorm=1.314, clip=0, train_wall=27, wall=1344
2020-10-12 20:10:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17809.71875Mb; avail=470863.2109375Mb
2020-10-12 20:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001215
2020-10-12 20:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.71875Mb; avail=470863.2109375Mb
2020-10-12 20:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021088
2020-10-12 20:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.41015625Mb; avail=470863.33203125Mb
2020-10-12 20:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016522
2020-10-12 20:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039625
2020-10-12 20:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.359375Mb; avail=470863.57421875Mb
2020-10-12 20:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17809.359375Mb; avail=470863.57421875Mb
2020-10-12 20:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000735
2020-10-12 20:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.359375Mb; avail=470863.57421875Mb
2020-10-12 20:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021100
2020-10-12 20:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.3359375Mb; avail=470863.2109375Mb
2020-10-12 20:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016553
2020-10-12 20:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039142
2020-10-12 20:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.3359375Mb; avail=470863.2109375Mb
2020-10-12 20:10:59 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.854 | nll_loss 5.53 | ppl 46.22 | wps 53101.5 | wpb 2186.6 | bsz 82.9 | num_updates 3745 | best_loss 6.854
2020-10-12 20:10:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:11:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 35 @ 3745 updates, score 6.854) (writing took 4.727860369000155 seconds)
2020-10-12 20:11:04 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 20:11:04 | INFO | train | epoch 035 | loss 6.131 | nll_loss 4.825 | ppl 28.35 | wps 18335.6 | ups 2.97 | wpb 6172 | bsz 242.4 | num_updates 3745 | lr 0.000187256 | gnorm 1.265 | clip 0 | train_wall 29 | wall 1362
2020-10-12 20:11:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 20:11:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 20:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17832.2109375Mb; avail=470839.96484375Mb
2020-10-12 20:11:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001101
2020-10-12 20:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005866
2020-10-12 20:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17832.25390625Mb; avail=470839.72265625Mb
2020-10-12 20:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000215
2020-10-12 20:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17832.25390625Mb; avail=470839.72265625Mb
2020-10-12 20:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097965
2020-10-12 20:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104828
2020-10-12 20:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17832.1875Mb; avail=470840.4609375Mb
2020-10-12 20:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17832.16015625Mb; avail=470840.4609375Mb
2020-10-12 20:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003670
2020-10-12 20:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17832.16015625Mb; avail=470840.4609375Mb
2020-10-12 20:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 20:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17832.16015625Mb; avail=470840.4609375Mb
2020-10-12 20:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098380
2020-10-12 20:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102979
2020-10-12 20:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17832.4140625Mb; avail=470840.21875Mb
2020-10-12 20:11:04 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 20:11:19 | INFO | train_inner | epoch 036:     55 / 107 loss=6.075, nll_loss=4.76, ppl=27.1, wps=18367.4, ups=2.97, wpb=6187.3, bsz=237.5, num_updates=3800, lr=0.000190005, gnorm=1.26, clip=0, train_wall=27, wall=1378
2020-10-12 20:11:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17848.6328125Mb; avail=470824.84375Mb
2020-10-12 20:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001208
2020-10-12 20:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17848.6328125Mb; avail=470824.84375Mb
2020-10-12 20:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021286
2020-10-12 20:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17848.59765625Mb; avail=470824.96484375Mb
2020-10-12 20:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016787
2020-10-12 20:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.040070
2020-10-12 20:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17848.59765625Mb; avail=470824.96484375Mb
2020-10-12 20:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17849.203125Mb; avail=470824.359375Mb
2020-10-12 20:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000705
2020-10-12 20:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.203125Mb; avail=470824.359375Mb
2020-10-12 20:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020977
2020-10-12 20:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.08203125Mb; avail=470824.48046875Mb
2020-10-12 20:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016550
2020-10-12 20:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039018
2020-10-12 20:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.2734375Mb; avail=470824.48046875Mb
2020-10-12 20:11:35 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.881 | nll_loss 5.579 | ppl 47.81 | wps 52695.6 | wpb 2186.6 | bsz 82.9 | num_updates 3852 | best_loss 6.854
2020-10-12 20:11:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:11:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_last.pt (epoch 36 @ 3852 updates, score 6.881) (writing took 3.6499479089998204 seconds)
2020-10-12 20:11:38 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 20:11:38 | INFO | train | epoch 036 | loss 6.044 | nll_loss 4.724 | ppl 26.42 | wps 19215.3 | ups 3.11 | wpb 6172 | bsz 242.4 | num_updates 3852 | lr 0.000192604 | gnorm 1.326 | clip 0 | train_wall 28 | wall 1396
2020-10-12 20:11:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 20:11:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 20:11:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18449.1171875Mb; avail=470223.4765625Mb
2020-10-12 20:11:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000895
2020-10-12 20:11:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005582
2020-10-12 20:11:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18448.65625Mb; avail=470223.7265625Mb
2020-10-12 20:11:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000199
2020-10-12 20:11:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18448.65625Mb; avail=470223.7265625Mb
2020-10-12 20:11:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099075
2020-10-12 20:11:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105626
2020-10-12 20:11:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18447.3359375Mb; avail=470225.2421875Mb
2020-10-12 20:11:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18447.3828125Mb; avail=470225.12109375Mb
2020-10-12 20:11:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004157
2020-10-12 20:11:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18447.3828125Mb; avail=470225.12109375Mb
2020-10-12 20:11:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000231
2020-10-12 20:11:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18447.3828125Mb; avail=470225.12109375Mb
2020-10-12 20:11:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098544
2020-10-12 20:11:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103726
2020-10-12 20:11:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18447.46484375Mb; avail=470225.25Mb
2020-10-12 20:11:38 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 20:11:52 | INFO | train_inner | epoch 037:     48 / 107 loss=6.024, nll_loss=4.701, ppl=26.01, wps=18515, ups=3.06, wpb=6054, bsz=237.9, num_updates=3900, lr=0.000195003, gnorm=1.394, clip=0, train_wall=27, wall=1410
2020-10-12 20:12:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17874.140625Mb; avail=470798.88671875Mb
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001208
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.140625Mb; avail=470798.88671875Mb
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021697
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.03515625Mb; avail=470798.88671875Mb
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016744
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.040438
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.03515625Mb; avail=470798.88671875Mb
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17874.03125Mb; avail=470798.9765625Mb
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000770
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.03125Mb; avail=470798.9765625Mb
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.034439
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.94140625Mb; avail=470798.9765625Mb
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016519
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.052510
2020-10-12 20:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.9453125Mb; avail=470799.09765625Mb
2020-10-12 20:12:10 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.792 | nll_loss 5.465 | ppl 44.17 | wps 51601.8 | wpb 2186.6 | bsz 82.9 | num_updates 3959 | best_loss 6.792
2020-10-12 20:12:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:12:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 37 @ 3959 updates, score 6.792) (writing took 4.470054225999775 seconds)
2020-10-12 20:12:14 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 20:12:14 | INFO | train | epoch 037 | loss 5.959 | nll_loss 4.625 | ppl 24.67 | wps 18435 | ups 2.99 | wpb 6172 | bsz 242.4 | num_updates 3959 | lr 0.000197951 | gnorm 1.327 | clip 0 | train_wall 29 | wall 1432
2020-10-12 20:12:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 20:12:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 20:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17851.98828125Mb; avail=470820.453125Mb
2020-10-12 20:12:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000858
2020-10-12 20:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005500
2020-10-12 20:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.98828125Mb; avail=470820.453125Mb
2020-10-12 20:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 20:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.98828125Mb; avail=470820.453125Mb
2020-10-12 20:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097999
2020-10-12 20:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104446
2020-10-12 20:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.4375Mb; avail=470821.1875Mb
2020-10-12 20:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17851.57421875Mb; avail=470821.06640625Mb
2020-10-12 20:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003682
2020-10-12 20:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.57421875Mb; avail=470821.06640625Mb
2020-10-12 20:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 20:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.57421875Mb; avail=470821.06640625Mb
2020-10-12 20:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096870
2020-10-12 20:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101474
2020-10-12 20:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.5625Mb; avail=470821.1875Mb
2020-10-12 20:12:14 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 20:12:26 | INFO | train_inner | epoch 038:     41 / 107 loss=5.908, nll_loss=4.566, ppl=23.68, wps=18607.6, ups=2.97, wpb=6256.2, bsz=238.9, num_updates=4000, lr=0.0002, gnorm=1.275, clip=0, train_wall=27, wall=1444
2020-10-12 20:12:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:12:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18837.265625Mb; avail=469846.48046875Mb
2020-10-12 20:12:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001264
2020-10-12 20:12:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18836.75Mb; avail=469845.9140625Mb
2020-10-12 20:12:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021121
2020-10-12 20:12:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18836.984375Mb; avail=469846.1640625Mb
2020-10-12 20:12:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.017501
2020-10-12 20:12:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.040692
2020-10-12 20:12:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18836.43359375Mb; avail=469845.3125Mb
2020-10-12 20:12:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18836.9375Mb; avail=469844.8046875Mb
2020-10-12 20:12:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000903
2020-10-12 20:12:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18837.28515625Mb; avail=469844.0859375Mb
2020-10-12 20:12:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020989
2020-10-12 20:12:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18836.41796875Mb; avail=469843.5Mb
2020-10-12 20:12:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016496
2020-10-12 20:12:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039379
2020-10-12 20:12:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18836.640625Mb; avail=469843.8203125Mb
2020-10-12 20:12:45 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.793 | nll_loss 5.446 | ppl 43.59 | wps 52113.3 | wpb 2186.6 | bsz 82.9 | num_updates 4066 | best_loss 6.792
2020-10-12 20:12:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:12:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_last.pt (epoch 38 @ 4066 updates, score 6.793) (writing took 5.472021870999924 seconds)
2020-10-12 20:12:51 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 20:12:51 | INFO | train | epoch 038 | loss 5.856 | nll_loss 4.504 | ppl 22.7 | wps 17937 | ups 2.91 | wpb 6172 | bsz 242.4 | num_updates 4066 | lr 0.00019837 | gnorm 1.292 | clip 0 | train_wall 29 | wall 1469
2020-10-12 20:12:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 20:12:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 20:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18572.21484375Mb; avail=470101.04296875Mb
2020-10-12 20:12:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000982
2020-10-12 20:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005563
2020-10-12 20:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18572.21484375Mb; avail=470101.04296875Mb
2020-10-12 20:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-12 20:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18572.21484375Mb; avail=470101.04296875Mb
2020-10-12 20:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.143561
2020-10-12 20:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.150178
2020-10-12 20:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18570.83203125Mb; avail=470102.390625Mb
2020-10-12 20:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18570.859375Mb; avail=470102.26953125Mb
2020-10-12 20:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003778
2020-10-12 20:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18570.859375Mb; avail=470102.26953125Mb
2020-10-12 20:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18570.859375Mb; avail=470102.26953125Mb
2020-10-12 20:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099923
2020-10-12 20:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104632
2020-10-12 20:12:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18570.79296875Mb; avail=470102.26953125Mb
2020-10-12 20:12:51 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 20:13:01 | INFO | train_inner | epoch 039:     34 / 107 loss=5.789, nll_loss=4.428, ppl=21.53, wps=17554.3, ups=2.86, wpb=6129.9, bsz=242.1, num_updates=4100, lr=0.000197546, gnorm=1.294, clip=0, train_wall=27, wall=1479
2020-10-12 20:13:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:13:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18202.48828125Mb; avail=470469.79296875Mb
2020-10-12 20:13:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001126
2020-10-12 20:13:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18204.3046875Mb; avail=470468.58203125Mb
2020-10-12 20:13:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.022113
2020-10-12 20:13:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18230.9453125Mb; avail=470441.94140625Mb
2020-10-12 20:13:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016980
2020-10-12 20:13:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.041180
2020-10-12 20:13:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18250.92578125Mb; avail=470421.9609375Mb
2020-10-12 20:13:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18271.51171875Mb; avail=470401.375Mb
2020-10-12 20:13:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000744
2020-10-12 20:13:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18272.1171875Mb; avail=470400.76953125Mb
2020-10-12 20:13:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021479
2020-10-12 20:13:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18297.58203125Mb; avail=470374.4921875Mb
2020-10-12 20:13:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.019149
2020-10-12 20:13:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.042295
2020-10-12 20:13:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18320.76953125Mb; avail=470352.08984375Mb
2020-10-12 20:13:22 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.84 | nll_loss 5.515 | ppl 45.72 | wps 51968.3 | wpb 2186.6 | bsz 82.9 | num_updates 4173 | best_loss 6.792
2020-10-12 20:13:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:13:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_last.pt (epoch 39 @ 4173 updates, score 6.84) (writing took 4.550063773000147 seconds)
2020-10-12 20:13:27 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 20:13:27 | INFO | train | epoch 039 | loss 5.765 | nll_loss 4.398 | ppl 21.08 | wps 18523.3 | ups 3 | wpb 6172 | bsz 242.4 | num_updates 4173 | lr 0.00019581 | gnorm 1.353 | clip 0 | train_wall 29 | wall 1505
2020-10-12 20:13:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 20:13:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18259.45703125Mb; avail=470413.33203125Mb
2020-10-12 20:13:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001029
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005739
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18266.23046875Mb; avail=470406.55859375Mb
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000199
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18266.8359375Mb; avail=470405.953125Mb
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100184
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107084
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18381.34375Mb; avail=470291.26171875Mb
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18414.0390625Mb; avail=470259.171875Mb
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003812
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18418.27734375Mb; avail=470254.93359375Mb
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000198
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18418.8828125Mb; avail=470254.328125Mb
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098290
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103065
2020-10-12 20:13:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18504.7421875Mb; avail=470170.43359375Mb
2020-10-12 20:13:27 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 20:13:34 | INFO | train_inner | epoch 040:     27 / 107 loss=5.804, nll_loss=4.442, ppl=21.73, wps=18603.3, ups=2.96, wpb=6293.7, bsz=248.8, num_updates=4200, lr=0.00019518, gnorm=1.397, clip=0, train_wall=27, wall=1513
2020-10-12 20:13:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:13:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17870.203125Mb; avail=470801.9609375Mb
2020-10-12 20:13:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001122
2020-10-12 20:13:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17870.3671875Mb; avail=470801.9609375Mb
2020-10-12 20:13:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021221
2020-10-12 20:13:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17870.34765625Mb; avail=470801.6875Mb
2020-10-12 20:13:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016494
2020-10-12 20:13:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039647
2020-10-12 20:13:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17870.34765625Mb; avail=470801.6875Mb
2020-10-12 20:13:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17870.296875Mb; avail=470801.9296875Mb
2020-10-12 20:13:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000753
2020-10-12 20:13:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17870.296875Mb; avail=470801.9296875Mb
2020-10-12 20:13:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021220
2020-10-12 20:13:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17870.35546875Mb; avail=470801.6875Mb
2020-10-12 20:13:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.025052
2020-10-12 20:13:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.047823
2020-10-12 20:13:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17870.3671875Mb; avail=470801.6875Mb
2020-10-12 20:13:58 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.756 | nll_loss 5.414 | ppl 42.63 | wps 52097.4 | wpb 2186.6 | bsz 82.9 | num_updates 4280 | best_loss 6.756
2020-10-12 20:13:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:14:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/M2O/checkpoint_best.pt (epoch 40 @ 4280 updates, score 6.756) (writing took 5.899557789999562 seconds)
2020-10-12 20:14:04 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 20:14:04 | INFO | train | epoch 040 | loss 5.67 | nll_loss 4.288 | ppl 19.54 | wps 17848.7 | ups 2.89 | wpb 6172 | bsz 242.4 | num_updates 4280 | lr 0.000193347 | gnorm 1.323 | clip 0 | train_wall 29 | wall 1542
2020-10-12 20:14:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 20:14:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 20:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19100.6953125Mb; avail=469571.91796875Mb
2020-10-12 20:14:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001037
2020-10-12 20:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006644
2020-10-12 20:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19100.81640625Mb; avail=469571.68359375Mb
2020-10-12 20:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000214
2020-10-12 20:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19100.81640625Mb; avail=469571.68359375Mb
2020-10-12 20:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.108089
2020-10-12 20:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115964
2020-10-12 20:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19100.84765625Mb; avail=469571.87109375Mb
2020-10-12 20:14:04 | INFO | fairseq_cli.train | done training in 1541.7 seconds
