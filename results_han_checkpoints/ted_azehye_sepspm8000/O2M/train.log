2020-10-12 20:15:35 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_azehye_sepspm8000/O2M/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok='tgt', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='eng-aze,eng-hye', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_azehye_sepspm8000/O2M/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 20:15:35 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 20:15:35 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['aze', 'eng', 'hye']
2020-10-12 20:15:35 | INFO | fairseq.data.multilingual.multilingual_data_manager | [aze] dictionary: 21875 types
2020-10-12 20:15:35 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 21875 types
2020-10-12 20:15:35 | INFO | fairseq.data.multilingual.multilingual_data_manager | [hye] dictionary: 21875 types
2020-10-12 20:15:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 20:15:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15621.625Mb; avail=473062.7734375Mb
2020-10-12 20:15:35 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 20:15:35 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:eng-aze': 1, 'main:eng-hye': 1}
2020-10-12 20:15:35 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-aze src_langtok: 21872; tgt_langtok: None
2020-10-12 20:15:35 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azehye_sepspm8000/O2M/valid.eng-aze.eng
2020-10-12 20:15:35 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azehye_sepspm8000/O2M/valid.eng-aze.aze
2020-10-12 20:15:35 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azehye_sepspm8000/O2M/ valid eng-aze 671 examples
2020-10-12 20:15:35 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-hye src_langtok: 21874; tgt_langtok: None
2020-10-12 20:15:35 | INFO | fairseq.data.data_utils | loaded 739 examples from: fairseq/data-bin/ted_azehye_sepspm8000/O2M/valid.eng-hye.eng
2020-10-12 20:15:35 | INFO | fairseq.data.data_utils | loaded 739 examples from: fairseq/data-bin/ted_azehye_sepspm8000/O2M/valid.eng-hye.hye
2020-10-12 20:15:35 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azehye_sepspm8000/O2M/ valid eng-hye 739 examples
2020-10-12 20:15:36 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21875, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21875, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=21875, bias=False)
  )
)
2020-10-12 20:15:36 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 20:15:36 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 20:15:36 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 20:15:36 | INFO | fairseq_cli.train | num. model params: 42743296 (num. trained: 42743296)
2020-10-12 20:15:40 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 20:15:40 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 20:15:40 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 20:15:40 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 20:15:40 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 20:15:40 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 20:15:40 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 20:15:40 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_last.pt
2020-10-12 20:15:40 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 20:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 20:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18372.3515625Mb; avail=470302.2890625Mb
2020-10-12 20:15:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 20:15:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:eng-aze': 1, 'main:eng-hye': 1}
2020-10-12 20:15:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-aze src_langtok: 21872; tgt_langtok: None
2020-10-12 20:15:40 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azehye_sepspm8000/O2M/train.eng-aze.eng
2020-10-12 20:15:40 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azehye_sepspm8000/O2M/train.eng-aze.aze
2020-10-12 20:15:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azehye_sepspm8000/O2M/ train eng-aze 5946 examples
2020-10-12 20:15:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-hye src_langtok: 21874; tgt_langtok: None
2020-10-12 20:15:40 | INFO | fairseq.data.data_utils | loaded 19995 examples from: fairseq/data-bin/ted_azehye_sepspm8000/O2M/train.eng-hye.eng
2020-10-12 20:15:40 | INFO | fairseq.data.data_utils | loaded 19995 examples from: fairseq/data-bin/ted_azehye_sepspm8000/O2M/train.eng-hye.hye
2020-10-12 20:15:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azehye_sepspm8000/O2M/ train eng-hye 19995 examples
2020-10-12 20:15:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:eng-aze', 5946), ('main:eng-hye', 19995)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 20:15:40 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 20:15:40 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 25941
2020-10-12 20:15:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 25941; virtual dataset size 25941
2020-10-12 20:15:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 20:15:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:eng-aze': 5946, 'main:eng-hye': 19995}; raw total size: 25941
2020-10-12 20:15:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:eng-aze': 5946, 'main:eng-hye': 19995}; resampled total size: 25941
2020-10-12 20:15:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 20:15:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.004293
2020-10-12 20:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18373.19140625Mb; avail=470301.44921875Mb
2020-10-12 20:15:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000510
2020-10-12 20:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004830
2020-10-12 20:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18373.2265625Mb; avail=470301.328125Mb
2020-10-12 20:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000200
2020-10-12 20:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18373.2265625Mb; avail=470301.328125Mb
2020-10-12 20:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102351
2020-10-12 20:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108164
2020-10-12 20:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18373.859375Mb; avail=470301.21484375Mb
2020-10-12 20:15:40 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 20:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18374.5859375Mb; avail=470300.75Mb
2020-10-12 20:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003664
2020-10-12 20:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18374.609375Mb; avail=470300.62890625Mb
2020-10-12 20:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18374.609375Mb; avail=470300.62890625Mb
2020-10-12 20:15:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099378
2020-10-12 20:15:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103978
2020-10-12 20:15:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18374.70703125Mb; avail=470300.62890625Mb
2020-10-12 20:15:41 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 20:16:10 | INFO | train_inner | epoch 001:    100 / 100 loss=14.415, nll_loss=14.331, ppl=20615.1, wps=20579.3, ups=3.44, wpb=5972.3, bsz=259.4, num_updates=100, lr=5.0975e-06, gnorm=3.599, clip=0, train_wall=29, wall=30
2020-10-12 20:16:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18524.80859375Mb; avail=470148.1328125Mb
2020-10-12 20:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001148
2020-10-12 20:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18524.80859375Mb; avail=470148.1328125Mb
2020-10-12 20:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020361
2020-10-12 20:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18524.6875Mb; avail=470148.25390625Mb
2020-10-12 20:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015317
2020-10-12 20:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037617
2020-10-12 20:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18524.61328125Mb; avail=470148.1328125Mb
2020-10-12 20:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18524.6640625Mb; avail=470148.25390625Mb
2020-10-12 20:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000693
2020-10-12 20:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18524.6640625Mb; avail=470148.25390625Mb
2020-10-12 20:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020282
2020-10-12 20:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18524.6640625Mb; avail=470148.25390625Mb
2020-10-12 20:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015367
2020-10-12 20:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037075
2020-10-12 20:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18524.6640625Mb; avail=470148.25390625Mb
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2020-10-12 20:16:11 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.644 | nll_loss 13.45 | ppl 11188.3 | wps 51940.1 | wpb 2151.6 | bsz 88.1 | num_updates 100
2020-10-12 20:16:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:16:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 1 @ 100 updates, score 13.644) (writing took 1.6554288039997118 seconds)
2020-10-12 20:16:13 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 20:16:13 | INFO | train | epoch 001 | loss 14.415 | nll_loss 14.331 | ppl 20615.1 | wps 18929.4 | ups 3.17 | wpb 5972.3 | bsz 259.4 | num_updates 100 | lr 5.0975e-06 | gnorm 3.599 | clip 0 | train_wall 29 | wall 33
2020-10-12 20:16:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 20:16:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 20:16:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16004.68359375Mb; avail=472664.0546875Mb
2020-10-12 20:16:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000778
2020-10-12 20:16:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005489
2020-10-12 20:16:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16013.765625Mb; avail=472654.97265625Mb
2020-10-12 20:16:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:16:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16014.37109375Mb; avail=472654.3671875Mb
2020-10-12 20:16:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102565
2020-10-12 20:16:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109204
2020-10-12 20:16:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16176.66015625Mb; avail=472491.25390625Mb
2020-10-12 20:16:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16215.38671875Mb; avail=472453.3515625Mb
2020-10-12 20:16:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004016
2020-10-12 20:16:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16216.6875Mb; avail=472452.26171875Mb
2020-10-12 20:16:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000188
2020-10-12 20:16:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16216.6875Mb; avail=472452.26171875Mb
2020-10-12 20:16:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103538
2020-10-12 20:16:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108591
2020-10-12 20:16:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16259.67578125Mb; avail=472409.2734375Mb
2020-10-12 20:16:13 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 20:16:42 | INFO | train_inner | epoch 002:    100 / 100 loss=13.209, nll_loss=12.985, ppl=8109.82, wps=18779.4, ups=3.14, wpb=5972.3, bsz=259.4, num_updates=200, lr=1.0095e-05, gnorm=1.543, clip=0, train_wall=28, wall=62
2020-10-12 20:16:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15727.21875Mb; avail=472942.5Mb
2020-10-12 20:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001095
2020-10-12 20:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15727.21875Mb; avail=472942.5Mb
2020-10-12 20:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020673
2020-10-12 20:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15729.73828125Mb; avail=472940.19921875Mb
2020-10-12 20:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016996
2020-10-12 20:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039547
2020-10-12 20:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15730.34375Mb; avail=472939.59375Mb
2020-10-12 20:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15731.04296875Mb; avail=472938.98828125Mb
2020-10-12 20:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000676
2020-10-12 20:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15731.6484375Mb; avail=472938.3828125Mb
2020-10-12 20:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020436
2020-10-12 20:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15731.6484375Mb; avail=472938.3828125Mb
2020-10-12 20:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.014885
2020-10-12 20:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.036728
2020-10-12 20:16:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15732.234375Mb; avail=472937.8984375Mb
2020-10-12 20:16:43 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 12.905 | nll_loss 12.637 | ppl 6371.18 | wps 53150.5 | wpb 2151.6 | bsz 88.1 | num_updates 200 | best_loss 12.905
2020-10-12 20:16:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:16:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 2 @ 200 updates, score 12.905) (writing took 5.5360376010003165 seconds)
2020-10-12 20:16:49 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 20:16:49 | INFO | train | epoch 002 | loss 13.209 | nll_loss 12.985 | ppl 8109.82 | wps 16737.4 | ups 2.8 | wpb 5972.3 | bsz 259.4 | num_updates 200 | lr 1.0095e-05 | gnorm 1.543 | clip 0 | train_wall 28 | wall 68
2020-10-12 20:16:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 20:16:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 20:16:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18758.98046875Mb; avail=469898.53125Mb
2020-10-12 20:16:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000761
2020-10-12 20:16:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005211
2020-10-12 20:16:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18758.98046875Mb; avail=469898.53125Mb
2020-10-12 20:16:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000188
2020-10-12 20:16:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18758.98046875Mb; avail=469898.53125Mb
2020-10-12 20:16:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104203
2020-10-12 20:16:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110380
2020-10-12 20:16:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18759.42578125Mb; avail=469898.109375Mb
2020-10-12 20:16:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18759.42578125Mb; avail=469898.109375Mb
2020-10-12 20:16:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003775
2020-10-12 20:16:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18760.03125Mb; avail=469897.50390625Mb
2020-10-12 20:16:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-12 20:16:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18760.03125Mb; avail=469897.50390625Mb
2020-10-12 20:16:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101053
2020-10-12 20:16:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105805
2020-10-12 20:16:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18760.64453125Mb; avail=469896.94921875Mb
2020-10-12 20:16:49 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 20:17:18 | INFO | train_inner | epoch 003:    100 / 100 loss=12.551, nll_loss=12.257, ppl=4894.85, wps=16749.4, ups=2.8, wpb=5972.3, bsz=259.4, num_updates=300, lr=1.50925e-05, gnorm=1.239, clip=0, train_wall=28, wall=98
2020-10-12 20:17:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16093.8984375Mb; avail=472567.41796875Mb
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001142
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16094.53125Mb; avail=472566.69140625Mb
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020483
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16117.9609375Mb; avail=472542.859375Mb
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015092
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037592
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16119.77734375Mb; avail=472541.6484375Mb
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16128.17578125Mb; avail=472533.02734375Mb
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000759
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16128.88671875Mb; avail=472532.30859375Mb
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020482
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16139.8828125Mb; avail=472521.16796875Mb
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015248
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037278
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16155.7421875Mb; avail=472504.69921875Mb
2020-10-12 20:17:19 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 12.112 | nll_loss 11.743 | ppl 3428.76 | wps 53158.7 | wpb 2151.6 | bsz 88.1 | num_updates 300 | best_loss 12.112
2020-10-12 20:17:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:17:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 3 @ 300 updates, score 12.112) (writing took 4.424616807999882 seconds)
2020-10-12 20:17:23 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 20:17:23 | INFO | train | epoch 003 | loss 12.551 | nll_loss 12.257 | ppl 4894.85 | wps 17287.3 | ups 2.89 | wpb 5972.3 | bsz 259.4 | num_updates 300 | lr 1.50925e-05 | gnorm 1.239 | clip 0 | train_wall 28 | wall 103
2020-10-12 20:17:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 20:17:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 20:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18099.98046875Mb; avail=470556.640625Mb
2020-10-12 20:17:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000814
2020-10-12 20:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006032
2020-10-12 20:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18100.5859375Mb; avail=470556.03515625Mb
2020-10-12 20:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18100.5859375Mb; avail=470556.03515625Mb
2020-10-12 20:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.110693
2020-10-12 20:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.117928
2020-10-12 20:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18100.375Mb; avail=470556.27734375Mb
2020-10-12 20:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18100.73828125Mb; avail=470555.9140625Mb
2020-10-12 20:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003681
2020-10-12 20:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18100.24609375Mb; avail=470556.40625Mb
2020-10-12 20:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 20:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18100.24609375Mb; avail=470556.40625Mb
2020-10-12 20:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104747
2020-10-12 20:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109364
2020-10-12 20:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18100.95703125Mb; avail=470555.6953125Mb
2020-10-12 20:17:23 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 20:17:52 | INFO | train_inner | epoch 004:    100 / 100 loss=11.825, nll_loss=11.439, ppl=2776.77, wps=17337.6, ups=2.9, wpb=5972.3, bsz=259.4, num_updates=400, lr=2.009e-05, gnorm=1.336, clip=0, train_wall=28, wall=132
2020-10-12 20:17:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18351.1171875Mb; avail=470291.09375Mb
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001546
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18352.93359375Mb; avail=470289.8828125Mb
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020582
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18376.18359375Mb; avail=470267.01953125Mb
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015309
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038312
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18393.765625Mb; avail=470248.85546875Mb
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18413.7421875Mb; avail=470228.99609375Mb
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000741
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18414.953125Mb; avail=470228.390625Mb
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020504
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18423.921875Mb; avail=470220.65234375Mb
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015091
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037092
2020-10-12 20:17:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18423.4296875Mb; avail=470222.8671875Mb
2020-10-12 20:17:53 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 11.486 | nll_loss 11.028 | ppl 2087.92 | wps 47473.6 | wpb 2151.6 | bsz 88.1 | num_updates 400 | best_loss 11.486
2020-10-12 20:17:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:18:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 4 @ 400 updates, score 11.486) (writing took 8.037043721999908 seconds)
2020-10-12 20:18:01 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 20:18:01 | INFO | train | epoch 004 | loss 11.825 | nll_loss 11.439 | ppl 2776.77 | wps 15662 | ups 2.62 | wpb 5972.3 | bsz 259.4 | num_updates 400 | lr 2.009e-05 | gnorm 1.336 | clip 0 | train_wall 28 | wall 141
2020-10-12 20:18:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 20:18:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 20:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17925.59375Mb; avail=470698.63671875Mb
2020-10-12 20:18:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001049
2020-10-12 20:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006059
2020-10-12 20:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17926.16796875Mb; avail=470698.0625Mb
2020-10-12 20:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17926.16796875Mb; avail=470698.0625Mb
2020-10-12 20:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.109538
2020-10-12 20:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.116668
2020-10-12 20:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17926.36328125Mb; avail=470697.69921875Mb
2020-10-12 20:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17925.94921875Mb; avail=470697.5859375Mb
2020-10-12 20:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003742
2020-10-12 20:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17925.94921875Mb; avail=470697.5859375Mb
2020-10-12 20:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 20:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17925.94921875Mb; avail=470697.5859375Mb
2020-10-12 20:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101382
2020-10-12 20:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106833
2020-10-12 20:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17925.90625Mb; avail=470697.94921875Mb
2020-10-12 20:18:01 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 20:18:31 | INFO | train_inner | epoch 005:    100 / 100 loss=11.237, nll_loss=10.756, ppl=1729.64, wps=15586.4, ups=2.61, wpb=5972.3, bsz=259.4, num_updates=500, lr=2.50875e-05, gnorm=1.201, clip=0, train_wall=28, wall=170
2020-10-12 20:18:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17824.75390625Mb; avail=470767.55078125Mb
2020-10-12 20:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001142
2020-10-12 20:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.75390625Mb; avail=470767.55078125Mb
2020-10-12 20:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021068
2020-10-12 20:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.66796875Mb; avail=470767.55078125Mb
2020-10-12 20:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015463
2020-10-12 20:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038488
2020-10-12 20:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.17578125Mb; avail=470767.55078125Mb
2020-10-12 20:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17824.40234375Mb; avail=470767.55078125Mb
2020-10-12 20:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000710
2020-10-12 20:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.40234375Mb; avail=470767.55078125Mb
2020-10-12 20:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020715
2020-10-12 20:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.375Mb; avail=470767.4296875Mb
2020-10-12 20:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015054
2020-10-12 20:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037251
2020-10-12 20:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.45703125Mb; avail=470767.55078125Mb
2020-10-12 20:18:31 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 11.07 | nll_loss 10.539 | ppl 1487.6 | wps 53404.8 | wpb 2151.6 | bsz 88.1 | num_updates 500 | best_loss 11.07
2020-10-12 20:18:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:18:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 5 @ 500 updates, score 11.07) (writing took 8.98097746700023 seconds)
2020-10-12 20:18:40 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 20:18:40 | INFO | train | epoch 005 | loss 11.237 | nll_loss 10.756 | ppl 1729.64 | wps 15242.1 | ups 2.55 | wpb 5972.3 | bsz 259.4 | num_updates 500 | lr 2.50875e-05 | gnorm 1.201 | clip 0 | train_wall 28 | wall 180
2020-10-12 20:18:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 20:18:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 20:18:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18421.96875Mb; avail=470170.3515625Mb
2020-10-12 20:18:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000986
2020-10-12 20:18:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006340
2020-10-12 20:18:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18422.453125Mb; avail=470169.8671875Mb
2020-10-12 20:18:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000226
2020-10-12 20:18:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18422.453125Mb; avail=470169.8671875Mb
2020-10-12 20:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.167893
2020-10-12 20:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.175646
2020-10-12 20:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18422.31640625Mb; avail=470169.49609375Mb
2020-10-12 20:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18422.3984375Mb; avail=470169.4140625Mb
2020-10-12 20:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004281
2020-10-12 20:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18422.3984375Mb; avail=470169.4140625Mb
2020-10-12 20:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000206
2020-10-12 20:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18422.3984375Mb; avail=470169.4140625Mb
2020-10-12 20:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.113915
2020-10-12 20:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.119312
2020-10-12 20:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18422.1640625Mb; avail=470169.65625Mb
2020-10-12 20:18:41 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 20:19:10 | INFO | train_inner | epoch 006:    100 / 100 loss=10.86, nll_loss=10.305, ppl=1264.72, wps=15159.9, ups=2.54, wpb=5972.3, bsz=259.4, num_updates=600, lr=3.0085e-05, gnorm=1.209, clip=0, train_wall=28, wall=210
2020-10-12 20:19:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19037.234375Mb; avail=469555.83984375Mb
2020-10-12 20:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001168
2020-10-12 20:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19037.234375Mb; avail=469555.83984375Mb
2020-10-12 20:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020585
2020-10-12 20:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19037.234375Mb; avail=469555.83984375Mb
2020-10-12 20:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015472
2020-10-12 20:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038026
2020-10-12 20:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19037.16015625Mb; avail=469555.8125Mb
2020-10-12 20:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18683.41015625Mb; avail=469915.86328125Mb
2020-10-12 20:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000696
2020-10-12 20:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18665.19921875Mb; avail=469931.61328125Mb
2020-10-12 20:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020298
2020-10-12 20:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18460.44921875Mb; avail=470133.65625Mb
2020-10-12 20:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015261
2020-10-12 20:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037020
2020-10-12 20:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18460.94140625Mb; avail=470135.37890625Mb
2020-10-12 20:19:11 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 10.684 | nll_loss 10.081 | ppl 1083.43 | wps 51019.5 | wpb 2151.6 | bsz 88.1 | num_updates 600 | best_loss 10.684
2020-10-12 20:19:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:19:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 6 @ 600 updates, score 10.684) (writing took 11.20938196499992 seconds)
2020-10-12 20:19:22 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 20:19:22 | INFO | train | epoch 006 | loss 10.86 | nll_loss 10.305 | ppl 1264.72 | wps 14330 | ups 2.4 | wpb 5972.3 | bsz 259.4 | num_updates 600 | lr 3.0085e-05 | gnorm 1.209 | clip 0 | train_wall 28 | wall 222
2020-10-12 20:19:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 20:19:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 20:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17853.203125Mb; avail=470739.12890625Mb
2020-10-12 20:19:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000888
2020-10-12 20:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005747
2020-10-12 20:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.80859375Mb; avail=470738.5234375Mb
2020-10-12 20:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000226
2020-10-12 20:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.80859375Mb; avail=470738.5234375Mb
2020-10-12 20:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103883
2020-10-12 20:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110817
2020-10-12 20:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17909.96875Mb; avail=470682.01171875Mb
2020-10-12 20:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17952.77734375Mb; avail=470638.91015625Mb
2020-10-12 20:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003794
2020-10-12 20:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17958.13671875Mb; avail=470633.4609375Mb
2020-10-12 20:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000210
2020-10-12 20:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17958.7421875Mb; avail=470632.85546875Mb
2020-10-12 20:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100429
2020-10-12 20:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105231
2020-10-12 20:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18082.73828125Mb; avail=470509.4609375Mb
2020-10-12 20:19:22 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 20:19:51 | INFO | train_inner | epoch 007:    100 / 100 loss=10.593, nll_loss=9.979, ppl=1009.24, wps=14451.2, ups=2.42, wpb=5972.3, bsz=259.4, num_updates=700, lr=3.50825e-05, gnorm=1.229, clip=0, train_wall=28, wall=251
2020-10-12 20:19:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17856.59765625Mb; avail=470736.01171875Mb
2020-10-12 20:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001208
2020-10-12 20:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17856.62890625Mb; avail=470735.890625Mb
2020-10-12 20:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020389
2020-10-12 20:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17856.6171875Mb; avail=470735.890625Mb
2020-10-12 20:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.014839
2020-10-12 20:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037212
2020-10-12 20:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17856.52734375Mb; avail=470736.01171875Mb
2020-10-12 20:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17856.52734375Mb; avail=470736.01171875Mb
2020-10-12 20:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000713
2020-10-12 20:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17856.52734375Mb; avail=470736.01171875Mb
2020-10-12 20:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021934
2020-10-12 20:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17856.50390625Mb; avail=470736.1484375Mb
2020-10-12 20:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.014835
2020-10-12 20:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038228
2020-10-12 20:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17856.50390625Mb; avail=470736.1484375Mb
2020-10-12 20:19:52 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 10.455 | nll_loss 9.806 | ppl 895.34 | wps 53235.4 | wpb 2151.6 | bsz 88.1 | num_updates 700 | best_loss 10.455
2020-10-12 20:19:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:20:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 7 @ 700 updates, score 10.455) (writing took 14.185350963000019 seconds)
2020-10-12 20:20:06 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 20:20:06 | INFO | train | epoch 007 | loss 10.593 | nll_loss 9.979 | ppl 1009.24 | wps 13495.8 | ups 2.26 | wpb 5972.3 | bsz 259.4 | num_updates 700 | lr 3.50825e-05 | gnorm 1.229 | clip 0 | train_wall 28 | wall 266
2020-10-12 20:20:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 20:20:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 20:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18445.3125Mb; avail=470151.68359375Mb
2020-10-12 20:20:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000745
2020-10-12 20:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005264
2020-10-12 20:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18444.2265625Mb; avail=470152.578125Mb
2020-10-12 20:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 20:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18443.36328125Mb; avail=470152.3515625Mb
2020-10-12 20:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102930
2020-10-12 20:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109235
2020-10-12 20:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18444.1796875Mb; avail=470149.7109375Mb
2020-10-12 20:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18444.3515625Mb; avail=470149.26171875Mb
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004041
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18444.89453125Mb; avail=470148.23046875Mb
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18444.765625Mb; avail=470148.1171875Mb
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101880
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106872
2020-10-12 20:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18444.8046875Mb; avail=470147.8515625Mb
2020-10-12 20:20:07 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 20:20:36 | INFO | train_inner | epoch 008:    100 / 100 loss=10.403, nll_loss=9.75, ppl=861.03, wps=13512.4, ups=2.26, wpb=5972.3, bsz=259.4, num_updates=800, lr=4.008e-05, gnorm=1.23, clip=0, train_wall=28, wall=295
2020-10-12 20:20:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17800.265625Mb; avail=470792.03515625Mb
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001131
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.265625Mb; avail=470792.03515625Mb
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021655
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.44921875Mb; avail=470791.77734375Mb
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015142
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038844
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.40625Mb; avail=470792.01953125Mb
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17800.40625Mb; avail=470792.01953125Mb
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000750
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.40625Mb; avail=470792.01953125Mb
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020364
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.3203125Mb; avail=470791.8984375Mb
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016247
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038131
2020-10-12 20:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.3203125Mb; avail=470791.8984375Mb
2020-10-12 20:20:36 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.32 | nll_loss 9.634 | ppl 794.68 | wps 53729 | wpb 2151.6 | bsz 88.1 | num_updates 800 | best_loss 10.32
2020-10-12 20:20:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:20:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 8 @ 800 updates, score 10.32) (writing took 4.478318626000146 seconds)
2020-10-12 20:20:41 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 20:20:41 | INFO | train | epoch 008 | loss 10.403 | nll_loss 9.75 | ppl 861.03 | wps 17311.7 | ups 2.9 | wpb 5972.3 | bsz 259.4 | num_updates 800 | lr 4.008e-05 | gnorm 1.23 | clip 0 | train_wall 28 | wall 301
2020-10-12 20:20:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 20:20:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 20:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17833.23046875Mb; avail=470758.7890625Mb
2020-10-12 20:20:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000835
2020-10-12 20:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005619
2020-10-12 20:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17833.8359375Mb; avail=470758.18359375Mb
2020-10-12 20:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-12 20:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17833.8359375Mb; avail=470758.18359375Mb
2020-10-12 20:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103247
2020-10-12 20:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109824
2020-10-12 20:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17833.765625Mb; avail=470758.0625Mb
2020-10-12 20:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17833.65625Mb; avail=470758.19140625Mb
2020-10-12 20:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004586
2020-10-12 20:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17833.67578125Mb; avail=470758.0703125Mb
2020-10-12 20:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000225
2020-10-12 20:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17833.67578125Mb; avail=470758.0703125Mb
2020-10-12 20:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101705
2020-10-12 20:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107277
2020-10-12 20:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.05859375Mb; avail=470757.49609375Mb
2020-10-12 20:20:41 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 20:21:10 | INFO | train_inner | epoch 009:    100 / 100 loss=10.266, nll_loss=9.584, ppl=767.6, wps=17260.3, ups=2.89, wpb=5972.3, bsz=259.4, num_updates=900, lr=4.50775e-05, gnorm=1.254, clip=0, train_wall=28, wall=330
2020-10-12 20:21:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:21:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18355.38671875Mb; avail=470236.72265625Mb
2020-10-12 20:21:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001133
2020-10-12 20:21:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18355.38671875Mb; avail=470236.72265625Mb
2020-10-12 20:21:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020447
2020-10-12 20:21:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18355.38671875Mb; avail=470236.72265625Mb
2020-10-12 20:21:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015130
2020-10-12 20:21:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037501
2020-10-12 20:21:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18355.29296875Mb; avail=470236.72265625Mb
2020-10-12 20:21:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18355.4765625Mb; avail=470236.72265625Mb
2020-10-12 20:21:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000703
2020-10-12 20:21:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18355.4765625Mb; avail=470236.72265625Mb
2020-10-12 20:21:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020385
2020-10-12 20:21:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18355.5Mb; avail=470236.6015625Mb
2020-10-12 20:21:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015144
2020-10-12 20:21:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.036981
2020-10-12 20:21:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18355.5Mb; avail=470236.6015625Mb
2020-10-12 20:21:11 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 10.139 | nll_loss 9.418 | ppl 684.3 | wps 53680.5 | wpb 2151.6 | bsz 88.1 | num_updates 900 | best_loss 10.139
2020-10-12 20:21:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:21:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 9 @ 900 updates, score 10.139) (writing took 12.435394051000003 seconds)
2020-10-12 20:21:23 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 20:21:23 | INFO | train | epoch 009 | loss 10.266 | nll_loss 9.584 | ppl 767.6 | wps 14038.6 | ups 2.35 | wpb 5972.3 | bsz 259.4 | num_updates 900 | lr 4.50775e-05 | gnorm 1.254 | clip 0 | train_wall 28 | wall 343
2020-10-12 20:21:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 20:21:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 20:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18558.10546875Mb; avail=470033.9609375Mb
2020-10-12 20:21:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000795
2020-10-12 20:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006021
2020-10-12 20:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18565.37109375Mb; avail=470027.30078125Mb
2020-10-12 20:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000197
2020-10-12 20:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18565.9765625Mb; avail=470026.6953125Mb
2020-10-12 20:21:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.114641
2020-10-12 20:21:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.121846
2020-10-12 20:21:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18710.859375Mb; avail=469881.75390625Mb
2020-10-12 20:21:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18765.88671875Mb; avail=469826.07421875Mb
2020-10-12 20:21:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004947
2020-10-12 20:21:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18772.546875Mb; avail=469820.01953125Mb
2020-10-12 20:21:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000265
2020-10-12 20:21:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18773.15234375Mb; avail=469819.4140625Mb
2020-10-12 20:21:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.114226
2020-10-12 20:21:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.120319
2020-10-12 20:21:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18901.8515625Mb; avail=469690.21484375Mb
2020-10-12 20:21:24 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 20:21:53 | INFO | train_inner | epoch 010:    100 / 100 loss=10.137, nll_loss=9.432, ppl=690.53, wps=14041.8, ups=2.35, wpb=5972.3, bsz=259.4, num_updates=1000, lr=5.0075e-05, gnorm=1.098, clip=0, train_wall=28, wall=372
2020-10-12 20:21:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18339.32421875Mb; avail=470253.06640625Mb
2020-10-12 20:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001100
2020-10-12 20:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18339.32421875Mb; avail=470253.06640625Mb
2020-10-12 20:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020249
2020-10-12 20:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18339.32421875Mb; avail=470253.06640625Mb
2020-10-12 20:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.014730
2020-10-12 20:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.036844
2020-10-12 20:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18339.32421875Mb; avail=470253.14453125Mb
2020-10-12 20:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18339.24609375Mb; avail=470253.14453125Mb
2020-10-12 20:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000699
2020-10-12 20:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18339.24609375Mb; avail=470253.14453125Mb
2020-10-12 20:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020187
2020-10-12 20:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18339.3984375Mb; avail=470252.78125Mb
2020-10-12 20:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.014910
2020-10-12 20:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.036516
2020-10-12 20:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18339.39453125Mb; avail=470252.78125Mb
2020-10-12 20:21:54 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 10.049 | nll_loss 9.311 | ppl 635.36 | wps 51444.8 | wpb 2151.6 | bsz 88.1 | num_updates 1000 | best_loss 10.049
2020-10-12 20:21:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:22:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 10 @ 1000 updates, score 10.049) (writing took 6.736516502000086 seconds)
2020-10-12 20:22:00 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 20:22:00 | INFO | train | epoch 010 | loss 10.137 | nll_loss 9.432 | ppl 690.53 | wps 16207.3 | ups 2.71 | wpb 5972.3 | bsz 259.4 | num_updates 1000 | lr 5.0075e-05 | gnorm 1.098 | clip 0 | train_wall 28 | wall 380
2020-10-12 20:22:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 20:22:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 20:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17803.26171875Mb; avail=470788.9609375Mb
2020-10-12 20:22:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000928
2020-10-12 20:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006003
2020-10-12 20:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.1484375Mb; avail=470788.9609375Mb
2020-10-12 20:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000253
2020-10-12 20:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.1484375Mb; avail=470788.9609375Mb
2020-10-12 20:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.107077
2020-10-12 20:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114168
2020-10-12 20:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.90234375Mb; avail=470788.11328125Mb
2020-10-12 20:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17803.92578125Mb; avail=470787.9921875Mb
2020-10-12 20:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003740
2020-10-12 20:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.92578125Mb; avail=470787.9921875Mb
2020-10-12 20:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 20:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.92578125Mb; avail=470787.9921875Mb
2020-10-12 20:22:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103738
2020-10-12 20:22:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108521
2020-10-12 20:22:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17804.05859375Mb; avail=470787.75Mb
2020-10-12 20:22:01 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 20:22:29 | INFO | train_inner | epoch 011:    100 / 100 loss=10.034, nll_loss=9.308, ppl=633.99, wps=16276.5, ups=2.73, wpb=5972.3, bsz=259.4, num_updates=1100, lr=5.50725e-05, gnorm=1.154, clip=0, train_wall=28, wall=409
2020-10-12 20:22:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:22:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18364.40625Mb; avail=470233.81640625Mb
2020-10-12 20:22:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001137
2020-10-12 20:22:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18364.1484375Mb; avail=470234.08203125Mb
2020-10-12 20:22:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020584
2020-10-12 20:22:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18364.28125Mb; avail=470233.73046875Mb
2020-10-12 20:22:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015062
2020-10-12 20:22:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037664
2020-10-12 20:22:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18365.28515625Mb; avail=470232.86328125Mb
2020-10-12 20:22:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18364.10546875Mb; avail=470232.56640625Mb
2020-10-12 20:22:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000820
2020-10-12 20:22:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18364.58203125Mb; avail=470232.453125Mb
2020-10-12 20:22:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021314
2020-10-12 20:22:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18364.484375Mb; avail=470232.35546875Mb
2020-10-12 20:22:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.014873
2020-10-12 20:22:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037873
2020-10-12 20:22:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18365.0859375Mb; avail=470232.3359375Mb
2020-10-12 20:22:30 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 9.964 | nll_loss 9.205 | ppl 590.04 | wps 46274.3 | wpb 2151.6 | bsz 88.1 | num_updates 1100 | best_loss 9.964
2020-10-12 20:22:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:22:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 11 @ 1100 updates, score 9.964) (writing took 12.202080441999897 seconds)
2020-10-12 20:22:43 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 20:22:43 | INFO | train | epoch 011 | loss 10.034 | nll_loss 9.308 | ppl 633.99 | wps 14090.2 | ups 2.36 | wpb 5972.3 | bsz 259.4 | num_updates 1100 | lr 5.50725e-05 | gnorm 1.154 | clip 0 | train_wall 28 | wall 422
2020-10-12 20:22:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 20:22:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 20:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18177.1953125Mb; avail=470425.30078125Mb
2020-10-12 20:22:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001065
2020-10-12 20:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006403
2020-10-12 20:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18178.41796875Mb; avail=470424.1328125Mb
2020-10-12 20:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000212
2020-10-12 20:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18178.16015625Mb; avail=470423.90625Mb
2020-10-12 20:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.107710
2020-10-12 20:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115239
2020-10-12 20:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18177.85546875Mb; avail=470421.58203125Mb
2020-10-12 20:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18178.7109375Mb; avail=470420.16796875Mb
2020-10-12 20:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003679
2020-10-12 20:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18177.83203125Mb; avail=470420.3671875Mb
2020-10-12 20:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 20:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18177.57421875Mb; avail=470420.25390625Mb
2020-10-12 20:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101815
2020-10-12 20:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106410
2020-10-12 20:22:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18178.140625Mb; avail=470418.19921875Mb
2020-10-12 20:22:43 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 20:23:12 | INFO | train_inner | epoch 012:    100 / 100 loss=9.943, nll_loss=9.201, ppl=588.63, wps=13946.8, ups=2.34, wpb=5972.3, bsz=259.4, num_updates=1200, lr=6.007e-05, gnorm=1.286, clip=0, train_wall=28, wall=452
2020-10-12 20:23:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17769.87109375Mb; avail=470823.46484375Mb
2020-10-12 20:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001184
2020-10-12 20:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17770.34765625Mb; avail=470823.23828125Mb
2020-10-12 20:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.022635
2020-10-12 20:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17770.35546875Mb; avail=470822.17578125Mb
2020-10-12 20:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.017133
2020-10-12 20:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.041973
2020-10-12 20:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17770.5703125Mb; avail=470821.80078125Mb
2020-10-12 20:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17770.30859375Mb; avail=470822.14453125Mb
2020-10-12 20:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000807
2020-10-12 20:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17770.1796875Mb; avail=470821.91796875Mb
2020-10-12 20:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.022627
2020-10-12 20:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17770.703125Mb; avail=470820.9765625Mb
2020-10-12 20:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015384
2020-10-12 20:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039846
2020-10-12 20:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17770.48046875Mb; avail=470821.26953125Mb
2020-10-12 20:23:13 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 9.904 | nll_loss 9.129 | ppl 559.93 | wps 49140.5 | wpb 2151.6 | bsz 88.1 | num_updates 1200 | best_loss 9.904
2020-10-12 20:23:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:23:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 12 @ 1200 updates, score 9.904) (writing took 5.101937707999241 seconds)
2020-10-12 20:23:18 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 20:23:18 | INFO | train | epoch 012 | loss 9.943 | nll_loss 9.201 | ppl 588.63 | wps 16792 | ups 2.81 | wpb 5972.3 | bsz 259.4 | num_updates 1200 | lr 6.007e-05 | gnorm 1.286 | clip 0 | train_wall 28 | wall 458
2020-10-12 20:23:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 20:23:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 20:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17789.34375Mb; avail=470802.515625Mb
2020-10-12 20:23:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000924
2020-10-12 20:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006432
2020-10-12 20:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17789.94921875Mb; avail=470801.91015625Mb
2020-10-12 20:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000235
2020-10-12 20:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17789.94921875Mb; avail=470801.91015625Mb
2020-10-12 20:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.107677
2020-10-12 20:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115226
2020-10-12 20:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17790.0234375Mb; avail=470802.03125Mb
2020-10-12 20:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17790.62890625Mb; avail=470801.42578125Mb
2020-10-12 20:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003768
2020-10-12 20:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17790.62890625Mb; avail=470801.42578125Mb
2020-10-12 20:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17790.62890625Mb; avail=470801.42578125Mb
2020-10-12 20:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105135
2020-10-12 20:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109831
2020-10-12 20:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17790.1796875Mb; avail=470801.42578125Mb
2020-10-12 20:23:18 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 20:23:48 | INFO | train_inner | epoch 013:    100 / 100 loss=9.838, nll_loss=9.08, ppl=541.15, wps=16904.5, ups=2.83, wpb=5972.3, bsz=259.4, num_updates=1300, lr=6.50675e-05, gnorm=1.303, clip=0, train_wall=28, wall=487
2020-10-12 20:23:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:23:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19560.0234375Mb; avail=469038.53125Mb
2020-10-12 20:23:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001190
2020-10-12 20:23:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19560.37109375Mb; avail=469038.19140625Mb
2020-10-12 20:23:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020842
2020-10-12 20:23:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19559.62890625Mb; avail=469037.9140625Mb
2020-10-12 20:23:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015238
2020-10-12 20:23:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038073
2020-10-12 20:23:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19560.5Mb; avail=469037.640625Mb
2020-10-12 20:23:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19560.25Mb; avail=469037.51953125Mb
2020-10-12 20:23:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000767
2020-10-12 20:23:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19560.015625Mb; avail=469037.171875Mb
2020-10-12 20:23:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020141
2020-10-12 20:23:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19559.98828125Mb; avail=469036.4921875Mb
2020-10-12 20:23:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015044
2020-10-12 20:23:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.036726
2020-10-12 20:23:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19559.99609375Mb; avail=469036.7109375Mb
2020-10-12 20:23:48 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 9.782 | nll_loss 8.995 | ppl 510.25 | wps 51134.9 | wpb 2151.6 | bsz 88.1 | num_updates 1300 | best_loss 9.782
2020-10-12 20:23:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:24:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 13 @ 1300 updates, score 9.782) (writing took 18.73224050099998 seconds)
2020-10-12 20:24:07 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 20:24:07 | INFO | train | epoch 013 | loss 9.838 | nll_loss 9.08 | ppl 541.15 | wps 12198.2 | ups 2.04 | wpb 5972.3 | bsz 259.4 | num_updates 1300 | lr 6.50675e-05 | gnorm 1.303 | clip 0 | train_wall 28 | wall 507
2020-10-12 20:24:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 20:24:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 20:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18355.5703125Mb; avail=470240.40625Mb
2020-10-12 20:24:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000885
2020-10-12 20:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005815
2020-10-12 20:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18355.53125Mb; avail=470240.234375Mb
2020-10-12 20:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000205
2020-10-12 20:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18355.40234375Mb; avail=470240.12109375Mb
2020-10-12 20:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102131
2020-10-12 20:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109030
2020-10-12 20:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18355.40234375Mb; avail=470238.58203125Mb
2020-10-12 20:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18355.8203125Mb; avail=470237.484375Mb
2020-10-12 20:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003898
2020-10-12 20:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18356.0390625Mb; avail=470237.65234375Mb
2020-10-12 20:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 20:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18355.91015625Mb; avail=470237.5390625Mb
2020-10-12 20:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101371
2020-10-12 20:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106288
2020-10-12 20:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18355.6953125Mb; avail=470237.18359375Mb
2020-10-12 20:24:07 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 20:24:36 | INFO | train_inner | epoch 014:    100 / 100 loss=9.73, nll_loss=8.954, ppl=495.93, wps=12230.4, ups=2.05, wpb=5972.3, bsz=259.4, num_updates=1400, lr=7.0065e-05, gnorm=1.304, clip=0, train_wall=28, wall=536
2020-10-12 20:24:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17850.95703125Mb; avail=470746.44921875Mb
2020-10-12 20:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001166
2020-10-12 20:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.69921875Mb; avail=470746.828125Mb
2020-10-12 20:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020702
2020-10-12 20:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.1328125Mb; avail=470746.4921875Mb
2020-10-12 20:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016957
2020-10-12 20:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039663
2020-10-12 20:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.796875Mb; avail=470746.80859375Mb
2020-10-12 20:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17850.47265625Mb; avail=470747.28125Mb
2020-10-12 20:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000779
2020-10-12 20:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.34375Mb; avail=470747.16796875Mb
2020-10-12 20:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020434
2020-10-12 20:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.7421875Mb; avail=470747.11328125Mb
2020-10-12 20:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015043
2020-10-12 20:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037033
2020-10-12 20:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.203125Mb; avail=470747.3828125Mb
2020-10-12 20:24:37 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 9.721 | nll_loss 8.935 | ppl 489.57 | wps 52391.7 | wpb 2151.6 | bsz 88.1 | num_updates 1400 | best_loss 9.721
2020-10-12 20:24:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:24:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 14 @ 1400 updates, score 9.721) (writing took 5.582895610000378 seconds)
2020-10-12 20:24:43 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 20:24:43 | INFO | train | epoch 014 | loss 9.73 | nll_loss 8.954 | ppl 495.93 | wps 16732.9 | ups 2.8 | wpb 5972.3 | bsz 259.4 | num_updates 1400 | lr 7.0065e-05 | gnorm 1.304 | clip 0 | train_wall 28 | wall 543
2020-10-12 20:24:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 20:24:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 20:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17887.38671875Mb; avail=470703.7578125Mb
2020-10-12 20:24:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001584
2020-10-12 20:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007090
2020-10-12 20:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17887.546875Mb; avail=470703.87890625Mb
2020-10-12 20:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000207
2020-10-12 20:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17887.546875Mb; avail=470703.87890625Mb
2020-10-12 20:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.125057
2020-10-12 20:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.133241
2020-10-12 20:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17887.84375Mb; avail=470704.015625Mb
2020-10-12 20:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17887.82421875Mb; avail=470704.13671875Mb
2020-10-12 20:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003793
2020-10-12 20:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17887.89453125Mb; avail=470704.015625Mb
2020-10-12 20:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 20:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17887.89453125Mb; avail=470704.015625Mb
2020-10-12 20:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.120112
2020-10-12 20:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.124866
2020-10-12 20:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17887.99609375Mb; avail=470703.65234375Mb
2020-10-12 20:24:43 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 20:25:12 | INFO | train_inner | epoch 015:    100 / 100 loss=9.634, nll_loss=8.843, ppl=459.31, wps=16624.6, ups=2.78, wpb=5972.3, bsz=259.4, num_updates=1500, lr=7.50625e-05, gnorm=1.464, clip=0, train_wall=28, wall=572
2020-10-12 20:25:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19121.921875Mb; avail=469470.8125Mb
2020-10-12 20:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001307
2020-10-12 20:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19121.921875Mb; avail=469470.8125Mb
2020-10-12 20:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020482
2020-10-12 20:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19121.921875Mb; avail=469470.8125Mb
2020-10-12 20:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015057
2020-10-12 20:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037624
2020-10-12 20:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19121.921875Mb; avail=469470.8125Mb
2020-10-12 20:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19121.921875Mb; avail=469470.8125Mb
2020-10-12 20:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000676
2020-10-12 20:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19121.921875Mb; avail=469470.8125Mb
2020-10-12 20:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020633
2020-10-12 20:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19121.921875Mb; avail=469470.8125Mb
2020-10-12 20:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.014893
2020-10-12 20:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.036941
2020-10-12 20:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19121.921875Mb; avail=469470.8125Mb
2020-10-12 20:25:13 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 9.601 | nll_loss 8.774 | ppl 437.68 | wps 51764.2 | wpb 2151.6 | bsz 88.1 | num_updates 1500 | best_loss 9.601
2020-10-12 20:25:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:25:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 15 @ 1500 updates, score 9.601) (writing took 12.154599524999867 seconds)
2020-10-12 20:25:25 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 20:25:25 | INFO | train | epoch 015 | loss 9.634 | nll_loss 8.843 | ppl 459.31 | wps 14079.7 | ups 2.36 | wpb 5972.3 | bsz 259.4 | num_updates 1500 | lr 7.50625e-05 | gnorm 1.464 | clip 0 | train_wall 28 | wall 585
2020-10-12 20:25:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 20:25:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 20:25:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17900.0625Mb; avail=470691.5546875Mb
2020-10-12 20:25:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000907
2020-10-12 20:25:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005890
2020-10-12 20:25:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17900.66796875Mb; avail=470690.94921875Mb
2020-10-12 20:25:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000285
2020-10-12 20:25:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17900.17578125Mb; avail=470691.44140625Mb
2020-10-12 20:25:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104229
2020-10-12 20:25:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111294
2020-10-12 20:25:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17900.921875Mb; avail=470690.47265625Mb
2020-10-12 20:25:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17900.4609375Mb; avail=470690.8515625Mb
2020-10-12 20:25:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003734
2020-10-12 20:25:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17900.4609375Mb; avail=470690.8515625Mb
2020-10-12 20:25:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000217
2020-10-12 20:25:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17900.4609375Mb; avail=470690.8515625Mb
2020-10-12 20:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104610
2020-10-12 20:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109300
2020-10-12 20:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17900.640625Mb; avail=470690.8515625Mb
2020-10-12 20:25:26 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 20:25:55 | INFO | train_inner | epoch 016:    100 / 100 loss=9.518, nll_loss=8.709, ppl=418.52, wps=14037.9, ups=2.35, wpb=5972.3, bsz=259.4, num_updates=1600, lr=8.006e-05, gnorm=1.362, clip=0, train_wall=28, wall=615
2020-10-12 20:25:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17819.11328125Mb; avail=470772.890625Mb
2020-10-12 20:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001249
2020-10-12 20:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17819.16796875Mb; avail=470772.6484375Mb
2020-10-12 20:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020369
2020-10-12 20:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17819.140625Mb; avail=470772.76953125Mb
2020-10-12 20:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015400
2020-10-12 20:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037839
2020-10-12 20:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17819.171875Mb; avail=470772.52734375Mb
2020-10-12 20:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17819.27734375Mb; avail=470772.52734375Mb
2020-10-12 20:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000726
2020-10-12 20:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17819.27734375Mb; avail=470772.52734375Mb
2020-10-12 20:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020375
2020-10-12 20:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17819.27734375Mb; avail=470772.52734375Mb
2020-10-12 20:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015292
2020-10-12 20:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037170
2020-10-12 20:25:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17819.42578125Mb; avail=470772.6484375Mb
2020-10-12 20:25:56 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 9.491 | nll_loss 8.65 | ppl 401.82 | wps 53631.6 | wpb 2151.6 | bsz 88.1 | num_updates 1600 | best_loss 9.491
2020-10-12 20:25:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:26:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 16 @ 1600 updates, score 9.491) (writing took 7.442388298999504 seconds)
2020-10-12 20:26:03 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 20:26:03 | INFO | train | epoch 016 | loss 9.518 | nll_loss 8.709 | ppl 418.52 | wps 15795 | ups 2.64 | wpb 5972.3 | bsz 259.4 | num_updates 1600 | lr 8.006e-05 | gnorm 1.362 | clip 0 | train_wall 28 | wall 623
2020-10-12 20:26:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 20:26:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 20:26:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17800.1015625Mb; avail=470792.30859375Mb
2020-10-12 20:26:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000982
2020-10-12 20:26:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006241
2020-10-12 20:26:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.1171875Mb; avail=470792.29296875Mb
2020-10-12 20:26:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000249
2020-10-12 20:26:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.1171875Mb; avail=470792.29296875Mb
2020-10-12 20:26:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.114691
2020-10-12 20:26:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.122060
2020-10-12 20:26:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.58203125Mb; avail=470793.83203125Mb
2020-10-12 20:26:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17798.4453125Mb; avail=470793.58984375Mb
2020-10-12 20:26:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004142
2020-10-12 20:26:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.4453125Mb; avail=470793.58984375Mb
2020-10-12 20:26:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 20:26:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.45703125Mb; avail=470793.46875Mb
2020-10-12 20:26:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.114570
2020-10-12 20:26:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.119849
2020-10-12 20:26:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.421875Mb; avail=470793.953125Mb
2020-10-12 20:26:03 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 20:26:33 | INFO | train_inner | epoch 017:    100 / 100 loss=9.395, nll_loss=8.569, ppl=379.82, wps=15831.5, ups=2.65, wpb=5972.3, bsz=259.4, num_updates=1700, lr=8.50575e-05, gnorm=1.344, clip=0, train_wall=28, wall=652
2020-10-12 20:26:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18491.8984375Mb; avail=470100.56640625Mb
2020-10-12 20:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001286
2020-10-12 20:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18491.8984375Mb; avail=470100.56640625Mb
2020-10-12 20:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.023199
2020-10-12 20:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18491.76953125Mb; avail=470100.6875Mb
2020-10-12 20:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.017119
2020-10-12 20:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.042400
2020-10-12 20:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18491.76953125Mb; avail=470100.6875Mb
2020-10-12 20:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18491.76953125Mb; avail=470100.6875Mb
2020-10-12 20:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000861
2020-10-12 20:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18491.76953125Mb; avail=470100.6875Mb
2020-10-12 20:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.023155
2020-10-12 20:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18491.76953125Mb; avail=470100.6875Mb
2020-10-12 20:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.018481
2020-10-12 20:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.043267
2020-10-12 20:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18491.76953125Mb; avail=470100.6875Mb
2020-10-12 20:26:33 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 9.417 | nll_loss 8.561 | ppl 377.6 | wps 52347.2 | wpb 2151.6 | bsz 88.1 | num_updates 1700 | best_loss 9.417
2020-10-12 20:26:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:26:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 17 @ 1700 updates, score 9.417) (writing took 9.200882825000008 seconds)
2020-10-12 20:26:43 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 20:26:43 | INFO | train | epoch 017 | loss 9.395 | nll_loss 8.569 | ppl 379.82 | wps 15108.7 | ups 2.53 | wpb 5972.3 | bsz 259.4 | num_updates 1700 | lr 8.50575e-05 | gnorm 1.344 | clip 0 | train_wall 28 | wall 662
2020-10-12 20:26:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 20:26:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 20:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17779.18359375Mb; avail=470812.828125Mb
2020-10-12 20:26:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000988
2020-10-12 20:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006467
2020-10-12 20:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17779.18359375Mb; avail=470812.828125Mb
2020-10-12 20:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000202
2020-10-12 20:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17779.18359375Mb; avail=470812.828125Mb
2020-10-12 20:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.117115
2020-10-12 20:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.125075
2020-10-12 20:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17777.5625Mb; avail=470814.0625Mb
2020-10-12 20:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17777.84375Mb; avail=470813.76953125Mb
2020-10-12 20:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004182
2020-10-12 20:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17777.84375Mb; avail=470813.76953125Mb
2020-10-12 20:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000263
2020-10-12 20:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17777.84375Mb; avail=470813.76953125Mb
2020-10-12 20:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.112665
2020-10-12 20:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.118027
2020-10-12 20:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17777.9296875Mb; avail=470814.40625Mb
2020-10-12 20:26:43 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 20:27:12 | INFO | train_inner | epoch 018:    100 / 100 loss=9.283, nll_loss=8.44, ppl=347.22, wps=15108.2, ups=2.53, wpb=5972.3, bsz=259.4, num_updates=1800, lr=9.0055e-05, gnorm=1.342, clip=0, train_wall=28, wall=692
2020-10-12 20:27:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:27:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17851.359375Mb; avail=470740.77734375Mb
2020-10-12 20:27:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001235
2020-10-12 20:27:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.359375Mb; avail=470740.77734375Mb
2020-10-12 20:27:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.022823
2020-10-12 20:27:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.46484375Mb; avail=470740.8828125Mb
2020-10-12 20:27:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016853
2020-10-12 20:27:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.041810
2020-10-12 20:27:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.46484375Mb; avail=470740.8828125Mb
2020-10-12 20:27:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17851.3984375Mb; avail=470740.73046875Mb
2020-10-12 20:27:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000822
2020-10-12 20:27:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.3984375Mb; avail=470740.71484375Mb
2020-10-12 20:27:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.022945
2020-10-12 20:27:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.5234375Mb; avail=470740.69921875Mb
2020-10-12 20:27:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016903
2020-10-12 20:27:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.041488
2020-10-12 20:27:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.38671875Mb; avail=470740.94140625Mb
2020-10-12 20:27:13 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 9.312 | nll_loss 8.438 | ppl 346.79 | wps 53610 | wpb 2151.6 | bsz 88.1 | num_updates 1800 | best_loss 9.312
2020-10-12 20:27:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:27:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 18 @ 1800 updates, score 9.312) (writing took 8.240909916000419 seconds)
2020-10-12 20:27:21 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 20:27:21 | INFO | train | epoch 018 | loss 9.283 | nll_loss 8.44 | ppl 347.22 | wps 15493 | ups 2.59 | wpb 5972.3 | bsz 259.4 | num_updates 1800 | lr 9.0055e-05 | gnorm 1.342 | clip 0 | train_wall 28 | wall 701
2020-10-12 20:27:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 20:27:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 20:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17828.109375Mb; avail=470764.61328125Mb
2020-10-12 20:27:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000893
2020-10-12 20:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005673
2020-10-12 20:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.44921875Mb; avail=470765.05078125Mb
2020-10-12 20:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000233
2020-10-12 20:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.3203125Mb; avail=470764.9375Mb
2020-10-12 20:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104097
2020-10-12 20:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110805
2020-10-12 20:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.87109375Mb; avail=470765.26171875Mb
2020-10-12 20:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17827.4765625Mb; avail=470764.65625Mb
2020-10-12 20:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003704
2020-10-12 20:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.4765625Mb; avail=470764.65625Mb
2020-10-12 20:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.4765625Mb; avail=470764.65625Mb
2020-10-12 20:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103017
2020-10-12 20:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107675
2020-10-12 20:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.75Mb; avail=470764.51171875Mb
2020-10-12 20:27:21 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 20:27:50 | INFO | train_inner | epoch 019:    100 / 100 loss=9.175, nll_loss=8.314, ppl=318.31, wps=15541.1, ups=2.6, wpb=5972.3, bsz=259.4, num_updates=1900, lr=9.50525e-05, gnorm=1.301, clip=0, train_wall=28, wall=730
2020-10-12 20:27:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:27:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18379.265625Mb; avail=470212.41796875Mb
2020-10-12 20:27:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001102
2020-10-12 20:27:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18380.4765625Mb; avail=470211.20703125Mb
2020-10-12 20:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020294
2020-10-12 20:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18402.16015625Mb; avail=470189.5234375Mb
2020-10-12 20:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015296
2020-10-12 20:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037477
2020-10-12 20:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18402.27734375Mb; avail=470191.24609375Mb
2020-10-12 20:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18402.76953125Mb; avail=470192.72265625Mb
2020-10-12 20:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000718
2020-10-12 20:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18402.76953125Mb; avail=470193.21484375Mb
2020-10-12 20:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020207
2020-10-12 20:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18402.22265625Mb; avail=470194.81640625Mb
2020-10-12 20:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015186
2020-10-12 20:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.036855
2020-10-12 20:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18402.20703125Mb; avail=470196.5390625Mb
2020-10-12 20:27:52 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 9.235 | nll_loss 8.348 | ppl 325.84 | wps 53020.4 | wpb 2151.6 | bsz 88.1 | num_updates 1900 | best_loss 9.235
2020-10-12 20:27:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:28:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 19 @ 1900 updates, score 9.235) (writing took 14.823944898000263 seconds)
2020-10-12 20:28:06 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 20:28:06 | INFO | train | epoch 019 | loss 9.175 | nll_loss 8.314 | ppl 318.31 | wps 13212.9 | ups 2.21 | wpb 5972.3 | bsz 259.4 | num_updates 1900 | lr 9.50525e-05 | gnorm 1.301 | clip 0 | train_wall 28 | wall 746
2020-10-12 20:28:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 20:28:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17827.45703125Mb; avail=470764.51953125Mb
2020-10-12 20:28:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001039
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006312
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.45703125Mb; avail=470764.51953125Mb
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000367
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.45703125Mb; avail=470764.51953125Mb
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.115022
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.122554
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17828.5859375Mb; avail=470763.7109375Mb
2020-10-12 20:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17828.69921875Mb; avail=470763.58984375Mb
2020-10-12 20:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004280
2020-10-12 20:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17828.69921875Mb; avail=470763.58984375Mb
2020-10-12 20:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 20:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17828.69921875Mb; avail=470763.58984375Mb
2020-10-12 20:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.114994
2020-10-12 20:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.120286
2020-10-12 20:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17828.765625Mb; avail=470763.34765625Mb
2020-10-12 20:28:07 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 20:28:36 | INFO | train_inner | epoch 020:    100 / 100 loss=9.066, nll_loss=8.189, ppl=291.82, wps=13235.3, ups=2.22, wpb=5972.3, bsz=259.4, num_updates=2000, lr=0.00010005, gnorm=1.291, clip=0, train_wall=28, wall=775
2020-10-12 20:28:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18555.4765625Mb; avail=470042.79296875Mb
2020-10-12 20:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001210
2020-10-12 20:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18557.03515625Mb; avail=470041.84765625Mb
2020-10-12 20:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020169
2020-10-12 20:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18582.91015625Mb; avail=470015.6171875Mb
2020-10-12 20:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.014944
2020-10-12 20:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037148
2020-10-12 20:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18604.625Mb; avail=469993.39453125Mb
2020-10-12 20:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18626.90625Mb; avail=469971.6015625Mb
2020-10-12 20:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000721
2020-10-12 20:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18627.859375Mb; avail=469970.1640625Mb
2020-10-12 20:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020131
2020-10-12 20:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18651.6015625Mb; avail=469946.37890625Mb
2020-10-12 20:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.014887
2020-10-12 20:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.036499
2020-10-12 20:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18669.3203125Mb; avail=469927.93359375Mb
2020-10-12 20:28:37 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 9.17 | nll_loss 8.263 | ppl 307.23 | wps 51638.5 | wpb 2151.6 | bsz 88.1 | num_updates 2000 | best_loss 9.17
2020-10-12 20:28:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:28:56 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 20 @ 2000 updates, score 9.17) (writing took 19.88990720899983 seconds)
2020-10-12 20:28:56 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 20:28:56 | INFO | train | epoch 020 | loss 9.066 | nll_loss 8.189 | ppl 291.82 | wps 11912.8 | ups 1.99 | wpb 5972.3 | bsz 259.4 | num_updates 2000 | lr 0.00010005 | gnorm 1.291 | clip 0 | train_wall 28 | wall 796
2020-10-12 20:28:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 20:28:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 20:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19097.3359375Mb; avail=469501.7890625Mb
2020-10-12 20:28:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000747
2020-10-12 20:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006044
2020-10-12 20:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19097.953125Mb; avail=469500.84765625Mb
2020-10-12 20:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 20:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19097.6953125Mb; avail=469501.359375Mb
2020-10-12 20:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.116591
2020-10-12 20:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.123815
2020-10-12 20:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19097.62890625Mb; avail=469497.6640625Mb
2020-10-12 20:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19097.984375Mb; avail=469497.21484375Mb
2020-10-12 20:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004174
2020-10-12 20:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19097.9609375Mb; avail=469497.80078125Mb
2020-10-12 20:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 20:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19097.83984375Mb; avail=469496.9609375Mb
2020-10-12 20:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.112985
2020-10-12 20:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.118291
2020-10-12 20:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19097.89453125Mb; avail=469494.33203125Mb
2020-10-12 20:28:57 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 20:29:26 | INFO | train_inner | epoch 021:    100 / 100 loss=8.964, nll_loss=8.071, ppl=268.98, wps=11956.7, ups=2, wpb=5972.3, bsz=259.4, num_updates=2100, lr=0.000105048, gnorm=1.336, clip=0, train_wall=28, wall=825
2020-10-12 20:29:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:29:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17851.1640625Mb; avail=470740.75390625Mb
2020-10-12 20:29:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001078
2020-10-12 20:29:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.1640625Mb; avail=470740.75390625Mb
2020-10-12 20:29:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020137
2020-10-12 20:29:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.1640625Mb; avail=470740.75390625Mb
2020-10-12 20:29:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015321
2020-10-12 20:29:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037302
2020-10-12 20:29:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.1640625Mb; avail=470740.75390625Mb
2020-10-12 20:29:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17851.1640625Mb; avail=470740.75390625Mb
2020-10-12 20:29:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000727
2020-10-12 20:29:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.1640625Mb; avail=470740.75390625Mb
2020-10-12 20:29:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020027
2020-10-12 20:29:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.1640625Mb; avail=470740.75390625Mb
2020-10-12 20:29:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.014903
2020-10-12 20:29:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.036391
2020-10-12 20:29:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.1640625Mb; avail=470740.75390625Mb
2020-10-12 20:29:26 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 9.129 | nll_loss 8.215 | ppl 297.07 | wps 51809.1 | wpb 2151.6 | bsz 88.1 | num_updates 2100 | best_loss 9.129
2020-10-12 20:29:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:29:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 21 @ 2100 updates, score 9.129) (writing took 4.505472513999848 seconds)
2020-10-12 20:29:31 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 20:29:31 | INFO | train | epoch 021 | loss 8.964 | nll_loss 8.071 | ppl 268.98 | wps 17347.8 | ups 2.9 | wpb 5972.3 | bsz 259.4 | num_updates 2100 | lr 0.000105048 | gnorm 1.336 | clip 0 | train_wall 28 | wall 831
2020-10-12 20:29:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 20:29:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 20:29:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18428.81640625Mb; avail=470163.03515625Mb
2020-10-12 20:29:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000658
2020-10-12 20:29:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005309
2020-10-12 20:29:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18428.81640625Mb; avail=470163.03515625Mb
2020-10-12 20:29:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 20:29:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18428.796875Mb; avail=470163.03515625Mb
2020-10-12 20:29:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102569
2020-10-12 20:29:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108849
2020-10-12 20:29:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18428.65625Mb; avail=470163.4453125Mb
2020-10-12 20:29:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18428.7265625Mb; avail=470163.203125Mb
2020-10-12 20:29:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003719
2020-10-12 20:29:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18428.7265625Mb; avail=470163.203125Mb
2020-10-12 20:29:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-12 20:29:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18428.7265625Mb; avail=470163.203125Mb
2020-10-12 20:29:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101331
2020-10-12 20:29:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105997
2020-10-12 20:29:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18429.37890625Mb; avail=470162.59765625Mb
2020-10-12 20:29:31 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 20:30:00 | INFO | train_inner | epoch 022:    100 / 100 loss=8.892, nll_loss=7.986, ppl=253.58, wps=17209.2, ups=2.88, wpb=5972.3, bsz=259.4, num_updates=2200, lr=0.000110045, gnorm=1.475, clip=0, train_wall=28, wall=860
2020-10-12 20:30:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:30:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17874.28125Mb; avail=470716.9765625Mb
2020-10-12 20:30:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001168
2020-10-12 20:30:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.28125Mb; avail=470716.9765625Mb
2020-10-12 20:30:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.019922
2020-10-12 20:30:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.3046875Mb; avail=470716.85546875Mb
2020-10-12 20:30:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015168
2020-10-12 20:30:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037039
2020-10-12 20:30:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.23046875Mb; avail=470716.85546875Mb
2020-10-12 20:30:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17874.23046875Mb; avail=470716.85546875Mb
2020-10-12 20:30:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000716
2020-10-12 20:30:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.23046875Mb; avail=470716.85546875Mb
2020-10-12 20:30:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.019997
2020-10-12 20:30:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.3203125Mb; avail=470716.85546875Mb
2020-10-12 20:30:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015038
2020-10-12 20:30:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.036493
2020-10-12 20:30:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.3203125Mb; avail=470716.85546875Mb
2020-10-12 20:30:01 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 9.049 | nll_loss 8.138 | ppl 281.7 | wps 53184.3 | wpb 2151.6 | bsz 88.1 | num_updates 2200 | best_loss 9.049
2020-10-12 20:30:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:30:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 22 @ 2200 updates, score 9.049) (writing took 7.179031337000197 seconds)
2020-10-12 20:30:08 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 20:30:08 | INFO | train | epoch 022 | loss 8.892 | nll_loss 7.986 | ppl 253.58 | wps 15982.9 | ups 2.68 | wpb 5972.3 | bsz 259.4 | num_updates 2200 | lr 0.000110045 | gnorm 1.475 | clip 0 | train_wall 28 | wall 868
2020-10-12 20:30:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 20:30:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 20:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17877.515625Mb; avail=470721.31640625Mb
2020-10-12 20:30:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000795
2020-10-12 20:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005764
2020-10-12 20:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.21875Mb; avail=470720.7890625Mb
2020-10-12 20:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000238
2020-10-12 20:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.08984375Mb; avail=470720.67578125Mb
2020-10-12 20:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.108048
2020-10-12 20:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114878
2020-10-12 20:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.94140625Mb; avail=470719.390625Mb
2020-10-12 20:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17877.0859375Mb; avail=470718.9140625Mb
2020-10-12 20:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003685
2020-10-12 20:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.046875Mb; avail=470718.83984375Mb
2020-10-12 20:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 20:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.65234375Mb; avail=470718.7265625Mb
2020-10-12 20:30:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103754
2020-10-12 20:30:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108477
2020-10-12 20:30:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.73046875Mb; avail=470717.00390625Mb
2020-10-12 20:30:09 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 20:30:37 | INFO | train_inner | epoch 023:    100 / 100 loss=8.774, nll_loss=7.851, ppl=230.81, wps=16125.3, ups=2.7, wpb=5972.3, bsz=259.4, num_updates=2300, lr=0.000115043, gnorm=1.284, clip=0, train_wall=28, wall=897
2020-10-12 20:30:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17835.64453125Mb; avail=470755.84765625Mb
2020-10-12 20:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001116
2020-10-12 20:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.64453125Mb; avail=470755.84765625Mb
2020-10-12 20:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020248
2020-10-12 20:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.64453125Mb; avail=470755.84765625Mb
2020-10-12 20:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015136
2020-10-12 20:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037287
2020-10-12 20:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.56640625Mb; avail=470756.08984375Mb
2020-10-12 20:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17835.578125Mb; avail=470755.96875Mb
2020-10-12 20:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000693
2020-10-12 20:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.578125Mb; avail=470755.96875Mb
2020-10-12 20:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020328
2020-10-12 20:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.65234375Mb; avail=470756.08984375Mb
2020-10-12 20:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015192
2020-10-12 20:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.036946
2020-10-12 20:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.66015625Mb; avail=470755.96875Mb
2020-10-12 20:30:38 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 8.971 | nll_loss 8.039 | ppl 263.05 | wps 52173.7 | wpb 2151.6 | bsz 88.1 | num_updates 2300 | best_loss 8.971
2020-10-12 20:30:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:30:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 23 @ 2300 updates, score 8.971) (writing took 4.5030520170003 seconds)
2020-10-12 20:30:43 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 20:30:43 | INFO | train | epoch 023 | loss 8.774 | nll_loss 7.851 | ppl 230.81 | wps 17361 | ups 2.91 | wpb 5972.3 | bsz 259.4 | num_updates 2300 | lr 0.000115043 | gnorm 1.284 | clip 0 | train_wall 28 | wall 902
2020-10-12 20:30:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 20:30:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 20:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17876.63671875Mb; avail=470715.52734375Mb
2020-10-12 20:30:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000801
2020-10-12 20:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005525
2020-10-12 20:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.20703125Mb; avail=470714.95703125Mb
2020-10-12 20:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 20:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.20703125Mb; avail=470714.95703125Mb
2020-10-12 20:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100974
2020-10-12 20:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107442
2020-10-12 20:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.24609375Mb; avail=470714.95703125Mb
2020-10-12 20:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17877.9765625Mb; avail=470714.109375Mb
2020-10-12 20:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003751
2020-10-12 20:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.9765625Mb; avail=470714.109375Mb
2020-10-12 20:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 20:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.9765625Mb; avail=470714.109375Mb
2020-10-12 20:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101672
2020-10-12 20:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106343
2020-10-12 20:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.59375Mb; avail=470714.4765625Mb
2020-10-12 20:30:43 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 20:31:12 | INFO | train_inner | epoch 024:    100 / 100 loss=8.678, nll_loss=7.74, ppl=213.72, wps=17302.1, ups=2.9, wpb=5972.3, bsz=259.4, num_updates=2400, lr=0.00012004, gnorm=1.326, clip=0, train_wall=28, wall=932
2020-10-12 20:31:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:31:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18430.5390625Mb; avail=470160.92578125Mb
2020-10-12 20:31:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001225
2020-10-12 20:31:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18430.5390625Mb; avail=470160.92578125Mb
2020-10-12 20:31:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020707
2020-10-12 20:31:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18430.5390625Mb; avail=470160.92578125Mb
2020-10-12 20:31:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015784
2020-10-12 20:31:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038574
2020-10-12 20:31:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18430.5390625Mb; avail=470160.92578125Mb
2020-10-12 20:31:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18430.41796875Mb; avail=470161.046875Mb
2020-10-12 20:31:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000786
2020-10-12 20:31:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18430.41796875Mb; avail=470161.046875Mb
2020-10-12 20:31:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020725
2020-10-12 20:31:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18429.51171875Mb; avail=470162.359375Mb
2020-10-12 20:31:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015696
2020-10-12 20:31:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037986
2020-10-12 20:31:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18429.51171875Mb; avail=470162.359375Mb
2020-10-12 20:31:13 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.931 | nll_loss 7.996 | ppl 255.25 | wps 51706.5 | wpb 2151.6 | bsz 88.1 | num_updates 2400 | best_loss 8.931
2020-10-12 20:31:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:31:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 24 @ 2400 updates, score 8.931) (writing took 8.87056706399926 seconds)
2020-10-12 20:31:22 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 20:31:22 | INFO | train | epoch 024 | loss 8.678 | nll_loss 7.74 | ppl 213.72 | wps 15361.5 | ups 2.57 | wpb 5972.3 | bsz 259.4 | num_updates 2400 | lr 0.00012004 | gnorm 1.326 | clip 0 | train_wall 28 | wall 941
2020-10-12 20:31:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 20:31:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 20:31:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17807.484375Mb; avail=470784.41796875Mb
2020-10-12 20:31:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000835
2020-10-12 20:31:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006051
2020-10-12 20:31:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.08984375Mb; avail=470783.8125Mb
2020-10-12 20:31:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000203
2020-10-12 20:31:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.06640625Mb; avail=470783.8125Mb
2020-10-12 20:31:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106784
2020-10-12 20:31:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113936
2020-10-12 20:31:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.2109375Mb; avail=470783.69140625Mb
2020-10-12 20:31:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17808.26953125Mb; avail=470783.328125Mb
2020-10-12 20:31:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003633
2020-10-12 20:31:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.26953125Mb; avail=470783.328125Mb
2020-10-12 20:31:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 20:31:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.26953125Mb; avail=470783.328125Mb
2020-10-12 20:31:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100576
2020-10-12 20:31:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105167
2020-10-12 20:31:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.4453125Mb; avail=470782.96484375Mb
2020-10-12 20:31:22 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 20:31:51 | INFO | train_inner | epoch 025:    100 / 100 loss=8.577, nll_loss=7.623, ppl=197.06, wps=15280.9, ups=2.56, wpb=5972.3, bsz=259.4, num_updates=2500, lr=0.000125037, gnorm=1.298, clip=0, train_wall=28, wall=971
2020-10-12 20:31:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17874.8125Mb; avail=470716.73046875Mb
2020-10-12 20:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001922
2020-10-12 20:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.8125Mb; avail=470716.73046875Mb
2020-10-12 20:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020451
2020-10-12 20:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.98828125Mb; avail=470716.3671875Mb
2020-10-12 20:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.017299
2020-10-12 20:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.040485
2020-10-12 20:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17875.15234375Mb; avail=470716.125Mb
2020-10-12 20:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17876.1015625Mb; avail=470715.16015625Mb
2020-10-12 20:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000705
2020-10-12 20:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.1015625Mb; avail=470715.16015625Mb
2020-10-12 20:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020519
2020-10-12 20:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.03515625Mb; avail=470715.0390625Mb
2020-10-12 20:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015053
2020-10-12 20:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037022
2020-10-12 20:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17875.94140625Mb; avail=470715.28125Mb
2020-10-12 20:31:52 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.9 | nll_loss 7.939 | ppl 245.43 | wps 53439 | wpb 2151.6 | bsz 88.1 | num_updates 2500 | best_loss 8.9
2020-10-12 20:31:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:32:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 25 @ 2500 updates, score 8.9) (writing took 9.004736038001283 seconds)
2020-10-12 20:32:01 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 20:32:01 | INFO | train | epoch 025 | loss 8.577 | nll_loss 7.623 | ppl 197.06 | wps 15235.7 | ups 2.55 | wpb 5972.3 | bsz 259.4 | num_updates 2500 | lr 0.000125037 | gnorm 1.298 | clip 0 | train_wall 28 | wall 981
2020-10-12 20:32:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 20:32:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 20:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18832.859375Mb; avail=469759.14453125Mb
2020-10-12 20:32:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001003
2020-10-12 20:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006383
2020-10-12 20:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18832.859375Mb; avail=469759.14453125Mb
2020-10-12 20:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000216
2020-10-12 20:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18832.859375Mb; avail=469759.14453125Mb
2020-10-12 20:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.115215
2020-10-12 20:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.122735
2020-10-12 20:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18833.7734375Mb; avail=469758.2421875Mb
2020-10-12 20:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18833.7734375Mb; avail=469758.2421875Mb
2020-10-12 20:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004126
2020-10-12 20:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18833.7734375Mb; avail=469758.2421875Mb
2020-10-12 20:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000235
2020-10-12 20:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18833.7734375Mb; avail=469758.2421875Mb
2020-10-12 20:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.114170
2020-10-12 20:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.119515
2020-10-12 20:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18834.04296875Mb; avail=469758.0Mb
2020-10-12 20:32:01 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 20:32:30 | INFO | train_inner | epoch 026:    100 / 100 loss=8.48, nll_loss=7.511, ppl=182.42, wps=15191, ups=2.54, wpb=5972.3, bsz=259.4, num_updates=2600, lr=0.000130035, gnorm=1.324, clip=0, train_wall=28, wall=1010
2020-10-12 20:32:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17863.49609375Mb; avail=470728.625Mb
2020-10-12 20:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001175
2020-10-12 20:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.97265625Mb; avail=470728.3984375Mb
2020-10-12 20:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020076
2020-10-12 20:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17864.421875Mb; avail=470728.38671875Mb
2020-10-12 20:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016692
2020-10-12 20:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038830
2020-10-12 20:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17864.28515625Mb; avail=470727.86328125Mb
2020-10-12 20:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17863.89453125Mb; avail=470727.48828125Mb
2020-10-12 20:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000825
2020-10-12 20:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17864.37109375Mb; avail=470727.8671875Mb
2020-10-12 20:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020023
2020-10-12 20:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17864.93359375Mb; avail=470727.15234375Mb
2020-10-12 20:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015429
2020-10-12 20:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037090
2020-10-12 20:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17864.81640625Mb; avail=470726.984375Mb
2020-10-12 20:32:31 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.882 | nll_loss 7.899 | ppl 238.73 | wps 53595.9 | wpb 2151.6 | bsz 88.1 | num_updates 2600 | best_loss 8.882
2020-10-12 20:32:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:32:36 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 26 @ 2600 updates, score 8.882) (writing took 5.257161442999859 seconds)
2020-10-12 20:32:36 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 20:32:36 | INFO | train | epoch 026 | loss 8.48 | nll_loss 7.511 | ppl 182.42 | wps 16793.8 | ups 2.81 | wpb 5972.3 | bsz 259.4 | num_updates 2600 | lr 0.000130035 | gnorm 1.324 | clip 0 | train_wall 28 | wall 1016
2020-10-12 20:32:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 20:32:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 20:32:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17867.95703125Mb; avail=470723.19140625Mb
2020-10-12 20:32:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000700
2020-10-12 20:32:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005084
2020-10-12 20:32:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17867.95703125Mb; avail=470723.19140625Mb
2020-10-12 20:32:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-12 20:32:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17867.95703125Mb; avail=470723.19140625Mb
2020-10-12 20:32:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101360
2020-10-12 20:32:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107395
2020-10-12 20:32:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17867.95703125Mb; avail=470723.19140625Mb
2020-10-12 20:32:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17867.99609375Mb; avail=470722.94921875Mb
2020-10-12 20:32:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003763
2020-10-12 20:32:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17867.99609375Mb; avail=470722.94921875Mb
2020-10-12 20:32:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000210
2020-10-12 20:32:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17867.90625Mb; avail=470722.94921875Mb
2020-10-12 20:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100468
2020-10-12 20:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105184
2020-10-12 20:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17867.94921875Mb; avail=470723.41015625Mb
2020-10-12 20:32:37 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 20:33:06 | INFO | train_inner | epoch 027:    100 / 100 loss=8.389, nll_loss=7.405, ppl=169.46, wps=16740.2, ups=2.8, wpb=5972.3, bsz=259.4, num_updates=2700, lr=0.000135032, gnorm=1.351, clip=0, train_wall=28, wall=1046
2020-10-12 20:33:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:33:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17857.80859375Mb; avail=470734.125Mb
2020-10-12 20:33:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001240
2020-10-12 20:33:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.80859375Mb; avail=470734.125Mb
2020-10-12 20:33:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020698
2020-10-12 20:33:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.99609375Mb; avail=470734.24609375Mb
2020-10-12 20:33:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015733
2020-10-12 20:33:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038488
2020-10-12 20:33:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.33984375Mb; avail=470734.00390625Mb
2020-10-12 20:33:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17858.33984375Mb; avail=470734.00390625Mb
2020-10-12 20:33:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000698
2020-10-12 20:33:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.33984375Mb; avail=470734.00390625Mb
2020-10-12 20:33:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020189
2020-10-12 20:33:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.82421875Mb; avail=470734.125Mb
2020-10-12 20:33:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015367
2020-10-12 20:33:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037009
2020-10-12 20:33:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17857.82421875Mb; avail=470734.125Mb
2020-10-12 20:33:07 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.735 | nll_loss 7.75 | ppl 215.29 | wps 52666.6 | wpb 2151.6 | bsz 88.1 | num_updates 2700 | best_loss 8.735
2020-10-12 20:33:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:33:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 27 @ 2700 updates, score 8.735) (writing took 8.32421021600021 seconds)
2020-10-12 20:33:15 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 20:33:15 | INFO | train | epoch 027 | loss 8.389 | nll_loss 7.405 | ppl 169.46 | wps 15411.6 | ups 2.58 | wpb 5972.3 | bsz 259.4 | num_updates 2700 | lr 0.000135032 | gnorm 1.351 | clip 0 | train_wall 28 | wall 1055
2020-10-12 20:33:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 20:33:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 20:33:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18255.453125Mb; avail=470336.00390625Mb
2020-10-12 20:33:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000712
2020-10-12 20:33:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006321
2020-10-12 20:33:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18255.453125Mb; avail=470336.00390625Mb
2020-10-12 20:33:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000221
2020-10-12 20:33:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18255.453125Mb; avail=470336.00390625Mb
2020-10-12 20:33:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.115000
2020-10-12 20:33:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.122393
2020-10-12 20:33:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18255.796875Mb; avail=470335.94140625Mb
2020-10-12 20:33:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18257.15234375Mb; avail=470334.5859375Mb
2020-10-12 20:33:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004097
2020-10-12 20:33:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18257.7734375Mb; avail=470333.96484375Mb
2020-10-12 20:33:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-12 20:33:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18257.7734375Mb; avail=470333.96484375Mb
2020-10-12 20:33:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.112965
2020-10-12 20:33:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.118216
2020-10-12 20:33:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18258.703125Mb; avail=470333.0078125Mb
2020-10-12 20:33:15 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 20:33:45 | INFO | train_inner | epoch 028:    100 / 100 loss=8.286, nll_loss=7.286, ppl=156.03, wps=15468, ups=2.59, wpb=5972.3, bsz=259.4, num_updates=2800, lr=0.00014003, gnorm=1.356, clip=0, train_wall=28, wall=1084
2020-10-12 20:33:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17318.4765625Mb; avail=471275.5546875Mb
2020-10-12 20:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001262
2020-10-12 20:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17319.6875Mb; avail=471274.34375Mb
2020-10-12 20:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020748
2020-10-12 20:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17330.8203125Mb; avail=471263.203125Mb
2020-10-12 20:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015532
2020-10-12 20:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038380
2020-10-12 20:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17331.42578125Mb; avail=471262.59765625Mb
2020-10-12 20:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17349.58984375Mb; avail=471244.43359375Mb
2020-10-12 20:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000787
2020-10-12 20:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17349.58984375Mb; avail=471244.43359375Mb
2020-10-12 20:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020487
2020-10-12 20:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17361.69921875Mb; avail=471232.32421875Mb
2020-10-12 20:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015288
2020-10-12 20:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037353
2020-10-12 20:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17372.70703125Mb; avail=471221.3046875Mb
2020-10-12 20:33:45 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.687 | nll_loss 7.68 | ppl 205.11 | wps 53119.8 | wpb 2151.6 | bsz 88.1 | num_updates 2800 | best_loss 8.687
2020-10-12 20:33:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:33:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 28 @ 2800 updates, score 8.687) (writing took 4.490868283000964 seconds)
2020-10-12 20:33:50 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 20:33:50 | INFO | train | epoch 028 | loss 8.286 | nll_loss 7.286 | ppl 156.03 | wps 17180.3 | ups 2.88 | wpb 5972.3 | bsz 259.4 | num_updates 2800 | lr 0.00014003 | gnorm 1.356 | clip 0 | train_wall 28 | wall 1090
2020-10-12 20:33:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 20:33:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 20:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17897.35546875Mb; avail=470694.1328125Mb
2020-10-12 20:33:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001007
2020-10-12 20:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006818
2020-10-12 20:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17897.35546875Mb; avail=470694.1328125Mb
2020-10-12 20:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000239
2020-10-12 20:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17897.35546875Mb; avail=470694.24609375Mb
2020-10-12 20:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104478
2020-10-12 20:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112609
2020-10-12 20:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17897.00390625Mb; avail=470694.6171875Mb
2020-10-12 20:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17897.14453125Mb; avail=470694.375Mb
2020-10-12 20:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003686
2020-10-12 20:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17897.14453125Mb; avail=470694.375Mb
2020-10-12 20:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000236
2020-10-12 20:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17897.14453125Mb; avail=470694.375Mb
2020-10-12 20:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102924
2020-10-12 20:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107649
2020-10-12 20:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17896.70703125Mb; avail=470695.109375Mb
2020-10-12 20:33:50 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 20:34:19 | INFO | train_inner | epoch 029:    100 / 100 loss=8.178, nll_loss=7.16, ppl=142.97, wps=17243.3, ups=2.89, wpb=5972.3, bsz=259.4, num_updates=2900, lr=0.000145028, gnorm=1.303, clip=0, train_wall=28, wall=1119
2020-10-12 20:34:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17874.44140625Mb; avail=470717.265625Mb
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001155
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.44140625Mb; avail=470717.265625Mb
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020604
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.44140625Mb; avail=470717.265625Mb
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015242
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037781
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.43359375Mb; avail=470717.2734375Mb
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17874.390625Mb; avail=470717.39453125Mb
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000718
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.390625Mb; avail=470717.39453125Mb
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020136
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.3984375Mb; avail=470717.2734375Mb
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015173
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.036761
2020-10-12 20:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.29296875Mb; avail=470717.2734375Mb
2020-10-12 20:34:20 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.617 | nll_loss 7.606 | ppl 194.88 | wps 53839.4 | wpb 2151.6 | bsz 88.1 | num_updates 2900 | best_loss 8.617
2020-10-12 20:34:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:34:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 29 @ 2900 updates, score 8.617) (writing took 4.463571689000673 seconds)
2020-10-12 20:34:24 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 20:34:24 | INFO | train | epoch 029 | loss 8.178 | nll_loss 7.16 | ppl 142.97 | wps 17245.9 | ups 2.89 | wpb 5972.3 | bsz 259.4 | num_updates 2900 | lr 0.000145028 | gnorm 1.303 | clip 0 | train_wall 28 | wall 1124
2020-10-12 20:34:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 20:34:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 20:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17915.66015625Mb; avail=470676.078125Mb
2020-10-12 20:34:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000955
2020-10-12 20:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006678
2020-10-12 20:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17915.66015625Mb; avail=470676.078125Mb
2020-10-12 20:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000274
2020-10-12 20:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17915.66015625Mb; avail=470676.078125Mb
2020-10-12 20:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106436
2020-10-12 20:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114483
2020-10-12 20:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17915.3046875Mb; avail=470676.20703125Mb
2020-10-12 20:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17914.7734375Mb; avail=470676.94140625Mb
2020-10-12 20:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003735
2020-10-12 20:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17914.7734375Mb; avail=470676.94140625Mb
2020-10-12 20:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-12 20:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17914.7734375Mb; avail=470676.94140625Mb
2020-10-12 20:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104042
2020-10-12 20:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108742
2020-10-12 20:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17914.83984375Mb; avail=470676.890625Mb
2020-10-12 20:34:25 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 20:34:54 | INFO | train_inner | epoch 030:    100 / 100 loss=8.079, nll_loss=7.046, ppl=132.13, wps=17258.1, ups=2.89, wpb=5972.3, bsz=259.4, num_updates=3000, lr=0.000150025, gnorm=1.389, clip=0, train_wall=28, wall=1154
2020-10-12 20:34:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18510.75Mb; avail=470090.06640625Mb
2020-10-12 20:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001220
2020-10-12 20:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18511.7578125Mb; avail=470089.70703125Mb
2020-10-12 20:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021586
2020-10-12 20:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18510.51171875Mb; avail=470092.25Mb
2020-10-12 20:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015439
2020-10-12 20:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039100
2020-10-12 20:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18512.25390625Mb; avail=470090.5625Mb
2020-10-12 20:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18512.7890625Mb; avail=470090.671875Mb
2020-10-12 20:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000861
2020-10-12 20:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18511.796875Mb; avail=470090.33203125Mb
2020-10-12 20:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020230
2020-10-12 20:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18511.0546875Mb; avail=470089.8359375Mb
2020-10-12 20:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015506
2020-10-12 20:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037458
2020-10-12 20:34:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18511.84375Mb; avail=470089.2421875Mb
2020-10-12 20:34:55 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.668 | nll_loss 7.671 | ppl 203.81 | wps 53004.1 | wpb 2151.6 | bsz 88.1 | num_updates 3000 | best_loss 8.617
2020-10-12 20:34:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:35:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_last.pt (epoch 30 @ 3000 updates, score 8.668) (writing took 5.620151324999824 seconds)
2020-10-12 20:35:00 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 20:35:00 | INFO | train | epoch 030 | loss 8.079 | nll_loss 7.046 | ppl 132.13 | wps 16613.7 | ups 2.78 | wpb 5972.3 | bsz 259.4 | num_updates 3000 | lr 0.000150025 | gnorm 1.389 | clip 0 | train_wall 28 | wall 1160
2020-10-12 20:35:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 20:35:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 20:35:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17929.890625Mb; avail=470661.96484375Mb
2020-10-12 20:35:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000705
2020-10-12 20:35:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005263
2020-10-12 20:35:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17925.953125Mb; avail=470665.90234375Mb
2020-10-12 20:35:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000207
2020-10-12 20:35:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17925.953125Mb; avail=470665.90234375Mb
2020-10-12 20:35:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103343
2020-10-12 20:35:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109722
2020-10-12 20:35:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18049.875Mb; avail=470541.98046875Mb
2020-10-12 20:35:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18089.23046875Mb; avail=470502.01953125Mb
2020-10-12 20:35:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003779
2020-10-12 20:35:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18094.07421875Mb; avail=470497.78125Mb
2020-10-12 20:35:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000223
2020-10-12 20:35:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18094.6796875Mb; avail=470497.17578125Mb
2020-10-12 20:35:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103814
2020-10-12 20:35:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108621
2020-10-12 20:35:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18226.40625Mb; avail=470365.546875Mb
2020-10-12 20:35:01 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 20:35:30 | INFO | train_inner | epoch 031:    100 / 100 loss=7.981, nll_loss=6.931, ppl=122.02, wps=16644.7, ups=2.79, wpb=5972.3, bsz=259.4, num_updates=3100, lr=0.000155023, gnorm=1.399, clip=0, train_wall=28, wall=1189
2020-10-12 20:35:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17903.74609375Mb; avail=470669.37109375Mb
2020-10-12 20:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001098
2020-10-12 20:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17903.74609375Mb; avail=470669.37109375Mb
2020-10-12 20:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.022305
2020-10-12 20:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17903.82421875Mb; avail=470669.37109375Mb
2020-10-12 20:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015098
2020-10-12 20:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039292
2020-10-12 20:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17903.83984375Mb; avail=470669.37109375Mb
2020-10-12 20:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17903.75390625Mb; avail=470669.25Mb
2020-10-12 20:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000708
2020-10-12 20:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17903.75390625Mb; avail=470669.25Mb
2020-10-12 20:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020414
2020-10-12 20:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17903.76171875Mb; avail=470669.25Mb
2020-10-12 20:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015218
2020-10-12 20:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037090
2020-10-12 20:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17903.78515625Mb; avail=470669.12890625Mb
2020-10-12 20:35:30 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.528 | nll_loss 7.496 | ppl 180.47 | wps 52976.4 | wpb 2151.6 | bsz 88.1 | num_updates 3100 | best_loss 8.528
2020-10-12 20:35:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:35:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 31 @ 3100 updates, score 8.528) (writing took 15.00796921699839 seconds)
2020-10-12 20:35:46 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 20:35:46 | INFO | train | epoch 031 | loss 7.981 | nll_loss 6.931 | ppl 122.02 | wps 13249 | ups 2.22 | wpb 5972.3 | bsz 259.4 | num_updates 3100 | lr 0.000155023 | gnorm 1.399 | clip 0 | train_wall 28 | wall 1205
2020-10-12 20:35:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 20:35:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18483.84765625Mb; avail=470089.91015625Mb
2020-10-12 20:35:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000917
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006797
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18486.77734375Mb; avail=470086.8828125Mb
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000272
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18486.77734375Mb; avail=470086.8828125Mb
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.115408
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.123456
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18487.0Mb; avail=470086.86328125Mb
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18487.10546875Mb; avail=470086.86328125Mb
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004189
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18487.10546875Mb; avail=470086.86328125Mb
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18487.10546875Mb; avail=470086.86328125Mb
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.115825
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.121151
2020-10-12 20:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18487.00390625Mb; avail=470087.1484375Mb
2020-10-12 20:35:46 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 20:36:15 | INFO | train_inner | epoch 032:    100 / 100 loss=7.861, nll_loss=6.792, ppl=110.81, wps=13183.5, ups=2.21, wpb=5972.3, bsz=259.4, num_updates=3200, lr=0.00016002, gnorm=1.328, clip=0, train_wall=28, wall=1235
2020-10-12 20:36:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:36:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17953.875Mb; avail=470620.08984375Mb
2020-10-12 20:36:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001104
2020-10-12 20:36:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17953.875Mb; avail=470620.08984375Mb
2020-10-12 20:36:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020419
2020-10-12 20:36:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17954.015625Mb; avail=470619.84765625Mb
2020-10-12 20:36:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016796
2020-10-12 20:36:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039132
2020-10-12 20:36:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17953.96875Mb; avail=470620.08984375Mb
2020-10-12 20:36:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17953.98828125Mb; avail=470619.96875Mb
2020-10-12 20:36:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000740
2020-10-12 20:36:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17953.98828125Mb; avail=470619.96875Mb
2020-10-12 20:36:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020112
2020-10-12 20:36:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17953.98828125Mb; avail=470619.96875Mb
2020-10-12 20:36:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015011
2020-10-12 20:36:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.036609
2020-10-12 20:36:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17953.87109375Mb; avail=470620.08984375Mb
2020-10-12 20:36:16 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.492 | nll_loss 7.444 | ppl 174.15 | wps 50452.6 | wpb 2151.6 | bsz 88.1 | num_updates 3200 | best_loss 8.492
2020-10-12 20:36:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:36:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 32 @ 3200 updates, score 8.492) (writing took 7.4487577010004316 seconds)
2020-10-12 20:36:23 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 20:36:23 | INFO | train | epoch 032 | loss 7.861 | nll_loss 6.792 | ppl 110.81 | wps 15808.5 | ups 2.65 | wpb 5972.3 | bsz 259.4 | num_updates 3200 | lr 0.00016002 | gnorm 1.328 | clip 0 | train_wall 28 | wall 1243
2020-10-12 20:36:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 20:36:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 20:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17961.66796875Mb; avail=470612.74609375Mb
2020-10-12 20:36:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000848
2020-10-12 20:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006288
2020-10-12 20:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17961.66796875Mb; avail=470612.74609375Mb
2020-10-12 20:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000213
2020-10-12 20:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17961.66796875Mb; avail=470612.74609375Mb
2020-10-12 20:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.114007
2020-10-12 20:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.121489
2020-10-12 20:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17960.69140625Mb; avail=470613.48828125Mb
2020-10-12 20:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17960.71484375Mb; avail=470613.3671875Mb
2020-10-12 20:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004214
2020-10-12 20:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17960.73046875Mb; avail=470613.3515625Mb
2020-10-12 20:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000233
2020-10-12 20:36:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17960.73046875Mb; avail=470613.3515625Mb
2020-10-12 20:36:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.112665
2020-10-12 20:36:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.117950
2020-10-12 20:36:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17960.390625Mb; avail=470613.5703125Mb
2020-10-12 20:36:24 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 20:36:53 | INFO | train_inner | epoch 033:    100 / 100 loss=7.767, nll_loss=6.683, ppl=102.72, wps=15815.7, ups=2.65, wpb=5972.3, bsz=259.4, num_updates=3300, lr=0.000165018, gnorm=1.401, clip=0, train_wall=28, wall=1272
2020-10-12 20:36:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17865.640625Mb; avail=470708.3359375Mb
2020-10-12 20:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001122
2020-10-12 20:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.640625Mb; avail=470708.3359375Mb
2020-10-12 20:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021729
2020-10-12 20:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.640625Mb; avail=470708.3359375Mb
2020-10-12 20:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016281
2020-10-12 20:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039927
2020-10-12 20:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.7265625Mb; avail=470708.21484375Mb
2020-10-12 20:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17865.69140625Mb; avail=470708.3359375Mb
2020-10-12 20:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000784
2020-10-12 20:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.69140625Mb; avail=470708.3359375Mb
2020-10-12 20:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021419
2020-10-12 20:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.69140625Mb; avail=470708.3359375Mb
2020-10-12 20:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.016121
2020-10-12 20:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.039101
2020-10-12 20:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.71484375Mb; avail=470708.21484375Mb
2020-10-12 20:36:54 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.469 | nll_loss 7.418 | ppl 171 | wps 53736.4 | wpb 2151.6 | bsz 88.1 | num_updates 3300 | best_loss 8.469
2020-10-12 20:36:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:36:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 33 @ 3300 updates, score 8.469) (writing took 4.442063575999782 seconds)
2020-10-12 20:36:58 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 20:36:58 | INFO | train | epoch 033 | loss 7.767 | nll_loss 6.683 | ppl 102.72 | wps 17205.5 | ups 2.88 | wpb 5972.3 | bsz 259.4 | num_updates 3300 | lr 0.000165018 | gnorm 1.401 | clip 0 | train_wall 28 | wall 1278
2020-10-12 20:36:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 20:36:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 20:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18468.22265625Mb; avail=470110.89453125Mb
2020-10-12 20:36:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000808
2020-10-12 20:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005903
2020-10-12 20:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18468.75Mb; avail=470111.01171875Mb
2020-10-12 20:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000257
2020-10-12 20:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18468.62109375Mb; avail=470110.29296875Mb
2020-10-12 20:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102240
2020-10-12 20:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109168
2020-10-12 20:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18468.40625Mb; avail=470108.3203125Mb
2020-10-12 20:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18468.6953125Mb; avail=470108.37890625Mb
2020-10-12 20:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003752
2020-10-12 20:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18468.421875Mb; avail=470108.6796875Mb
2020-10-12 20:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 20:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18468.8984375Mb; avail=470108.56640625Mb
2020-10-12 20:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102584
2020-10-12 20:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107252
2020-10-12 20:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18468.39453125Mb; avail=470106.84375Mb
2020-10-12 20:36:58 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 20:37:28 | INFO | train_inner | epoch 034:    100 / 100 loss=7.675, nll_loss=6.575, ppl=95.31, wps=17082.7, ups=2.86, wpb=5972.3, bsz=259.4, num_updates=3400, lr=0.000170015, gnorm=1.408, clip=0, train_wall=29, wall=1307
2020-10-12 20:37:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:37:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17876.75Mb; avail=470696.34765625Mb
2020-10-12 20:37:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001211
2020-10-12 20:37:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.75Mb; avail=470696.34765625Mb
2020-10-12 20:37:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020480
2020-10-12 20:37:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.75Mb; avail=470696.34765625Mb
2020-10-12 20:37:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015302
2020-10-12 20:37:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037766
2020-10-12 20:37:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.75Mb; avail=470696.34765625Mb
2020-10-12 20:37:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17876.75Mb; avail=470696.34765625Mb
2020-10-12 20:37:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000710
2020-10-12 20:37:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.75Mb; avail=470696.34765625Mb
2020-10-12 20:37:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020471
2020-10-12 20:37:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.69921875Mb; avail=470696.10546875Mb
2020-10-12 20:37:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015603
2020-10-12 20:37:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037525
2020-10-12 20:37:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.82421875Mb; avail=470695.87109375Mb
2020-10-12 20:37:29 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.389 | nll_loss 7.307 | ppl 158.4 | wps 53495.3 | wpb 2151.6 | bsz 88.1 | num_updates 3400 | best_loss 8.389
2020-10-12 20:37:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:37:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 34 @ 3400 updates, score 8.389) (writing took 4.458681543999774 seconds)
2020-10-12 20:37:33 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 20:37:33 | INFO | train | epoch 034 | loss 7.675 | nll_loss 6.575 | ppl 95.31 | wps 17077.7 | ups 2.86 | wpb 5972.3 | bsz 259.4 | num_updates 3400 | lr 0.000170015 | gnorm 1.408 | clip 0 | train_wall 29 | wall 1313
2020-10-12 20:37:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 20:37:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17921.1796875Mb; avail=470652.2734375Mb
2020-10-12 20:37:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000701
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005574
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17921.1796875Mb; avail=470652.2734375Mb
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000286
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17921.1796875Mb; avail=470652.2734375Mb
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106634
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113342
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17921.734375Mb; avail=470651.7890625Mb
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17921.26171875Mb; avail=470652.28125Mb
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003719
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17921.23828125Mb; avail=470652.28125Mb
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000215
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17921.23828125Mb; avail=470652.28125Mb
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103917
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108602
2020-10-12 20:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17921.03515625Mb; avail=470652.40234375Mb
2020-10-12 20:37:33 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 20:38:02 | INFO | train_inner | epoch 035:    100 / 100 loss=7.549, nll_loss=6.428, ppl=86.11, wps=17229.1, ups=2.88, wpb=5972.3, bsz=259.4, num_updates=3500, lr=0.000175013, gnorm=1.36, clip=0, train_wall=28, wall=1342
2020-10-12 20:38:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:38:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19073.06640625Mb; avail=469503.875Mb
2020-10-12 20:38:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001239
2020-10-12 20:38:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19072.80859375Mb; avail=469504.140625Mb
2020-10-12 20:38:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020707
2020-10-12 20:38:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19072.1953125Mb; avail=469503.3125Mb
2020-10-12 20:38:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015650
2020-10-12 20:38:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038563
2020-10-12 20:38:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19072.5Mb; avail=469503.484375Mb
2020-10-12 20:38:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19072.546875Mb; avail=469502.7109375Mb
2020-10-12 20:38:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000896
2020-10-12 20:38:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19072.2890625Mb; avail=469502.484375Mb
2020-10-12 20:38:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020390
2020-10-12 20:38:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19072.62890625Mb; avail=469502.4140625Mb
2020-10-12 20:38:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015418
2020-10-12 20:38:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037628
2020-10-12 20:38:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19072.18359375Mb; avail=469501.875Mb
2020-10-12 20:38:03 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.333 | nll_loss 7.256 | ppl 152.87 | wps 51642.2 | wpb 2151.6 | bsz 88.1 | num_updates 3500 | best_loss 8.333
2020-10-12 20:38:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:38:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 35 @ 3500 updates, score 8.333) (writing took 14.677312097999675 seconds)
2020-10-12 20:38:18 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 20:38:18 | INFO | train | epoch 035 | loss 7.549 | nll_loss 6.428 | ppl 86.11 | wps 13303.1 | ups 2.23 | wpb 5972.3 | bsz 259.4 | num_updates 3500 | lr 0.000175013 | gnorm 1.36 | clip 0 | train_wall 28 | wall 1358
2020-10-12 20:38:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 20:38:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 20:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19114.4921875Mb; avail=469459.64453125Mb
2020-10-12 20:38:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001143
2020-10-12 20:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006723
2020-10-12 20:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19114.43359375Mb; avail=469459.88671875Mb
2020-10-12 20:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000299
2020-10-12 20:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19114.43359375Mb; avail=469459.88671875Mb
2020-10-12 20:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.117648
2020-10-12 20:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.125715
2020-10-12 20:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19114.98046875Mb; avail=469459.5859375Mb
2020-10-12 20:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19113.02734375Mb; avail=469461.5390625Mb
2020-10-12 20:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004220
2020-10-12 20:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19112.53515625Mb; avail=469462.03125Mb
2020-10-12 20:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000245
2020-10-12 20:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19112.53515625Mb; avail=469462.03125Mb
2020-10-12 20:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.116872
2020-10-12 20:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.122390
2020-10-12 20:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19235.54296875Mb; avail=469339.2421875Mb
2020-10-12 20:38:18 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 20:38:47 | INFO | train_inner | epoch 036:    100 / 100 loss=7.442, nll_loss=6.304, ppl=79.02, wps=13340.8, ups=2.23, wpb=5972.3, bsz=259.4, num_updates=3600, lr=0.00018001, gnorm=1.37, clip=0, train_wall=28, wall=1387
2020-10-12 20:38:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17852.9375Mb; avail=470701.984375Mb
2020-10-12 20:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001113
2020-10-12 20:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.9375Mb; avail=470701.984375Mb
2020-10-12 20:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020332
2020-10-12 20:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.0390625Mb; avail=470701.13671875Mb
2020-10-12 20:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015157
2020-10-12 20:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037423
2020-10-12 20:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.03125Mb; avail=470701.2578125Mb
2020-10-12 20:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17852.96875Mb; avail=470701.37890625Mb
2020-10-12 20:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000733
2020-10-12 20:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.96875Mb; avail=470701.37890625Mb
2020-10-12 20:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020273
2020-10-12 20:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.02734375Mb; avail=470701.5Mb
2020-10-12 20:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015089
2020-10-12 20:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.036865
2020-10-12 20:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.078125Mb; avail=470701.2578125Mb
2020-10-12 20:38:48 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.338 | nll_loss 7.243 | ppl 151.46 | wps 53660.4 | wpb 2151.6 | bsz 88.1 | num_updates 3600 | best_loss 8.333
2020-10-12 20:38:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:39:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_last.pt (epoch 36 @ 3600 updates, score 8.338) (writing took 13.252990663000674 seconds)
2020-10-12 20:39:01 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 20:39:01 | INFO | train | epoch 036 | loss 7.442 | nll_loss 6.304 | ppl 79.02 | wps 13782.4 | ups 2.31 | wpb 5972.3 | bsz 259.4 | num_updates 3600 | lr 0.00018001 | gnorm 1.37 | clip 0 | train_wall 28 | wall 1401
2020-10-12 20:39:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 20:39:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 20:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19336.27734375Mb; avail=469218.86328125Mb
2020-10-12 20:39:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000988
2020-10-12 20:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005932
2020-10-12 20:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19335.78515625Mb; avail=469219.35546875Mb
2020-10-12 20:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000188
2020-10-12 20:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19335.78515625Mb; avail=469219.35546875Mb
2020-10-12 20:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.113862
2020-10-12 20:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.120958
2020-10-12 20:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19335.30859375Mb; avail=469219.83203125Mb
2020-10-12 20:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19335.30859375Mb; avail=469219.83203125Mb
2020-10-12 20:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004226
2020-10-12 20:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19335.30859375Mb; avail=469219.83203125Mb
2020-10-12 20:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000243
2020-10-12 20:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19335.30859375Mb; avail=469219.83203125Mb
2020-10-12 20:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.114881
2020-10-12 20:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.120374
2020-10-12 20:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19335.45703125Mb; avail=469219.46875Mb
2020-10-12 20:39:01 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 20:39:31 | INFO | train_inner | epoch 037:    100 / 100 loss=7.339, nll_loss=6.183, ppl=72.65, wps=13717.3, ups=2.3, wpb=5972.3, bsz=259.4, num_updates=3700, lr=0.000185008, gnorm=1.432, clip=0, train_wall=28, wall=1430
2020-10-12 20:39:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:39:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17811.4375Mb; avail=470743.625Mb
2020-10-12 20:39:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001176
2020-10-12 20:39:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.4375Mb; avail=470743.625Mb
2020-10-12 20:39:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020653
2020-10-12 20:39:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17798.63671875Mb; avail=470756.42578125Mb
2020-10-12 20:39:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015245
2020-10-12 20:39:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037861
2020-10-12 20:39:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17793.22265625Mb; avail=470761.83984375Mb
2020-10-12 20:39:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17793.22265625Mb; avail=470761.83984375Mb
2020-10-12 20:39:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000719
2020-10-12 20:39:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17793.22265625Mb; avail=470761.83984375Mb
2020-10-12 20:39:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020080
2020-10-12 20:39:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17793.22265625Mb; avail=470761.83984375Mb
2020-10-12 20:39:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015031
2020-10-12 20:39:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.036579
2020-10-12 20:39:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17793.22265625Mb; avail=470761.83984375Mb
2020-10-12 20:39:31 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.288 | nll_loss 7.187 | ppl 145.73 | wps 53604.1 | wpb 2151.6 | bsz 88.1 | num_updates 3700 | best_loss 8.288
2020-10-12 20:39:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:39:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 37 @ 3700 updates, score 8.288) (writing took 5.243852206000156 seconds)
2020-10-12 20:39:37 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 20:39:37 | INFO | train | epoch 037 | loss 7.339 | nll_loss 6.183 | ppl 72.65 | wps 16808.9 | ups 2.81 | wpb 5972.3 | bsz 259.4 | num_updates 3700 | lr 0.000185008 | gnorm 1.432 | clip 0 | train_wall 28 | wall 1436
2020-10-12 20:39:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 20:39:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17814.87109375Mb; avail=470739.4453125Mb
2020-10-12 20:39:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000696
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005431
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17814.87109375Mb; avail=470739.4453125Mb
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000215
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17814.83984375Mb; avail=470739.6875Mb
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101557
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107955
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17814.8359375Mb; avail=470740.11328125Mb
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17814.73828125Mb; avail=470740.11328125Mb
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003696
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17814.73828125Mb; avail=470740.11328125Mb
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17814.73828125Mb; avail=470740.11328125Mb
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102242
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106871
2020-10-12 20:39:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17829.3515625Mb; avail=470725.12890625Mb
2020-10-12 20:39:37 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 20:40:06 | INFO | train_inner | epoch 038:    100 / 100 loss=7.217, nll_loss=6.041, ppl=65.84, wps=16816.6, ups=2.82, wpb=5972.3, bsz=259.4, num_updates=3800, lr=0.000190005, gnorm=1.371, clip=0, train_wall=28, wall=1466
2020-10-12 20:40:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17818.1796875Mb; avail=470736.39453125Mb
2020-10-12 20:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001204
2020-10-12 20:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.1796875Mb; avail=470736.39453125Mb
2020-10-12 20:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020514
2020-10-12 20:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.1796875Mb; avail=470736.39453125Mb
2020-10-12 20:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.018002
2020-10-12 20:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.040561
2020-10-12 20:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.1796875Mb; avail=470736.39453125Mb
2020-10-12 20:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17818.1484375Mb; avail=470736.03125Mb
2020-10-12 20:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001134
2020-10-12 20:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.1484375Mb; avail=470736.03125Mb
2020-10-12 20:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.021033
2020-10-12 20:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.08203125Mb; avail=470736.515625Mb
2020-10-12 20:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015115
2020-10-12 20:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038255
2020-10-12 20:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.08203125Mb; avail=470736.515625Mb
2020-10-12 20:40:07 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.267 | nll_loss 7.15 | ppl 142.05 | wps 53518.7 | wpb 2151.6 | bsz 88.1 | num_updates 3800 | best_loss 8.267
2020-10-12 20:40:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:40:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 38 @ 3800 updates, score 8.267) (writing took 4.470428347998677 seconds)
2020-10-12 20:40:11 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 20:40:11 | INFO | train | epoch 038 | loss 7.217 | nll_loss 6.041 | ppl 65.84 | wps 17180.7 | ups 2.88 | wpb 5972.3 | bsz 259.4 | num_updates 3800 | lr 0.000190005 | gnorm 1.371 | clip 0 | train_wall 28 | wall 1471
2020-10-12 20:40:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 20:40:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 20:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17818.9609375Mb; avail=470735.97265625Mb
2020-10-12 20:40:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000943
2020-10-12 20:40:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006674
2020-10-12 20:40:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.9609375Mb; avail=470735.97265625Mb
2020-10-12 20:40:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000215
2020-10-12 20:40:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.9609375Mb; avail=470735.97265625Mb
2020-10-12 20:40:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105949
2020-10-12 20:40:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113754
2020-10-12 20:40:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.85546875Mb; avail=470736.09375Mb
2020-10-12 20:40:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17818.37890625Mb; avail=470736.5859375Mb
2020-10-12 20:40:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003684
2020-10-12 20:40:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.359375Mb; avail=470736.5859375Mb
2020-10-12 20:40:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 20:40:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.4140625Mb; avail=470736.46484375Mb
2020-10-12 20:40:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102682
2020-10-12 20:40:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107286
2020-10-12 20:40:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.6015625Mb; avail=470736.7109375Mb
2020-10-12 20:40:12 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 20:40:41 | INFO | train_inner | epoch 039:    100 / 100 loss=7.122, nll_loss=5.929, ppl=60.94, wps=17243.4, ups=2.89, wpb=5972.3, bsz=259.4, num_updates=3900, lr=0.000195003, gnorm=1.448, clip=0, train_wall=28, wall=1501
2020-10-12 20:40:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:40:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19111.46484375Mb; avail=469443.7109375Mb
2020-10-12 20:40:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001207
2020-10-12 20:40:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19111.46484375Mb; avail=469443.7109375Mb
2020-10-12 20:40:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020599
2020-10-12 20:40:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19111.4921875Mb; avail=469443.58984375Mb
2020-10-12 20:40:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015262
2020-10-12 20:40:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037862
2020-10-12 20:40:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19111.5078125Mb; avail=469443.46875Mb
2020-10-12 20:40:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19111.44921875Mb; avail=469443.7109375Mb
2020-10-12 20:40:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000705
2020-10-12 20:40:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19111.44921875Mb; avail=469443.7109375Mb
2020-10-12 20:40:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020194
2020-10-12 20:40:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19110.96875Mb; avail=469443.58984375Mb
2020-10-12 20:40:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015084
2020-10-12 20:40:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.036752
2020-10-12 20:40:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19110.96875Mb; avail=469443.58984375Mb
2020-10-12 20:40:42 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.212 | nll_loss 7.076 | ppl 134.96 | wps 51741.9 | wpb 2151.6 | bsz 88.1 | num_updates 3900 | best_loss 8.212
2020-10-12 20:40:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:40:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 39 @ 3900 updates, score 8.212) (writing took 11.547170988998914 seconds)
2020-10-12 20:40:53 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 20:40:53 | INFO | train | epoch 039 | loss 7.122 | nll_loss 5.929 | ppl 60.94 | wps 14319.4 | ups 2.4 | wpb 5972.3 | bsz 259.4 | num_updates 3900 | lr 0.000195003 | gnorm 1.448 | clip 0 | train_wall 28 | wall 1513
2020-10-12 20:40:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 20:40:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 20:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17821.98046875Mb; avail=470732.26171875Mb
2020-10-12 20:40:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000931
2020-10-12 20:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006860
2020-10-12 20:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.09375Mb; avail=470732.1484375Mb
2020-10-12 20:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000268
2020-10-12 20:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.09375Mb; avail=470732.1484375Mb
2020-10-12 20:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106775
2020-10-12 20:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114728
2020-10-12 20:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.0078125Mb; avail=470732.140625Mb
2020-10-12 20:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17821.7421875Mb; avail=470732.51171875Mb
2020-10-12 20:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003692
2020-10-12 20:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17821.7421875Mb; avail=470732.51171875Mb
2020-10-12 20:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 20:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17821.7421875Mb; avail=470732.51171875Mb
2020-10-12 20:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100557
2020-10-12 20:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105152
2020-10-12 20:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17821.72265625Mb; avail=470732.6328125Mb
2020-10-12 20:40:53 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 20:41:22 | INFO | train_inner | epoch 040:    100 / 100 loss=7.002, nll_loss=5.789, ppl=55.28, wps=14366.3, ups=2.41, wpb=5972.3, bsz=259.4, num_updates=4000, lr=0.0002, gnorm=1.381, clip=0, train_wall=28, wall=1542
2020-10-12 20:41:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:41:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17846.2890625Mb; avail=470707.71875Mb
2020-10-12 20:41:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001121
2020-10-12 20:41:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17846.2890625Mb; avail=470707.71875Mb
2020-10-12 20:41:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020658
2020-10-12 20:41:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17846.26171875Mb; avail=470707.83984375Mb
2020-10-12 20:41:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015422
2020-10-12 20:41:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.038006
2020-10-12 20:41:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17846.26171875Mb; avail=470707.83984375Mb
2020-10-12 20:41:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17846.26171875Mb; avail=470707.83984375Mb
2020-10-12 20:41:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000769
2020-10-12 20:41:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17846.26171875Mb; avail=470707.83984375Mb
2020-10-12 20:41:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020195
2020-10-12 20:41:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17846.26171875Mb; avail=470707.83984375Mb
2020-10-12 20:41:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.015307
2020-10-12 20:41:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.037025
2020-10-12 20:41:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17846.26953125Mb; avail=470707.71875Mb
2020-10-12 20:41:23 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.195 | nll_loss 7.042 | ppl 131.81 | wps 53811.1 | wpb 2151.6 | bsz 88.1 | num_updates 4000 | best_loss 8.195
2020-10-12 20:41:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:41:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azehye_sepspm8000/O2M/checkpoint_best.pt (epoch 40 @ 4000 updates, score 8.195) (writing took 4.456878589999178 seconds)
2020-10-12 20:41:28 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 20:41:28 | INFO | train | epoch 040 | loss 7.002 | nll_loss 5.789 | ppl 55.28 | wps 17337.4 | ups 2.9 | wpb 5972.3 | bsz 259.4 | num_updates 4000 | lr 0.0002 | gnorm 1.381 | clip 0 | train_wall 28 | wall 1547
2020-10-12 20:41:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 20:41:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 20:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17884.01171875Mb; avail=470669.8125Mb
2020-10-12 20:41:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000683
2020-10-12 20:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005315
2020-10-12 20:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17883.83984375Mb; avail=470669.93359375Mb
2020-10-12 20:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000197
2020-10-12 20:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17883.83984375Mb; avail=470669.93359375Mb
2020-10-12 20:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101367
2020-10-12 20:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107635
2020-10-12 20:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17883.3828125Mb; avail=470670.0625Mb
2020-10-12 20:41:28 | INFO | fairseq_cli.train | done training in 1547.4 seconds
