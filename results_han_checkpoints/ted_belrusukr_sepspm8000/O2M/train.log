2020-10-12 10:44:03 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_belrusukr_sepspm8000/O2M/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok='tgt', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='eng-bel,eng-rus,eng-ukr', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 10:44:03 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 10:44:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['bel', 'eng', 'rus', 'ukr']
2020-10-12 10:44:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bel] dictionary: 27012 types
2020-10-12 10:44:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 27012 types
2020-10-12 10:44:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | [rus] dictionary: 27012 types
2020-10-12 10:44:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | [ukr] dictionary: 27012 types
2020-10-12 10:44:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 10:44:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:44:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 10:44:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:eng-bel': 1, 'main:eng-rus': 1, 'main:eng-ukr': 1}
2020-10-12 10:44:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-bel src_langtok: 27008; tgt_langtok: None
2020-10-12 10:44:04 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/O2M/valid.eng-bel.eng
2020-10-12 10:44:04 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/O2M/valid.eng-bel.bel
2020-10-12 10:44:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrusukr_sepspm8000/O2M/ valid eng-bel 248 examples
2020-10-12 10:44:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-rus src_langtok: 27010; tgt_langtok: None
2020-10-12 10:44:04 | INFO | fairseq.data.data_utils | loaded 4814 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/O2M/valid.eng-rus.eng
2020-10-12 10:44:04 | INFO | fairseq.data.data_utils | loaded 4814 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/O2M/valid.eng-rus.rus
2020-10-12 10:44:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrusukr_sepspm8000/O2M/ valid eng-rus 4814 examples
2020-10-12 10:44:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-ukr src_langtok: 27011; tgt_langtok: None
2020-10-12 10:44:04 | INFO | fairseq.data.data_utils | loaded 3060 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/O2M/valid.eng-ukr.eng
2020-10-12 10:44:04 | INFO | fairseq.data.data_utils | loaded 3060 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/O2M/valid.eng-ukr.ukr
2020-10-12 10:44:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrusukr_sepspm8000/O2M/ valid eng-ukr 3060 examples
2020-10-12 10:44:04 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(27012, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(27012, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=27012, bias=False)
  )
)
2020-10-12 10:44:04 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 10:44:04 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 10:44:04 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 10:44:04 | INFO | fairseq_cli.train | num. model params: 45373440 (num. trained: 45373440)
2020-10-12 10:44:06 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 10:44:06 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 10:44:06 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 10:44:06 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.732 GB ; name = GeForce RTX 2080 Ti                     
2020-10-12 10:44:06 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 10:44:06 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 10:44:06 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 10:44:06 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_last.pt
2020-10-12 10:44:06 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 10:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 10:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:44:06 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 10:44:06 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:eng-bel': 1, 'main:eng-rus': 1, 'main:eng-ukr': 1}
2020-10-12 10:44:06 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-bel src_langtok: 27008; tgt_langtok: None
2020-10-12 10:44:06 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/O2M/train.eng-bel.eng
2020-10-12 10:44:06 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/O2M/train.eng-bel.bel
2020-10-12 10:44:06 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrusukr_sepspm8000/O2M/ train eng-bel 4509 examples
2020-10-12 10:44:06 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-rus src_langtok: 27010; tgt_langtok: None
2020-10-12 10:44:06 | INFO | fairseq.data.data_utils | loaded 208397 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/O2M/train.eng-rus.eng
2020-10-12 10:44:06 | INFO | fairseq.data.data_utils | loaded 208397 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/O2M/train.eng-rus.rus
2020-10-12 10:44:06 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrusukr_sepspm8000/O2M/ train eng-rus 208397 examples
2020-10-12 10:44:06 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-ukr src_langtok: 27011; tgt_langtok: None
2020-10-12 10:44:06 | INFO | fairseq.data.data_utils | loaded 108463 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/O2M/train.eng-ukr.eng
2020-10-12 10:44:06 | INFO | fairseq.data.data_utils | loaded 108463 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/O2M/train.eng-ukr.ukr
2020-10-12 10:44:06 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrusukr_sepspm8000/O2M/ train eng-ukr 108463 examples
2020-10-12 10:44:06 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:eng-bel', 4509), ('main:eng-rus', 208397), ('main:eng-ukr', 108463)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 10:44:06 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 10:44:06 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 321369
2020-10-12 10:44:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 321369; virtual dataset size 321369
2020-10-12 10:44:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 10:44:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:eng-bel': 4509, 'main:eng-rus': 208397, 'main:eng-ukr': 108463}; raw total size: 321369
2020-10-12 10:44:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:eng-bel': 4509, 'main:eng-rus': 208397, 'main:eng-ukr': 108463}; resampled total size: 321369
2020-10-12 10:44:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 10:44:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.022157
2020-10-12 10:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:44:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003781
2020-10-12 10:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044209
2020-10-12 10:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002029
2020-10-12 10:44:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.703858
2020-10-12 10:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.750628
2020-10-12 10:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:44:07 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 10:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035375
2020-10-12 10:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001952
2020-10-12 10:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.703844
2020-10-12 10:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.741758
2020-10-12 10:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:44:08 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 10:44:42 | INFO | train_inner | epoch 001:    100 / 1138 loss=14.575, nll_loss=14.475, ppl=22766.9, wps=21714.5, ups=2.93, wpb=7401.7, bsz=273, num_updates=100, lr=5.0975e-06, gnorm=3.791, clip=0, train_wall=34, wall=36
2020-10-12 10:45:17 | INFO | train_inner | epoch 001:    200 / 1138 loss=13.42, nll_loss=13.186, ppl=9318.23, wps=20994, ups=2.85, wpb=7355.9, bsz=292.2, num_updates=200, lr=1.0095e-05, gnorm=1.527, clip=0, train_wall=35, wall=71
2020-10-12 10:45:53 | INFO | train_inner | epoch 001:    300 / 1138 loss=12.771, nll_loss=12.466, ppl=5658.79, wps=21016.1, ups=2.82, wpb=7456.4, bsz=300.7, num_updates=300, lr=1.50925e-05, gnorm=1.433, clip=0, train_wall=35, wall=107
2020-10-12 10:46:28 | INFO | train_inner | epoch 001:    400 / 1138 loss=12.061, nll_loss=11.665, ppl=3248.3, wps=20761.8, ups=2.83, wpb=7323.4, bsz=286.4, num_updates=400, lr=2.009e-05, gnorm=1.49, clip=0, train_wall=35, wall=142
2020-10-12 10:47:04 | INFO | train_inner | epoch 001:    500 / 1138 loss=11.51, nll_loss=11.019, ppl=2075.16, wps=20885.3, ups=2.79, wpb=7490.1, bsz=268.6, num_updates=500, lr=2.50875e-05, gnorm=1.348, clip=0, train_wall=35, wall=178
2020-10-12 10:47:39 | INFO | train_inner | epoch 001:    600 / 1138 loss=11.094, nll_loss=10.514, ppl=1461.79, wps=20388.3, ups=2.81, wpb=7266.7, bsz=266, num_updates=600, lr=3.0085e-05, gnorm=1.304, clip=0, train_wall=35, wall=213
2020-10-12 10:48:15 | INFO | train_inner | epoch 001:    700 / 1138 loss=10.901, nll_loss=10.269, ppl=1234.27, wps=20853.3, ups=2.79, wpb=7468.2, bsz=301.1, num_updates=700, lr=3.50825e-05, gnorm=1.289, clip=0, train_wall=35, wall=249
2020-10-12 10:48:51 | INFO | train_inner | epoch 001:    800 / 1138 loss=10.763, nll_loss=10.102, ppl=1099.18, wps=20458.4, ups=2.82, wpb=7252.8, bsz=268.4, num_updates=800, lr=4.008e-05, gnorm=1.107, clip=0, train_wall=35, wall=285
2020-10-12 10:49:26 | INFO | train_inner | epoch 001:    900 / 1138 loss=10.649, nll_loss=9.97, ppl=1002.8, wps=20732.7, ups=2.81, wpb=7377.3, bsz=277.5, num_updates=900, lr=4.50775e-05, gnorm=1.137, clip=0, train_wall=35, wall=320
2020-10-12 10:50:02 | INFO | train_inner | epoch 001:   1000 / 1138 loss=10.522, nll_loss=9.822, ppl=905.3, wps=21119.8, ups=2.76, wpb=7664, bsz=298.8, num_updates=1000, lr=5.0075e-05, gnorm=1.164, clip=0, train_wall=36, wall=356
2020-10-12 10:50:38 | INFO | train_inner | epoch 001:   1100 / 1138 loss=10.341, nll_loss=9.613, ppl=782.82, wps=20830.2, ups=2.82, wpb=7396.2, bsz=284.2, num_updates=1100, lr=5.50725e-05, gnorm=1.188, clip=0, train_wall=35, wall=392
2020-10-12 10:50:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 10:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001025
2020-10-12 10:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071029
2020-10-12 10:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050967
2020-10-12 10:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123352
2020-10-12 10:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000930
2020-10-12 10:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072105
2020-10-12 10:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050102
2020-10-12 10:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123460
2020-10-12 10:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
/home/han/Documents/11737-hw/assign2/fairseq/fairseq/utils.py:340: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2020-10-12 10:50:57 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.05 | nll_loss 9.255 | ppl 611.04 | wps 44859.8 | wpb 2418.3 | bsz 92.3 | num_updates 1138
2020-10-12 10:50:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 10:50:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 1 @ 1138 updates, score 10.05) (writing took 1.0601687189773656 seconds)
2020-10-12 10:50:58 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 10:50:58 | INFO | train | epoch 001 | loss 11.645 | nll_loss 11.136 | ppl 2250.69 | wps 20566.8 | ups 2.78 | wpb 7410.9 | bsz 282.4 | num_updates 1138 | lr 5.69716e-05 | gnorm 1.506 | clip 0 | train_wall 399 | wall 412
2020-10-12 10:50:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 10:50:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 10:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:50:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004037
2020-10-12 10:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044638
2020-10-12 10:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001886
2020-10-12 10:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:50:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.710126
2020-10-12 10:50:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.757177
2020-10-12 10:50:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:50:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:50:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035936
2020-10-12 10:50:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:50:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001972
2020-10-12 10:50:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:51:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.712168
2020-10-12 10:51:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.750611
2020-10-12 10:51:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:51:00 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 10:51:22 | INFO | train_inner | epoch 002:     62 / 1138 loss=10.336, nll_loss=9.601, ppl=776.59, wps=17089.4, ups=2.29, wpb=7452.4, bsz=267.4, num_updates=1200, lr=6.007e-05, gnorm=1.034, clip=0, train_wall=35, wall=436
2020-10-12 10:51:57 | INFO | train_inner | epoch 002:    162 / 1138 loss=10.252, nll_loss=9.502, ppl=724.84, wps=20885.5, ups=2.79, wpb=7476.4, bsz=270, num_updates=1300, lr=6.50675e-05, gnorm=1.069, clip=0, train_wall=35, wall=471
2020-10-12 10:52:34 | INFO | train_inner | epoch 002:    262 / 1138 loss=10.143, nll_loss=9.374, ppl=663.66, wps=20752.4, ups=2.76, wpb=7515.2, bsz=305.4, num_updates=1400, lr=7.0065e-05, gnorm=1.073, clip=0, train_wall=36, wall=508
2020-10-12 10:53:09 | INFO | train_inner | epoch 002:    362 / 1138 loss=10.092, nll_loss=9.314, ppl=636.4, wps=20678.3, ups=2.8, wpb=7397.4, bsz=277.9, num_updates=1500, lr=7.50625e-05, gnorm=1.119, clip=0, train_wall=35, wall=543
2020-10-12 10:53:45 | INFO | train_inner | epoch 002:    462 / 1138 loss=10.004, nll_loss=9.212, ppl=592.92, wps=20579.8, ups=2.8, wpb=7346.9, bsz=286.3, num_updates=1600, lr=8.006e-05, gnorm=1.199, clip=0, train_wall=35, wall=579
2020-10-12 10:54:21 | INFO | train_inner | epoch 002:    562 / 1138 loss=9.931, nll_loss=9.126, ppl=558.68, wps=20596.9, ups=2.82, wpb=7306.1, bsz=268.1, num_updates=1700, lr=8.50575e-05, gnorm=1.135, clip=0, train_wall=35, wall=615
2020-10-12 10:54:56 | INFO | train_inner | epoch 002:    662 / 1138 loss=9.826, nll_loss=9.007, ppl=514.45, wps=20966.7, ups=2.82, wpb=7440.6, bsz=290.3, num_updates=1800, lr=9.0055e-05, gnorm=1.2, clip=0, train_wall=35, wall=650
2020-10-12 10:55:32 | INFO | train_inner | epoch 002:    762 / 1138 loss=9.763, nll_loss=8.933, ppl=488.66, wps=20567.6, ups=2.79, wpb=7378.3, bsz=258.6, num_updates=1900, lr=9.50525e-05, gnorm=1.084, clip=0, train_wall=35, wall=686
2020-10-12 10:56:08 | INFO | train_inner | epoch 002:    862 / 1138 loss=9.625, nll_loss=8.777, ppl=438.64, wps=20695.3, ups=2.8, wpb=7381, bsz=315.5, num_updates=2000, lr=0.00010005, gnorm=1.233, clip=0, train_wall=35, wall=722
2020-10-12 10:56:43 | INFO | train_inner | epoch 002:    962 / 1138 loss=9.53, nll_loss=8.666, ppl=406.1, wps=20649.9, ups=2.8, wpb=7386.9, bsz=287.9, num_updates=2100, lr=0.000105048, gnorm=1.209, clip=0, train_wall=35, wall=757
2020-10-12 10:57:19 | INFO | train_inner | epoch 002:   1062 / 1138 loss=9.511, nll_loss=8.642, ppl=399.48, wps=20562, ups=2.78, wpb=7408.6, bsz=268.4, num_updates=2200, lr=0.000110045, gnorm=1.155, clip=0, train_wall=36, wall=793
2020-10-12 10:57:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 10:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001083
2020-10-12 10:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070110
2020-10-12 10:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050935
2020-10-12 10:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122472
2020-10-12 10:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000938
2020-10-12 10:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068542
2020-10-12 10:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050972
2020-10-12 10:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120777
2020-10-12 10:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:57:52 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.081 | nll_loss 8.113 | ppl 276.95 | wps 45083.1 | wpb 2418.3 | bsz 92.3 | num_updates 2276 | best_loss 9.081
2020-10-12 10:57:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 10:57:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 2 @ 2276 updates, score 9.081) (writing took 1.595931781019317 seconds)
2020-10-12 10:57:53 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 10:57:53 | INFO | train | epoch 002 | loss 9.86 | nll_loss 9.046 | ppl 528.64 | wps 20299.1 | ups 2.74 | wpb 7410.9 | bsz 282.4 | num_updates 2276 | lr 0.000113843 | gnorm 1.135 | clip 0 | train_wall 402 | wall 827
2020-10-12 10:57:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 10:57:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 10:57:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:57:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003837
2020-10-12 10:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.046906
2020-10-12 10:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002127
2020-10-12 10:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.703098
2020-10-12 10:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.752685
2020-10-12 10:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034480
2020-10-12 10:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001770
2020-10-12 10:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.699343
2020-10-12 10:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.736132
2020-10-12 10:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:57:55 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 10:58:03 | INFO | train_inner | epoch 003:     24 / 1138 loss=9.391, nll_loss=8.503, ppl=362.84, wps=17075.2, ups=2.27, wpb=7528, bsz=280.4, num_updates=2300, lr=0.000115043, gnorm=1.012, clip=0, train_wall=35, wall=837
2020-10-12 10:58:39 | INFO | train_inner | epoch 003:    124 / 1138 loss=9.312, nll_loss=8.412, ppl=340.58, wps=20724, ups=2.83, wpb=7315.4, bsz=280.2, num_updates=2400, lr=0.00012004, gnorm=1.109, clip=0, train_wall=35, wall=873
2020-10-12 10:59:15 | INFO | train_inner | epoch 003:    224 / 1138 loss=9.263, nll_loss=8.354, ppl=327.26, wps=20733.3, ups=2.8, wpb=7409.7, bsz=272, num_updates=2500, lr=0.000125037, gnorm=1.085, clip=0, train_wall=35, wall=908
2020-10-12 10:59:51 | INFO | train_inner | epoch 003:    324 / 1138 loss=9.189, nll_loss=8.268, ppl=308.24, wps=20892.6, ups=2.77, wpb=7551, bsz=283.6, num_updates=2600, lr=0.000130035, gnorm=1.08, clip=0, train_wall=36, wall=945
2020-10-12 11:00:26 | INFO | train_inner | epoch 003:    424 / 1138 loss=9.143, nll_loss=8.214, ppl=296.83, wps=20751.5, ups=2.81, wpb=7376.6, bsz=257.6, num_updates=2700, lr=0.000135032, gnorm=1.044, clip=0, train_wall=35, wall=980
2020-10-12 11:01:02 | INFO | train_inner | epoch 003:    524 / 1138 loss=8.974, nll_loss=8.021, ppl=259.8, wps=20488.5, ups=2.79, wpb=7341.3, bsz=299.7, num_updates=2800, lr=0.00014003, gnorm=1.098, clip=0, train_wall=35, wall=1016
2020-10-12 11:01:38 | INFO | train_inner | epoch 003:    624 / 1138 loss=8.968, nll_loss=8.013, ppl=258.3, wps=20771.1, ups=2.78, wpb=7462.4, bsz=300.3, num_updates=2900, lr=0.000145028, gnorm=1.071, clip=0, train_wall=35, wall=1052
2020-10-12 11:02:14 | INFO | train_inner | epoch 003:    724 / 1138 loss=8.959, nll_loss=8.001, ppl=256.17, wps=20492.5, ups=2.81, wpb=7288.2, bsz=256.6, num_updates=3000, lr=0.000150025, gnorm=1.04, clip=0, train_wall=35, wall=1088
2020-10-12 11:02:50 | INFO | train_inner | epoch 003:    824 / 1138 loss=8.892, nll_loss=7.924, ppl=242.85, wps=20831.6, ups=2.78, wpb=7502.3, bsz=275.3, num_updates=3100, lr=0.000155023, gnorm=1.075, clip=0, train_wall=36, wall=1124
2020-10-12 11:03:25 | INFO | train_inner | epoch 003:    924 / 1138 loss=8.822, nll_loss=7.844, ppl=229.77, wps=20484.7, ups=2.8, wpb=7314.4, bsz=268.3, num_updates=3200, lr=0.00016002, gnorm=1.02, clip=0, train_wall=35, wall=1159
2020-10-12 11:04:01 | INFO | train_inner | epoch 003:   1024 / 1138 loss=8.687, nll_loss=7.692, ppl=206.75, wps=20591.5, ups=2.76, wpb=7459.1, bsz=304.6, num_updates=3300, lr=0.000165018, gnorm=1.122, clip=0, train_wall=36, wall=1195
2020-10-12 11:04:38 | INFO | train_inner | epoch 003:   1124 / 1138 loss=8.584, nll_loss=7.573, ppl=190.36, wps=20702.3, ups=2.76, wpb=7488.3, bsz=315.4, num_updates=3400, lr=0.000170015, gnorm=1.017, clip=0, train_wall=36, wall=1232
2020-10-12 11:04:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 11:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001071
2020-10-12 11:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070178
2020-10-12 11:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049924
2020-10-12 11:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121512
2020-10-12 11:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000944
2020-10-12 11:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068435
2020-10-12 11:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050435
2020-10-12 11:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120135
2020-10-12 11:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:04:48 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.273 | nll_loss 7.167 | ppl 143.73 | wps 44727.9 | wpb 2418.3 | bsz 92.3 | num_updates 3414 | best_loss 8.273
2020-10-12 11:04:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 11:04:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 3 @ 3414 updates, score 8.273) (writing took 1.665403516992228 seconds)
2020-10-12 11:04:49 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 11:04:49 | INFO | train | epoch 003 | loss 8.985 | nll_loss 8.033 | ppl 261.88 | wps 20276.9 | ups 2.74 | wpb 7410.9 | bsz 282.4 | num_updates 3414 | lr 0.000170715 | gnorm 1.069 | clip 0 | train_wall 403 | wall 1243
2020-10-12 11:04:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 11:04:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 11:04:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:04:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003557
2020-10-12 11:04:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.042639
2020-10-12 11:04:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:04:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001881
2020-10-12 11:04:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:04:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.686108
2020-10-12 11:04:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.731161
2020-10-12 11:04:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:04:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:04:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034207
2020-10-12 11:04:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:04:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001782
2020-10-12 11:04:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:04:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.696050
2020-10-12 11:04:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.732562
2020-10-12 11:04:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:04:51 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 11:05:21 | INFO | train_inner | epoch 004:     86 / 1138 loss=8.548, nll_loss=7.531, ppl=184.9, wps=17035.7, ups=2.3, wpb=7410.8, bsz=275.3, num_updates=3500, lr=0.000175013, gnorm=1.088, clip=0, train_wall=35, wall=1275
2020-10-12 11:05:57 | INFO | train_inner | epoch 004:    186 / 1138 loss=8.488, nll_loss=7.46, ppl=176.1, wps=20973.8, ups=2.78, wpb=7547.6, bsz=274.3, num_updates=3600, lr=0.00018001, gnorm=1.07, clip=0, train_wall=36, wall=1311
2020-10-12 11:06:33 | INFO | train_inner | epoch 004:    286 / 1138 loss=8.436, nll_loss=7.4, ppl=168.92, wps=20691.9, ups=2.82, wpb=7350.5, bsz=274.9, num_updates=3700, lr=0.000185008, gnorm=1.116, clip=0, train_wall=35, wall=1347
2020-10-12 11:07:08 | INFO | train_inner | epoch 004:    386 / 1138 loss=8.366, nll_loss=7.32, ppl=159.74, wps=20804.7, ups=2.79, wpb=7448.3, bsz=269.4, num_updates=3800, lr=0.000190005, gnorm=1.064, clip=0, train_wall=35, wall=1382
2020-10-12 11:07:44 | INFO | train_inner | epoch 004:    486 / 1138 loss=8.188, nll_loss=7.117, ppl=138.84, wps=20341.1, ups=2.8, wpb=7272.8, bsz=297.7, num_updates=3900, lr=0.000195003, gnorm=1.169, clip=0, train_wall=35, wall=1418
2020-10-12 11:08:20 | INFO | train_inner | epoch 004:    586 / 1138 loss=8.159, nll_loss=7.082, ppl=135.49, wps=20650.4, ups=2.79, wpb=7391.6, bsz=280.3, num_updates=4000, lr=0.0002, gnorm=1.072, clip=0, train_wall=35, wall=1454
2020-10-12 11:08:56 | INFO | train_inner | epoch 004:    686 / 1138 loss=8.097, nll_loss=7.01, ppl=128.9, wps=20482.6, ups=2.77, wpb=7403.1, bsz=295.3, num_updates=4100, lr=0.000197546, gnorm=1.141, clip=0, train_wall=36, wall=1490
2020-10-12 11:09:33 | INFO | train_inner | epoch 004:    786 / 1138 loss=8.067, nll_loss=6.974, ppl=125.68, wps=20836.5, ups=2.75, wpb=7586.6, bsz=277.6, num_updates=4200, lr=0.00019518, gnorm=1.179, clip=0, train_wall=36, wall=1527
2020-10-12 11:10:08 | INFO | train_inner | epoch 004:    886 / 1138 loss=7.935, nll_loss=6.825, ppl=113.38, wps=20464.5, ups=2.81, wpb=7283, bsz=287.2, num_updates=4300, lr=0.000192897, gnorm=1.116, clip=0, train_wall=35, wall=1562
2020-10-12 11:10:44 | INFO | train_inner | epoch 004:    986 / 1138 loss=7.842, nll_loss=6.717, ppl=105.23, wps=20638.3, ups=2.77, wpb=7440.9, bsz=292.1, num_updates=4400, lr=0.000190693, gnorm=1.116, clip=0, train_wall=36, wall=1598
2020-10-12 11:11:20 | INFO | train_inner | epoch 004:   1086 / 1138 loss=7.785, nll_loss=6.651, ppl=100.51, wps=20596.7, ups=2.81, wpb=7333, bsz=284.8, num_updates=4500, lr=0.000188562, gnorm=1.131, clip=0, train_wall=35, wall=1634
2020-10-12 11:11:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 11:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001003
2020-10-12 11:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070107
2020-10-12 11:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050414
2020-10-12 11:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121854
2020-10-12 11:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000870
2020-10-12 11:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070300
2020-10-12 11:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050628
2020-10-12 11:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122114
2020-10-12 11:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:11:44 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.381 | nll_loss 6.119 | ppl 69.51 | wps 44345.5 | wpb 2418.3 | bsz 92.3 | num_updates 4552 | best_loss 7.381
2020-10-12 11:11:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 11:11:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 4 @ 4552 updates, score 7.381) (writing took 1.6089286020142026 seconds)
2020-10-12 11:11:45 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 11:11:45 | INFO | train | epoch 004 | loss 8.15 | nll_loss 7.072 | ppl 134.54 | wps 20278.6 | ups 2.74 | wpb 7410.9 | bsz 282.4 | num_updates 4552 | lr 0.000187482 | gnorm 1.112 | clip 0 | train_wall 403 | wall 1659
2020-10-12 11:11:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 11:11:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 11:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:11:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003732
2020-10-12 11:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.046229
2020-10-12 11:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002177
2020-10-12 11:11:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.695068
2020-10-12 11:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.744024
2020-10-12 11:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034066
2020-10-12 11:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001809
2020-10-12 11:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:11:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.685972
2020-10-12 11:11:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.722371
2020-10-12 11:11:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:11:47 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 11:12:04 | INFO | train_inner | epoch 005:     48 / 1138 loss=7.719, nll_loss=6.577, ppl=95.46, wps=16837.5, ups=2.28, wpb=7393.4, bsz=269.5, num_updates=4600, lr=0.000186501, gnorm=1.138, clip=0, train_wall=35, wall=1678
2020-10-12 11:12:39 | INFO | train_inner | epoch 005:    148 / 1138 loss=7.628, nll_loss=6.471, ppl=88.72, wps=20644.5, ups=2.8, wpb=7371, bsz=297.4, num_updates=4700, lr=0.000184506, gnorm=1.165, clip=0, train_wall=35, wall=1713
2020-10-12 11:13:15 | INFO | train_inner | epoch 005:    248 / 1138 loss=7.578, nll_loss=6.414, ppl=85.29, wps=20469.2, ups=2.78, wpb=7372.7, bsz=279.5, num_updates=4800, lr=0.000182574, gnorm=1.092, clip=0, train_wall=36, wall=1749
2020-10-12 11:13:51 | INFO | train_inner | epoch 005:    348 / 1138 loss=7.45, nll_loss=6.267, ppl=77.03, wps=20793.6, ups=2.83, wpb=7351.8, bsz=300.6, num_updates=4900, lr=0.000180702, gnorm=1.113, clip=0, train_wall=35, wall=1785
2020-10-12 11:14:27 | INFO | train_inner | epoch 005:    448 / 1138 loss=7.416, nll_loss=6.227, ppl=74.91, wps=20607.9, ups=2.79, wpb=7391, bsz=288.9, num_updates=5000, lr=0.000178885, gnorm=1.129, clip=0, train_wall=35, wall=1821
2020-10-12 11:15:03 | INFO | train_inner | epoch 005:    548 / 1138 loss=7.421, nll_loss=6.231, ppl=75.11, wps=20884.4, ups=2.77, wpb=7545.4, bsz=274.2, num_updates=5100, lr=0.000177123, gnorm=1.101, clip=0, train_wall=36, wall=1857
2020-10-12 11:15:39 | INFO | train_inner | epoch 005:    648 / 1138 loss=7.331, nll_loss=6.128, ppl=69.96, wps=20573.6, ups=2.79, wpb=7383.9, bsz=294.4, num_updates=5200, lr=0.000175412, gnorm=1.148, clip=0, train_wall=35, wall=1893
2020-10-12 11:16:14 | INFO | train_inner | epoch 005:    748 / 1138 loss=7.281, nll_loss=6.071, ppl=67.21, wps=20696.3, ups=2.8, wpb=7393.7, bsz=277.2, num_updates=5300, lr=0.000173749, gnorm=1.091, clip=0, train_wall=35, wall=1928
2020-10-12 11:16:51 | INFO | train_inner | epoch 005:    848 / 1138 loss=7.295, nll_loss=6.084, ppl=67.83, wps=20984.6, ups=2.74, wpb=7662.4, bsz=261.8, num_updates=5400, lr=0.000172133, gnorm=1.147, clip=0, train_wall=36, wall=1965
2020-10-12 11:17:27 | INFO | train_inner | epoch 005:    948 / 1138 loss=7.196, nll_loss=5.972, ppl=62.76, wps=20590.6, ups=2.77, wpb=7424.6, bsz=274.3, num_updates=5500, lr=0.000170561, gnorm=1.088, clip=0, train_wall=36, wall=2001
2020-10-12 11:18:03 | INFO | train_inner | epoch 005:   1048 / 1138 loss=7.125, nll_loss=5.889, ppl=59.28, wps=20636.4, ups=2.78, wpb=7431.8, bsz=290.2, num_updates=5600, lr=0.000169031, gnorm=1.062, clip=0, train_wall=36, wall=2037
2020-10-12 11:18:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 11:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001183
2020-10-12 11:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070891
2020-10-12 11:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050699
2020-10-12 11:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123116
2020-10-12 11:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000920
2020-10-12 11:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069627
2020-10-12 11:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051140
2020-10-12 11:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122013
2020-10-12 11:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:18:40 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.749 | nll_loss 5.368 | ppl 41.31 | wps 44786.4 | wpb 2418.3 | bsz 92.3 | num_updates 5690 | best_loss 6.749
2020-10-12 11:18:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 11:18:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 5 @ 5690 updates, score 6.749) (writing took 1.5840333219966851 seconds)
2020-10-12 11:18:41 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 11:18:41 | INFO | train | epoch 005 | loss 7.367 | nll_loss 6.169 | ppl 71.96 | wps 20272.3 | ups 2.74 | wpb 7410.9 | bsz 282.4 | num_updates 5690 | lr 0.000167689 | gnorm 1.122 | clip 0 | train_wall 403 | wall 2075
2020-10-12 11:18:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 11:18:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 11:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:18:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004113
2020-10-12 11:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.048300
2020-10-12 11:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002114
2020-10-12 11:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:18:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.692960
2020-10-12 11:18:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.743997
2020-10-12 11:18:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:18:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:18:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034628
2020-10-12 11:18:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:18:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001896
2020-10-12 11:18:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:18:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.686882
2020-10-12 11:18:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.723933
2020-10-12 11:18:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:18:43 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 11:18:46 | INFO | train_inner | epoch 006:     10 / 1138 loss=7.129, nll_loss=5.894, ppl=59.48, wps=16788.6, ups=2.3, wpb=7285.4, bsz=274.7, num_updates=5700, lr=0.000167542, gnorm=1.146, clip=0, train_wall=35, wall=2080
2020-10-12 11:19:22 | INFO | train_inner | epoch 006:    110 / 1138 loss=6.958, nll_loss=5.701, ppl=52.01, wps=21118.2, ups=2.82, wpb=7482.9, bsz=292.3, num_updates=5800, lr=0.000166091, gnorm=1.093, clip=0, train_wall=35, wall=2116
2020-10-12 11:19:58 | INFO | train_inner | epoch 006:    210 / 1138 loss=6.962, nll_loss=5.703, ppl=52.09, wps=20682.3, ups=2.79, wpb=7420.6, bsz=274, num_updates=5900, lr=0.000164677, gnorm=1.114, clip=0, train_wall=35, wall=2152
2020-10-12 11:20:33 | INFO | train_inner | epoch 006:    310 / 1138 loss=6.969, nll_loss=5.709, ppl=52.31, wps=20840.2, ups=2.81, wpb=7423.2, bsz=258.4, num_updates=6000, lr=0.000163299, gnorm=1.101, clip=0, train_wall=35, wall=2187
2020-10-12 11:21:09 | INFO | train_inner | epoch 006:    410 / 1138 loss=6.857, nll_loss=5.581, ppl=47.88, wps=20738, ups=2.79, wpb=7436.1, bsz=288.4, num_updates=6100, lr=0.000161955, gnorm=1.124, clip=0, train_wall=35, wall=2223
2020-10-12 11:21:45 | INFO | train_inner | epoch 006:    510 / 1138 loss=6.802, nll_loss=5.519, ppl=45.85, wps=20555.4, ups=2.78, wpb=7404.7, bsz=304, num_updates=6200, lr=0.000160644, gnorm=1.107, clip=0, train_wall=36, wall=2259
2020-10-12 11:22:21 | INFO | train_inner | epoch 006:    610 / 1138 loss=6.852, nll_loss=5.574, ppl=47.63, wps=20548.4, ups=2.78, wpb=7387.8, bsz=270.3, num_updates=6300, lr=0.000159364, gnorm=1.087, clip=0, train_wall=36, wall=2295
2020-10-12 11:22:57 | INFO | train_inner | epoch 006:    710 / 1138 loss=6.74, nll_loss=5.447, ppl=43.62, wps=20640.9, ups=2.81, wpb=7345.1, bsz=276.6, num_updates=6400, lr=0.000158114, gnorm=1.077, clip=0, train_wall=35, wall=2331
2020-10-12 11:23:33 | INFO | train_inner | epoch 006:    810 / 1138 loss=6.692, nll_loss=5.39, ppl=41.94, wps=20602.4, ups=2.77, wpb=7429.2, bsz=300.1, num_updates=6500, lr=0.000156893, gnorm=1.056, clip=0, train_wall=36, wall=2367
2020-10-12 11:24:08 | INFO | train_inner | epoch 006:    910 / 1138 loss=6.709, nll_loss=5.41, ppl=42.53, wps=20402.4, ups=2.81, wpb=7255.3, bsz=274.1, num_updates=6600, lr=0.0001557, gnorm=1.077, clip=0, train_wall=35, wall=2402
2020-10-12 11:24:44 | INFO | train_inner | epoch 006:   1010 / 1138 loss=6.632, nll_loss=5.322, ppl=40, wps=20766.4, ups=2.78, wpb=7479.7, bsz=299.1, num_updates=6700, lr=0.000154533, gnorm=1.06, clip=0, train_wall=36, wall=2438
2020-10-12 11:25:20 | INFO | train_inner | epoch 006:   1110 / 1138 loss=6.651, nll_loss=5.342, ppl=40.55, wps=20831.9, ups=2.78, wpb=7481.2, bsz=271.8, num_updates=6800, lr=0.000153393, gnorm=1.077, clip=0, train_wall=35, wall=2474
2020-10-12 11:25:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 11:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000985
2020-10-12 11:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068043
2020-10-12 11:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050151
2020-10-12 11:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119513
2020-10-12 11:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000905
2020-10-12 11:25:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:25:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068854
2020-10-12 11:25:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:25:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050526
2020-10-12 11:25:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120606
2020-10-12 11:25:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:25:35 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.331 | nll_loss 4.878 | ppl 29.41 | wps 45208 | wpb 2418.3 | bsz 92.3 | num_updates 6828 | best_loss 6.331
2020-10-12 11:25:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 11:25:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 6 @ 6828 updates, score 6.331) (writing took 1.6412388500175439 seconds)
2020-10-12 11:25:37 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 11:25:37 | INFO | train | epoch 006 | loss 6.8 | nll_loss 5.515 | ppl 45.74 | wps 20289.6 | ups 2.74 | wpb 7410.9 | bsz 282.4 | num_updates 6828 | lr 0.000153078 | gnorm 1.087 | clip 0 | train_wall 402 | wall 2491
2020-10-12 11:25:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 11:25:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 11:25:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:25:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003669
2020-10-12 11:25:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043842
2020-10-12 11:25:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:25:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001890
2020-10-12 11:25:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.708083
2020-10-12 11:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.754355
2020-10-12 11:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034360
2020-10-12 11:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001837
2020-10-12 11:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.687326
2020-10-12 11:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.724048
2020-10-12 11:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:25:38 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 11:26:04 | INFO | train_inner | epoch 007:     72 / 1138 loss=6.562, nll_loss=5.242, ppl=37.84, wps=16679.3, ups=2.3, wpb=7261.8, bsz=271.1, num_updates=6900, lr=0.000152277, gnorm=1.087, clip=0, train_wall=35, wall=2518
2020-10-12 11:26:39 | INFO | train_inner | epoch 007:    172 / 1138 loss=6.536, nll_loss=5.211, ppl=37.03, wps=20896, ups=2.81, wpb=7425.4, bsz=258.3, num_updates=7000, lr=0.000151186, gnorm=1.043, clip=0, train_wall=35, wall=2553
2020-10-12 11:27:15 | INFO | train_inner | epoch 007:    272 / 1138 loss=6.461, nll_loss=5.125, ppl=34.9, wps=20738.7, ups=2.81, wpb=7375.3, bsz=287.4, num_updates=7100, lr=0.000150117, gnorm=1.046, clip=0, train_wall=35, wall=2589
2020-10-12 11:27:51 | INFO | train_inner | epoch 007:    372 / 1138 loss=6.422, nll_loss=5.08, ppl=33.82, wps=20731.2, ups=2.78, wpb=7459.8, bsz=292.2, num_updates=7200, lr=0.000149071, gnorm=1.059, clip=0, train_wall=36, wall=2625
2020-10-12 11:28:27 | INFO | train_inner | epoch 007:    472 / 1138 loss=6.414, nll_loss=5.071, ppl=33.6, wps=20797.2, ups=2.78, wpb=7486.6, bsz=297, num_updates=7300, lr=0.000148047, gnorm=1.105, clip=0, train_wall=36, wall=2661
2020-10-12 11:29:03 | INFO | train_inner | epoch 007:    572 / 1138 loss=6.431, nll_loss=5.089, ppl=34.03, wps=20698, ups=2.8, wpb=7394.9, bsz=270.1, num_updates=7400, lr=0.000147043, gnorm=1.081, clip=0, train_wall=35, wall=2697
2020-10-12 11:29:39 | INFO | train_inner | epoch 007:    672 / 1138 loss=6.379, nll_loss=5.029, ppl=32.65, wps=20735.7, ups=2.79, wpb=7445.3, bsz=275.1, num_updates=7500, lr=0.000146059, gnorm=1.038, clip=0, train_wall=35, wall=2733
2020-10-12 11:30:14 | INFO | train_inner | epoch 007:    772 / 1138 loss=6.358, nll_loss=5.006, ppl=32.13, wps=20922, ups=2.8, wpb=7476.6, bsz=281.8, num_updates=7600, lr=0.000145095, gnorm=1.054, clip=0, train_wall=35, wall=2768
2020-10-12 11:30:50 | INFO | train_inner | epoch 007:    872 / 1138 loss=6.29, nll_loss=4.929, ppl=30.46, wps=20451.7, ups=2.8, wpb=7295.6, bsz=304.2, num_updates=7700, lr=0.00014415, gnorm=1.027, clip=0, train_wall=35, wall=2804
2020-10-12 11:31:26 | INFO | train_inner | epoch 007:    972 / 1138 loss=6.294, nll_loss=4.932, ppl=30.53, wps=20812.4, ups=2.77, wpb=7522.9, bsz=292.7, num_updates=7800, lr=0.000143223, gnorm=1.029, clip=0, train_wall=36, wall=2840
2020-10-12 11:32:02 | INFO | train_inner | epoch 007:   1072 / 1138 loss=6.282, nll_loss=4.918, ppl=30.23, wps=20779.7, ups=2.78, wpb=7474.8, bsz=284.2, num_updates=7900, lr=0.000142314, gnorm=1.032, clip=0, train_wall=36, wall=2876
2020-10-12 11:32:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 11:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001124
2020-10-12 11:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069605
2020-10-12 11:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050625
2020-10-12 11:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121708
2020-10-12 11:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000931
2020-10-12 11:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069062
2020-10-12 11:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050518
2020-10-12 11:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120832
2020-10-12 11:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:32:31 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.024 | nll_loss 4.522 | ppl 22.98 | wps 44936.4 | wpb 2418.3 | bsz 92.3 | num_updates 7966 | best_loss 6.024
2020-10-12 11:32:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 11:32:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 7 @ 7966 updates, score 6.024) (writing took 1.6698066950193606 seconds)
2020-10-12 11:32:32 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 11:32:32 | INFO | train | epoch 007 | loss 6.391 | nll_loss 5.044 | ppl 33 | wps 20306.5 | ups 2.74 | wpb 7410.9 | bsz 282.4 | num_updates 7966 | lr 0.000141723 | gnorm 1.055 | clip 0 | train_wall 402 | wall 2906
2020-10-12 11:32:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 11:32:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 11:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:32:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003992
2020-10-12 11:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.046331
2020-10-12 11:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001881
2020-10-12 11:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.705246
2020-10-12 11:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.754015
2020-10-12 11:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034398
2020-10-12 11:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001908
2020-10-12 11:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.709727
2020-10-12 11:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.746573
2020-10-12 11:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:32:34 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 11:32:46 | INFO | train_inner | epoch 008:     34 / 1138 loss=6.258, nll_loss=4.891, ppl=29.67, wps=16680.1, ups=2.28, wpb=7319.3, bsz=275.4, num_updates=8000, lr=0.000141421, gnorm=1.048, clip=0, train_wall=35, wall=2920
2020-10-12 11:33:22 | INFO | train_inner | epoch 008:    134 / 1138 loss=6.161, nll_loss=4.782, ppl=27.51, wps=20646.1, ups=2.8, wpb=7386.4, bsz=295.3, num_updates=8100, lr=0.000140546, gnorm=1.04, clip=0, train_wall=35, wall=2956
2020-10-12 11:33:58 | INFO | train_inner | epoch 008:    234 / 1138 loss=6.138, nll_loss=4.755, ppl=27, wps=20627.4, ups=2.79, wpb=7390.6, bsz=291.4, num_updates=8200, lr=0.000139686, gnorm=1.026, clip=0, train_wall=35, wall=2992
2020-10-12 11:34:33 | INFO | train_inner | epoch 008:    334 / 1138 loss=6.139, nll_loss=4.755, ppl=27, wps=20442.9, ups=2.82, wpb=7251.4, bsz=291.3, num_updates=8300, lr=0.000138842, gnorm=1.055, clip=0, train_wall=35, wall=3027
2020-10-12 11:35:09 | INFO | train_inner | epoch 008:    434 / 1138 loss=6.155, nll_loss=4.771, ppl=27.31, wps=20688.8, ups=2.78, wpb=7442.2, bsz=265.5, num_updates=8400, lr=0.000138013, gnorm=1.003, clip=0, train_wall=36, wall=3063
2020-10-12 11:35:45 | INFO | train_inner | epoch 008:    534 / 1138 loss=6.159, nll_loss=4.776, ppl=27.39, wps=20526.1, ups=2.79, wpb=7362.7, bsz=260.7, num_updates=8500, lr=0.000137199, gnorm=1.061, clip=0, train_wall=35, wall=3099
2020-10-12 11:36:21 | INFO | train_inner | epoch 008:    634 / 1138 loss=6.079, nll_loss=4.685, ppl=25.72, wps=20935.3, ups=2.79, wpb=7498.6, bsz=280.5, num_updates=8600, lr=0.000136399, gnorm=1.012, clip=0, train_wall=35, wall=3135
2020-10-12 11:36:56 | INFO | train_inner | epoch 008:    734 / 1138 loss=6.091, nll_loss=4.7, ppl=25.98, wps=20566, ups=2.81, wpb=7313.9, bsz=269.1, num_updates=8700, lr=0.000135613, gnorm=1.064, clip=0, train_wall=35, wall=3170
2020-10-12 11:37:32 | INFO | train_inner | epoch 008:    834 / 1138 loss=6.068, nll_loss=4.673, ppl=25.5, wps=20662.2, ups=2.78, wpb=7442.5, bsz=287.1, num_updates=8800, lr=0.00013484, gnorm=1.028, clip=0, train_wall=36, wall=3206
2020-10-12 11:38:09 | INFO | train_inner | epoch 008:    934 / 1138 loss=6.081, nll_loss=4.688, ppl=25.77, wps=20730.6, ups=2.76, wpb=7516.6, bsz=258.1, num_updates=8900, lr=0.00013408, gnorm=1.011, clip=0, train_wall=36, wall=3243
2020-10-12 11:38:44 | INFO | train_inner | epoch 008:   1034 / 1138 loss=5.997, nll_loss=4.592, ppl=24.12, wps=20771, ups=2.78, wpb=7464.8, bsz=314.6, num_updates=9000, lr=0.000133333, gnorm=1.001, clip=0, train_wall=36, wall=3278
2020-10-12 11:39:20 | INFO | train_inner | epoch 008:   1134 / 1138 loss=6.013, nll_loss=4.611, ppl=24.43, wps=20566.2, ups=2.79, wpb=7379, bsz=289.8, num_updates=9100, lr=0.000132599, gnorm=1.048, clip=0, train_wall=35, wall=3314
2020-10-12 11:39:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 11:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001087
2020-10-12 11:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070330
2020-10-12 11:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050298
2020-10-12 11:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122065
2020-10-12 11:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000964
2020-10-12 11:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069823
2020-10-12 11:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050076
2020-10-12 11:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121186
2020-10-12 11:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:39:27 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.824 | nll_loss 4.28 | ppl 19.43 | wps 44580.9 | wpb 2418.3 | bsz 92.3 | num_updates 9104 | best_loss 5.824
2020-10-12 11:39:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 11:39:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 8 @ 9104 updates, score 5.824) (writing took 1.5901829540089238 seconds)
2020-10-12 11:39:28 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 11:39:28 | INFO | train | epoch 008 | loss 6.1 | nll_loss 4.71 | ppl 26.17 | wps 20264.9 | ups 2.73 | wpb 7410.9 | bsz 282.4 | num_updates 9104 | lr 0.00013257 | gnorm 1.031 | clip 0 | train_wall 403 | wall 3322
2020-10-12 11:39:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 11:39:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 11:39:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:39:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003800
2020-10-12 11:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.045760
2020-10-12 11:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002034
2020-10-12 11:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.701812
2020-10-12 11:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.750157
2020-10-12 11:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034521
2020-10-12 11:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001839
2020-10-12 11:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:39:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.724021
2020-10-12 11:39:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.760906
2020-10-12 11:39:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:39:30 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 11:40:04 | INFO | train_inner | epoch 009:     96 / 1138 loss=5.918, nll_loss=4.503, ppl=22.67, wps=16721.9, ups=2.3, wpb=7283.9, bsz=293.8, num_updates=9200, lr=0.000131876, gnorm=1.012, clip=0, train_wall=35, wall=3358
2020-10-12 11:40:40 | INFO | train_inner | epoch 009:    196 / 1138 loss=5.901, nll_loss=4.483, ppl=22.37, wps=20854.7, ups=2.78, wpb=7495.8, bsz=298.5, num_updates=9300, lr=0.000131165, gnorm=1.005, clip=0, train_wall=36, wall=3394
2020-10-12 11:41:16 | INFO | train_inner | epoch 009:    296 / 1138 loss=5.929, nll_loss=4.514, ppl=22.85, wps=21023.3, ups=2.8, wpb=7514.6, bsz=275.4, num_updates=9400, lr=0.000130466, gnorm=1.022, clip=0, train_wall=35, wall=3430
2020-10-12 11:41:52 | INFO | train_inner | epoch 009:    396 / 1138 loss=5.927, nll_loss=4.511, ppl=22.8, wps=20588.3, ups=2.78, wpb=7396.9, bsz=283.5, num_updates=9500, lr=0.000129777, gnorm=1.06, clip=0, train_wall=35, wall=3466
2020-10-12 11:42:28 | INFO | train_inner | epoch 009:    496 / 1138 loss=5.879, nll_loss=4.458, ppl=21.97, wps=20717, ups=2.78, wpb=7452.2, bsz=283.4, num_updates=9600, lr=0.000129099, gnorm=0.99, clip=0, train_wall=36, wall=3501
2020-10-12 11:43:04 | INFO | train_inner | epoch 009:    596 / 1138 loss=5.913, nll_loss=4.494, ppl=22.53, wps=21084.6, ups=2.76, wpb=7644.7, bsz=270, num_updates=9700, lr=0.000128432, gnorm=0.998, clip=0, train_wall=36, wall=3538
2020-10-12 11:43:39 | INFO | train_inner | epoch 009:    696 / 1138 loss=5.886, nll_loss=4.465, ppl=22.09, wps=20433.4, ups=2.81, wpb=7261.5, bsz=278.2, num_updates=9800, lr=0.000127775, gnorm=1.039, clip=0, train_wall=35, wall=3573
2020-10-12 11:44:15 | INFO | train_inner | epoch 009:    796 / 1138 loss=5.872, nll_loss=4.449, ppl=21.83, wps=20631.4, ups=2.8, wpb=7366, bsz=271.5, num_updates=9900, lr=0.000127128, gnorm=1.008, clip=0, train_wall=35, wall=3609
2020-10-12 11:44:51 | INFO | train_inner | epoch 009:    896 / 1138 loss=5.849, nll_loss=4.424, ppl=21.46, wps=20381.5, ups=2.82, wpb=7232.9, bsz=282.5, num_updates=10000, lr=0.000126491, gnorm=1.01, clip=0, train_wall=35, wall=3644
2020-10-12 11:45:27 | INFO | train_inner | epoch 009:    996 / 1138 loss=5.831, nll_loss=4.403, ppl=21.15, wps=20642.3, ups=2.77, wpb=7460.7, bsz=292.2, num_updates=10100, lr=0.000125863, gnorm=1.002, clip=0, train_wall=36, wall=3681
2020-10-12 11:46:02 | INFO | train_inner | epoch 009:   1096 / 1138 loss=5.85, nll_loss=4.423, ppl=21.45, wps=20799.4, ups=2.79, wpb=7453.4, bsz=272.6, num_updates=10200, lr=0.000125245, gnorm=0.992, clip=0, train_wall=35, wall=3716
2020-10-12 11:46:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 11:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001140
2020-10-12 11:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069871
2020-10-12 11:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051282
2020-10-12 11:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122628
2020-10-12 11:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000880
2020-10-12 11:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070342
2020-10-12 11:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050510
2020-10-12 11:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122054
2020-10-12 11:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:46:23 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.657 | nll_loss 4.099 | ppl 17.13 | wps 44820.4 | wpb 2418.3 | bsz 92.3 | num_updates 10242 | best_loss 5.657
2020-10-12 11:46:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 11:46:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 9 @ 10242 updates, score 5.657) (writing took 1.7013688370061573 seconds)
2020-10-12 11:46:24 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 11:46:24 | INFO | train | epoch 009 | loss 5.883 | nll_loss 4.462 | ppl 22.04 | wps 20281.2 | ups 2.74 | wpb 7410.9 | bsz 282.4 | num_updates 10242 | lr 0.000124988 | gnorm 1.012 | clip 0 | train_wall 403 | wall 3738
2020-10-12 11:46:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 11:46:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 11:46:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:46:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004042
2020-10-12 11:46:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.048704
2020-10-12 11:46:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:46:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002142
2020-10-12 11:46:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.703546
2020-10-12 11:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.755106
2020-10-12 11:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034357
2020-10-12 11:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001882
2020-10-12 11:46:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.689043
2020-10-12 11:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.725814
2020-10-12 11:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:46:26 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 11:46:46 | INFO | train_inner | epoch 010:     58 / 1138 loss=5.767, nll_loss=4.33, ppl=20.12, wps=16917, ups=2.28, wpb=7427, bsz=278.3, num_updates=10300, lr=0.000124635, gnorm=0.99, clip=0, train_wall=35, wall=3760
2020-10-12 11:47:22 | INFO | train_inner | epoch 010:    158 / 1138 loss=5.73, nll_loss=4.288, ppl=19.54, wps=20730.8, ups=2.81, wpb=7370.4, bsz=293.1, num_updates=10400, lr=0.000124035, gnorm=1.003, clip=0, train_wall=35, wall=3796
2020-10-12 11:47:58 | INFO | train_inner | epoch 010:    258 / 1138 loss=5.738, nll_loss=4.296, ppl=19.65, wps=20708, ups=2.8, wpb=7407.3, bsz=274.7, num_updates=10500, lr=0.000123443, gnorm=0.993, clip=0, train_wall=35, wall=3832
2020-10-12 11:48:34 | INFO | train_inner | epoch 010:    358 / 1138 loss=5.672, nll_loss=4.222, ppl=18.66, wps=20431.3, ups=2.79, wpb=7331.9, bsz=314.8, num_updates=10600, lr=0.000122859, gnorm=1.005, clip=0, train_wall=35, wall=3868
2020-10-12 11:49:09 | INFO | train_inner | epoch 010:    458 / 1138 loss=5.741, nll_loss=4.3, ppl=19.7, wps=20707, ups=2.81, wpb=7359.3, bsz=287.1, num_updates=10700, lr=0.000122284, gnorm=1.036, clip=0, train_wall=35, wall=3903
2020-10-12 11:49:45 | INFO | train_inner | epoch 010:    558 / 1138 loss=5.736, nll_loss=4.293, ppl=19.6, wps=20968.9, ups=2.79, wpb=7520.3, bsz=290.7, num_updates=10800, lr=0.000121716, gnorm=1.039, clip=0, train_wall=35, wall=3939
2020-10-12 11:50:21 | INFO | train_inner | epoch 010:    658 / 1138 loss=5.685, nll_loss=4.235, ppl=18.84, wps=20859.6, ups=2.77, wpb=7518.1, bsz=280.3, num_updates=10900, lr=0.000121157, gnorm=0.964, clip=0, train_wall=36, wall=3975
2020-10-12 11:50:57 | INFO | train_inner | epoch 010:    758 / 1138 loss=5.727, nll_loss=4.284, ppl=19.47, wps=20408.9, ups=2.78, wpb=7343.9, bsz=267.9, num_updates=11000, lr=0.000120605, gnorm=1.011, clip=0, train_wall=36, wall=4011
2020-10-12 11:51:33 | INFO | train_inner | epoch 010:    858 / 1138 loss=5.712, nll_loss=4.267, ppl=19.26, wps=20425.2, ups=2.77, wpb=7363.6, bsz=268.6, num_updates=11100, lr=0.00012006, gnorm=0.993, clip=0, train_wall=36, wall=4047
2020-10-12 11:52:09 | INFO | train_inner | epoch 010:    958 / 1138 loss=5.724, nll_loss=4.28, ppl=19.43, wps=20761.5, ups=2.78, wpb=7462.9, bsz=255.8, num_updates=11200, lr=0.000119523, gnorm=1.007, clip=0, train_wall=36, wall=4083
2020-10-12 11:52:45 | INFO | train_inner | epoch 010:   1058 / 1138 loss=5.692, nll_loss=4.245, ppl=18.96, wps=20563.8, ups=2.81, wpb=7321.4, bsz=283.9, num_updates=11300, lr=0.000118993, gnorm=1.01, clip=0, train_wall=35, wall=4119
2020-10-12 11:53:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 11:53:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:53:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001113
2020-10-12 11:53:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072089
2020-10-12 11:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051645
2020-10-12 11:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125194
2020-10-12 11:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000982
2020-10-12 11:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069020
2020-10-12 11:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051265
2020-10-12 11:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121590
2020-10-12 11:53:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:53:19 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.566 | nll_loss 3.99 | ppl 15.89 | wps 44212.3 | wpb 2418.3 | bsz 92.3 | num_updates 11380 | best_loss 5.566
2020-10-12 11:53:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 11:53:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 10 @ 11380 updates, score 5.566) (writing took 1.937526819005143 seconds)
2020-10-12 11:53:21 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 11:53:21 | INFO | train | epoch 010 | loss 5.713 | nll_loss 4.268 | ppl 19.27 | wps 20261.9 | ups 2.73 | wpb 7410.9 | bsz 282.4 | num_updates 11380 | lr 0.000118574 | gnorm 1.003 | clip 0 | train_wall 403 | wall 4154
2020-10-12 11:53:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 11:53:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 11:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:53:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003568
2020-10-12 11:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043228
2020-10-12 11:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001879
2020-10-12 11:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.709058
2020-10-12 11:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.754700
2020-10-12 11:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 11:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034610
2020-10-12 11:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001853
2020-10-12 11:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:53:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.722915
2020-10-12 11:53:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.759908
2020-10-12 11:53:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 11:53:22 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 11:53:29 | INFO | train_inner | epoch 011:     20 / 1138 loss=5.645, nll_loss=4.191, ppl=18.26, wps=16877.8, ups=2.24, wpb=7520, bsz=296.2, num_updates=11400, lr=0.00011847, gnorm=0.988, clip=0, train_wall=35, wall=4163
2020-10-12 11:54:04 | INFO | train_inner | epoch 011:    120 / 1138 loss=5.586, nll_loss=4.125, ppl=17.45, wps=20804.1, ups=2.83, wpb=7342.4, bsz=283.4, num_updates=11500, lr=0.000117954, gnorm=0.981, clip=0, train_wall=35, wall=4198
2020-10-12 11:54:40 | INFO | train_inner | epoch 011:    220 / 1138 loss=5.575, nll_loss=4.111, ppl=17.28, wps=20502.4, ups=2.78, wpb=7368.9, bsz=291, num_updates=11600, lr=0.000117444, gnorm=0.983, clip=0, train_wall=36, wall=4234
2020-10-12 11:55:16 | INFO | train_inner | epoch 011:    320 / 1138 loss=5.571, nll_loss=4.107, ppl=17.24, wps=20807.4, ups=2.79, wpb=7457.9, bsz=298.9, num_updates=11700, lr=0.000116941, gnorm=1.001, clip=0, train_wall=35, wall=4270
2020-10-12 11:55:52 | INFO | train_inner | epoch 011:    420 / 1138 loss=5.575, nll_loss=4.111, ppl=17.29, wps=20748.6, ups=2.78, wpb=7453.7, bsz=275, num_updates=11800, lr=0.000116445, gnorm=0.973, clip=0, train_wall=35, wall=4306
2020-10-12 11:56:28 | INFO | train_inner | epoch 011:    520 / 1138 loss=5.576, nll_loss=4.112, ppl=17.29, wps=20590.9, ups=2.79, wpb=7392.5, bsz=283.8, num_updates=11900, lr=0.000115954, gnorm=1.008, clip=0, train_wall=35, wall=4342
2020-10-12 11:57:04 | INFO | train_inner | epoch 011:    620 / 1138 loss=5.574, nll_loss=4.109, ppl=17.26, wps=20718.9, ups=2.77, wpb=7479.8, bsz=272.7, num_updates=12000, lr=0.00011547, gnorm=0.992, clip=0, train_wall=36, wall=4378
2020-10-12 11:57:40 | INFO | train_inner | epoch 011:    720 / 1138 loss=5.577, nll_loss=4.113, ppl=17.31, wps=20622.7, ups=2.8, wpb=7371.4, bsz=285.7, num_updates=12100, lr=0.000114992, gnorm=0.991, clip=0, train_wall=35, wall=4414
2020-10-12 11:58:16 | INFO | train_inner | epoch 011:    820 / 1138 loss=5.562, nll_loss=4.097, ppl=17.11, wps=20786.1, ups=2.78, wpb=7468.9, bsz=291.3, num_updates=12200, lr=0.00011452, gnorm=0.99, clip=0, train_wall=35, wall=4450
2020-10-12 11:58:52 | INFO | train_inner | epoch 011:    920 / 1138 loss=5.556, nll_loss=4.09, ppl=17.03, wps=20302.7, ups=2.76, wpb=7367.6, bsz=282.7, num_updates=12300, lr=0.000114053, gnorm=0.991, clip=0, train_wall=36, wall=4486
2020-10-12 11:59:28 | INFO | train_inner | epoch 011:   1020 / 1138 loss=5.578, nll_loss=4.115, ppl=17.33, wps=20713.2, ups=2.78, wpb=7446.4, bsz=283.1, num_updates=12400, lr=0.000113592, gnorm=0.992, clip=0, train_wall=36, wall=4522
2020-10-12 12:00:04 | INFO | train_inner | epoch 011:   1120 / 1138 loss=5.577, nll_loss=4.114, ppl=17.31, wps=20510.9, ups=2.77, wpb=7391.5, bsz=260.6, num_updates=12500, lr=0.000113137, gnorm=0.988, clip=0, train_wall=36, wall=4558
2020-10-12 12:00:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 12:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001134
2020-10-12 12:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070460
2020-10-12 12:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:00:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050501
2020-10-12 12:00:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122441
2020-10-12 12:00:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:00:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:00:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000879
2020-10-12 12:00:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:00:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068492
2020-10-12 12:00:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:00:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051443
2020-10-12 12:00:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121139
2020-10-12 12:00:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:00:15 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 5.45 | nll_loss 3.865 | ppl 14.57 | wps 44669.1 | wpb 2418.3 | bsz 92.3 | num_updates 12518 | best_loss 5.45
2020-10-12 12:00:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 12:00:17 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 11 @ 12518 updates, score 5.45) (writing took 1.7154346089810133 seconds)
2020-10-12 12:00:17 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 12:00:17 | INFO | train | epoch 011 | loss 5.574 | nll_loss 4.11 | ppl 17.27 | wps 20238.6 | ups 2.73 | wpb 7410.9 | bsz 282.4 | num_updates 12518 | lr 0.000113056 | gnorm 0.991 | clip 0 | train_wall 403 | wall 4571
2020-10-12 12:00:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 12:00:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 12:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:00:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004094
2020-10-12 12:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.045628
2020-10-12 12:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001983
2020-10-12 12:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:00:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.694146
2020-10-12 12:00:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.742288
2020-10-12 12:00:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:00:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:00:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035305
2020-10-12 12:00:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:00:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001923
2020-10-12 12:00:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:00:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.686928
2020-10-12 12:00:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.724690
2020-10-12 12:00:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:00:19 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 12:00:48 | INFO | train_inner | epoch 012:     82 / 1138 loss=5.469, nll_loss=3.992, ppl=15.91, wps=16885.1, ups=2.28, wpb=7410.7, bsz=294.1, num_updates=12600, lr=0.000112687, gnorm=0.984, clip=0, train_wall=35, wall=4602
2020-10-12 12:01:24 | INFO | train_inner | epoch 012:    182 / 1138 loss=5.433, nll_loss=3.951, ppl=15.47, wps=20583, ups=2.79, wpb=7377.9, bsz=310.3, num_updates=12700, lr=0.000112243, gnorm=0.973, clip=0, train_wall=35, wall=4638
2020-10-12 12:02:00 | INFO | train_inner | epoch 012:    282 / 1138 loss=5.494, nll_loss=4.019, ppl=16.21, wps=20505.7, ups=2.8, wpb=7335.5, bsz=267, num_updates=12800, lr=0.000111803, gnorm=0.975, clip=0, train_wall=35, wall=4674
2020-10-12 12:02:35 | INFO | train_inner | epoch 012:    382 / 1138 loss=5.486, nll_loss=4.011, ppl=16.12, wps=20498.7, ups=2.8, wpb=7333.6, bsz=270.2, num_updates=12900, lr=0.000111369, gnorm=0.995, clip=0, train_wall=35, wall=4709
2020-10-12 12:03:11 | INFO | train_inner | epoch 012:    482 / 1138 loss=5.472, nll_loss=3.993, ppl=15.92, wps=20526.6, ups=2.78, wpb=7389.7, bsz=270, num_updates=13000, lr=0.00011094, gnorm=0.978, clip=0, train_wall=36, wall=4745
2020-10-12 12:03:47 | INFO | train_inner | epoch 012:    582 / 1138 loss=5.494, nll_loss=4.019, ppl=16.21, wps=20560.5, ups=2.78, wpb=7392.3, bsz=256.7, num_updates=13100, lr=0.000110516, gnorm=1.004, clip=0, train_wall=36, wall=4781
2020-10-12 12:04:23 | INFO | train_inner | epoch 012:    682 / 1138 loss=5.471, nll_loss=3.992, ppl=15.91, wps=20785.7, ups=2.78, wpb=7477.9, bsz=283.8, num_updates=13200, lr=0.000110096, gnorm=0.972, clip=0, train_wall=36, wall=4817
2020-10-12 12:05:00 | INFO | train_inner | epoch 012:    782 / 1138 loss=5.424, nll_loss=3.941, ppl=15.35, wps=20646.1, ups=2.76, wpb=7474.3, bsz=305.7, num_updates=13300, lr=0.000109682, gnorm=0.96, clip=0, train_wall=36, wall=4854
2020-10-12 12:05:35 | INFO | train_inner | epoch 012:    882 / 1138 loss=5.443, nll_loss=3.961, ppl=15.58, wps=20715.2, ups=2.8, wpb=7405.2, bsz=286.1, num_updates=13400, lr=0.000109272, gnorm=0.986, clip=0, train_wall=35, wall=4889
2020-10-12 12:06:11 | INFO | train_inner | epoch 012:    982 / 1138 loss=5.446, nll_loss=3.966, ppl=15.63, wps=20483.5, ups=2.8, wpb=7317.1, bsz=279.2, num_updates=13500, lr=0.000108866, gnorm=0.982, clip=0, train_wall=35, wall=4925
2020-10-12 12:06:47 | INFO | train_inner | epoch 012:   1082 / 1138 loss=5.429, nll_loss=3.946, ppl=15.41, wps=20896.9, ups=2.78, wpb=7529.4, bsz=289.7, num_updates=13600, lr=0.000108465, gnorm=0.964, clip=0, train_wall=36, wall=4961
2020-10-12 12:07:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 12:07:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:07:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001093
2020-10-12 12:07:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:07:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071375
2020-10-12 12:07:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:07:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050957
2020-10-12 12:07:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123767
2020-10-12 12:07:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:07:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:07:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000946
2020-10-12 12:07:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:07:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070849
2020-10-12 12:07:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:07:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051288
2020-10-12 12:07:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123410
2020-10-12 12:07:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:07:12 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.358 | nll_loss 3.752 | ppl 13.47 | wps 44737.6 | wpb 2418.3 | bsz 92.3 | num_updates 13656 | best_loss 5.358
2020-10-12 12:07:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 12:07:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 12 @ 13656 updates, score 5.358) (writing took 1.689673769986257 seconds)
2020-10-12 12:07:14 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 12:07:14 | INFO | train | epoch 012 | loss 5.458 | nll_loss 3.979 | ppl 15.77 | wps 20240.2 | ups 2.73 | wpb 7410.9 | bsz 282.4 | num_updates 13656 | lr 0.000108243 | gnorm 0.979 | clip 0 | train_wall 403 | wall 4988
2020-10-12 12:07:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 12:07:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 12:07:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:07:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003807
2020-10-12 12:07:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044480
2020-10-12 12:07:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:07:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001971
2020-10-12 12:07:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.687867
2020-10-12 12:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.734844
2020-10-12 12:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035373
2020-10-12 12:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001877
2020-10-12 12:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.716328
2020-10-12 12:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.754111
2020-10-12 12:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:07:15 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 12:07:31 | INFO | train_inner | epoch 013:     44 / 1138 loss=5.404, nll_loss=3.919, ppl=15.13, wps=16720.3, ups=2.28, wpb=7328.4, bsz=263.5, num_updates=13700, lr=0.000108069, gnorm=0.97, clip=0, train_wall=35, wall=5005
2020-10-12 12:08:07 | INFO | train_inner | epoch 013:    144 / 1138 loss=5.356, nll_loss=3.864, ppl=14.56, wps=20665.1, ups=2.78, wpb=7434.1, bsz=287.5, num_updates=13800, lr=0.000107676, gnorm=0.98, clip=0, train_wall=36, wall=5041
2020-10-12 12:08:42 | INFO | train_inner | epoch 013:    244 / 1138 loss=5.363, nll_loss=3.871, ppl=14.64, wps=20542.2, ups=2.81, wpb=7316.9, bsz=283.6, num_updates=13900, lr=0.000107288, gnorm=0.994, clip=0, train_wall=35, wall=5076
2020-10-12 12:09:18 | INFO | train_inner | epoch 013:    344 / 1138 loss=5.373, nll_loss=3.883, ppl=14.76, wps=20621.9, ups=2.81, wpb=7343.7, bsz=270.6, num_updates=14000, lr=0.000106904, gnorm=0.979, clip=0, train_wall=35, wall=5112
2020-10-12 12:09:54 | INFO | train_inner | epoch 013:    444 / 1138 loss=5.382, nll_loss=3.892, ppl=14.84, wps=20937.8, ups=2.78, wpb=7523.5, bsz=283.9, num_updates=14100, lr=0.000106525, gnorm=0.969, clip=0, train_wall=35, wall=5148
2020-10-12 12:10:30 | INFO | train_inner | epoch 013:    544 / 1138 loss=5.365, nll_loss=3.872, ppl=14.64, wps=20685.9, ups=2.8, wpb=7386.3, bsz=289.4, num_updates=14200, lr=0.000106149, gnorm=0.974, clip=0, train_wall=35, wall=5184
2020-10-12 12:11:06 | INFO | train_inner | epoch 013:    644 / 1138 loss=5.363, nll_loss=3.87, ppl=14.62, wps=21080.5, ups=2.77, wpb=7606, bsz=285.8, num_updates=14300, lr=0.000105777, gnorm=1.009, clip=0, train_wall=36, wall=5220
2020-10-12 12:11:42 | INFO | train_inner | epoch 013:    744 / 1138 loss=5.366, nll_loss=3.873, ppl=14.66, wps=20663, ups=2.8, wpb=7391.9, bsz=289.8, num_updates=14400, lr=0.000105409, gnorm=0.961, clip=0, train_wall=35, wall=5256
2020-10-12 12:12:17 | INFO | train_inner | epoch 013:    844 / 1138 loss=5.367, nll_loss=3.876, ppl=14.69, wps=20548.3, ups=2.81, wpb=7314.7, bsz=277.6, num_updates=14500, lr=0.000105045, gnorm=0.98, clip=0, train_wall=35, wall=5291
2020-10-12 12:12:53 | INFO | train_inner | epoch 013:    944 / 1138 loss=5.359, nll_loss=3.867, ppl=14.59, wps=20699.7, ups=2.78, wpb=7437.8, bsz=285.9, num_updates=14600, lr=0.000104685, gnorm=0.976, clip=0, train_wall=36, wall=5327
2020-10-12 12:13:29 | INFO | train_inner | epoch 013:   1044 / 1138 loss=5.372, nll_loss=3.88, ppl=14.72, wps=20903.2, ups=2.77, wpb=7538.6, bsz=273.4, num_updates=14700, lr=0.000104328, gnorm=0.964, clip=0, train_wall=36, wall=5363
2020-10-12 12:14:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 12:14:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:14:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001160
2020-10-12 12:14:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:14:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070063
2020-10-12 12:14:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050039
2020-10-12 12:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121607
2020-10-12 12:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000879
2020-10-12 12:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067500
2020-10-12 12:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048967
2020-10-12 12:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117665
2020-10-12 12:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:14:07 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 5.292 | nll_loss 3.682 | ppl 12.83 | wps 44998.9 | wpb 2418.3 | bsz 92.3 | num_updates 14794 | best_loss 5.292
2020-10-12 12:14:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 12:14:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 13 @ 14794 updates, score 5.292) (writing took 1.5932275529776234 seconds)
2020-10-12 12:14:09 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 12:14:09 | INFO | train | epoch 013 | loss 5.362 | nll_loss 3.87 | ppl 14.62 | wps 20314.2 | ups 2.74 | wpb 7410.9 | bsz 282.4 | num_updates 14794 | lr 0.000103996 | gnorm 0.976 | clip 0 | train_wall 402 | wall 5403
2020-10-12 12:14:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 12:14:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 12:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:14:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003946
2020-10-12 12:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.046690
2020-10-12 12:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002089
2020-10-12 12:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:14:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.691378
2020-10-12 12:14:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.740758
2020-10-12 12:14:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:14:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:14:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034817
2020-10-12 12:14:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:14:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001900
2020-10-12 12:14:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:14:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.699382
2020-10-12 12:14:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.736639
2020-10-12 12:14:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:14:11 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 12:14:13 | INFO | train_inner | epoch 014:      6 / 1138 loss=5.324, nll_loss=3.828, ppl=14.2, wps=16805.2, ups=2.29, wpb=7324.4, bsz=286.4, num_updates=14800, lr=0.000103975, gnorm=0.96, clip=0, train_wall=35, wall=5407
2020-10-12 12:14:48 | INFO | train_inner | epoch 014:    106 / 1138 loss=5.28, nll_loss=3.777, ppl=13.71, wps=20794.7, ups=2.82, wpb=7385.6, bsz=278.3, num_updates=14900, lr=0.000103626, gnorm=0.961, clip=0, train_wall=35, wall=5442
2020-10-12 12:15:24 | INFO | train_inner | epoch 014:    206 / 1138 loss=5.27, nll_loss=3.766, ppl=13.6, wps=20896.6, ups=2.81, wpb=7444.2, bsz=285.2, num_updates=15000, lr=0.00010328, gnorm=0.979, clip=0, train_wall=35, wall=5478
2020-10-12 12:16:00 | INFO | train_inner | epoch 014:    306 / 1138 loss=5.287, nll_loss=3.785, ppl=13.78, wps=20850.9, ups=2.79, wpb=7481.3, bsz=286.6, num_updates=15100, lr=0.000102937, gnorm=0.971, clip=0, train_wall=35, wall=5514
2020-10-12 12:16:35 | INFO | train_inner | epoch 014:    406 / 1138 loss=5.286, nll_loss=3.784, ppl=13.78, wps=20430.6, ups=2.82, wpb=7256, bsz=280, num_updates=15200, lr=0.000102598, gnorm=0.97, clip=0, train_wall=35, wall=5549
2020-10-12 12:17:11 | INFO | train_inner | epoch 014:    506 / 1138 loss=5.27, nll_loss=3.765, ppl=13.6, wps=20580.7, ups=2.77, wpb=7427.4, bsz=288.7, num_updates=15300, lr=0.000102262, gnorm=0.984, clip=0, train_wall=36, wall=5585
2020-10-12 12:17:47 | INFO | train_inner | epoch 014:    606 / 1138 loss=5.268, nll_loss=3.763, ppl=13.57, wps=20862.8, ups=2.8, wpb=7462.5, bsz=289.1, num_updates=15400, lr=0.000101929, gnorm=0.957, clip=0, train_wall=35, wall=5621
2020-10-12 12:18:23 | INFO | train_inner | epoch 014:    706 / 1138 loss=5.301, nll_loss=3.8, ppl=13.93, wps=20675.2, ups=2.81, wpb=7362.2, bsz=269.4, num_updates=15500, lr=0.0001016, gnorm=0.978, clip=0, train_wall=35, wall=5657
2020-10-12 12:18:59 | INFO | train_inner | epoch 014:    806 / 1138 loss=5.278, nll_loss=3.774, ppl=13.68, wps=20968.6, ups=2.8, wpb=7494.6, bsz=277.4, num_updates=15600, lr=0.000101274, gnorm=0.961, clip=0, train_wall=35, wall=5692
2020-10-12 12:19:34 | INFO | train_inner | epoch 014:    906 / 1138 loss=5.269, nll_loss=3.765, ppl=13.59, wps=20777.1, ups=2.79, wpb=7443.7, bsz=285.4, num_updates=15700, lr=0.000100951, gnorm=0.971, clip=0, train_wall=35, wall=5728
2020-10-12 12:20:10 | INFO | train_inner | epoch 014:   1006 / 1138 loss=5.302, nll_loss=3.802, ppl=13.95, wps=20729.1, ups=2.79, wpb=7436.5, bsz=271.8, num_updates=15800, lr=0.000100631, gnorm=0.983, clip=0, train_wall=35, wall=5764
2020-10-12 12:20:46 | INFO | train_inner | epoch 014:   1106 / 1138 loss=5.268, nll_loss=3.764, ppl=13.59, wps=20528.7, ups=2.79, wpb=7350, bsz=284.3, num_updates=15900, lr=0.000100314, gnorm=0.982, clip=0, train_wall=35, wall=5800
2020-10-12 12:20:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 12:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001213
2020-10-12 12:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070193
2020-10-12 12:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050632
2020-10-12 12:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122414
2020-10-12 12:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000960
2020-10-12 12:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070146
2020-10-12 12:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050505
2020-10-12 12:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121933
2020-10-12 12:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:21:03 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 5.242 | nll_loss 3.621 | ppl 12.3 | wps 45165.1 | wpb 2418.3 | bsz 92.3 | num_updates 15932 | best_loss 5.242
2020-10-12 12:21:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 12:21:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 14 @ 15932 updates, score 5.242) (writing took 1.9215622930205427 seconds)
2020-10-12 12:21:05 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 12:21:05 | INFO | train | epoch 014 | loss 5.279 | nll_loss 3.776 | ppl 13.7 | wps 20297.8 | ups 2.74 | wpb 7410.9 | bsz 282.4 | num_updates 15932 | lr 0.000100213 | gnorm 0.973 | clip 0 | train_wall 402 | wall 5819
2020-10-12 12:21:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 12:21:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 12:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:21:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003645
2020-10-12 12:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043447
2020-10-12 12:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001954
2020-10-12 12:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.697354
2020-10-12 12:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.743298
2020-10-12 12:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034889
2020-10-12 12:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001898
2020-10-12 12:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.722115
2020-10-12 12:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.759437
2020-10-12 12:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:21:06 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 12:21:30 | INFO | train_inner | epoch 015:     68 / 1138 loss=5.209, nll_loss=3.697, ppl=12.97, wps=16531.4, ups=2.26, wpb=7299.9, bsz=298.5, num_updates=16000, lr=0.0001, gnorm=0.977, clip=0, train_wall=35, wall=5844
2020-10-12 12:22:05 | INFO | train_inner | epoch 015:    168 / 1138 loss=5.209, nll_loss=3.697, ppl=12.97, wps=20667.3, ups=2.83, wpb=7297, bsz=279.5, num_updates=16100, lr=9.9689e-05, gnorm=0.977, clip=0, train_wall=35, wall=5879
2020-10-12 12:22:41 | INFO | train_inner | epoch 015:    268 / 1138 loss=5.219, nll_loss=3.708, ppl=13.07, wps=20632, ups=2.8, wpb=7373.5, bsz=270.1, num_updates=16200, lr=9.93808e-05, gnorm=0.968, clip=0, train_wall=35, wall=5915
2020-10-12 12:23:17 | INFO | train_inner | epoch 015:    368 / 1138 loss=5.185, nll_loss=3.67, ppl=12.73, wps=20657.6, ups=2.81, wpb=7345.9, bsz=295.5, num_updates=16300, lr=9.90755e-05, gnorm=0.968, clip=0, train_wall=35, wall=5951
2020-10-12 12:23:53 | INFO | train_inner | epoch 015:    468 / 1138 loss=5.232, nll_loss=3.722, ppl=13.2, wps=20642.1, ups=2.79, wpb=7409.6, bsz=264.6, num_updates=16400, lr=9.8773e-05, gnorm=0.98, clip=0, train_wall=35, wall=5987
2020-10-12 12:24:29 | INFO | train_inner | epoch 015:    568 / 1138 loss=5.186, nll_loss=3.671, ppl=12.74, wps=20818.7, ups=2.79, wpb=7472.1, bsz=310.6, num_updates=16500, lr=9.84732e-05, gnorm=0.987, clip=0, train_wall=35, wall=6023
2020-10-12 12:25:04 | INFO | train_inner | epoch 015:    668 / 1138 loss=5.208, nll_loss=3.696, ppl=12.96, wps=20754.4, ups=2.79, wpb=7442.9, bsz=278.3, num_updates=16600, lr=9.81761e-05, gnorm=0.954, clip=0, train_wall=35, wall=6058
2020-10-12 12:25:40 | INFO | train_inner | epoch 015:    768 / 1138 loss=5.195, nll_loss=3.681, ppl=12.83, wps=20735.1, ups=2.8, wpb=7400.9, bsz=293.4, num_updates=16700, lr=9.78818e-05, gnorm=0.983, clip=0, train_wall=35, wall=6094
2020-10-12 12:26:16 | INFO | train_inner | epoch 015:    868 / 1138 loss=5.225, nll_loss=3.713, ppl=13.12, wps=20816.5, ups=2.76, wpb=7551.3, bsz=269.3, num_updates=16800, lr=9.759e-05, gnorm=0.966, clip=0, train_wall=36, wall=6130
2020-10-12 12:26:52 | INFO | train_inner | epoch 015:    968 / 1138 loss=5.225, nll_loss=3.715, ppl=13.13, wps=20728.8, ups=2.79, wpb=7421.3, bsz=277.8, num_updates=16900, lr=9.73009e-05, gnorm=0.973, clip=0, train_wall=35, wall=6166
2020-10-12 12:27:28 | INFO | train_inner | epoch 015:   1068 / 1138 loss=5.2, nll_loss=3.687, ppl=12.88, wps=20612.4, ups=2.79, wpb=7389.9, bsz=273, num_updates=17000, lr=9.70143e-05, gnorm=0.965, clip=0, train_wall=35, wall=6202
2020-10-12 12:27:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 12:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001163
2020-10-12 12:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070116
2020-10-12 12:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049753
2020-10-12 12:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121379
2020-10-12 12:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000886
2020-10-12 12:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069453
2020-10-12 12:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050314
2020-10-12 12:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120967
2020-10-12 12:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:27:58 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 5.193 | nll_loss 3.573 | ppl 11.9 | wps 44625.3 | wpb 2418.3 | bsz 92.3 | num_updates 17070 | best_loss 5.193
2020-10-12 12:27:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 12:28:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 15 @ 17070 updates, score 5.193) (writing took 1.443868810980348 seconds)
2020-10-12 12:28:00 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 12:28:00 | INFO | train | epoch 015 | loss 5.206 | nll_loss 3.693 | ppl 12.94 | wps 20304.1 | ups 2.74 | wpb 7410.9 | bsz 282.4 | num_updates 17070 | lr 9.68151e-05 | gnorm 0.97 | clip 0 | train_wall 402 | wall 6234
2020-10-12 12:28:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 12:28:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 12:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:28:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004078
2020-10-12 12:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.047614
2020-10-12 12:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002191
2020-10-12 12:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.708312
2020-10-12 12:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.758728
2020-10-12 12:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034891
2020-10-12 12:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001894
2020-10-12 12:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.695567
2020-10-12 12:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.732881
2020-10-12 12:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:28:01 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 12:28:12 | INFO | train_inner | epoch 016:     30 / 1138 loss=5.16, nll_loss=3.642, ppl=12.48, wps=16978.1, ups=2.27, wpb=7469.1, bsz=292.3, num_updates=17100, lr=9.67302e-05, gnorm=0.95, clip=0, train_wall=35, wall=6246
2020-10-12 12:28:48 | INFO | train_inner | epoch 016:    130 / 1138 loss=5.138, nll_loss=3.616, ppl=12.26, wps=20617.1, ups=2.81, wpb=7330.1, bsz=276.5, num_updates=17200, lr=9.64486e-05, gnorm=0.967, clip=0, train_wall=35, wall=6282
2020-10-12 12:29:23 | INFO | train_inner | epoch 016:    230 / 1138 loss=5.124, nll_loss=3.6, ppl=12.13, wps=20742.3, ups=2.8, wpb=7408.3, bsz=299.4, num_updates=17300, lr=9.61694e-05, gnorm=0.951, clip=0, train_wall=35, wall=6317
2020-10-12 12:29:59 | INFO | train_inner | epoch 016:    330 / 1138 loss=5.139, nll_loss=3.616, ppl=12.26, wps=20918.2, ups=2.79, wpb=7491.2, bsz=269.1, num_updates=17400, lr=9.58927e-05, gnorm=0.959, clip=0, train_wall=35, wall=6353
2020-10-12 12:30:35 | INFO | train_inner | epoch 016:    430 / 1138 loss=5.125, nll_loss=3.601, ppl=12.14, wps=20538.9, ups=2.79, wpb=7373.3, bsz=291.3, num_updates=17500, lr=9.56183e-05, gnorm=0.964, clip=0, train_wall=35, wall=6389
2020-10-12 12:31:11 | INFO | train_inner | epoch 016:    530 / 1138 loss=5.131, nll_loss=3.608, ppl=12.2, wps=21015.4, ups=2.77, wpb=7577, bsz=295.7, num_updates=17600, lr=9.53463e-05, gnorm=0.972, clip=0, train_wall=36, wall=6425
2020-10-12 12:31:47 | INFO | train_inner | epoch 016:    630 / 1138 loss=5.15, nll_loss=3.63, ppl=12.38, wps=20220.2, ups=2.8, wpb=7219, bsz=275.9, num_updates=17700, lr=9.50765e-05, gnorm=0.997, clip=0, train_wall=35, wall=6461
2020-10-12 12:32:22 | INFO | train_inner | epoch 016:    730 / 1138 loss=5.126, nll_loss=3.603, ppl=12.15, wps=20638.1, ups=2.8, wpb=7365.9, bsz=276.2, num_updates=17800, lr=9.48091e-05, gnorm=0.966, clip=0, train_wall=35, wall=6496
2020-10-12 12:32:59 | INFO | train_inner | epoch 016:    830 / 1138 loss=5.156, nll_loss=3.637, ppl=12.44, wps=20581.5, ups=2.76, wpb=7460.6, bsz=278.5, num_updates=17900, lr=9.45439e-05, gnorm=0.954, clip=0, train_wall=36, wall=6533
2020-10-12 12:33:35 | INFO | train_inner | epoch 016:    930 / 1138 loss=5.147, nll_loss=3.627, ppl=12.35, wps=20907.1, ups=2.8, wpb=7478.7, bsz=287, num_updates=18000, lr=9.42809e-05, gnorm=0.942, clip=0, train_wall=35, wall=6568
2020-10-12 12:34:10 | INFO | train_inner | epoch 016:   1030 / 1138 loss=5.174, nll_loss=3.658, ppl=12.62, wps=20551.3, ups=2.81, wpb=7321.4, bsz=275.5, num_updates=18100, lr=9.40201e-05, gnorm=0.987, clip=0, train_wall=35, wall=6604
2020-10-12 12:34:46 | INFO | train_inner | epoch 016:   1130 / 1138 loss=5.134, nll_loss=3.613, ppl=12.23, wps=21019, ups=2.79, wpb=7522.1, bsz=280.4, num_updates=18200, lr=9.37614e-05, gnorm=0.957, clip=0, train_wall=35, wall=6640
2020-10-12 12:34:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 12:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001114
2020-10-12 12:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070205
2020-10-12 12:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051359
2020-10-12 12:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123019
2020-10-12 12:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000938
2020-10-12 12:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068309
2020-10-12 12:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050798
2020-10-12 12:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120365
2020-10-12 12:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:34:54 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 5.145 | nll_loss 3.52 | ppl 11.47 | wps 44546 | wpb 2418.3 | bsz 92.3 | num_updates 18208 | best_loss 5.145
2020-10-12 12:34:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 12:34:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 16 @ 18208 updates, score 5.145) (writing took 1.6037521609978285 seconds)
2020-10-12 12:34:55 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 12:34:55 | INFO | train | epoch 016 | loss 5.14 | nll_loss 3.619 | ppl 12.28 | wps 20294.9 | ups 2.74 | wpb 7410.9 | bsz 282.4 | num_updates 18208 | lr 9.37408e-05 | gnorm 0.965 | clip 0 | train_wall 402 | wall 6649
2020-10-12 12:34:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 12:34:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 12:34:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:34:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004091
2020-10-12 12:34:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.047949
2020-10-12 12:34:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:34:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002181
2020-10-12 12:34:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:34:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.697884
2020-10-12 12:34:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.748551
2020-10-12 12:34:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:34:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:34:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034700
2020-10-12 12:34:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:34:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001906
2020-10-12 12:34:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:34:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.725067
2020-10-12 12:34:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.762210
2020-10-12 12:34:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:34:57 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 12:35:30 | INFO | train_inner | epoch 017:     92 / 1138 loss=5.076, nll_loss=3.545, ppl=11.68, wps=17125.8, ups=2.27, wpb=7556.6, bsz=287.4, num_updates=18300, lr=9.35049e-05, gnorm=0.974, clip=0, train_wall=35, wall=6684
2020-10-12 12:36:06 | INFO | train_inner | epoch 017:    192 / 1138 loss=5.083, nll_loss=3.553, ppl=11.74, wps=21016.8, ups=2.78, wpb=7571.6, bsz=277.8, num_updates=18400, lr=9.32505e-05, gnorm=0.958, clip=0, train_wall=36, wall=6720
2020-10-12 12:36:42 | INFO | train_inner | epoch 017:    292 / 1138 loss=5.095, nll_loss=3.568, ppl=11.86, wps=20678.8, ups=2.8, wpb=7376.3, bsz=283.1, num_updates=18500, lr=9.29981e-05, gnorm=0.977, clip=0, train_wall=35, wall=6756
2020-10-12 12:37:18 | INFO | train_inner | epoch 017:    392 / 1138 loss=5.078, nll_loss=3.549, ppl=11.7, wps=20669.1, ups=2.77, wpb=7459.1, bsz=270.5, num_updates=18600, lr=9.27478e-05, gnorm=0.954, clip=0, train_wall=36, wall=6792
2020-10-12 12:37:54 | INFO | train_inner | epoch 017:    492 / 1138 loss=5.08, nll_loss=3.551, ppl=11.72, wps=20775, ups=2.79, wpb=7445.4, bsz=290.8, num_updates=18700, lr=9.24995e-05, gnorm=0.958, clip=0, train_wall=35, wall=6828
2020-10-12 12:38:29 | INFO | train_inner | epoch 017:    592 / 1138 loss=5.094, nll_loss=3.567, ppl=11.85, wps=20310.6, ups=2.81, wpb=7230.7, bsz=253.3, num_updates=18800, lr=9.22531e-05, gnorm=0.994, clip=0, train_wall=35, wall=6863
2020-10-12 12:39:05 | INFO | train_inner | epoch 017:    692 / 1138 loss=5.095, nll_loss=3.568, ppl=11.86, wps=20442.5, ups=2.78, wpb=7341, bsz=287.8, num_updates=18900, lr=9.20087e-05, gnorm=0.955, clip=0, train_wall=35, wall=6899
2020-10-12 12:39:41 | INFO | train_inner | epoch 017:    792 / 1138 loss=5.089, nll_loss=3.56, ppl=11.79, wps=20719.6, ups=2.79, wpb=7427.9, bsz=263.9, num_updates=19000, lr=9.17663e-05, gnorm=0.954, clip=0, train_wall=35, wall=6935
2020-10-12 12:40:16 | INFO | train_inner | epoch 017:    892 / 1138 loss=5.073, nll_loss=3.544, ppl=11.67, wps=20479.7, ups=2.82, wpb=7261.5, bsz=308.2, num_updates=19100, lr=9.15258e-05, gnorm=0.962, clip=0, train_wall=35, wall=6970
2020-10-12 12:40:52 | INFO | train_inner | epoch 017:    992 / 1138 loss=5.087, nll_loss=3.56, ppl=11.79, wps=20674.9, ups=2.81, wpb=7357.9, bsz=283.3, num_updates=19200, lr=9.12871e-05, gnorm=0.978, clip=0, train_wall=35, wall=7006
2020-10-12 12:41:28 | INFO | train_inner | epoch 017:   1092 / 1138 loss=5.086, nll_loss=3.558, ppl=11.78, wps=20470.8, ups=2.79, wpb=7328.9, bsz=280, num_updates=19300, lr=9.10503e-05, gnorm=0.974, clip=0, train_wall=35, wall=7042
2020-10-12 12:41:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001182
2020-10-12 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070540
2020-10-12 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050313
2020-10-12 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122383
2020-10-12 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000912
2020-10-12 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070681
2020-10-12 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050681
2020-10-12 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122594
2020-10-12 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:41:50 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 5.107 | nll_loss 3.473 | ppl 11.11 | wps 44652 | wpb 2418.3 | bsz 92.3 | num_updates 19346 | best_loss 5.107
2020-10-12 12:41:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 12:41:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 17 @ 19346 updates, score 5.107) (writing took 1.6679170749848709 seconds)
2020-10-12 12:41:51 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 12:41:51 | INFO | train | epoch 017 | loss 5.083 | nll_loss 3.554 | ppl 11.74 | wps 20275.5 | ups 2.74 | wpb 7410.9 | bsz 282.4 | num_updates 19346 | lr 9.0942e-05 | gnorm 0.966 | clip 0 | train_wall 403 | wall 7065
2020-10-12 12:41:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 12:41:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 12:41:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:41:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003924
2020-10-12 12:41:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044096
2020-10-12 12:41:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:41:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001881
2020-10-12 12:41:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.692546
2020-10-12 12:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.739051
2020-10-12 12:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034774
2020-10-12 12:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001884
2020-10-12 12:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.693364
2020-10-12 12:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.730552
2020-10-12 12:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:41:53 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 12:42:12 | INFO | train_inner | epoch 018:     54 / 1138 loss=5.029, nll_loss=3.494, ppl=11.27, wps=16854, ups=2.27, wpb=7428.4, bsz=295, num_updates=19400, lr=9.08153e-05, gnorm=0.951, clip=0, train_wall=35, wall=7086
2020-10-12 12:42:48 | INFO | train_inner | epoch 018:    154 / 1138 loss=5.032, nll_loss=3.495, ppl=11.28, wps=20908.3, ups=2.79, wpb=7502.7, bsz=269.4, num_updates=19500, lr=9.05822e-05, gnorm=0.96, clip=0, train_wall=35, wall=7122
2020-10-12 12:43:24 | INFO | train_inner | epoch 018:    254 / 1138 loss=4.989, nll_loss=3.449, ppl=10.92, wps=20821.1, ups=2.77, wpb=7516.1, bsz=325.1, num_updates=19600, lr=9.03508e-05, gnorm=0.945, clip=0, train_wall=36, wall=7158
2020-10-12 12:44:00 | INFO | train_inner | epoch 018:    354 / 1138 loss=5.042, nll_loss=3.508, ppl=11.37, wps=20723.2, ups=2.81, wpb=7371.9, bsz=262.1, num_updates=19700, lr=9.01212e-05, gnorm=0.969, clip=0, train_wall=35, wall=7193
2020-10-12 12:44:35 | INFO | train_inner | epoch 018:    454 / 1138 loss=5.014, nll_loss=3.476, ppl=11.13, wps=20836.8, ups=2.8, wpb=7449.6, bsz=294.2, num_updates=19800, lr=8.98933e-05, gnorm=0.962, clip=0, train_wall=35, wall=7229
2020-10-12 12:45:11 | INFO | train_inner | epoch 018:    554 / 1138 loss=5.038, nll_loss=3.504, ppl=11.34, wps=20414.9, ups=2.79, wpb=7327.9, bsz=286.5, num_updates=19900, lr=8.96672e-05, gnorm=0.994, clip=0, train_wall=35, wall=7265
2020-10-12 12:45:47 | INFO | train_inner | epoch 018:    654 / 1138 loss=5.034, nll_loss=3.498, ppl=11.3, wps=20533.1, ups=2.79, wpb=7370, bsz=292.2, num_updates=20000, lr=8.94427e-05, gnorm=0.964, clip=0, train_wall=35, wall=7301
2020-10-12 12:46:22 | INFO | train_inner | epoch 018:    754 / 1138 loss=5.046, nll_loss=3.512, ppl=11.41, wps=20826.5, ups=2.83, wpb=7359.2, bsz=256.7, num_updates=20100, lr=8.92199e-05, gnorm=0.968, clip=0, train_wall=35, wall=7336
2020-10-12 12:46:58 | INFO | train_inner | epoch 018:    854 / 1138 loss=5.011, nll_loss=3.474, ppl=11.11, wps=20894.4, ups=2.77, wpb=7536, bsz=302.4, num_updates=20200, lr=8.89988e-05, gnorm=0.935, clip=0, train_wall=36, wall=7372
2020-10-12 12:47:34 | INFO | train_inner | epoch 018:    954 / 1138 loss=5.051, nll_loss=3.518, ppl=11.45, wps=20723.4, ups=2.78, wpb=7450.4, bsz=264.4, num_updates=20300, lr=8.87794e-05, gnorm=0.971, clip=0, train_wall=36, wall=7408
2020-10-12 12:48:10 | INFO | train_inner | epoch 018:   1054 / 1138 loss=5.038, nll_loss=3.504, ppl=11.34, wps=20486.4, ups=2.78, wpb=7359.6, bsz=287.6, num_updates=20400, lr=8.85615e-05, gnorm=0.986, clip=0, train_wall=35, wall=7444
2020-10-12 12:48:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 12:48:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:48:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001101
2020-10-12 12:48:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:48:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069433
2020-10-12 12:48:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:48:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050240
2020-10-12 12:48:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121112
2020-10-12 12:48:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:48:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:48:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000936
2020-10-12 12:48:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:48:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070197
2020-10-12 12:48:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:48:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050998
2020-10-12 12:48:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122456
2020-10-12 12:48:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:48:46 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 5.072 | nll_loss 3.429 | ppl 10.77 | wps 44921.7 | wpb 2418.3 | bsz 92.3 | num_updates 20484 | best_loss 5.072
2020-10-12 12:48:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 12:48:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 18 @ 20484 updates, score 5.072) (writing took 1.4447697479918133 seconds)
2020-10-12 12:48:47 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 12:48:47 | INFO | train | epoch 018 | loss 5.031 | nll_loss 3.496 | ppl 11.28 | wps 20292.3 | ups 2.74 | wpb 7410.9 | bsz 282.4 | num_updates 20484 | lr 8.83797e-05 | gnorm 0.967 | clip 0 | train_wall 403 | wall 7481
2020-10-12 12:48:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 12:48:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 12:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:48:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003632
2020-10-12 12:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043772
2020-10-12 12:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001904
2020-10-12 12:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.698060
2020-10-12 12:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.744272
2020-10-12 12:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034347
2020-10-12 12:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001865
2020-10-12 12:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:48:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.721859
2020-10-12 12:48:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.758604
2020-10-12 12:48:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:48:49 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 12:48:54 | INFO | train_inner | epoch 019:     16 / 1138 loss=5.043, nll_loss=3.509, ppl=11.38, wps=16798, ups=2.28, wpb=7364.3, bsz=266.5, num_updates=20500, lr=8.83452e-05, gnorm=0.984, clip=0, train_wall=35, wall=7488
2020-10-12 12:49:30 | INFO | train_inner | epoch 019:    116 / 1138 loss=4.973, nll_loss=3.429, ppl=10.77, wps=20805.3, ups=2.81, wpb=7405.4, bsz=278.2, num_updates=20600, lr=8.81305e-05, gnorm=0.961, clip=0, train_wall=35, wall=7524
2020-10-12 12:50:05 | INFO | train_inner | epoch 019:    216 / 1138 loss=4.969, nll_loss=3.426, ppl=10.75, wps=20568.7, ups=2.8, wpb=7345.2, bsz=290.7, num_updates=20700, lr=8.79174e-05, gnorm=0.961, clip=0, train_wall=35, wall=7559
2020-10-12 12:50:42 | INFO | train_inner | epoch 019:    316 / 1138 loss=5.003, nll_loss=3.463, ppl=11.02, wps=20625.7, ups=2.77, wpb=7438.5, bsz=261, num_updates=20800, lr=8.77058e-05, gnorm=0.95, clip=0, train_wall=36, wall=7596
2020-10-12 12:51:18 | INFO | train_inner | epoch 019:    416 / 1138 loss=4.979, nll_loss=3.436, ppl=10.82, wps=20537.1, ups=2.77, wpb=7409.6, bsz=277.8, num_updates=20900, lr=8.74957e-05, gnorm=0.968, clip=0, train_wall=36, wall=7632
2020-10-12 12:51:53 | INFO | train_inner | epoch 019:    516 / 1138 loss=4.977, nll_loss=3.435, ppl=10.82, wps=20868.7, ups=2.81, wpb=7426.3, bsz=295.1, num_updates=21000, lr=8.72872e-05, gnorm=0.951, clip=0, train_wall=35, wall=7667
2020-10-12 12:52:29 | INFO | train_inner | epoch 019:    616 / 1138 loss=4.983, nll_loss=3.44, ppl=10.86, wps=20506.7, ups=2.77, wpb=7402.9, bsz=288.7, num_updates=21100, lr=8.70801e-05, gnorm=0.956, clip=0, train_wall=36, wall=7703
2020-10-12 12:53:05 | INFO | train_inner | epoch 019:    716 / 1138 loss=4.996, nll_loss=3.456, ppl=10.97, wps=20797.2, ups=2.78, wpb=7472.9, bsz=271.7, num_updates=21200, lr=8.68744e-05, gnorm=0.956, clip=0, train_wall=35, wall=7739
2020-10-12 12:53:41 | INFO | train_inner | epoch 019:    816 / 1138 loss=4.98, nll_loss=3.438, ppl=10.84, wps=20720, ups=2.8, wpb=7387.1, bsz=288, num_updates=21300, lr=8.66703e-05, gnorm=0.958, clip=0, train_wall=35, wall=7775
2020-10-12 12:54:17 | INFO | train_inner | epoch 019:    916 / 1138 loss=4.979, nll_loss=3.436, ppl=10.82, wps=20731.1, ups=2.77, wpb=7494.9, bsz=295, num_updates=21400, lr=8.64675e-05, gnorm=0.953, clip=0, train_wall=36, wall=7811
2020-10-12 12:54:53 | INFO | train_inner | epoch 019:   1016 / 1138 loss=4.987, nll_loss=3.446, ppl=10.9, wps=20312.6, ups=2.8, wpb=7261.3, bsz=270.6, num_updates=21500, lr=8.62662e-05, gnorm=0.969, clip=0, train_wall=35, wall=7847
2020-10-12 12:55:29 | INFO | train_inner | epoch 019:   1116 / 1138 loss=4.976, nll_loss=3.434, ppl=10.81, wps=20873.7, ups=2.78, wpb=7505.9, bsz=300.6, num_updates=21600, lr=8.60663e-05, gnorm=0.952, clip=0, train_wall=36, wall=7883
2020-10-12 12:55:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 12:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001154
2020-10-12 12:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070057
2020-10-12 12:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052623
2020-10-12 12:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124175
2020-10-12 12:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000925
2020-10-12 12:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068364
2020-10-12 12:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052191
2020-10-12 12:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121806
2020-10-12 12:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:55:42 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 5.041 | nll_loss 3.399 | ppl 10.55 | wps 44237.9 | wpb 2418.3 | bsz 92.3 | num_updates 21622 | best_loss 5.041
2020-10-12 12:55:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 12:55:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 19 @ 21622 updates, score 5.041) (writing took 2.0387167219887488 seconds)
2020-10-12 12:55:44 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 12:55:44 | INFO | train | epoch 019 | loss 4.983 | nll_loss 3.441 | ppl 10.86 | wps 20239.2 | ups 2.73 | wpb 7410.9 | bsz 282.4 | num_updates 21622 | lr 8.60225e-05 | gnorm 0.959 | clip 0 | train_wall 403 | wall 7898
2020-10-12 12:55:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 12:55:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 12:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:55:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003605
2020-10-12 12:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043380
2020-10-12 12:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001887
2020-10-12 12:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.710112
2020-10-12 12:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.755905
2020-10-12 12:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 12:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034783
2020-10-12 12:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001869
2020-10-12 12:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.697935
2020-10-12 12:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.735109
2020-10-12 12:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 12:55:45 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 12:56:13 | INFO | train_inner | epoch 020:     78 / 1138 loss=4.963, nll_loss=3.419, ppl=10.69, wps=16859.5, ups=2.26, wpb=7459.4, bsz=268.6, num_updates=21700, lr=8.58678e-05, gnorm=0.971, clip=0, train_wall=35, wall=7927
2020-10-12 12:56:49 | INFO | train_inner | epoch 020:    178 / 1138 loss=4.923, nll_loss=3.373, ppl=10.36, wps=20885.6, ups=2.8, wpb=7463.5, bsz=284.9, num_updates=21800, lr=8.56706e-05, gnorm=0.949, clip=0, train_wall=35, wall=7963
2020-10-12 12:57:25 | INFO | train_inner | epoch 020:    278 / 1138 loss=4.927, nll_loss=3.378, ppl=10.39, wps=20543.2, ups=2.79, wpb=7353.5, bsz=289.1, num_updates=21900, lr=8.54748e-05, gnorm=0.959, clip=0, train_wall=35, wall=7998
2020-10-12 12:58:00 | INFO | train_inner | epoch 020:    378 / 1138 loss=4.926, nll_loss=3.377, ppl=10.39, wps=20837.8, ups=2.8, wpb=7445.8, bsz=281.4, num_updates=22000, lr=8.52803e-05, gnorm=0.963, clip=0, train_wall=35, wall=8034
2020-10-12 12:58:36 | INFO | train_inner | epoch 020:    478 / 1138 loss=4.94, nll_loss=3.393, ppl=10.5, wps=20482.6, ups=2.82, wpb=7257.6, bsz=280.9, num_updates=22100, lr=8.50871e-05, gnorm=0.98, clip=0, train_wall=35, wall=8070
2020-10-12 12:59:12 | INFO | train_inner | epoch 020:    578 / 1138 loss=4.94, nll_loss=3.392, ppl=10.5, wps=20753.8, ups=2.77, wpb=7503.7, bsz=279.5, num_updates=22200, lr=8.48953e-05, gnorm=0.943, clip=0, train_wall=36, wall=8106
2020-10-12 12:59:48 | INFO | train_inner | epoch 020:    678 / 1138 loss=4.951, nll_loss=3.405, ppl=10.59, wps=20727.4, ups=2.78, wpb=7444.1, bsz=289.3, num_updates=22300, lr=8.47047e-05, gnorm=0.955, clip=0, train_wall=35, wall=8142
2020-10-12 13:00:24 | INFO | train_inner | epoch 020:    778 / 1138 loss=4.946, nll_loss=3.399, ppl=10.55, wps=20593.1, ups=2.79, wpb=7372, bsz=268.1, num_updates=22400, lr=8.45154e-05, gnorm=0.969, clip=0, train_wall=35, wall=8178
2020-10-12 13:01:00 | INFO | train_inner | epoch 020:    878 / 1138 loss=4.947, nll_loss=3.4, ppl=10.55, wps=20861.6, ups=2.78, wpb=7504.5, bsz=282.6, num_updates=22500, lr=8.43274e-05, gnorm=0.957, clip=0, train_wall=36, wall=8213
2020-10-12 13:01:35 | INFO | train_inner | epoch 020:    978 / 1138 loss=4.928, nll_loss=3.379, ppl=10.4, wps=20737.5, ups=2.79, wpb=7436.6, bsz=298.9, num_updates=22600, lr=8.41406e-05, gnorm=0.981, clip=0, train_wall=35, wall=8249
2020-10-12 13:02:11 | INFO | train_inner | epoch 020:   1078 / 1138 loss=4.957, nll_loss=3.413, ppl=10.65, wps=20508.2, ups=2.81, wpb=7307.9, bsz=287.2, num_updates=22700, lr=8.39551e-05, gnorm=0.968, clip=0, train_wall=35, wall=8285
2020-10-12 13:02:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 13:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001094
2020-10-12 13:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069200
2020-10-12 13:02:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:02:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050483
2020-10-12 13:02:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121110
2020-10-12 13:02:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:02:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:02:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000955
2020-10-12 13:02:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:02:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069283
2020-10-12 13:02:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:02:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049613
2020-10-12 13:02:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120165
2020-10-12 13:02:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:02:37 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 5.023 | nll_loss 3.376 | ppl 10.38 | wps 45025.1 | wpb 2418.3 | bsz 92.3 | num_updates 22760 | best_loss 5.023
2020-10-12 13:02:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 13:02:39 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 20 @ 22760 updates, score 5.023) (writing took 1.6036824870097917 seconds)
2020-10-12 13:02:39 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 13:02:39 | INFO | train | epoch 020 | loss 4.941 | nll_loss 3.394 | ppl 10.51 | wps 20305.9 | ups 2.74 | wpb 7410.9 | bsz 282.4 | num_updates 22760 | lr 8.38444e-05 | gnorm 0.964 | clip 0 | train_wall 402 | wall 8313
2020-10-12 13:02:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 13:02:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 13:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:02:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003907
2020-10-12 13:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.048066
2020-10-12 13:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002128
2020-10-12 13:02:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:02:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.701811
2020-10-12 13:02:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.752726
2020-10-12 13:02:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:02:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:02:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034312
2020-10-12 13:02:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:02:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001877
2020-10-12 13:02:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:02:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.721021
2020-10-12 13:02:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.757740
2020-10-12 13:02:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:02:41 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 13:02:55 | INFO | train_inner | epoch 021:     40 / 1138 loss=4.944, nll_loss=3.396, ppl=10.53, wps=16984.2, ups=2.28, wpb=7450.1, bsz=270, num_updates=22800, lr=8.37708e-05, gnorm=0.96, clip=0, train_wall=35, wall=8329
2020-10-12 13:03:31 | INFO | train_inner | epoch 021:    140 / 1138 loss=4.892, nll_loss=3.337, ppl=10.1, wps=20952.1, ups=2.79, wpb=7511.8, bsz=276.4, num_updates=22900, lr=8.35877e-05, gnorm=0.955, clip=0, train_wall=35, wall=8365
2020-10-12 13:04:07 | INFO | train_inner | epoch 021:    240 / 1138 loss=4.902, nll_loss=3.349, ppl=10.19, wps=20640.9, ups=2.79, wpb=7394.7, bsz=279.4, num_updates=23000, lr=8.34058e-05, gnorm=0.954, clip=0, train_wall=35, wall=8401
2020-10-12 13:04:42 | INFO | train_inner | epoch 021:    340 / 1138 loss=4.867, nll_loss=3.31, ppl=9.92, wps=20344.8, ups=2.79, wpb=7296.7, bsz=300.8, num_updates=23100, lr=8.3225e-05, gnorm=0.962, clip=0, train_wall=35, wall=8436
2020-10-12 13:05:18 | INFO | train_inner | epoch 021:    440 / 1138 loss=4.924, nll_loss=3.374, ppl=10.36, wps=20743.8, ups=2.79, wpb=7443.1, bsz=268, num_updates=23200, lr=8.30455e-05, gnorm=0.955, clip=0, train_wall=35, wall=8472
2020-10-12 13:05:54 | INFO | train_inner | epoch 021:    540 / 1138 loss=4.912, nll_loss=3.361, ppl=10.28, wps=20659.9, ups=2.8, wpb=7385.9, bsz=285.5, num_updates=23300, lr=8.28671e-05, gnorm=0.978, clip=0, train_wall=35, wall=8508
2020-10-12 13:06:30 | INFO | train_inner | epoch 021:    640 / 1138 loss=4.896, nll_loss=3.343, ppl=10.15, wps=20539.4, ups=2.76, wpb=7440.1, bsz=280.3, num_updates=23400, lr=8.26898e-05, gnorm=0.963, clip=0, train_wall=36, wall=8544
2020-10-12 13:07:06 | INFO | train_inner | epoch 021:    740 / 1138 loss=4.911, nll_loss=3.36, ppl=10.27, wps=20763.2, ups=2.77, wpb=7485.3, bsz=274.5, num_updates=23500, lr=8.25137e-05, gnorm=0.955, clip=0, train_wall=36, wall=8580
2020-10-12 13:07:42 | INFO | train_inner | epoch 021:    840 / 1138 loss=4.881, nll_loss=3.325, ppl=10.02, wps=20716.5, ups=2.78, wpb=7462.7, bsz=290.5, num_updates=23600, lr=8.23387e-05, gnorm=0.957, clip=0, train_wall=36, wall=8616
2020-10-12 13:08:18 | INFO | train_inner | epoch 021:    940 / 1138 loss=4.936, nll_loss=3.387, ppl=10.46, wps=20603.4, ups=2.79, wpb=7375.6, bsz=261.4, num_updates=23700, lr=8.21648e-05, gnorm=0.987, clip=0, train_wall=35, wall=8652
2020-10-12 13:08:54 | INFO | train_inner | epoch 021:   1040 / 1138 loss=4.884, nll_loss=3.33, ppl=10.05, wps=20303.4, ups=2.78, wpb=7309.1, bsz=300.6, num_updates=23800, lr=8.1992e-05, gnorm=0.953, clip=0, train_wall=36, wall=8688
2020-10-12 13:09:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 13:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001132
2020-10-12 13:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071100
2020-10-12 13:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051103
2020-10-12 13:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123674
2020-10-12 13:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000922
2020-10-12 13:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070756
2020-10-12 13:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051415
2020-10-12 13:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123415
2020-10-12 13:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:09:34 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 4.996 | nll_loss 3.344 | ppl 10.16 | wps 44880.5 | wpb 2418.3 | bsz 92.3 | num_updates 23898 | best_loss 4.996
2020-10-12 13:09:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 13:09:36 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 21 @ 23898 updates, score 4.996) (writing took 1.6320886660250835 seconds)
2020-10-12 13:09:36 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 13:09:36 | INFO | train | epoch 021 | loss 4.901 | nll_loss 3.348 | ppl 10.18 | wps 20223.6 | ups 2.73 | wpb 7410.9 | bsz 282.4 | num_updates 23898 | lr 8.18237e-05 | gnorm 0.962 | clip 0 | train_wall 404 | wall 8730
2020-10-12 13:09:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 13:09:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 13:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:09:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002946
2020-10-12 13:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043471
2020-10-12 13:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001981
2020-10-12 13:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:09:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.705620
2020-10-12 13:09:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.751607
2020-10-12 13:09:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:09:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:09:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034601
2020-10-12 13:09:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:09:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001881
2020-10-12 13:09:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.680026
2020-10-12 13:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.717035
2020-10-12 13:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:09:38 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 13:09:38 | INFO | train_inner | epoch 022:      2 / 1138 loss=4.911, nll_loss=3.361, ppl=10.27, wps=16697.8, ups=2.26, wpb=7375.6, bsz=290.3, num_updates=23900, lr=8.18203e-05, gnorm=0.972, clip=0, train_wall=35, wall=8732
2020-10-12 13:10:14 | INFO | train_inner | epoch 022:    102 / 1138 loss=4.854, nll_loss=3.296, ppl=9.82, wps=20899.2, ups=2.83, wpb=7383.3, bsz=284.3, num_updates=24000, lr=8.16497e-05, gnorm=0.983, clip=0, train_wall=35, wall=8768
2020-10-12 13:10:50 | INFO | train_inner | epoch 022:    202 / 1138 loss=4.843, nll_loss=3.283, ppl=9.73, wps=20790.2, ups=2.78, wpb=7474.3, bsz=293.5, num_updates=24100, lr=8.14801e-05, gnorm=0.927, clip=0, train_wall=36, wall=8804
2020-10-12 13:11:25 | INFO | train_inner | epoch 022:    302 / 1138 loss=4.849, nll_loss=3.288, ppl=9.77, wps=20578.4, ups=2.79, wpb=7383, bsz=280.2, num_updates=24200, lr=8.13116e-05, gnorm=0.962, clip=0, train_wall=35, wall=8839
2020-10-12 13:12:02 | INFO | train_inner | epoch 022:    402 / 1138 loss=4.885, nll_loss=3.33, ppl=10.06, wps=20838.9, ups=2.77, wpb=7511.9, bsz=259.8, num_updates=24300, lr=8.11441e-05, gnorm=0.95, clip=0, train_wall=36, wall=8876
2020-10-12 13:12:38 | INFO | train_inner | epoch 022:    502 / 1138 loss=4.881, nll_loss=3.325, ppl=10.02, wps=20792.5, ups=2.78, wpb=7487.7, bsz=268, num_updates=24400, lr=8.09776e-05, gnorm=0.964, clip=0, train_wall=36, wall=8912
2020-10-12 13:13:13 | INFO | train_inner | epoch 022:    602 / 1138 loss=4.849, nll_loss=3.289, ppl=9.78, wps=20183.9, ups=2.8, wpb=7209.8, bsz=287.7, num_updates=24500, lr=8.08122e-05, gnorm=0.968, clip=0, train_wall=35, wall=8947
2020-10-12 13:13:50 | INFO | train_inner | epoch 022:    702 / 1138 loss=4.901, nll_loss=3.347, ppl=10.17, wps=20702.8, ups=2.74, wpb=7545.3, bsz=258.6, num_updates=24600, lr=8.06478e-05, gnorm=0.979, clip=0, train_wall=36, wall=8984
2020-10-12 13:14:25 | INFO | train_inner | epoch 022:    802 / 1138 loss=4.862, nll_loss=3.305, ppl=9.89, wps=20385.9, ups=2.8, wpb=7284.7, bsz=290.6, num_updates=24700, lr=8.04844e-05, gnorm=0.975, clip=0, train_wall=35, wall=9019
2020-10-12 13:15:02 | INFO | train_inner | epoch 022:    902 / 1138 loss=4.878, nll_loss=3.322, ppl=10, wps=20353.9, ups=2.77, wpb=7359.9, bsz=287.5, num_updates=24800, lr=8.03219e-05, gnorm=0.983, clip=0, train_wall=36, wall=9056
2020-10-12 13:15:37 | INFO | train_inner | epoch 022:   1002 / 1138 loss=4.85, nll_loss=3.291, ppl=9.79, wps=20884.8, ups=2.79, wpb=7473.8, bsz=298.8, num_updates=24900, lr=8.01605e-05, gnorm=0.964, clip=0, train_wall=35, wall=9091
2020-10-12 13:16:13 | INFO | train_inner | epoch 022:   1102 / 1138 loss=4.882, nll_loss=3.327, ppl=10.04, wps=20533.7, ups=2.79, wpb=7357.1, bsz=284.1, num_updates=25000, lr=8e-05, gnorm=0.965, clip=0, train_wall=35, wall=9127
2020-10-12 13:16:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 13:16:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:16:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001114
2020-10-12 13:16:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:16:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069451
2020-10-12 13:16:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:16:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051069
2020-10-12 13:16:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121974
2020-10-12 13:16:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:16:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:16:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000945
2020-10-12 13:16:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:16:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070104
2020-10-12 13:16:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:16:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050831
2020-10-12 13:16:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122200
2020-10-12 13:16:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:16:31 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 4.972 | nll_loss 3.315 | ppl 9.95 | wps 44764.4 | wpb 2418.3 | bsz 92.3 | num_updates 25036 | best_loss 4.972
2020-10-12 13:16:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 13:16:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 22 @ 25036 updates, score 4.972) (writing took 1.5809817069966812 seconds)
2020-10-12 13:16:33 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 13:16:33 | INFO | train | epoch 022 | loss 4.865 | nll_loss 3.307 | ppl 9.9 | wps 20227.9 | ups 2.73 | wpb 7410.9 | bsz 282.4 | num_updates 25036 | lr 7.99425e-05 | gnorm 0.964 | clip 0 | train_wall 404 | wall 9147
2020-10-12 13:16:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 13:16:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 13:16:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:16:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003214
2020-10-12 13:16:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.047042
2020-10-12 13:16:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:16:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002161
2020-10-12 13:16:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:16:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.700707
2020-10-12 13:16:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.750598
2020-10-12 13:16:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:16:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:16:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034536
2020-10-12 13:16:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:16:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001888
2020-10-12 13:16:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:16:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.731595
2020-10-12 13:16:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.768547
2020-10-12 13:16:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:16:35 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 13:16:57 | INFO | train_inner | epoch 023:     64 / 1138 loss=4.812, nll_loss=3.248, ppl=9.5, wps=16783.5, ups=2.27, wpb=7403.3, bsz=285.9, num_updates=25100, lr=7.98405e-05, gnorm=0.957, clip=0, train_wall=35, wall=9171
2020-10-12 13:17:33 | INFO | train_inner | epoch 023:    164 / 1138 loss=4.815, nll_loss=3.251, ppl=9.52, wps=20637.4, ups=2.82, wpb=7328.7, bsz=279, num_updates=25200, lr=7.96819e-05, gnorm=0.968, clip=0, train_wall=35, wall=9207
2020-10-12 13:18:09 | INFO | train_inner | epoch 023:    264 / 1138 loss=4.799, nll_loss=3.233, ppl=9.41, wps=20477.6, ups=2.77, wpb=7392.4, bsz=305.3, num_updates=25300, lr=7.95243e-05, gnorm=0.951, clip=0, train_wall=36, wall=9243
2020-10-12 13:18:45 | INFO | train_inner | epoch 023:    364 / 1138 loss=4.827, nll_loss=3.265, ppl=9.61, wps=20356.7, ups=2.81, wpb=7239.5, bsz=286.2, num_updates=25400, lr=7.93676e-05, gnorm=0.995, clip=0, train_wall=35, wall=9278
2020-10-12 13:19:20 | INFO | train_inner | epoch 023:    464 / 1138 loss=4.817, nll_loss=3.253, ppl=9.53, wps=20505.7, ups=2.79, wpb=7354.6, bsz=280.3, num_updates=25500, lr=7.92118e-05, gnorm=0.967, clip=0, train_wall=35, wall=9314
2020-10-12 13:19:56 | INFO | train_inner | epoch 023:    564 / 1138 loss=4.861, nll_loss=3.303, ppl=9.87, wps=20922.4, ups=2.77, wpb=7546, bsz=254.4, num_updates=25600, lr=7.90569e-05, gnorm=0.976, clip=0, train_wall=36, wall=9350
2020-10-12 13:20:32 | INFO | train_inner | epoch 023:    664 / 1138 loss=4.858, nll_loss=3.299, ppl=9.84, wps=20818.4, ups=2.79, wpb=7459.3, bsz=256.3, num_updates=25700, lr=7.8903e-05, gnorm=0.961, clip=0, train_wall=35, wall=9386
2020-10-12 13:21:08 | INFO | train_inner | epoch 023:    764 / 1138 loss=4.845, nll_loss=3.285, ppl=9.75, wps=20761.6, ups=2.78, wpb=7461.2, bsz=289.3, num_updates=25800, lr=7.87499e-05, gnorm=0.957, clip=0, train_wall=35, wall=9422
2020-10-12 13:21:44 | INFO | train_inner | epoch 023:    864 / 1138 loss=4.833, nll_loss=3.27, ppl=9.65, wps=20676.3, ups=2.79, wpb=7417.5, bsz=278.5, num_updates=25900, lr=7.85977e-05, gnorm=0.962, clip=0, train_wall=35, wall=9458
2020-10-12 13:22:20 | INFO | train_inner | epoch 023:    964 / 1138 loss=4.836, nll_loss=3.276, ppl=9.68, wps=20856.1, ups=2.77, wpb=7531.2, bsz=286.7, num_updates=26000, lr=7.84465e-05, gnorm=0.948, clip=0, train_wall=36, wall=9494
2020-10-12 13:22:56 | INFO | train_inner | epoch 023:   1064 / 1138 loss=4.84, nll_loss=3.28, ppl=9.72, wps=20706.4, ups=2.79, wpb=7411.3, bsz=294.5, num_updates=26100, lr=7.8296e-05, gnorm=0.992, clip=0, train_wall=35, wall=9530
2020-10-12 13:23:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 13:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001137
2020-10-12 13:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069366
2020-10-12 13:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050804
2020-10-12 13:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121647
2020-10-12 13:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000894
2020-10-12 13:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069106
2020-10-12 13:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050632
2020-10-12 13:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120956
2020-10-12 13:23:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:23:28 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 4.963 | nll_loss 3.306 | ppl 9.89 | wps 44659.4 | wpb 2418.3 | bsz 92.3 | num_updates 26174 | best_loss 4.963
2020-10-12 13:23:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 13:23:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 23 @ 26174 updates, score 4.963) (writing took 1.4539843719976489 seconds)
2020-10-12 13:23:29 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 13:23:29 | INFO | train | epoch 023 | loss 4.832 | nll_loss 3.27 | ppl 9.65 | wps 20267.2 | ups 2.73 | wpb 7410.9 | bsz 282.4 | num_updates 26174 | lr 7.81853e-05 | gnorm 0.968 | clip 0 | train_wall 403 | wall 9563
2020-10-12 13:23:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 13:23:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 13:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:23:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002969
2020-10-12 13:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043172
2020-10-12 13:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001907
2020-10-12 13:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:23:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.700390
2020-10-12 13:23:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.746002
2020-10-12 13:23:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:23:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:23:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034887
2020-10-12 13:23:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:23:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001897
2020-10-12 13:23:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:23:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.677297
2020-10-12 13:23:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.714604
2020-10-12 13:23:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:23:31 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 13:23:40 | INFO | train_inner | epoch 024:     26 / 1138 loss=4.806, nll_loss=3.243, ppl=9.47, wps=16827.3, ups=2.28, wpb=7365, bsz=316.5, num_updates=26200, lr=7.81465e-05, gnorm=0.967, clip=0, train_wall=35, wall=9574
2020-10-12 13:24:15 | INFO | train_inner | epoch 024:    126 / 1138 loss=4.812, nll_loss=3.247, ppl=9.49, wps=20678.9, ups=2.83, wpb=7317.3, bsz=260.4, num_updates=26300, lr=7.79978e-05, gnorm=0.974, clip=0, train_wall=35, wall=9609
2020-10-12 13:24:51 | INFO | train_inner | epoch 024:    226 / 1138 loss=4.797, nll_loss=3.231, ppl=9.39, wps=20682.2, ups=2.79, wpb=7404, bsz=267.6, num_updates=26400, lr=7.78499e-05, gnorm=0.954, clip=0, train_wall=35, wall=9645
2020-10-12 13:25:26 | INFO | train_inner | epoch 024:    326 / 1138 loss=4.786, nll_loss=3.218, ppl=9.3, wps=20615.2, ups=2.82, wpb=7307.4, bsz=280.4, num_updates=26500, lr=7.77029e-05, gnorm=0.975, clip=0, train_wall=35, wall=9680
2020-10-12 13:26:03 | INFO | train_inner | epoch 024:    426 / 1138 loss=4.795, nll_loss=3.228, ppl=9.37, wps=20783.7, ups=2.76, wpb=7528.4, bsz=283.1, num_updates=26600, lr=7.75567e-05, gnorm=0.96, clip=0, train_wall=36, wall=9717
2020-10-12 13:26:39 | INFO | train_inner | epoch 024:    526 / 1138 loss=4.785, nll_loss=3.217, ppl=9.3, wps=20799, ups=2.78, wpb=7490.6, bsz=295.2, num_updates=26700, lr=7.74113e-05, gnorm=0.959, clip=0, train_wall=36, wall=9753
2020-10-12 13:27:15 | INFO | train_inner | epoch 024:    626 / 1138 loss=4.795, nll_loss=3.228, ppl=9.37, wps=20676.7, ups=2.77, wpb=7471.8, bsz=275.3, num_updates=26800, lr=7.72667e-05, gnorm=0.978, clip=0, train_wall=36, wall=9789
2020-10-12 13:27:51 | INFO | train_inner | epoch 024:    726 / 1138 loss=4.82, nll_loss=3.256, ppl=9.55, wps=20729.2, ups=2.78, wpb=7456.8, bsz=278.2, num_updates=26900, lr=7.7123e-05, gnorm=0.969, clip=0, train_wall=36, wall=9825
2020-10-12 13:28:27 | INFO | train_inner | epoch 024:    826 / 1138 loss=4.789, nll_loss=3.223, ppl=9.34, wps=20621.4, ups=2.77, wpb=7444.8, bsz=302.8, num_updates=27000, lr=7.698e-05, gnorm=0.965, clip=0, train_wall=36, wall=9861
2020-10-12 13:29:02 | INFO | train_inner | epoch 024:    926 / 1138 loss=4.823, nll_loss=3.261, ppl=9.59, wps=20300.5, ups=2.81, wpb=7236.8, bsz=279.8, num_updates=27100, lr=7.68379e-05, gnorm=0.986, clip=0, train_wall=35, wall=9896
2020-10-12 13:29:39 | INFO | train_inner | epoch 024:   1026 / 1138 loss=4.807, nll_loss=3.242, ppl=9.46, wps=20504, ups=2.77, wpb=7390.3, bsz=279.4, num_updates=27200, lr=7.66965e-05, gnorm=0.969, clip=0, train_wall=36, wall=9933
2020-10-12 13:30:15 | INFO | train_inner | epoch 024:   1126 / 1138 loss=4.791, nll_loss=3.224, ppl=9.34, wps=20735.6, ups=2.75, wpb=7541, bsz=308.8, num_updates=27300, lr=7.65559e-05, gnorm=0.96, clip=0, train_wall=36, wall=9969
2020-10-12 13:30:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 13:30:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:30:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001094
2020-10-12 13:30:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:30:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069505
2020-10-12 13:30:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:30:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050273
2020-10-12 13:30:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121206
2020-10-12 13:30:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:30:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:30:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000960
2020-10-12 13:30:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:30:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070266
2020-10-12 13:30:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:30:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050428
2020-10-12 13:30:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121975
2020-10-12 13:30:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:30:24 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 4.946 | nll_loss 3.282 | ppl 9.73 | wps 44906.5 | wpb 2418.3 | bsz 92.3 | num_updates 27312 | best_loss 4.946
2020-10-12 13:30:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 13:30:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 24 @ 27312 updates, score 4.946) (writing took 2.052918508008588 seconds)
2020-10-12 13:30:26 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 13:30:26 | INFO | train | epoch 024 | loss 4.8 | nll_loss 3.234 | ppl 9.41 | wps 20218 | ups 2.73 | wpb 7410.9 | bsz 282.4 | num_updates 27312 | lr 7.65391e-05 | gnorm 0.969 | clip 0 | train_wall 404 | wall 9980
2020-10-12 13:30:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 13:30:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 13:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:30:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002905
2020-10-12 13:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043946
2020-10-12 13:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001981
2020-10-12 13:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.688421
2020-10-12 13:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.734915
2020-10-12 13:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035492
2020-10-12 13:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001964
2020-10-12 13:30:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:30:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.722558
2020-10-12 13:30:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.760546
2020-10-12 13:30:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:30:28 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 13:30:59 | INFO | train_inner | epoch 025:     88 / 1138 loss=4.738, nll_loss=3.165, ppl=8.97, wps=16889, ups=2.27, wpb=7437.8, bsz=292.6, num_updates=27400, lr=7.64161e-05, gnorm=0.953, clip=0, train_wall=35, wall=10013
2020-10-12 13:31:35 | INFO | train_inner | epoch 025:    188 / 1138 loss=4.731, nll_loss=3.156, ppl=8.92, wps=20778.4, ups=2.78, wpb=7478.4, bsz=318.9, num_updates=27500, lr=7.6277e-05, gnorm=0.946, clip=0, train_wall=36, wall=10049
2020-10-12 13:32:11 | INFO | train_inner | epoch 025:    288 / 1138 loss=4.769, nll_loss=3.197, ppl=9.17, wps=20865.1, ups=2.75, wpb=7585.3, bsz=285, num_updates=27600, lr=7.61387e-05, gnorm=0.951, clip=0, train_wall=36, wall=10085
2020-10-12 13:32:47 | INFO | train_inner | epoch 025:    388 / 1138 loss=4.754, nll_loss=3.181, ppl=9.07, wps=20564.7, ups=2.79, wpb=7361.3, bsz=282, num_updates=27700, lr=7.60011e-05, gnorm=0.979, clip=0, train_wall=35, wall=10121
2020-10-12 13:33:23 | INFO | train_inner | epoch 025:    488 / 1138 loss=4.787, nll_loss=3.221, ppl=9.32, wps=20273.4, ups=2.8, wpb=7246.2, bsz=247.4, num_updates=27800, lr=7.58643e-05, gnorm=0.996, clip=0, train_wall=35, wall=10157
2020-10-12 13:33:59 | INFO | train_inner | epoch 025:    588 / 1138 loss=4.777, nll_loss=3.209, ppl=9.24, wps=20634.2, ups=2.8, wpb=7365.4, bsz=275.8, num_updates=27900, lr=7.57282e-05, gnorm=0.978, clip=0, train_wall=35, wall=10192
2020-10-12 13:34:34 | INFO | train_inner | epoch 025:    688 / 1138 loss=4.785, nll_loss=3.218, ppl=9.3, wps=20563.1, ups=2.8, wpb=7339.6, bsz=285.6, num_updates=28000, lr=7.55929e-05, gnorm=0.977, clip=0, train_wall=35, wall=10228
2020-10-12 13:35:10 | INFO | train_inner | epoch 025:    788 / 1138 loss=4.772, nll_loss=3.203, ppl=9.21, wps=20467.3, ups=2.77, wpb=7377.1, bsz=275, num_updates=28100, lr=7.54583e-05, gnorm=0.959, clip=0, train_wall=36, wall=10264
2020-10-12 13:35:46 | INFO | train_inner | epoch 025:    888 / 1138 loss=4.804, nll_loss=3.239, ppl=9.44, wps=20510.2, ups=2.76, wpb=7421.4, bsz=263.8, num_updates=28200, lr=7.53244e-05, gnorm=0.994, clip=0, train_wall=36, wall=10300
2020-10-12 13:36:22 | INFO | train_inner | epoch 025:    988 / 1138 loss=4.78, nll_loss=3.211, ppl=9.26, wps=20328.6, ups=2.79, wpb=7277.4, bsz=269.8, num_updates=28300, lr=7.51912e-05, gnorm=0.973, clip=0, train_wall=35, wall=10336
2020-10-12 13:36:59 | INFO | train_inner | epoch 025:   1088 / 1138 loss=4.786, nll_loss=3.218, ppl=9.3, wps=20941.6, ups=2.74, wpb=7653.1, bsz=292.2, num_updates=28400, lr=7.50587e-05, gnorm=0.951, clip=0, train_wall=36, wall=10373
2020-10-12 13:37:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 13:37:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:37:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001126
2020-10-12 13:37:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:37:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071605
2020-10-12 13:37:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:37:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050327
2020-10-12 13:37:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123405
2020-10-12 13:37:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:37:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:37:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000901
2020-10-12 13:37:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:37:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068376
2020-10-12 13:37:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:37:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050467
2020-10-12 13:37:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120063
2020-10-12 13:37:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:37:22 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 4.927 | nll_loss 3.268 | ppl 9.63 | wps 44986.7 | wpb 2418.3 | bsz 92.3 | num_updates 28450 | best_loss 4.927
2020-10-12 13:37:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 13:37:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 25 @ 28450 updates, score 4.927) (writing took 1.6332411690091249 seconds)
2020-10-12 13:37:23 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 13:37:23 | INFO | train | epoch 025 | loss 4.771 | nll_loss 3.201 | ppl 9.2 | wps 20217.5 | ups 2.73 | wpb 7410.9 | bsz 282.4 | num_updates 28450 | lr 7.49927e-05 | gnorm 0.968 | clip 0 | train_wall 404 | wall 10397
2020-10-12 13:37:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 13:37:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 13:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:37:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003025
2020-10-12 13:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043790
2020-10-12 13:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001956
2020-10-12 13:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.692259
2020-10-12 13:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.738534
2020-10-12 13:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034710
2020-10-12 13:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001898
2020-10-12 13:37:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:37:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.672068
2020-10-12 13:37:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.709213
2020-10-12 13:37:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:37:25 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 13:37:42 | INFO | train_inner | epoch 026:     50 / 1138 loss=4.745, nll_loss=3.173, ppl=9.02, wps=16531.8, ups=2.3, wpb=7187.1, bsz=299, num_updates=28500, lr=7.49269e-05, gnorm=0.982, clip=0, train_wall=35, wall=10416
2020-10-12 13:38:18 | INFO | train_inner | epoch 026:    150 / 1138 loss=4.73, nll_loss=3.155, ppl=8.9, wps=20832.7, ups=2.82, wpb=7388.6, bsz=272.6, num_updates=28600, lr=7.47958e-05, gnorm=0.964, clip=0, train_wall=35, wall=10452
2020-10-12 13:38:53 | INFO | train_inner | epoch 026:    250 / 1138 loss=4.756, nll_loss=3.184, ppl=9.09, wps=20865, ups=2.8, wpb=7445.4, bsz=259, num_updates=28700, lr=7.46653e-05, gnorm=0.982, clip=0, train_wall=35, wall=10487
2020-10-12 13:39:29 | INFO | train_inner | epoch 026:    350 / 1138 loss=4.723, nll_loss=3.148, ppl=8.86, wps=20412.9, ups=2.77, wpb=7361.1, bsz=303, num_updates=28800, lr=7.45356e-05, gnorm=0.953, clip=0, train_wall=36, wall=10523
2020-10-12 13:40:05 | INFO | train_inner | epoch 026:    450 / 1138 loss=4.746, nll_loss=3.173, ppl=9.02, wps=20745, ups=2.78, wpb=7466.9, bsz=292.3, num_updates=28900, lr=7.44065e-05, gnorm=0.965, clip=0, train_wall=36, wall=10559
2020-10-12 13:40:41 | INFO | train_inner | epoch 026:    550 / 1138 loss=4.753, nll_loss=3.18, ppl=9.07, wps=20842.4, ups=2.78, wpb=7494.7, bsz=275.2, num_updates=29000, lr=7.42781e-05, gnorm=0.957, clip=0, train_wall=36, wall=10595
2020-10-12 13:41:17 | INFO | train_inner | epoch 026:    650 / 1138 loss=4.75, nll_loss=3.177, ppl=9.05, wps=20671.2, ups=2.78, wpb=7441.9, bsz=281, num_updates=29100, lr=7.41504e-05, gnorm=0.956, clip=0, train_wall=36, wall=10631
2020-10-12 13:41:54 | INFO | train_inner | epoch 026:    750 / 1138 loss=4.764, nll_loss=3.193, ppl=9.15, wps=20734.5, ups=2.77, wpb=7489.4, bsz=263.7, num_updates=29200, lr=7.40233e-05, gnorm=0.969, clip=0, train_wall=36, wall=10668
2020-10-12 13:42:29 | INFO | train_inner | epoch 026:    850 / 1138 loss=4.726, nll_loss=3.151, ppl=8.88, wps=20511, ups=2.78, wpb=7367.4, bsz=293.8, num_updates=29300, lr=7.38969e-05, gnorm=0.97, clip=0, train_wall=35, wall=10703
2020-10-12 13:43:06 | INFO | train_inner | epoch 026:    950 / 1138 loss=4.761, nll_loss=3.19, ppl=9.12, wps=20723.9, ups=2.76, wpb=7509.5, bsz=286, num_updates=29400, lr=7.37711e-05, gnorm=0.972, clip=0, train_wall=36, wall=10740
2020-10-12 13:43:41 | INFO | train_inner | epoch 026:   1050 / 1138 loss=4.742, nll_loss=3.169, ppl=8.99, wps=20277.1, ups=2.81, wpb=7207.8, bsz=285.4, num_updates=29500, lr=7.3646e-05, gnorm=0.977, clip=0, train_wall=35, wall=10775
2020-10-12 13:44:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 13:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001085
2020-10-12 13:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069132
2020-10-12 13:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050155
2020-10-12 13:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120710
2020-10-12 13:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000949
2020-10-12 13:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069949
2020-10-12 13:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:44:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050359
2020-10-12 13:44:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121576
2020-10-12 13:44:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:44:18 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 4.919 | nll_loss 3.251 | ppl 9.52 | wps 44790.9 | wpb 2418.3 | bsz 92.3 | num_updates 29588 | best_loss 4.919
2020-10-12 13:44:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 13:44:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 26 @ 29588 updates, score 4.919) (writing took 1.5947555980237667 seconds)
2020-10-12 13:44:20 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 13:44:20 | INFO | train | epoch 026 | loss 4.743 | nll_loss 3.17 | ppl 9 | wps 20245.9 | ups 2.73 | wpb 7410.9 | bsz 282.4 | num_updates 29588 | lr 7.35364e-05 | gnorm 0.967 | clip 0 | train_wall 403 | wall 10814
2020-10-12 13:44:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 13:44:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 13:44:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:44:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003325
2020-10-12 13:44:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.047659
2020-10-12 13:44:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:44:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002188
2020-10-12 13:44:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.693988
2020-10-12 13:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.744382
2020-10-12 13:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034746
2020-10-12 13:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001917
2020-10-12 13:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.717081
2020-10-12 13:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.754283
2020-10-12 13:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:44:21 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 13:44:26 | INFO | train_inner | epoch 027:     12 / 1138 loss=4.736, nll_loss=3.162, ppl=8.95, wps=16937.3, ups=2.25, wpb=7524.6, bsz=281.7, num_updates=29600, lr=7.35215e-05, gnorm=0.95, clip=0, train_wall=36, wall=10820
2020-10-12 13:45:01 | INFO | train_inner | epoch 027:    112 / 1138 loss=4.705, nll_loss=3.126, ppl=8.73, wps=20952.3, ups=2.82, wpb=7428.5, bsz=276.1, num_updates=29700, lr=7.33976e-05, gnorm=0.962, clip=0, train_wall=35, wall=10855
2020-10-12 13:45:37 | INFO | train_inner | epoch 027:    212 / 1138 loss=4.687, nll_loss=3.107, ppl=8.61, wps=20672.6, ups=2.8, wpb=7395.5, bsz=294.3, num_updates=29800, lr=7.32743e-05, gnorm=0.964, clip=0, train_wall=35, wall=10891
2020-10-12 13:46:13 | INFO | train_inner | epoch 027:    312 / 1138 loss=4.701, nll_loss=3.123, ppl=8.71, wps=20466.3, ups=2.77, wpb=7383.6, bsz=287.9, num_updates=29900, lr=7.31517e-05, gnorm=0.965, clip=0, train_wall=36, wall=10927
2020-10-12 13:46:49 | INFO | train_inner | epoch 027:    412 / 1138 loss=4.714, nll_loss=3.136, ppl=8.79, wps=20538.3, ups=2.78, wpb=7400, bsz=267.9, num_updates=30000, lr=7.30297e-05, gnorm=0.965, clip=0, train_wall=36, wall=10963
2020-10-12 13:47:25 | INFO | train_inner | epoch 027:    512 / 1138 loss=4.715, nll_loss=3.139, ppl=8.81, wps=20489.2, ups=2.76, wpb=7411.5, bsz=278.8, num_updates=30100, lr=7.29083e-05, gnorm=0.968, clip=0, train_wall=36, wall=10999
2020-10-12 13:48:01 | INFO | train_inner | epoch 027:    612 / 1138 loss=4.717, nll_loss=3.14, ppl=8.82, wps=20586.7, ups=2.76, wpb=7449.5, bsz=285.4, num_updates=30200, lr=7.27875e-05, gnorm=0.967, clip=0, train_wall=36, wall=11035
2020-10-12 13:48:37 | INFO | train_inner | epoch 027:    712 / 1138 loss=4.729, nll_loss=3.155, ppl=8.91, wps=20516.4, ups=2.79, wpb=7353.8, bsz=295.7, num_updates=30300, lr=7.26672e-05, gnorm=0.974, clip=0, train_wall=35, wall=11071
2020-10-12 13:49:13 | INFO | train_inner | epoch 027:    812 / 1138 loss=4.728, nll_loss=3.151, ppl=8.89, wps=20607.9, ups=2.76, wpb=7462.1, bsz=281.1, num_updates=30400, lr=7.25476e-05, gnorm=0.977, clip=0, train_wall=36, wall=11107
2020-10-12 13:49:50 | INFO | train_inner | epoch 027:    912 / 1138 loss=4.729, nll_loss=3.153, ppl=8.9, wps=20627.7, ups=2.77, wpb=7453.2, bsz=290.2, num_updates=30500, lr=7.24286e-05, gnorm=0.968, clip=0, train_wall=36, wall=11144
2020-10-12 13:50:26 | INFO | train_inner | epoch 027:   1012 / 1138 loss=4.74, nll_loss=3.166, ppl=8.97, wps=20257.9, ups=2.74, wpb=7390.2, bsz=264.2, num_updates=30600, lr=7.23102e-05, gnorm=0.976, clip=0, train_wall=36, wall=11180
2020-10-12 13:51:02 | INFO | train_inner | epoch 027:   1112 / 1138 loss=4.727, nll_loss=3.152, ppl=8.89, wps=20477.8, ups=2.79, wpb=7352.7, bsz=289, num_updates=30700, lr=7.21923e-05, gnorm=0.974, clip=0, train_wall=35, wall=11216
2020-10-12 13:51:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 13:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001145
2020-10-12 13:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069313
2020-10-12 13:51:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:51:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050904
2020-10-12 13:51:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121701
2020-10-12 13:51:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:51:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:51:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000939
2020-10-12 13:51:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:51:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069002
2020-10-12 13:51:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:51:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050940
2020-10-12 13:51:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121212
2020-10-12 13:51:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:51:16 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 4.909 | nll_loss 3.247 | ppl 9.49 | wps 44883.5 | wpb 2418.3 | bsz 92.3 | num_updates 30726 | best_loss 4.909
2020-10-12 13:51:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 13:51:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 27 @ 30726 updates, score 4.909) (writing took 1.6084142760082614 seconds)
2020-10-12 13:51:18 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 13:51:18 | INFO | train | epoch 027 | loss 4.718 | nll_loss 3.141 | ppl 8.82 | wps 20169.4 | ups 2.72 | wpb 7410.9 | bsz 282.4 | num_updates 30726 | lr 7.21617e-05 | gnorm 0.969 | clip 0 | train_wall 405 | wall 11232
2020-10-12 13:51:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 13:51:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 13:51:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:51:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003175
2020-10-12 13:51:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.046862
2020-10-12 13:51:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:51:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001910
2020-10-12 13:51:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:51:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.718692
2020-10-12 13:51:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.767995
2020-10-12 13:51:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:51:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:51:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034949
2020-10-12 13:51:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:51:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001944
2020-10-12 13:51:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:51:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.691181
2020-10-12 13:51:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.728594
2020-10-12 13:51:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:51:20 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 13:51:46 | INFO | train_inner | epoch 028:     74 / 1138 loss=4.707, nll_loss=3.129, ppl=8.75, wps=17071, ups=2.28, wpb=7476.5, bsz=272.8, num_updates=30800, lr=7.2075e-05, gnorm=0.977, clip=0, train_wall=35, wall=11260
2020-10-12 13:52:22 | INFO | train_inner | epoch 028:    174 / 1138 loss=4.703, nll_loss=3.124, ppl=8.72, wps=20522.6, ups=2.79, wpb=7352.9, bsz=256.6, num_updates=30900, lr=7.19583e-05, gnorm=0.96, clip=0, train_wall=35, wall=11296
2020-10-12 13:52:57 | INFO | train_inner | epoch 028:    274 / 1138 loss=4.681, nll_loss=3.1, ppl=8.57, wps=20120.2, ups=2.81, wpb=7170.3, bsz=275.5, num_updates=31000, lr=7.18421e-05, gnorm=0.985, clip=0, train_wall=35, wall=11331
2020-10-12 13:53:33 | INFO | train_inner | epoch 028:    374 / 1138 loss=4.685, nll_loss=3.103, ppl=8.59, wps=20738, ups=2.77, wpb=7488.4, bsz=285.7, num_updates=31100, lr=7.17265e-05, gnorm=0.955, clip=0, train_wall=36, wall=11367
2020-10-12 13:54:10 | INFO | train_inner | epoch 028:    474 / 1138 loss=4.71, nll_loss=3.132, ppl=8.77, wps=20524.9, ups=2.75, wpb=7467, bsz=277.3, num_updates=31200, lr=7.16115e-05, gnorm=0.957, clip=0, train_wall=36, wall=11404
2020-10-12 13:54:45 | INFO | train_inner | epoch 028:    574 / 1138 loss=4.679, nll_loss=3.098, ppl=8.56, wps=20644.9, ups=2.79, wpb=7387.5, bsz=286, num_updates=31300, lr=7.1497e-05, gnorm=0.963, clip=0, train_wall=35, wall=11439
2020-10-12 13:55:22 | INFO | train_inner | epoch 028:    674 / 1138 loss=4.715, nll_loss=3.137, ppl=8.79, wps=21027.8, ups=2.76, wpb=7623, bsz=260.5, num_updates=31400, lr=7.13831e-05, gnorm=0.97, clip=0, train_wall=36, wall=11476
2020-10-12 13:55:58 | INFO | train_inner | epoch 028:    774 / 1138 loss=4.688, nll_loss=3.108, ppl=8.62, wps=20510.7, ups=2.78, wpb=7386.9, bsz=289.5, num_updates=31500, lr=7.12697e-05, gnorm=0.965, clip=0, train_wall=36, wall=11512
2020-10-12 13:56:33 | INFO | train_inner | epoch 028:    874 / 1138 loss=4.695, nll_loss=3.116, ppl=8.67, wps=20484.6, ups=2.81, wpb=7287, bsz=291.2, num_updates=31600, lr=7.11568e-05, gnorm=0.974, clip=0, train_wall=35, wall=11547
2020-10-12 13:57:09 | INFO | train_inner | epoch 028:    974 / 1138 loss=4.696, nll_loss=3.117, ppl=8.68, wps=20620.3, ups=2.78, wpb=7422.7, bsz=289.8, num_updates=31700, lr=7.10445e-05, gnorm=0.975, clip=0, train_wall=36, wall=11583
2020-10-12 13:57:45 | INFO | train_inner | epoch 028:   1074 / 1138 loss=4.667, nll_loss=3.085, ppl=8.48, wps=20384.7, ups=2.77, wpb=7351.8, bsz=309.4, num_updates=31800, lr=7.09327e-05, gnorm=0.977, clip=0, train_wall=36, wall=11619
2020-10-12 13:58:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 13:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001100
2020-10-12 13:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071340
2020-10-12 13:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051041
2020-10-12 13:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123822
2020-10-12 13:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000944
2020-10-12 13:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070675
2020-10-12 13:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051221
2020-10-12 13:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123165
2020-10-12 13:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:58:14 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 4.884 | nll_loss 3.214 | ppl 9.28 | wps 44617.4 | wpb 2418.3 | bsz 92.3 | num_updates 31864 | best_loss 4.884
2020-10-12 13:58:14 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 13:58:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 28 @ 31864 updates, score 4.884) (writing took 1.609482272004243 seconds)
2020-10-12 13:58:16 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 13:58:16 | INFO | train | epoch 028 | loss 4.693 | nll_loss 3.113 | ppl 8.65 | wps 20200.7 | ups 2.73 | wpb 7410.9 | bsz 282.4 | num_updates 31864 | lr 7.08614e-05 | gnorm 0.968 | clip 0 | train_wall 404 | wall 11650
2020-10-12 13:58:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 13:58:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 13:58:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:58:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003704
2020-10-12 13:58:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.051327
2020-10-12 13:58:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:58:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001943
2020-10-12 13:58:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:58:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.709530
2020-10-12 13:58:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.763314
2020-10-12 13:58:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:58:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 13:58:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034335
2020-10-12 13:58:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:58:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001865
2020-10-12 13:58:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:58:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.719914
2020-10-12 13:58:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.756644
2020-10-12 13:58:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 13:58:17 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 13:58:30 | INFO | train_inner | epoch 029:     36 / 1138 loss=4.673, nll_loss=3.091, ppl=8.52, wps=17070.1, ups=2.25, wpb=7587, bsz=300.5, num_updates=31900, lr=7.08214e-05, gnorm=0.96, clip=0, train_wall=36, wall=11664
2020-10-12 13:59:06 | INFO | train_inner | epoch 029:    136 / 1138 loss=4.667, nll_loss=3.082, ppl=8.47, wps=21282.3, ups=2.8, wpb=7597.6, bsz=276.7, num_updates=32000, lr=7.07107e-05, gnorm=0.95, clip=0, train_wall=35, wall=11700
2020-10-12 13:59:41 | INFO | train_inner | epoch 029:    236 / 1138 loss=4.649, nll_loss=3.063, ppl=8.36, wps=20621.4, ups=2.8, wpb=7370.5, bsz=285, num_updates=32100, lr=7.06005e-05, gnorm=0.965, clip=0, train_wall=35, wall=11735
2020-10-12 14:00:17 | INFO | train_inner | epoch 029:    336 / 1138 loss=4.667, nll_loss=3.083, ppl=8.47, wps=20812.9, ups=2.77, wpb=7524, bsz=288.8, num_updates=32200, lr=7.04907e-05, gnorm=0.959, clip=0, train_wall=36, wall=11771
2020-10-12 14:00:53 | INFO | train_inner | epoch 029:    436 / 1138 loss=4.68, nll_loss=3.098, ppl=8.56, wps=20722.8, ups=2.77, wpb=7472.1, bsz=284.9, num_updates=32300, lr=7.03815e-05, gnorm=0.968, clip=0, train_wall=36, wall=11807
2020-10-12 14:01:30 | INFO | train_inner | epoch 029:    536 / 1138 loss=4.666, nll_loss=3.081, ppl=8.46, wps=20562.5, ups=2.77, wpb=7415.2, bsz=272.8, num_updates=32400, lr=7.02728e-05, gnorm=0.963, clip=0, train_wall=36, wall=11844
2020-10-12 14:02:05 | INFO | train_inner | epoch 029:    636 / 1138 loss=4.68, nll_loss=3.099, ppl=8.57, wps=20297.4, ups=2.79, wpb=7269.6, bsz=264.9, num_updates=32500, lr=7.01646e-05, gnorm=0.984, clip=0, train_wall=35, wall=11879
2020-10-12 14:02:41 | INFO | train_inner | epoch 029:    736 / 1138 loss=4.659, nll_loss=3.074, ppl=8.42, wps=20530.6, ups=2.8, wpb=7338.9, bsz=290.4, num_updates=32600, lr=7.00569e-05, gnorm=0.977, clip=0, train_wall=35, wall=11915
2020-10-12 14:03:17 | INFO | train_inner | epoch 029:    836 / 1138 loss=4.682, nll_loss=3.101, ppl=8.58, wps=20462.2, ups=2.79, wpb=7343.7, bsz=287, num_updates=32700, lr=6.99497e-05, gnorm=0.981, clip=0, train_wall=35, wall=11951
2020-10-12 14:03:53 | INFO | train_inner | epoch 029:    936 / 1138 loss=4.66, nll_loss=3.076, ppl=8.43, wps=20357.5, ups=2.78, wpb=7317.5, bsz=292.7, num_updates=32800, lr=6.9843e-05, gnorm=0.967, clip=0, train_wall=36, wall=11987
2020-10-12 14:04:29 | INFO | train_inner | epoch 029:   1036 / 1138 loss=4.686, nll_loss=3.106, ppl=8.61, wps=20644, ups=2.79, wpb=7399.7, bsz=280.5, num_updates=32900, lr=6.97368e-05, gnorm=0.974, clip=0, train_wall=35, wall=12023
2020-10-12 14:05:05 | INFO | train_inner | epoch 029:   1136 / 1138 loss=4.684, nll_loss=3.102, ppl=8.59, wps=20504.7, ups=2.75, wpb=7446.2, bsz=270, num_updates=33000, lr=6.96311e-05, gnorm=0.971, clip=0, train_wall=36, wall=12059
2020-10-12 14:05:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 14:05:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:05:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001148
2020-10-12 14:05:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:05:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070850
2020-10-12 14:05:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:05:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051086
2020-10-12 14:05:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123428
2020-10-12 14:05:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:05:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:05:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000893
2020-10-12 14:05:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:05:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069447
2020-10-12 14:05:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:05:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051345
2020-10-12 14:05:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122009
2020-10-12 14:05:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:05:11 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 4.884 | nll_loss 3.216 | ppl 9.29 | wps 44808.4 | wpb 2418.3 | bsz 92.3 | num_updates 33002 | best_loss 4.884
2020-10-12 14:05:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 14:05:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 29 @ 33002 updates, score 4.884) (writing took 1.5851245660160203 seconds)
2020-10-12 14:05:12 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 14:05:12 | INFO | train | epoch 029 | loss 4.67 | nll_loss 3.086 | ppl 8.49 | wps 20229.4 | ups 2.73 | wpb 7410.9 | bsz 282.4 | num_updates 33002 | lr 6.9629e-05 | gnorm 0.969 | clip 0 | train_wall 404 | wall 12066
2020-10-12 14:05:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 14:05:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 14:05:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:05:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003313
2020-10-12 14:05:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.048155
2020-10-12 14:05:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:05:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002218
2020-10-12 14:05:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:05:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.712144
2020-10-12 14:05:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.763047
2020-10-12 14:05:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:05:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:05:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035353
2020-10-12 14:05:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:05:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001928
2020-10-12 14:05:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:05:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.691903
2020-10-12 14:05:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.729712
2020-10-12 14:05:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:05:14 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 14:05:49 | INFO | train_inner | epoch 030:     98 / 1138 loss=4.626, nll_loss=3.036, ppl=8.2, wps=17022.6, ups=2.29, wpb=7421.2, bsz=281.4, num_updates=33100, lr=6.95258e-05, gnorm=0.958, clip=0, train_wall=35, wall=12103
2020-10-12 14:06:25 | INFO | train_inner | epoch 030:    198 / 1138 loss=4.623, nll_loss=3.033, ppl=8.19, wps=20718.6, ups=2.79, wpb=7435.8, bsz=281.7, num_updates=33200, lr=6.9421e-05, gnorm=0.962, clip=0, train_wall=35, wall=12139
2020-10-12 14:07:01 | INFO | train_inner | epoch 030:    298 / 1138 loss=4.636, nll_loss=3.048, ppl=8.27, wps=20606.2, ups=2.75, wpb=7488.6, bsz=293.1, num_updates=33300, lr=6.93167e-05, gnorm=0.976, clip=0, train_wall=36, wall=12175
2020-10-12 14:07:37 | INFO | train_inner | epoch 030:    398 / 1138 loss=4.649, nll_loss=3.062, ppl=8.35, wps=20497.7, ups=2.78, wpb=7381, bsz=286.4, num_updates=33400, lr=6.92129e-05, gnorm=0.984, clip=0, train_wall=36, wall=12211
2020-10-12 14:08:13 | INFO | train_inner | epoch 030:    498 / 1138 loss=4.666, nll_loss=3.083, ppl=8.47, wps=20272.2, ups=2.78, wpb=7287.5, bsz=256.9, num_updates=33500, lr=6.91095e-05, gnorm=0.994, clip=0, train_wall=36, wall=12247
2020-10-12 14:08:49 | INFO | train_inner | epoch 030:    598 / 1138 loss=4.661, nll_loss=3.076, ppl=8.43, wps=20270.1, ups=2.76, wpb=7345.5, bsz=271.4, num_updates=33600, lr=6.90066e-05, gnorm=0.992, clip=0, train_wall=36, wall=12283
2020-10-12 14:09:25 | INFO | train_inner | epoch 030:    698 / 1138 loss=4.639, nll_loss=3.053, ppl=8.3, wps=20427.6, ups=2.77, wpb=7366, bsz=302.1, num_updates=33700, lr=6.89041e-05, gnorm=0.977, clip=0, train_wall=36, wall=12319
2020-10-12 14:10:01 | INFO | train_inner | epoch 030:    798 / 1138 loss=4.662, nll_loss=3.078, ppl=8.45, wps=20659.4, ups=2.78, wpb=7437.9, bsz=255.9, num_updates=33800, lr=6.88021e-05, gnorm=0.97, clip=0, train_wall=36, wall=12355
2020-10-12 14:10:37 | INFO | train_inner | epoch 030:    898 / 1138 loss=4.655, nll_loss=3.07, ppl=8.4, wps=20374.7, ups=2.77, wpb=7368.7, bsz=268.6, num_updates=33900, lr=6.87005e-05, gnorm=0.993, clip=0, train_wall=36, wall=12391
2020-10-12 14:11:13 | INFO | train_inner | epoch 030:    998 / 1138 loss=4.66, nll_loss=3.077, ppl=8.44, wps=20769.8, ups=2.8, wpb=7424.9, bsz=308.4, num_updates=34000, lr=6.85994e-05, gnorm=0.981, clip=0, train_wall=35, wall=12427
2020-10-12 14:11:49 | INFO | train_inner | epoch 030:   1098 / 1138 loss=4.664, nll_loss=3.08, ppl=8.46, wps=20820.1, ups=2.77, wpb=7521.8, bsz=287.8, num_updates=34100, lr=6.84988e-05, gnorm=0.975, clip=0, train_wall=36, wall=12463
2020-10-12 14:12:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 14:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001102
2020-10-12 14:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069221
2020-10-12 14:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050108
2020-10-12 14:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120764
2020-10-12 14:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000927
2020-10-12 14:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069662
2020-10-12 14:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050192
2020-10-12 14:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121094
2020-10-12 14:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:12:09 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 4.865 | nll_loss 3.193 | ppl 9.14 | wps 44814.7 | wpb 2418.3 | bsz 92.3 | num_updates 34140 | best_loss 4.865
2020-10-12 14:12:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 14:12:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 30 @ 34140 updates, score 4.865) (writing took 1.9281540229858365 seconds)
2020-10-12 14:12:11 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 14:12:11 | INFO | train | epoch 030 | loss 4.649 | nll_loss 3.063 | ppl 8.36 | wps 20169.4 | ups 2.72 | wpb 7410.9 | bsz 282.4 | num_updates 34140 | lr 6.84586e-05 | gnorm 0.978 | clip 0 | train_wall 405 | wall 12485
2020-10-12 14:12:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 14:12:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 14:12:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:12:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002960
2020-10-12 14:12:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043375
2020-10-12 14:12:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:12:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001926
2020-10-12 14:12:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:12:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.706162
2020-10-12 14:12:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.751998
2020-10-12 14:12:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:12:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:12:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034206
2020-10-12 14:12:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:12:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001875
2020-10-12 14:12:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:12:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.720473
2020-10-12 14:12:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.757091
2020-10-12 14:12:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:12:12 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 14:12:33 | INFO | train_inner | epoch 031:     60 / 1138 loss=4.643, nll_loss=3.057, ppl=8.32, wps=16743.8, ups=2.27, wpb=7367.1, bsz=291.5, num_updates=34200, lr=6.83986e-05, gnorm=0.974, clip=0, train_wall=35, wall=12507
2020-10-12 14:13:09 | INFO | train_inner | epoch 031:    160 / 1138 loss=4.592, nll_loss=2.999, ppl=7.99, wps=20748.6, ups=2.81, wpb=7387.5, bsz=287.3, num_updates=34300, lr=6.82988e-05, gnorm=0.974, clip=0, train_wall=35, wall=12543
2020-10-12 14:13:45 | INFO | train_inner | epoch 031:    260 / 1138 loss=4.593, nll_loss=2.999, ppl=7.99, wps=20764, ups=2.79, wpb=7429.7, bsz=300.8, num_updates=34400, lr=6.81994e-05, gnorm=0.954, clip=0, train_wall=35, wall=12579
2020-10-12 14:14:21 | INFO | train_inner | epoch 031:    360 / 1138 loss=4.62, nll_loss=3.03, ppl=8.17, wps=20286.2, ups=2.76, wpb=7338.5, bsz=297.3, num_updates=34500, lr=6.81005e-05, gnorm=0.986, clip=0, train_wall=36, wall=12615
2020-10-12 14:14:57 | INFO | train_inner | epoch 031:    460 / 1138 loss=4.622, nll_loss=3.032, ppl=8.18, wps=20490, ups=2.77, wpb=7397, bsz=278.9, num_updates=34600, lr=6.8002e-05, gnorm=0.973, clip=0, train_wall=36, wall=12651
2020-10-12 14:15:33 | INFO | train_inner | epoch 031:    560 / 1138 loss=4.63, nll_loss=3.043, ppl=8.24, wps=20344, ups=2.8, wpb=7265.3, bsz=274.1, num_updates=34700, lr=6.7904e-05, gnorm=0.993, clip=0, train_wall=35, wall=12687
2020-10-12 14:16:09 | INFO | train_inner | epoch 031:    660 / 1138 loss=4.645, nll_loss=3.058, ppl=8.33, wps=20543, ups=2.78, wpb=7392.4, bsz=263.6, num_updates=34800, lr=6.78064e-05, gnorm=0.984, clip=0, train_wall=36, wall=12723
2020-10-12 14:16:45 | INFO | train_inner | epoch 031:    760 / 1138 loss=4.638, nll_loss=3.049, ppl=8.28, wps=20716.9, ups=2.74, wpb=7569.9, bsz=288.8, num_updates=34900, lr=6.77091e-05, gnorm=0.973, clip=0, train_wall=36, wall=12759
2020-10-12 14:17:21 | INFO | train_inner | epoch 031:    860 / 1138 loss=4.624, nll_loss=3.035, ppl=8.19, wps=20714, ups=2.78, wpb=7450.3, bsz=285.1, num_updates=35000, lr=6.76123e-05, gnorm=0.981, clip=0, train_wall=36, wall=12795
2020-10-12 14:17:57 | INFO | train_inner | epoch 031:    960 / 1138 loss=4.678, nll_loss=3.095, ppl=8.55, wps=20429, ups=2.79, wpb=7311.6, bsz=240.8, num_updates=35100, lr=6.7516e-05, gnorm=1.006, clip=0, train_wall=35, wall=12831
2020-10-12 14:18:33 | INFO | train_inner | epoch 031:   1060 / 1138 loss=4.636, nll_loss=3.049, ppl=8.27, wps=20760.6, ups=2.76, wpb=7533.6, bsz=292.6, num_updates=35200, lr=6.742e-05, gnorm=0.969, clip=0, train_wall=36, wall=12867
2020-10-12 14:19:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 14:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.007316
2020-10-12 14:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070010
2020-10-12 14:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051172
2020-10-12 14:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128855
2020-10-12 14:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000900
2020-10-12 14:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069326
2020-10-12 14:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050854
2020-10-12 14:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121400
2020-10-12 14:19:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:19:07 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 4.861 | nll_loss 3.191 | ppl 9.13 | wps 43973.8 | wpb 2418.3 | bsz 92.3 | num_updates 35278 | best_loss 4.861
2020-10-12 14:19:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 14:19:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 31 @ 35278 updates, score 4.861) (writing took 1.591956629010383 seconds)
2020-10-12 14:19:08 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 14:19:08 | INFO | train | epoch 031 | loss 4.628 | nll_loss 3.039 | ppl 8.22 | wps 20187.2 | ups 2.72 | wpb 7410.9 | bsz 282.4 | num_updates 35278 | lr 6.73454e-05 | gnorm 0.978 | clip 0 | train_wall 404 | wall 12902
2020-10-12 14:19:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 14:19:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 14:19:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:19:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003158
2020-10-12 14:19:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.045104
2020-10-12 14:19:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:19:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001953
2020-10-12 14:19:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.702290
2020-10-12 14:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.749893
2020-10-12 14:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034556
2020-10-12 14:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001888
2020-10-12 14:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.681987
2020-10-12 14:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.718949
2020-10-12 14:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:19:10 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 14:19:18 | INFO | train_inner | epoch 032:     22 / 1138 loss=4.614, nll_loss=3.023, ppl=8.13, wps=16936.5, ups=2.25, wpb=7539.2, bsz=301.6, num_updates=35300, lr=6.73244e-05, gnorm=0.959, clip=0, train_wall=36, wall=12912
2020-10-12 14:19:54 | INFO | train_inner | epoch 032:    122 / 1138 loss=4.609, nll_loss=3.017, ppl=8.09, wps=21036.2, ups=2.78, wpb=7555.4, bsz=280.4, num_updates=35400, lr=6.72293e-05, gnorm=0.974, clip=0, train_wall=35, wall=12948
2020-10-12 14:20:29 | INFO | train_inner | epoch 032:    222 / 1138 loss=4.611, nll_loss=3.019, ppl=8.11, wps=20750.4, ups=2.79, wpb=7432.2, bsz=268.9, num_updates=35500, lr=6.71345e-05, gnorm=0.969, clip=0, train_wall=35, wall=12983
2020-10-12 14:21:06 | INFO | train_inner | epoch 032:    322 / 1138 loss=4.611, nll_loss=3.02, ppl=8.11, wps=20769, ups=2.75, wpb=7556.1, bsz=286.2, num_updates=35600, lr=6.70402e-05, gnorm=0.956, clip=0, train_wall=36, wall=13020
2020-10-12 14:21:42 | INFO | train_inner | epoch 032:    422 / 1138 loss=4.598, nll_loss=3.006, ppl=8.03, wps=20595, ups=2.79, wpb=7369.4, bsz=282.6, num_updates=35700, lr=6.69462e-05, gnorm=0.97, clip=0, train_wall=35, wall=13056
2020-10-12 14:22:18 | INFO | train_inner | epoch 032:    522 / 1138 loss=4.59, nll_loss=2.994, ppl=7.97, wps=20544.9, ups=2.75, wpb=7471.3, bsz=284.6, num_updates=35800, lr=6.68526e-05, gnorm=0.97, clip=0, train_wall=36, wall=13092
2020-10-12 14:22:54 | INFO | train_inner | epoch 032:    622 / 1138 loss=4.6, nll_loss=3.008, ppl=8.04, wps=20642.9, ups=2.78, wpb=7423, bsz=306.4, num_updates=35900, lr=6.67595e-05, gnorm=0.972, clip=0, train_wall=36, wall=13128
2020-10-12 14:23:30 | INFO | train_inner | epoch 032:    722 / 1138 loss=4.613, nll_loss=3.022, ppl=8.12, wps=20480.9, ups=2.77, wpb=7405.1, bsz=282.6, num_updates=36000, lr=6.66667e-05, gnorm=0.976, clip=0, train_wall=36, wall=13164
2020-10-12 14:24:06 | INFO | train_inner | epoch 032:    822 / 1138 loss=4.62, nll_loss=3.031, ppl=8.17, wps=20339.5, ups=2.76, wpb=7356.6, bsz=284.1, num_updates=36100, lr=6.65743e-05, gnorm=0.979, clip=0, train_wall=36, wall=13200
2020-10-12 14:24:42 | INFO | train_inner | epoch 032:    922 / 1138 loss=4.632, nll_loss=3.044, ppl=8.25, wps=20639.1, ups=2.79, wpb=7399.4, bsz=245.3, num_updates=36200, lr=6.64822e-05, gnorm=0.988, clip=0, train_wall=35, wall=13236
2020-10-12 14:25:18 | INFO | train_inner | epoch 032:   1022 / 1138 loss=4.603, nll_loss=3.012, ppl=8.07, wps=20212.1, ups=2.79, wpb=7240.8, bsz=287, num_updates=36300, lr=6.63906e-05, gnorm=0.995, clip=0, train_wall=35, wall=13272
2020-10-12 14:25:54 | INFO | train_inner | epoch 032:   1122 / 1138 loss=4.598, nll_loss=3.006, ppl=8.03, wps=20175.7, ups=2.77, wpb=7292.9, bsz=302.4, num_updates=36400, lr=6.62994e-05, gnorm=0.975, clip=0, train_wall=36, wall=13308
2020-10-12 14:26:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 14:26:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:26:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.007281
2020-10-12 14:26:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:26:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072815
2020-10-12 14:26:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:26:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050817
2020-10-12 14:26:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.131266
2020-10-12 14:26:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:26:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:26:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000936
2020-10-12 14:26:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:26:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070357
2020-10-12 14:26:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:26:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050334
2020-10-12 14:26:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121938
2020-10-12 14:26:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:26:05 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 4.864 | nll_loss 3.191 | ppl 9.13 | wps 43954.3 | wpb 2418.3 | bsz 92.3 | num_updates 36416 | best_loss 4.861
2020-10-12 14:26:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 14:26:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_last.pt (epoch 32 @ 36416 updates, score 4.864) (writing took 1.1676302389823832 seconds)
2020-10-12 14:26:06 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 14:26:06 | INFO | train | epoch 032 | loss 4.608 | nll_loss 3.017 | ppl 8.09 | wps 20182 | ups 2.72 | wpb 7410.9 | bsz 282.4 | num_updates 36416 | lr 6.62848e-05 | gnorm 0.975 | clip 0 | train_wall 405 | wall 13320
2020-10-12 14:26:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 14:26:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 14:26:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:26:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003172
2020-10-12 14:26:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043450
2020-10-12 14:26:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:26:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001998
2020-10-12 14:26:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:26:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.713210
2020-10-12 14:26:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.759195
2020-10-12 14:26:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:26:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:26:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034978
2020-10-12 14:26:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:26:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002082
2020-10-12 14:26:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.740416
2020-10-12 14:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.778017
2020-10-12 14:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:26:08 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 14:26:37 | INFO | train_inner | epoch 033:     84 / 1138 loss=4.577, nll_loss=2.982, ppl=7.9, wps=16725.9, ups=2.31, wpb=7239.3, bsz=273.2, num_updates=36500, lr=6.62085e-05, gnorm=1, clip=0, train_wall=35, wall=13351
2020-10-12 14:27:13 | INFO | train_inner | epoch 033:    184 / 1138 loss=4.547, nll_loss=2.948, ppl=7.72, wps=20592.8, ups=2.78, wpb=7410.2, bsz=282.8, num_updates=36600, lr=6.6118e-05, gnorm=0.969, clip=0, train_wall=36, wall=13387
2020-10-12 14:27:49 | INFO | train_inner | epoch 033:    284 / 1138 loss=4.601, nll_loss=3.008, ppl=8.04, wps=20509, ups=2.77, wpb=7413.8, bsz=276.4, num_updates=36700, lr=6.60278e-05, gnorm=0.973, clip=0, train_wall=36, wall=13423
2020-10-12 14:28:26 | INFO | train_inner | epoch 033:    384 / 1138 loss=4.575, nll_loss=2.978, ppl=7.88, wps=20684.1, ups=2.75, wpb=7514.3, bsz=275.3, num_updates=36800, lr=6.5938e-05, gnorm=0.972, clip=0, train_wall=36, wall=13460
2020-10-12 14:29:02 | INFO | train_inner | epoch 033:    484 / 1138 loss=4.584, nll_loss=2.99, ppl=7.95, wps=20490.6, ups=2.79, wpb=7348.9, bsz=296.6, num_updates=36900, lr=6.58486e-05, gnorm=0.997, clip=0, train_wall=35, wall=13496
2020-10-12 14:29:38 | INFO | train_inner | epoch 033:    584 / 1138 loss=4.599, nll_loss=3.005, ppl=8.03, wps=20522, ups=2.76, wpb=7427.4, bsz=270.2, num_updates=37000, lr=6.57596e-05, gnorm=0.99, clip=0, train_wall=36, wall=13532
2020-10-12 14:30:14 | INFO | train_inner | epoch 033:    684 / 1138 loss=4.587, nll_loss=2.992, ppl=7.96, wps=20620.4, ups=2.77, wpb=7440.9, bsz=293.4, num_updates=37100, lr=6.56709e-05, gnorm=0.969, clip=0, train_wall=36, wall=13568
2020-10-12 14:30:50 | INFO | train_inner | epoch 033:    784 / 1138 loss=4.589, nll_loss=2.995, ppl=7.97, wps=20612.2, ups=2.77, wpb=7446.4, bsz=299.8, num_updates=37200, lr=6.55826e-05, gnorm=0.966, clip=0, train_wall=36, wall=13604
2020-10-12 14:31:26 | INFO | train_inner | epoch 033:    884 / 1138 loss=4.595, nll_loss=3.003, ppl=8.02, wps=20442.3, ups=2.79, wpb=7327.5, bsz=287.1, num_updates=37300, lr=6.54946e-05, gnorm=0.984, clip=0, train_wall=35, wall=13640
2020-10-12 14:32:02 | INFO | train_inner | epoch 033:    984 / 1138 loss=4.608, nll_loss=3.016, ppl=8.09, wps=20424.2, ups=2.74, wpb=7441.6, bsz=276.4, num_updates=37400, lr=6.5407e-05, gnorm=0.966, clip=0, train_wall=36, wall=13676
2020-10-12 14:32:38 | INFO | train_inner | epoch 033:   1084 / 1138 loss=4.593, nll_loss=3, ppl=8, wps=20623.7, ups=2.78, wpb=7430.9, bsz=280.6, num_updates=37500, lr=6.53197e-05, gnorm=0.97, clip=0, train_wall=36, wall=13712
2020-10-12 14:32:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 14:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001144
2020-10-12 14:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071046
2020-10-12 14:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050891
2020-10-12 14:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123420
2020-10-12 14:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000879
2020-10-12 14:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069506
2020-10-12 14:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050975
2020-10-12 14:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121678
2020-10-12 14:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:33:03 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 4.842 | nll_loss 3.162 | ppl 8.95 | wps 44690 | wpb 2418.3 | bsz 92.3 | num_updates 37554 | best_loss 4.842
2020-10-12 14:33:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 14:33:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 33 @ 37554 updates, score 4.842) (writing took 1.6012642990099266 seconds)
2020-10-12 14:33:04 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 14:33:04 | INFO | train | epoch 033 | loss 4.589 | nll_loss 2.995 | ppl 7.97 | wps 20165.6 | ups 2.72 | wpb 7410.9 | bsz 282.4 | num_updates 37554 | lr 6.52727e-05 | gnorm 0.977 | clip 0 | train_wall 405 | wall 13738
2020-10-12 14:33:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 14:33:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 14:33:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:33:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003319
2020-10-12 14:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.046440
2020-10-12 14:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001952
2020-10-12 14:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.689963
2020-10-12 14:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.738876
2020-10-12 14:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034879
2020-10-12 14:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001904
2020-10-12 14:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:33:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.675849
2020-10-12 14:33:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.713157
2020-10-12 14:33:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:33:06 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 14:33:22 | INFO | train_inner | epoch 034:     46 / 1138 loss=4.598, nll_loss=3.005, ppl=8.03, wps=17185.1, ups=2.27, wpb=7553.9, bsz=281.2, num_updates=37600, lr=6.52328e-05, gnorm=0.958, clip=0, train_wall=35, wall=13756
2020-10-12 14:33:58 | INFO | train_inner | epoch 034:    146 / 1138 loss=4.563, nll_loss=2.965, ppl=7.81, wps=20561.7, ups=2.8, wpb=7330.8, bsz=274, num_updates=37700, lr=6.51462e-05, gnorm=0.978, clip=0, train_wall=35, wall=13792
2020-10-12 14:34:35 | INFO | train_inner | epoch 034:    246 / 1138 loss=4.549, nll_loss=2.949, ppl=7.72, wps=20519.2, ups=2.74, wpb=7497.8, bsz=283.3, num_updates=37800, lr=6.506e-05, gnorm=0.962, clip=0, train_wall=36, wall=13829
2020-10-12 14:35:11 | INFO | train_inner | epoch 034:    346 / 1138 loss=4.553, nll_loss=2.953, ppl=7.75, wps=20751.5, ups=2.75, wpb=7535.4, bsz=292.6, num_updates=37900, lr=6.49741e-05, gnorm=0.966, clip=0, train_wall=36, wall=13865
2020-10-12 14:35:47 | INFO | train_inner | epoch 034:    446 / 1138 loss=4.573, nll_loss=2.977, ppl=7.87, wps=20588.2, ups=2.77, wpb=7424.6, bsz=285.1, num_updates=38000, lr=6.48886e-05, gnorm=0.982, clip=0, train_wall=36, wall=13901
2020-10-12 14:36:23 | INFO | train_inner | epoch 034:    546 / 1138 loss=4.572, nll_loss=2.975, ppl=7.86, wps=20757.1, ups=2.76, wpb=7508.8, bsz=296.5, num_updates=38100, lr=6.48034e-05, gnorm=0.963, clip=0, train_wall=36, wall=13937
2020-10-12 14:36:59 | INFO | train_inner | epoch 034:    646 / 1138 loss=4.589, nll_loss=2.995, ppl=7.97, wps=20476.6, ups=2.8, wpb=7312.1, bsz=262.4, num_updates=38200, lr=6.47185e-05, gnorm=0.997, clip=0, train_wall=35, wall=13973
2020-10-12 14:37:35 | INFO | train_inner | epoch 034:    746 / 1138 loss=4.572, nll_loss=2.977, ppl=7.87, wps=20303.7, ups=2.77, wpb=7324.2, bsz=274.2, num_updates=38300, lr=6.46339e-05, gnorm=0.985, clip=0, train_wall=36, wall=14009
2020-10-12 14:38:11 | INFO | train_inner | epoch 034:    846 / 1138 loss=4.597, nll_loss=3.004, ppl=8.02, wps=20587.6, ups=2.76, wpb=7456.6, bsz=270.7, num_updates=38400, lr=6.45497e-05, gnorm=0.991, clip=0, train_wall=36, wall=14045
2020-10-12 14:38:47 | INFO | train_inner | epoch 034:    946 / 1138 loss=4.576, nll_loss=2.981, ppl=7.89, wps=20647.6, ups=2.79, wpb=7394.6, bsz=300.8, num_updates=38500, lr=6.44658e-05, gnorm=0.99, clip=0, train_wall=35, wall=14081
2020-10-12 14:39:23 | INFO | train_inner | epoch 034:   1046 / 1138 loss=4.587, nll_loss=2.993, ppl=7.96, wps=20202, ups=2.78, wpb=7263.7, bsz=265, num_updates=38600, lr=6.43823e-05, gnorm=1.006, clip=0, train_wall=36, wall=14117
2020-10-12 14:39:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 14:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001097
2020-10-12 14:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070444
2020-10-12 14:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050349
2020-10-12 14:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122239
2020-10-12 14:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000945
2020-10-12 14:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070348
2020-10-12 14:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050144
2020-10-12 14:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121759
2020-10-12 14:39:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:40:01 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.838 | nll_loss 3.164 | ppl 8.97 | wps 44808.5 | wpb 2418.3 | bsz 92.3 | num_updates 38692 | best_loss 4.838
2020-10-12 14:40:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 14:40:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 34 @ 38692 updates, score 4.838) (writing took 1.5988397560140584 seconds)
2020-10-12 14:40:03 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 14:40:03 | INFO | train | epoch 034 | loss 4.57 | nll_loss 2.974 | ppl 7.86 | wps 20156.8 | ups 2.72 | wpb 7410.9 | bsz 282.4 | num_updates 38692 | lr 6.43057e-05 | gnorm 0.98 | clip 0 | train_wall 405 | wall 14157
2020-10-12 14:40:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 14:40:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 14:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:40:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003294
2020-10-12 14:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.047633
2020-10-12 14:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002155
2020-10-12 14:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.692430
2020-10-12 14:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.742834
2020-10-12 14:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034730
2020-10-12 14:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001899
2020-10-12 14:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.711982
2020-10-12 14:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.749150
2020-10-12 14:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:40:04 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 14:40:07 | INFO | train_inner | epoch 035:      8 / 1138 loss=4.55, nll_loss=2.951, ppl=7.73, wps=16684.4, ups=2.25, wpb=7399.2, bsz=292.2, num_updates=38700, lr=6.4299e-05, gnorm=0.978, clip=0, train_wall=36, wall=14161
2020-10-12 14:40:42 | INFO | train_inner | epoch 035:    108 / 1138 loss=4.53, nll_loss=2.928, ppl=7.61, wps=20509.6, ups=2.84, wpb=7231.9, bsz=275.4, num_updates=38800, lr=6.42161e-05, gnorm=0.993, clip=0, train_wall=35, wall=14196
2020-10-12 14:41:19 | INFO | train_inner | epoch 035:    208 / 1138 loss=4.513, nll_loss=2.909, ppl=7.51, wps=20495.8, ups=2.76, wpb=7431.7, bsz=294.6, num_updates=38900, lr=6.41335e-05, gnorm=0.974, clip=0, train_wall=36, wall=14233
2020-10-12 14:41:55 | INFO | train_inner | epoch 035:    308 / 1138 loss=4.537, nll_loss=2.936, ppl=7.65, wps=20718.7, ups=2.78, wpb=7463.9, bsz=302, num_updates=39000, lr=6.40513e-05, gnorm=0.974, clip=0, train_wall=36, wall=14269
2020-10-12 14:42:30 | INFO | train_inner | epoch 035:    408 / 1138 loss=4.571, nll_loss=2.975, ppl=7.86, wps=20087.4, ups=2.81, wpb=7157.2, bsz=252.6, num_updates=39100, lr=6.39693e-05, gnorm=1.007, clip=0, train_wall=35, wall=14304
2020-10-12 14:43:07 | INFO | train_inner | epoch 035:    508 / 1138 loss=4.539, nll_loss=2.937, ppl=7.66, wps=20686.3, ups=2.77, wpb=7467.5, bsz=276.3, num_updates=39200, lr=6.38877e-05, gnorm=0.972, clip=0, train_wall=36, wall=14340
2020-10-12 14:43:43 | INFO | train_inner | epoch 035:    608 / 1138 loss=4.558, nll_loss=2.96, ppl=7.78, wps=20490.1, ups=2.77, wpb=7407.2, bsz=279.4, num_updates=39300, lr=6.38063e-05, gnorm=0.992, clip=0, train_wall=36, wall=14377
2020-10-12 14:44:19 | INFO | train_inner | epoch 035:    708 / 1138 loss=4.552, nll_loss=2.953, ppl=7.75, wps=20508.2, ups=2.74, wpb=7478.4, bsz=307, num_updates=39400, lr=6.37253e-05, gnorm=0.984, clip=0, train_wall=36, wall=14413
2020-10-12 14:44:55 | INFO | train_inner | epoch 035:    808 / 1138 loss=4.578, nll_loss=2.982, ppl=7.9, wps=20636.5, ups=2.78, wpb=7419.9, bsz=267.8, num_updates=39500, lr=6.36446e-05, gnorm=0.992, clip=0, train_wall=36, wall=14449
2020-10-12 14:45:31 | INFO | train_inner | epoch 035:    908 / 1138 loss=4.583, nll_loss=2.988, ppl=7.93, wps=20508.6, ups=2.78, wpb=7383.6, bsz=283.4, num_updates=39600, lr=6.35642e-05, gnorm=0.983, clip=0, train_wall=36, wall=14485
2020-10-12 14:46:07 | INFO | train_inner | epoch 035:   1008 / 1138 loss=4.577, nll_loss=2.98, ppl=7.89, wps=20735, ups=2.76, wpb=7524.4, bsz=259, num_updates=39700, lr=6.34841e-05, gnorm=0.98, clip=0, train_wall=36, wall=14521
2020-10-12 14:46:44 | INFO | train_inner | epoch 035:   1108 / 1138 loss=4.553, nll_loss=2.954, ppl=7.75, wps=20658.9, ups=2.76, wpb=7483.5, bsz=299.8, num_updates=39800, lr=6.34043e-05, gnorm=0.972, clip=0, train_wall=36, wall=14558
2020-10-12 14:46:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 14:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001158
2020-10-12 14:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070054
2020-10-12 14:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049699
2020-10-12 14:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121252
2020-10-12 14:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000926
2020-10-12 14:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068598
2020-10-12 14:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050165
2020-10-12 14:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120001
2020-10-12 14:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:47:00 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.819 | nll_loss 3.141 | ppl 8.82 | wps 45069.5 | wpb 2418.3 | bsz 92.3 | num_updates 39830 | best_loss 4.819
2020-10-12 14:47:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 14:47:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 35 @ 39830 updates, score 4.819) (writing took 1.6056966559845023 seconds)
2020-10-12 14:47:01 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 14:47:01 | INFO | train | epoch 035 | loss 4.553 | nll_loss 2.955 | ppl 7.75 | wps 20150.2 | ups 2.72 | wpb 7410.9 | bsz 282.4 | num_updates 39830 | lr 6.33804e-05 | gnorm 0.983 | clip 0 | train_wall 405 | wall 14575
2020-10-12 14:47:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 14:47:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 14:47:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:47:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003435
2020-10-12 14:47:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.047460
2020-10-12 14:47:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:47:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002094
2020-10-12 14:47:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:47:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.697243
2020-10-12 14:47:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.747414
2020-10-12 14:47:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:47:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:47:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034302
2020-10-12 14:47:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:47:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001880
2020-10-12 14:47:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.685586
2020-10-12 14:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.722305
2020-10-12 14:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:47:03 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 14:47:28 | INFO | train_inner | epoch 036:     70 / 1138 loss=4.519, nll_loss=2.917, ppl=7.55, wps=16985.2, ups=2.26, wpb=7503.9, bsz=301.3, num_updates=39900, lr=6.33248e-05, gnorm=0.965, clip=0, train_wall=36, wall=14602
2020-10-12 14:48:04 | INFO | train_inner | epoch 036:    170 / 1138 loss=4.531, nll_loss=2.928, ppl=7.61, wps=20560.3, ups=2.76, wpb=7446.8, bsz=285.1, num_updates=40000, lr=6.32456e-05, gnorm=0.975, clip=0, train_wall=36, wall=14638
2020-10-12 14:48:40 | INFO | train_inner | epoch 036:    270 / 1138 loss=4.53, nll_loss=2.927, ppl=7.61, wps=20702.5, ups=2.8, wpb=7388.5, bsz=285.4, num_updates=40100, lr=6.31666e-05, gnorm=0.979, clip=0, train_wall=35, wall=14674
2020-10-12 14:49:16 | INFO | train_inner | epoch 036:    370 / 1138 loss=4.528, nll_loss=2.926, ppl=7.6, wps=20385.4, ups=2.75, wpb=7407.4, bsz=287.5, num_updates=40200, lr=6.3088e-05, gnorm=0.973, clip=0, train_wall=36, wall=14710
2020-10-12 14:49:52 | INFO | train_inner | epoch 036:    470 / 1138 loss=4.541, nll_loss=2.94, ppl=7.67, wps=20644, ups=2.77, wpb=7457.6, bsz=266.2, num_updates=40300, lr=6.30097e-05, gnorm=0.99, clip=0, train_wall=36, wall=14746
2020-10-12 14:50:28 | INFO | train_inner | epoch 036:    570 / 1138 loss=4.54, nll_loss=2.938, ppl=7.66, wps=20535.4, ups=2.79, wpb=7352.2, bsz=269.9, num_updates=40400, lr=6.29317e-05, gnorm=0.995, clip=0, train_wall=35, wall=14782
2020-10-12 14:51:04 | INFO | train_inner | epoch 036:    670 / 1138 loss=4.515, nll_loss=2.91, ppl=7.52, wps=20320.5, ups=2.74, wpb=7410, bsz=297, num_updates=40500, lr=6.28539e-05, gnorm=0.981, clip=0, train_wall=36, wall=14818
2020-10-12 14:51:41 | INFO | train_inner | epoch 036:    770 / 1138 loss=4.557, nll_loss=2.958, ppl=7.77, wps=20573.7, ups=2.76, wpb=7456.2, bsz=279.1, num_updates=40600, lr=6.27765e-05, gnorm=0.981, clip=0, train_wall=36, wall=14855
2020-10-12 14:52:17 | INFO | train_inner | epoch 036:    870 / 1138 loss=4.531, nll_loss=2.929, ppl=7.62, wps=20454, ups=2.77, wpb=7381.3, bsz=291.1, num_updates=40700, lr=6.26993e-05, gnorm=0.989, clip=0, train_wall=36, wall=14891
2020-10-12 14:52:53 | INFO | train_inner | epoch 036:    970 / 1138 loss=4.559, nll_loss=2.96, ppl=7.78, wps=20792.3, ups=2.79, wpb=7461.1, bsz=259.3, num_updates=40800, lr=6.26224e-05, gnorm=1.003, clip=0, train_wall=35, wall=14927
2020-10-12 14:53:29 | INFO | train_inner | epoch 036:   1070 / 1138 loss=4.556, nll_loss=2.959, ppl=7.77, wps=20264.2, ups=2.77, wpb=7311.5, bsz=293.1, num_updates=40900, lr=6.25458e-05, gnorm=0.984, clip=0, train_wall=36, wall=14963
2020-10-12 14:53:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 14:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001113
2020-10-12 14:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071334
2020-10-12 14:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050946
2020-10-12 14:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123732
2020-10-12 14:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000926
2020-10-12 14:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070093
2020-10-12 14:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050107
2020-10-12 14:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121453
2020-10-12 14:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:53:58 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 4.837 | nll_loss 3.164 | ppl 8.96 | wps 44560.9 | wpb 2418.3 | bsz 92.3 | num_updates 40968 | best_loss 4.819
2020-10-12 14:53:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 14:53:59 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_last.pt (epoch 36 @ 40968 updates, score 4.837) (writing took 1.122780473990133 seconds)
2020-10-12 14:53:59 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 14:53:59 | INFO | train | epoch 036 | loss 4.537 | nll_loss 2.936 | ppl 7.66 | wps 20172.8 | ups 2.72 | wpb 7410.9 | bsz 282.4 | num_updates 40968 | lr 6.24939e-05 | gnorm 0.984 | clip 0 | train_wall 405 | wall 14993
2020-10-12 14:53:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 14:53:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 14:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:53:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003002
2020-10-12 14:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043167
2020-10-12 14:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001928
2020-10-12 14:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.706896
2020-10-12 14:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.752512
2020-10-12 14:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 14:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034679
2020-10-12 14:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001910
2020-10-12 14:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:54:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.723447
2020-10-12 14:54:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.760568
2020-10-12 14:54:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 14:54:01 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 14:54:12 | INFO | train_inner | epoch 037:     32 / 1138 loss=4.535, nll_loss=2.934, ppl=7.64, wps=17066, ups=2.29, wpb=7446.3, bsz=280.6, num_updates=41000, lr=6.24695e-05, gnorm=0.986, clip=0, train_wall=35, wall=15006
2020-10-12 14:54:48 | INFO | train_inner | epoch 037:    132 / 1138 loss=4.491, nll_loss=2.884, ppl=7.38, wps=20720.2, ups=2.77, wpb=7490.8, bsz=300, num_updates=41100, lr=6.23935e-05, gnorm=0.972, clip=0, train_wall=36, wall=15042
2020-10-12 14:55:24 | INFO | train_inner | epoch 037:    232 / 1138 loss=4.507, nll_loss=2.902, ppl=7.47, wps=20469, ups=2.8, wpb=7306.4, bsz=278.2, num_updates=41200, lr=6.23177e-05, gnorm=0.986, clip=0, train_wall=35, wall=15078
2020-10-12 14:56:00 | INFO | train_inner | epoch 037:    332 / 1138 loss=4.525, nll_loss=2.921, ppl=7.57, wps=20366.9, ups=2.76, wpb=7385.4, bsz=255.9, num_updates=41300, lr=6.22422e-05, gnorm=0.99, clip=0, train_wall=36, wall=15114
2020-10-12 14:56:37 | INFO | train_inner | epoch 037:    432 / 1138 loss=4.525, nll_loss=2.921, ppl=7.57, wps=20912.8, ups=2.77, wpb=7550.1, bsz=273, num_updates=41400, lr=6.2167e-05, gnorm=0.971, clip=0, train_wall=36, wall=15151
2020-10-12 14:57:13 | INFO | train_inner | epoch 037:    532 / 1138 loss=4.491, nll_loss=2.884, ppl=7.38, wps=20628.9, ups=2.76, wpb=7482.8, bsz=302.4, num_updates=41500, lr=6.2092e-05, gnorm=0.97, clip=0, train_wall=36, wall=15187
2020-10-12 14:57:49 | INFO | train_inner | epoch 037:    632 / 1138 loss=4.523, nll_loss=2.92, ppl=7.57, wps=20455.1, ups=2.78, wpb=7353.1, bsz=274.9, num_updates=41600, lr=6.20174e-05, gnorm=1.01, clip=0, train_wall=36, wall=15223
2020-10-12 14:58:25 | INFO | train_inner | epoch 037:    732 / 1138 loss=4.533, nll_loss=2.932, ppl=7.63, wps=20753.7, ups=2.75, wpb=7544.8, bsz=300, num_updates=41700, lr=6.1943e-05, gnorm=0.978, clip=0, train_wall=36, wall=15259
2020-10-12 14:59:01 | INFO | train_inner | epoch 037:    832 / 1138 loss=4.55, nll_loss=2.95, ppl=7.73, wps=20364.7, ups=2.79, wpb=7294, bsz=259.3, num_updates=41800, lr=6.18688e-05, gnorm=1.019, clip=0, train_wall=35, wall=15295
2020-10-12 14:59:37 | INFO | train_inner | epoch 037:    932 / 1138 loss=4.512, nll_loss=2.908, ppl=7.51, wps=20303.9, ups=2.8, wpb=7260.5, bsz=292.3, num_updates=41900, lr=6.17949e-05, gnorm=0.987, clip=0, train_wall=35, wall=15331
2020-10-12 15:00:13 | INFO | train_inner | epoch 037:   1032 / 1138 loss=4.552, nll_loss=2.953, ppl=7.74, wps=20600.1, ups=2.77, wpb=7449.8, bsz=280, num_updates=42000, lr=6.17213e-05, gnorm=0.979, clip=0, train_wall=36, wall=15367
2020-10-12 15:00:49 | INFO | train_inner | epoch 037:   1132 / 1138 loss=4.537, nll_loss=2.936, ppl=7.65, wps=20625.5, ups=2.77, wpb=7455.2, bsz=295.1, num_updates=42100, lr=6.1648e-05, gnorm=0.98, clip=0, train_wall=36, wall=15403
2020-10-12 15:00:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 15:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 15:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001125
2020-10-12 15:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069523
2020-10-12 15:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050366
2020-10-12 15:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121350
2020-10-12 15:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 15:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000882
2020-10-12 15:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070643
2020-10-12 15:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050786
2020-10-12 15:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122635
2020-10-12 15:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:00:56 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 4.807 | nll_loss 3.131 | ppl 8.76 | wps 44694.3 | wpb 2418.3 | bsz 92.3 | num_updates 42106 | best_loss 4.807
2020-10-12 15:00:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 15:00:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 37 @ 42106 updates, score 4.807) (writing took 1.900234510016162 seconds)
2020-10-12 15:00:58 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 15:00:58 | INFO | train | epoch 037 | loss 4.522 | nll_loss 2.918 | ppl 7.56 | wps 20145.2 | ups 2.72 | wpb 7410.9 | bsz 282.4 | num_updates 42106 | lr 6.16436e-05 | gnorm 0.986 | clip 0 | train_wall 405 | wall 15412
2020-10-12 15:00:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 15:00:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 15:00:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 15:00:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002997
2020-10-12 15:00:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.042397
2020-10-12 15:00:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:00:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001882
2020-10-12 15:00:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:00:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.692511
2020-10-12 15:00:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.737313
2020-10-12 15:00:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:00:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 15:00:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034493
2020-10-12 15:00:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:00:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001871
2020-10-12 15:00:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.691948
2020-10-12 15:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.728842
2020-10-12 15:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:01:00 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 15:01:33 | INFO | train_inner | epoch 038:     94 / 1138 loss=4.478, nll_loss=2.869, ppl=7.31, wps=16871, ups=2.27, wpb=7440, bsz=279.6, num_updates=42200, lr=6.15749e-05, gnorm=0.97, clip=0, train_wall=35, wall=15447
2020-10-12 15:02:09 | INFO | train_inner | epoch 038:    194 / 1138 loss=4.503, nll_loss=2.896, ppl=7.45, wps=20584.4, ups=2.79, wpb=7378.2, bsz=259.9, num_updates=42300, lr=6.15021e-05, gnorm=0.986, clip=0, train_wall=35, wall=15483
2020-10-12 15:02:45 | INFO | train_inner | epoch 038:    294 / 1138 loss=4.5, nll_loss=2.893, ppl=7.43, wps=20667.1, ups=2.75, wpb=7517.7, bsz=285.8, num_updates=42400, lr=6.14295e-05, gnorm=0.982, clip=0, train_wall=36, wall=15519
2020-10-12 15:03:22 | INFO | train_inner | epoch 038:    394 / 1138 loss=4.499, nll_loss=2.893, ppl=7.43, wps=20306.9, ups=2.76, wpb=7357.1, bsz=296.4, num_updates=42500, lr=6.13572e-05, gnorm=0.977, clip=0, train_wall=36, wall=15556
2020-10-12 15:03:58 | INFO | train_inner | epoch 038:    494 / 1138 loss=4.496, nll_loss=2.889, ppl=7.41, wps=20691.2, ups=2.72, wpb=7599.6, bsz=306.2, num_updates=42600, lr=6.12851e-05, gnorm=0.982, clip=0, train_wall=36, wall=15592
2020-10-12 15:04:34 | INFO | train_inner | epoch 038:    594 / 1138 loss=4.486, nll_loss=2.878, ppl=7.35, wps=20293.7, ups=2.77, wpb=7316, bsz=298.5, num_updates=42700, lr=6.12133e-05, gnorm=0.996, clip=0, train_wall=36, wall=15628
2020-10-12 15:05:10 | INFO | train_inner | epoch 038:    694 / 1138 loss=4.528, nll_loss=2.926, ppl=7.6, wps=20556.2, ups=2.78, wpb=7386.7, bsz=271.1, num_updates=42800, lr=6.11418e-05, gnorm=1.003, clip=0, train_wall=35, wall=15664
2020-10-12 15:05:46 | INFO | train_inner | epoch 038:    794 / 1138 loss=4.53, nll_loss=2.927, ppl=7.6, wps=20623, ups=2.77, wpb=7453.7, bsz=263.8, num_updates=42900, lr=6.10705e-05, gnorm=0.989, clip=0, train_wall=36, wall=15700
2020-10-12 15:06:22 | INFO | train_inner | epoch 038:    894 / 1138 loss=4.499, nll_loss=2.893, ppl=7.43, wps=20609, ups=2.77, wpb=7435, bsz=295.4, num_updates=43000, lr=6.09994e-05, gnorm=0.986, clip=0, train_wall=36, wall=15736
2020-10-12 15:06:58 | INFO | train_inner | epoch 038:    994 / 1138 loss=4.518, nll_loss=2.916, ppl=7.55, wps=20349.3, ups=2.79, wpb=7303.1, bsz=283.5, num_updates=43100, lr=6.09286e-05, gnorm=1.001, clip=0, train_wall=35, wall=15772
2020-10-12 15:07:34 | INFO | train_inner | epoch 038:   1094 / 1138 loss=4.527, nll_loss=2.926, ppl=7.6, wps=20295.4, ups=2.78, wpb=7307, bsz=276.2, num_updates=43200, lr=6.08581e-05, gnorm=1.007, clip=0, train_wall=36, wall=15808
2020-10-12 15:07:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 15:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 15:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001090
2020-10-12 15:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070184
2020-10-12 15:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051110
2020-10-12 15:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122722
2020-10-12 15:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 15:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000996
2020-10-12 15:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069628
2020-10-12 15:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050970
2020-10-12 15:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121913
2020-10-12 15:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:07:55 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 4.81 | nll_loss 3.132 | ppl 8.77 | wps 44884.3 | wpb 2418.3 | bsz 92.3 | num_updates 43244 | best_loss 4.807
2020-10-12 15:07:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 15:07:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_last.pt (epoch 38 @ 43244 updates, score 4.81) (writing took 1.5212665679864585 seconds)
2020-10-12 15:07:57 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 15:07:57 | INFO | train | epoch 038 | loss 4.506 | nll_loss 2.901 | ppl 7.47 | wps 20144 | ups 2.72 | wpb 7410.9 | bsz 282.4 | num_updates 43244 | lr 6.08271e-05 | gnorm 0.989 | clip 0 | train_wall 406 | wall 15831
2020-10-12 15:07:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 15:07:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 15:07:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 15:07:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003075
2020-10-12 15:07:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044976
2020-10-12 15:07:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:07:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002037
2020-10-12 15:07:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:07:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.708621
2020-10-12 15:07:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.756181
2020-10-12 15:07:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:07:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 15:07:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034647
2020-10-12 15:07:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:07:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001918
2020-10-12 15:07:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:07:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.724771
2020-10-12 15:07:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.761871
2020-10-12 15:07:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:07:58 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 15:08:18 | INFO | train_inner | epoch 039:     56 / 1138 loss=4.489, nll_loss=2.882, ppl=7.37, wps=16827.7, ups=2.29, wpb=7346.1, bsz=277.5, num_updates=43300, lr=6.07877e-05, gnorm=0.987, clip=0, train_wall=35, wall=15852
2020-10-12 15:08:54 | INFO | train_inner | epoch 039:    156 / 1138 loss=4.484, nll_loss=2.875, ppl=7.34, wps=20620.5, ups=2.78, wpb=7417.2, bsz=275.9, num_updates=43400, lr=6.07177e-05, gnorm=0.983, clip=0, train_wall=36, wall=15888
2020-10-12 15:09:30 | INFO | train_inner | epoch 039:    256 / 1138 loss=4.466, nll_loss=2.856, ppl=7.24, wps=20620.4, ups=2.81, wpb=7331.4, bsz=275, num_updates=43500, lr=6.06478e-05, gnorm=1.003, clip=0, train_wall=35, wall=15924
2020-10-12 15:10:06 | INFO | train_inner | epoch 039:    356 / 1138 loss=4.506, nll_loss=2.9, ppl=7.47, wps=20177, ups=2.78, wpb=7269.2, bsz=270.5, num_updates=43600, lr=6.05783e-05, gnorm=1.015, clip=0, train_wall=36, wall=15960
2020-10-12 15:10:42 | INFO | train_inner | epoch 039:    456 / 1138 loss=4.483, nll_loss=2.874, ppl=7.33, wps=20556.4, ups=2.77, wpb=7421, bsz=301.5, num_updates=43700, lr=6.05089e-05, gnorm=0.982, clip=0, train_wall=36, wall=15996
2020-10-12 15:11:17 | INFO | train_inner | epoch 039:    556 / 1138 loss=4.482, nll_loss=2.873, ppl=7.32, wps=20478.3, ups=2.8, wpb=7325.2, bsz=284.9, num_updates=43800, lr=6.04398e-05, gnorm=0.993, clip=0, train_wall=35, wall=16031
2020-10-12 15:11:54 | INFO | train_inner | epoch 039:    656 / 1138 loss=4.501, nll_loss=2.895, ppl=7.44, wps=20624.1, ups=2.75, wpb=7486.1, bsz=268, num_updates=43900, lr=6.03709e-05, gnorm=0.98, clip=0, train_wall=36, wall=16068
2020-10-12 15:12:30 | INFO | train_inner | epoch 039:    756 / 1138 loss=4.51, nll_loss=2.904, ppl=7.48, wps=20623.5, ups=2.73, wpb=7545.3, bsz=271, num_updates=44000, lr=6.03023e-05, gnorm=0.994, clip=0, train_wall=36, wall=16104
2020-10-12 15:13:06 | INFO | train_inner | epoch 039:    856 / 1138 loss=4.499, nll_loss=2.893, ppl=7.43, wps=20495.1, ups=2.8, wpb=7321, bsz=283, num_updates=44100, lr=6.02339e-05, gnorm=1.004, clip=0, train_wall=35, wall=16140
2020-10-12 15:13:42 | INFO | train_inner | epoch 039:    956 / 1138 loss=4.509, nll_loss=2.903, ppl=7.48, wps=21020.3, ups=2.76, wpb=7629.8, bsz=278.6, num_updates=44200, lr=6.01657e-05, gnorm=0.969, clip=0, train_wall=36, wall=16176
2020-10-12 15:14:19 | INFO | train_inner | epoch 039:   1056 / 1138 loss=4.499, nll_loss=2.893, ppl=7.43, wps=20575.5, ups=2.76, wpb=7454.5, bsz=306.6, num_updates=44300, lr=6.00977e-05, gnorm=0.983, clip=0, train_wall=36, wall=16213
2020-10-12 15:14:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 15:14:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 15:14:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001127
2020-10-12 15:14:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:14:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070386
2020-10-12 15:14:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:14:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049832
2020-10-12 15:14:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121677
2020-10-12 15:14:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:14:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 15:14:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000915
2020-10-12 15:14:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:14:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068175
2020-10-12 15:14:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049328
2020-10-12 15:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118732
2020-10-12 15:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:14:53 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 4.809 | nll_loss 3.13 | ppl 8.76 | wps 43951.6 | wpb 2418.3 | bsz 92.3 | num_updates 44382 | best_loss 4.807
2020-10-12 15:14:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 15:14:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_last.pt (epoch 39 @ 44382 updates, score 4.809) (writing took 1.4060907030070666 seconds)
2020-10-12 15:14:55 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 15:14:55 | INFO | train | epoch 039 | loss 4.492 | nll_loss 2.885 | ppl 7.39 | wps 20173.3 | ups 2.72 | wpb 7410.9 | bsz 282.4 | num_updates 44382 | lr 6.00422e-05 | gnorm 0.989 | clip 0 | train_wall 405 | wall 16249
2020-10-12 15:14:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 15:14:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 15:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 15:14:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003173
2020-10-12 15:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.042827
2020-10-12 15:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001903
2020-10-12 15:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:14:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.707483
2020-10-12 15:14:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.752740
2020-10-12 15:14:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:14:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 15:14:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034327
2020-10-12 15:14:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:14:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001852
2020-10-12 15:14:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:14:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.701077
2020-10-12 15:14:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.737813
2020-10-12 15:14:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:14:56 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 15:15:03 | INFO | train_inner | epoch 040:     18 / 1138 loss=4.489, nll_loss=2.881, ppl=7.37, wps=16729.3, ups=2.26, wpb=7399.4, bsz=284, num_updates=44400, lr=6.003e-05, gnorm=0.984, clip=0, train_wall=36, wall=16257
2020-10-12 15:15:38 | INFO | train_inner | epoch 040:    118 / 1138 loss=4.468, nll_loss=2.856, ppl=7.24, wps=20916.8, ups=2.81, wpb=7430.6, bsz=279.2, num_updates=44500, lr=5.99625e-05, gnorm=0.972, clip=0, train_wall=35, wall=16292
2020-10-12 15:16:15 | INFO | train_inner | epoch 040:    218 / 1138 loss=4.463, nll_loss=2.851, ppl=7.21, wps=20905.1, ups=2.74, wpb=7634.2, bsz=298.7, num_updates=44600, lr=5.98953e-05, gnorm=0.964, clip=0, train_wall=36, wall=16329
2020-10-12 15:16:51 | INFO | train_inner | epoch 040:    318 / 1138 loss=4.474, nll_loss=2.864, ppl=7.28, wps=20431.4, ups=2.76, wpb=7395.4, bsz=285.8, num_updates=44700, lr=5.98282e-05, gnorm=0.983, clip=0, train_wall=36, wall=16365
2020-10-12 15:17:27 | INFO | train_inner | epoch 040:    418 / 1138 loss=4.461, nll_loss=2.85, ppl=7.21, wps=20582.2, ups=2.78, wpb=7405.7, bsz=284.5, num_updates=44800, lr=5.97614e-05, gnorm=0.988, clip=0, train_wall=36, wall=16401
2020-10-12 15:18:03 | INFO | train_inner | epoch 040:    518 / 1138 loss=4.504, nll_loss=2.897, ppl=7.45, wps=20682.4, ups=2.76, wpb=7504.5, bsz=260.3, num_updates=44900, lr=5.96948e-05, gnorm=0.992, clip=0, train_wall=36, wall=16437
2020-10-12 15:18:39 | INFO | train_inner | epoch 040:    618 / 1138 loss=4.477, nll_loss=2.867, ppl=7.3, wps=20485.5, ups=2.8, wpb=7323.9, bsz=279.8, num_updates=45000, lr=5.96285e-05, gnorm=1.002, clip=0, train_wall=35, wall=16473
2020-10-12 15:19:15 | INFO | train_inner | epoch 040:    718 / 1138 loss=4.434, nll_loss=2.819, ppl=7.06, wps=20343.1, ups=2.76, wpb=7366.5, bsz=342.4, num_updates=45100, lr=5.95623e-05, gnorm=0.982, clip=0, train_wall=36, wall=16509
2020-10-12 15:19:51 | INFO | train_inner | epoch 040:    818 / 1138 loss=4.497, nll_loss=2.891, ppl=7.42, wps=20340.9, ups=2.77, wpb=7347.6, bsz=276.3, num_updates=45200, lr=5.94964e-05, gnorm=1.005, clip=0, train_wall=36, wall=16545
2020-10-12 15:20:27 | INFO | train_inner | epoch 040:    918 / 1138 loss=4.495, nll_loss=2.888, ppl=7.4, wps=20435.5, ups=2.78, wpb=7340.8, bsz=265.9, num_updates=45300, lr=5.94307e-05, gnorm=0.998, clip=0, train_wall=35, wall=16581
2020-10-12 15:21:03 | INFO | train_inner | epoch 040:   1018 / 1138 loss=4.473, nll_loss=2.865, ppl=7.28, wps=20565.4, ups=2.8, wpb=7350.8, bsz=290.6, num_updates=45400, lr=5.93652e-05, gnorm=0.993, clip=0, train_wall=35, wall=16617
2020-10-12 15:21:39 | INFO | train_inner | epoch 040:   1118 / 1138 loss=4.517, nll_loss=2.912, ppl=7.53, wps=20358.1, ups=2.79, wpb=7296.2, bsz=246, num_updates=45500, lr=5.92999e-05, gnorm=1.012, clip=0, train_wall=35, wall=16653
2020-10-12 15:21:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 15:21:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 15:21:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001109
2020-10-12 15:21:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:21:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071271
2020-10-12 15:21:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051091
2020-10-12 15:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123816
2020-10-12 15:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 15:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000965
2020-10-12 15:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069070
2020-10-12 15:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051218
2020-10-12 15:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121575
2020-10-12 15:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:21:51 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 4.792 | nll_loss 3.11 | ppl 8.63 | wps 44856.4 | wpb 2418.3 | bsz 92.3 | num_updates 45520 | best_loss 4.792
2020-10-12 15:21:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 15:21:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/O2M/checkpoint_best.pt (epoch 40 @ 45520 updates, score 4.792) (writing took 1.6050468049943447 seconds)
2020-10-12 15:21:53 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 15:21:53 | INFO | train | epoch 040 | loss 4.478 | nll_loss 2.869 | ppl 7.3 | wps 20164 | ups 2.72 | wpb 7410.9 | bsz 282.4 | num_updates 45520 | lr 5.92869e-05 | gnorm 0.99 | clip 0 | train_wall 405 | wall 16667
2020-10-12 15:21:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 15:21:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 15:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 15:21:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003596
2020-10-12 15:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.047835
2020-10-12 15:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002251
2020-10-12 15:21:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:21:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.697663
2020-10-12 15:21:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.748376
2020-10-12 15:21:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 15:21:54 | INFO | fairseq_cli.train | done training in 16667.0 seconds
