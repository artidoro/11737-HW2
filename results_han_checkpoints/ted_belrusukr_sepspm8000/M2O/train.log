2020-10-12 06:10:23 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_belrusukr_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='bel-eng,rus-eng,ukr-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 06:10:23 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 06:10:23 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['bel', 'eng', 'rus', 'ukr']
2020-10-12 06:10:23 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bel] dictionary: 27012 types
2020-10-12 06:10:23 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 27012 types
2020-10-12 06:10:23 | INFO | fairseq.data.multilingual.multilingual_data_manager | [rus] dictionary: 27012 types
2020-10-12 06:10:23 | INFO | fairseq.data.multilingual.multilingual_data_manager | [ukr] dictionary: 27012 types
2020-10-12 06:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 06:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:10:23 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 06:10:23 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:bel-eng': 1, 'main:rus-eng': 1, 'main:ukr-eng': 1}
2020-10-12 06:10:23 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-12 06:10:23 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/M2O/valid.bel-eng.bel
2020-10-12 06:10:23 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/M2O/valid.bel-eng.eng
2020-10-12 06:10:23 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrusukr_sepspm8000/M2O/ valid bel-eng 248 examples
2020-10-12 06:10:23 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:rus-eng src_langtok: None; tgt_langtok: None
2020-10-12 06:10:23 | INFO | fairseq.data.data_utils | loaded 4814 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/M2O/valid.rus-eng.rus
2020-10-12 06:10:23 | INFO | fairseq.data.data_utils | loaded 4814 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/M2O/valid.rus-eng.eng
2020-10-12 06:10:23 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrusukr_sepspm8000/M2O/ valid rus-eng 4814 examples
2020-10-12 06:10:23 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:ukr-eng src_langtok: None; tgt_langtok: None
2020-10-12 06:10:23 | INFO | fairseq.data.data_utils | loaded 3060 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/M2O/valid.ukr-eng.ukr
2020-10-12 06:10:23 | INFO | fairseq.data.data_utils | loaded 3060 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/M2O/valid.ukr-eng.eng
2020-10-12 06:10:23 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrusukr_sepspm8000/M2O/ valid ukr-eng 3060 examples
2020-10-12 06:10:23 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(27012, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(27012, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=27012, bias=False)
  )
)
2020-10-12 06:10:23 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 06:10:23 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 06:10:23 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 06:10:23 | INFO | fairseq_cli.train | num. model params: 45373440 (num. trained: 45373440)
2020-10-12 06:10:25 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 06:10:25 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 06:10:25 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 06:10:25 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.732 GB ; name = GeForce RTX 2080 Ti                     
2020-10-12 06:10:25 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 06:10:25 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 06:10:25 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 06:10:25 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_last.pt
2020-10-12 06:10:25 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 06:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 06:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:10:25 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 06:10:25 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:bel-eng': 1, 'main:rus-eng': 1, 'main:ukr-eng': 1}
2020-10-12 06:10:25 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-12 06:10:25 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/M2O/train.bel-eng.bel
2020-10-12 06:10:25 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/M2O/train.bel-eng.eng
2020-10-12 06:10:25 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrusukr_sepspm8000/M2O/ train bel-eng 4509 examples
2020-10-12 06:10:25 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:rus-eng src_langtok: None; tgt_langtok: None
2020-10-12 06:10:25 | INFO | fairseq.data.data_utils | loaded 208397 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/M2O/train.rus-eng.rus
2020-10-12 06:10:25 | INFO | fairseq.data.data_utils | loaded 208397 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/M2O/train.rus-eng.eng
2020-10-12 06:10:25 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrusukr_sepspm8000/M2O/ train rus-eng 208397 examples
2020-10-12 06:10:25 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:ukr-eng src_langtok: None; tgt_langtok: None
2020-10-12 06:10:25 | INFO | fairseq.data.data_utils | loaded 108463 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/M2O/train.ukr-eng.ukr
2020-10-12 06:10:25 | INFO | fairseq.data.data_utils | loaded 108463 examples from: fairseq/data-bin/ted_belrusukr_sepspm8000/M2O/train.ukr-eng.eng
2020-10-12 06:10:25 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrusukr_sepspm8000/M2O/ train ukr-eng 108463 examples
2020-10-12 06:10:25 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:bel-eng', 4509), ('main:rus-eng', 208397), ('main:ukr-eng', 108463)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 06:10:25 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 06:10:25 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 321369
2020-10-12 06:10:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 321369; virtual dataset size 321369
2020-10-12 06:10:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 06:10:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:bel-eng': 4509, 'main:rus-eng': 208397, 'main:ukr-eng': 108463}; raw total size: 321369
2020-10-12 06:10:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:bel-eng': 4509, 'main:rus-eng': 208397, 'main:ukr-eng': 108463}; resampled total size: 321369
2020-10-12 06:10:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 06:10:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.022219
2020-10-12 06:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:10:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003774
2020-10-12 06:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044312
2020-10-12 06:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002100
2020-10-12 06:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.716916
2020-10-12 06:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.763871
2020-10-12 06:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:10:26 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 06:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035593
2020-10-12 06:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001956
2020-10-12 06:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.718903
2020-10-12 06:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.756987
2020-10-12 06:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:10:27 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 06:11:00 | INFO | train_inner | epoch 001:    100 / 1120 loss=14.33, nll_loss=14.206, ppl=18895.2, wps=24495.1, ups=3.11, wpb=7885.2, bsz=293.9, num_updates=100, lr=5.0975e-06, gnorm=4.467, clip=0, train_wall=32, wall=34
2020-10-12 06:11:32 | INFO | train_inner | epoch 001:    200 / 1120 loss=12.718, nll_loss=12.41, ppl=5442.53, wps=23901.2, ups=3.11, wpb=7681.4, bsz=307.2, num_updates=200, lr=1.0095e-05, gnorm=2.273, clip=0, train_wall=32, wall=66
2020-10-12 06:12:04 | INFO | train_inner | epoch 001:    300 / 1120 loss=11.851, nll_loss=11.442, ppl=2782.72, wps=23817.5, ups=3.07, wpb=7761.8, bsz=289.8, num_updates=300, lr=1.50925e-05, gnorm=1.881, clip=0, train_wall=32, wall=99
2020-10-12 06:12:37 | INFO | train_inner | epoch 001:    400 / 1120 loss=10.825, nll_loss=10.278, ppl=1241.47, wps=23613.5, ups=3.04, wpb=7756.9, bsz=286, num_updates=400, lr=2.009e-05, gnorm=1.971, clip=0, train_wall=32, wall=132
2020-10-12 06:13:10 | INFO | train_inner | epoch 001:    500 / 1120 loss=9.825, nll_loss=9.102, ppl=549.35, wps=23512.5, ups=3.04, wpb=7722.7, bsz=279.4, num_updates=500, lr=2.50875e-05, gnorm=1.74, clip=0, train_wall=32, wall=165
2020-10-12 06:13:43 | INFO | train_inner | epoch 001:    600 / 1120 loss=9.451, nll_loss=8.629, ppl=396.01, wps=23145.6, ups=3.06, wpb=7559.2, bsz=278.6, num_updates=600, lr=3.0085e-05, gnorm=1.541, clip=0, train_wall=32, wall=197
2020-10-12 06:14:16 | INFO | train_inner | epoch 001:    700 / 1120 loss=9.305, nll_loss=8.431, ppl=345.19, wps=22818.8, ups=3, wpb=7610.6, bsz=292.2, num_updates=700, lr=3.50825e-05, gnorm=1.367, clip=0, train_wall=33, wall=231
2020-10-12 06:14:49 | INFO | train_inner | epoch 001:    800 / 1120 loss=9.211, nll_loss=8.314, ppl=318.19, wps=22853.9, ups=2.99, wpb=7636.5, bsz=265.9, num_updates=800, lr=4.008e-05, gnorm=1.345, clip=0, train_wall=33, wall=264
2020-10-12 06:15:23 | INFO | train_inner | epoch 001:    900 / 1120 loss=8.973, nll_loss=8.042, ppl=263.56, wps=23008.8, ups=2.95, wpb=7795.1, bsz=283.9, num_updates=900, lr=4.50775e-05, gnorm=1.426, clip=0, train_wall=33, wall=298
2020-10-12 06:15:57 | INFO | train_inner | epoch 001:   1000 / 1120 loss=8.744, nll_loss=7.782, ppl=220.06, wps=23014.9, ups=2.93, wpb=7849.6, bsz=305.7, num_updates=1000, lr=5.0075e-05, gnorm=1.545, clip=0, train_wall=33, wall=332
2020-10-12 06:16:32 | INFO | train_inner | epoch 001:   1100 / 1120 loss=8.735, nll_loss=7.77, ppl=218.22, wps=22167.8, ups=2.92, wpb=7591.4, bsz=283.8, num_updates=1100, lr=5.50725e-05, gnorm=1.445, clip=0, train_wall=34, wall=366
2020-10-12 06:16:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 06:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001002
2020-10-12 06:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066512
2020-10-12 06:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051764
2020-10-12 06:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119622
2020-10-12 06:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000901
2020-10-12 06:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067164
2020-10-12 06:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050598
2020-10-12 06:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118993
2020-10-12 06:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
/home/han/Documents/11737-hw/assign2/fairseq/fairseq/utils.py:340: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2020-10-12 06:16:43 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.333 | nll_loss 7.265 | ppl 153.78 | wps 47085.8 | wpb 2406.1 | bsz 90.2 | num_updates 1120
2020-10-12 06:16:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 06:16:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 1 @ 1120 updates, score 8.333) (writing took 1.062404071999481 seconds)
2020-10-12 06:16:44 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 06:16:44 | INFO | train | epoch 001 | loss 10.343 | nll_loss 9.652 | ppl 804.75 | wps 22900.4 | ups 2.97 | wpb 7707.9 | bsz 286.9 | num_updates 1120 | lr 5.6072e-05 | gnorm 1.896 | clip 0 | train_wall 365 | wall 379
2020-10-12 06:16:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 06:16:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 06:16:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:16:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003975
2020-10-12 06:16:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044682
2020-10-12 06:16:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:16:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001922
2020-10-12 06:16:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:16:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.710618
2020-10-12 06:16:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.757756
2020-10-12 06:16:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:16:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:16:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035853
2020-10-12 06:16:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:16:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001923
2020-10-12 06:16:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:16:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.707520
2020-10-12 06:16:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.745836
2020-10-12 06:16:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:16:46 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 06:17:13 | INFO | train_inner | epoch 002:     80 / 1120 loss=8.556, nll_loss=7.567, ppl=189.67, wps=18409.9, ups=2.4, wpb=7683, bsz=275, num_updates=1200, lr=6.007e-05, gnorm=1.333, clip=0, train_wall=34, wall=408
2020-10-12 06:17:48 | INFO | train_inner | epoch 002:    180 / 1120 loss=8.492, nll_loss=7.495, ppl=180.34, wps=22159.9, ups=2.88, wpb=7684.8, bsz=293.2, num_updates=1300, lr=6.50675e-05, gnorm=1.349, clip=0, train_wall=34, wall=443
2020-10-12 06:18:23 | INFO | train_inner | epoch 002:    280 / 1120 loss=8.277, nll_loss=7.249, ppl=152.06, wps=22055.2, ups=2.87, wpb=7678.9, bsz=300.1, num_updates=1400, lr=7.0065e-05, gnorm=1.392, clip=0, train_wall=34, wall=478
2020-10-12 06:18:58 | INFO | train_inner | epoch 002:    380 / 1120 loss=8.258, nll_loss=7.227, ppl=149.82, wps=22035.8, ups=2.88, wpb=7660.3, bsz=265.5, num_updates=1500, lr=7.50625e-05, gnorm=1.321, clip=0, train_wall=34, wall=512
2020-10-12 06:19:33 | INFO | train_inner | epoch 002:    480 / 1120 loss=8.185, nll_loss=7.145, ppl=141.5, wps=21797.4, ups=2.85, wpb=7658.4, bsz=298.9, num_updates=1600, lr=8.006e-05, gnorm=1.369, clip=0, train_wall=35, wall=547
2020-10-12 06:20:08 | INFO | train_inner | epoch 002:    580 / 1120 loss=8.067, nll_loss=7.009, ppl=128.84, wps=21918.5, ups=2.84, wpb=7719.7, bsz=271.1, num_updates=1700, lr=8.50575e-05, gnorm=1.236, clip=0, train_wall=35, wall=583
2020-10-12 06:20:43 | INFO | train_inner | epoch 002:    680 / 1120 loss=8.075, nll_loss=7.018, ppl=129.62, wps=21662.4, ups=2.82, wpb=7668.8, bsz=302.7, num_updates=1800, lr=9.0055e-05, gnorm=1.326, clip=0, train_wall=35, wall=618
2020-10-12 06:21:19 | INFO | train_inner | epoch 002:    780 / 1120 loss=7.998, nll_loss=6.93, ppl=121.92, wps=21991.5, ups=2.83, wpb=7777.7, bsz=267.3, num_updates=1900, lr=9.50525e-05, gnorm=1.191, clip=0, train_wall=35, wall=653
2020-10-12 06:21:54 | INFO | train_inner | epoch 002:    880 / 1120 loss=7.78, nll_loss=6.681, ppl=102.58, wps=22050.9, ups=2.8, wpb=7866.3, bsz=319.6, num_updates=2000, lr=0.00010005, gnorm=1.38, clip=0, train_wall=35, wall=689
2020-10-12 06:22:29 | INFO | train_inner | epoch 002:    980 / 1120 loss=7.731, nll_loss=6.627, ppl=98.83, wps=21653, ups=2.85, wpb=7593.2, bsz=289.2, num_updates=2100, lr=0.000105048, gnorm=1.276, clip=0, train_wall=34, wall=724
2020-10-12 06:23:05 | INFO | train_inner | epoch 002:   1080 / 1120 loss=7.787, nll_loss=6.688, ppl=103.12, wps=22020.7, ups=2.84, wpb=7764.3, bsz=257.4, num_updates=2200, lr=0.000110045, gnorm=1.149, clip=0, train_wall=35, wall=759
2020-10-12 06:23:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 06:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001039
2020-10-12 06:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069078
2020-10-12 06:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052141
2020-10-12 06:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122601
2020-10-12 06:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000918
2020-10-12 06:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068785
2020-10-12 06:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051731
2020-10-12 06:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121763
2020-10-12 06:23:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:23:24 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.436 | nll_loss 6.254 | ppl 76.3 | wps 45290.9 | wpb 2406.1 | bsz 90.2 | num_updates 2240 | best_loss 7.436
2020-10-12 06:23:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 06:23:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 2 @ 2240 updates, score 7.436) (writing took 1.9803496419917792 seconds)
2020-10-12 06:23:26 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 06:23:26 | INFO | train | epoch 002 | loss 8.086 | nll_loss 7.03 | ppl 130.73 | wps 21494.7 | ups 2.79 | wpb 7707.9 | bsz 286.9 | num_updates 2240 | lr 0.000112044 | gnorm 1.303 | clip 0 | train_wall 386 | wall 781
2020-10-12 06:23:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 06:23:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 06:23:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:23:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003692
2020-10-12 06:23:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.045190
2020-10-12 06:23:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:23:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001865
2020-10-12 06:23:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:23:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.728207
2020-10-12 06:23:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.775792
2020-10-12 06:23:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:23:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:23:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034680
2020-10-12 06:23:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:23:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001763
2020-10-12 06:23:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:23:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.724054
2020-10-12 06:23:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.761024
2020-10-12 06:23:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:23:28 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 06:23:49 | INFO | train_inner | epoch 003:     60 / 1120 loss=7.717, nll_loss=6.609, ppl=97.59, wps=17800, ups=2.27, wpb=7833.3, bsz=292.2, num_updates=2300, lr=0.000115043, gnorm=1.187, clip=0, train_wall=35, wall=803
2020-10-12 06:24:24 | INFO | train_inner | epoch 003:    160 / 1120 loss=7.617, nll_loss=6.497, ppl=90.29, wps=21958.7, ups=2.83, wpb=7756.4, bsz=301.6, num_updates=2400, lr=0.00012004, gnorm=1.122, clip=0, train_wall=35, wall=839
2020-10-12 06:25:00 | INFO | train_inner | epoch 003:    260 / 1120 loss=7.602, nll_loss=6.476, ppl=89.04, wps=21734.5, ups=2.8, wpb=7763.8, bsz=292.5, num_updates=2500, lr=0.000125037, gnorm=1.157, clip=0, train_wall=35, wall=874
2020-10-12 06:25:35 | INFO | train_inner | epoch 003:    360 / 1120 loss=7.616, nll_loss=6.491, ppl=89.98, wps=21541.5, ups=2.83, wpb=7624.3, bsz=262.6, num_updates=2600, lr=0.000130035, gnorm=1.113, clip=0, train_wall=35, wall=910
2020-10-12 06:26:11 | INFO | train_inner | epoch 003:    460 / 1120 loss=7.538, nll_loss=6.404, ppl=84.69, wps=21714.3, ups=2.78, wpb=7804.3, bsz=293, num_updates=2700, lr=0.000135032, gnorm=1.098, clip=0, train_wall=35, wall=946
2020-10-12 06:26:47 | INFO | train_inner | epoch 003:    560 / 1120 loss=7.508, nll_loss=6.37, ppl=82.7, wps=21209.5, ups=2.82, wpb=7525, bsz=297.8, num_updates=2800, lr=0.00014003, gnorm=1.176, clip=0, train_wall=35, wall=981
2020-10-12 06:27:22 | INFO | train_inner | epoch 003:    660 / 1120 loss=7.497, nll_loss=6.356, ppl=81.91, wps=21433.2, ups=2.82, wpb=7613.6, bsz=264.5, num_updates=2900, lr=0.000145028, gnorm=1.032, clip=0, train_wall=35, wall=1017
2020-10-12 06:27:58 | INFO | train_inner | epoch 003:    760 / 1120 loss=7.374, nll_loss=6.218, ppl=74.42, wps=21693.3, ups=2.82, wpb=7687, bsz=272, num_updates=3000, lr=0.000150025, gnorm=1.039, clip=0, train_wall=35, wall=1052
2020-10-12 06:28:33 | INFO | train_inner | epoch 003:    860 / 1120 loss=7.343, nll_loss=6.181, ppl=72.57, wps=21631.4, ups=2.82, wpb=7675.5, bsz=271.6, num_updates=3100, lr=0.000155023, gnorm=1.046, clip=0, train_wall=35, wall=1088
2020-10-12 06:29:09 | INFO | train_inner | epoch 003:    960 / 1120 loss=7.257, nll_loss=6.083, ppl=67.81, wps=21807.4, ups=2.8, wpb=7778.2, bsz=286.4, num_updates=3200, lr=0.00016002, gnorm=1.03, clip=0, train_wall=35, wall=1123
2020-10-12 06:29:44 | INFO | train_inner | epoch 003:   1060 / 1120 loss=7.215, nll_loss=6.036, ppl=65.6, wps=21714.8, ups=2.8, wpb=7750.4, bsz=307, num_updates=3300, lr=0.000165018, gnorm=1.045, clip=0, train_wall=35, wall=1159
2020-10-12 06:30:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001023
2020-10-12 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069025
2020-10-12 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052167
2020-10-12 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122555
2020-10-12 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000940
2020-10-12 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067159
2020-10-12 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051574
2020-10-12 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120002
2020-10-12 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:30:11 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.929 | nll_loss 5.659 | ppl 50.52 | wps 44798.8 | wpb 2406.1 | bsz 90.2 | num_updates 3360 | best_loss 6.929
2020-10-12 06:30:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 06:30:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 3 @ 3360 updates, score 6.929) (writing took 1.927161850995617 seconds)
2020-10-12 06:30:13 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 06:30:13 | INFO | train | epoch 003 | loss 7.458 | nll_loss 6.313 | ppl 79.51 | wps 21214 | ups 2.75 | wpb 7707.9 | bsz 286.9 | num_updates 3360 | lr 0.000168016 | gnorm 1.09 | clip 0 | train_wall 392 | wall 1188
2020-10-12 06:30:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 06:30:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 06:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:30:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003748
2020-10-12 06:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.045101
2020-10-12 06:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001798
2020-10-12 06:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:30:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.719161
2020-10-12 06:30:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.766588
2020-10-12 06:30:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:30:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:30:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034747
2020-10-12 06:30:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:30:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001869
2020-10-12 06:30:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:30:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.709532
2020-10-12 06:30:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.746676
2020-10-12 06:30:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:30:14 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 06:30:29 | INFO | train_inner | epoch 004:     40 / 1120 loss=7.193, nll_loss=6.011, ppl=64.5, wps=17485, ups=2.26, wpb=7738.4, bsz=315.8, num_updates=3400, lr=0.000170015, gnorm=1.049, clip=0, train_wall=35, wall=1203
2020-10-12 06:31:04 | INFO | train_inner | epoch 004:    140 / 1120 loss=7.154, nll_loss=5.967, ppl=62.54, wps=21745.4, ups=2.8, wpb=7758, bsz=273.7, num_updates=3500, lr=0.000175013, gnorm=1.021, clip=0, train_wall=35, wall=1239
2020-10-12 06:31:40 | INFO | train_inner | epoch 004:    240 / 1120 loss=7.079, nll_loss=5.88, ppl=58.88, wps=21812.5, ups=2.82, wpb=7725.9, bsz=265.6, num_updates=3600, lr=0.00018001, gnorm=1.13, clip=0, train_wall=35, wall=1274
2020-10-12 06:32:16 | INFO | train_inner | epoch 004:    340 / 1120 loss=7.125, nll_loss=5.932, ppl=61.07, wps=21583.6, ups=2.79, wpb=7743.8, bsz=279.4, num_updates=3700, lr=0.000185008, gnorm=0.995, clip=0, train_wall=35, wall=1310
2020-10-12 06:32:52 | INFO | train_inner | epoch 004:    440 / 1120 loss=7.019, nll_loss=5.812, ppl=56.19, wps=21531.8, ups=2.78, wpb=7743.5, bsz=297.6, num_updates=3800, lr=0.000190005, gnorm=1.057, clip=0, train_wall=35, wall=1346
2020-10-12 06:33:27 | INFO | train_inner | epoch 004:    540 / 1120 loss=6.891, nll_loss=5.665, ppl=50.75, wps=21694.4, ups=2.82, wpb=7694.1, bsz=281.8, num_updates=3900, lr=0.000195003, gnorm=0.991, clip=0, train_wall=35, wall=1382
2020-10-12 06:34:03 | INFO | train_inner | epoch 004:    640 / 1120 loss=6.942, nll_loss=5.723, ppl=52.82, wps=21410.6, ups=2.79, wpb=7681, bsz=314.2, num_updates=4000, lr=0.0002, gnorm=1.077, clip=0, train_wall=35, wall=1418
2020-10-12 06:34:38 | INFO | train_inner | epoch 004:    740 / 1120 loss=6.791, nll_loss=5.55, ppl=46.86, wps=21747.1, ups=2.82, wpb=7711.4, bsz=278.6, num_updates=4100, lr=0.000197546, gnorm=1.067, clip=0, train_wall=35, wall=1453
2020-10-12 06:35:14 | INFO | train_inner | epoch 004:    840 / 1120 loss=6.774, nll_loss=5.532, ppl=46.26, wps=21606.2, ups=2.8, wpb=7705.8, bsz=274.6, num_updates=4200, lr=0.00019518, gnorm=1.055, clip=0, train_wall=35, wall=1489
2020-10-12 06:35:50 | INFO | train_inner | epoch 004:    940 / 1120 loss=6.698, nll_loss=5.445, ppl=43.57, wps=21608.8, ups=2.8, wpb=7708.9, bsz=309.7, num_updates=4300, lr=0.000192897, gnorm=1.121, clip=0, train_wall=35, wall=1524
2020-10-12 06:36:26 | INFO | train_inner | epoch 004:   1040 / 1120 loss=6.665, nll_loss=5.406, ppl=42.41, wps=21386.9, ups=2.78, wpb=7696.5, bsz=309.8, num_updates=4400, lr=0.000190693, gnorm=1.083, clip=0, train_wall=35, wall=1560
2020-10-12 06:36:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 06:36:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:36:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000953
2020-10-12 06:36:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:36:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067510
2020-10-12 06:36:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:36:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051994
2020-10-12 06:36:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120804
2020-10-12 06:36:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:36:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:36:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000932
2020-10-12 06:36:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:36:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068509
2020-10-12 06:36:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:36:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050979
2020-10-12 06:36:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120752
2020-10-12 06:36:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:36:59 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.373 | nll_loss 4.999 | ppl 31.98 | wps 44964 | wpb 2406.1 | bsz 90.2 | num_updates 4480 | best_loss 6.373
2020-10-12 06:36:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 06:37:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 4 @ 4480 updates, score 6.373) (writing took 1.5917691150098108 seconds)
2020-10-12 06:37:01 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 06:37:01 | INFO | train | epoch 004 | loss 6.907 | nll_loss 5.683 | ppl 51.38 | wps 21173.1 | ups 2.75 | wpb 7707.9 | bsz 286.9 | num_updates 4480 | lr 0.000188982 | gnorm 1.053 | clip 0 | train_wall 393 | wall 1595
2020-10-12 06:37:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 06:37:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 06:37:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:37:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004376
2020-10-12 06:37:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.048420
2020-10-12 06:37:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:37:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002161
2020-10-12 06:37:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:37:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.730333
2020-10-12 06:37:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.781469
2020-10-12 06:37:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:37:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:37:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034710
2020-10-12 06:37:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:37:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001885
2020-10-12 06:37:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.707613
2020-10-12 06:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.744747
2020-10-12 06:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:37:02 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 06:37:09 | INFO | train_inner | epoch 005:     20 / 1120 loss=6.671, nll_loss=5.414, ppl=42.62, wps=17270.4, ups=2.3, wpb=7515.3, bsz=266.2, num_updates=4500, lr=0.000188562, gnorm=1.017, clip=0, train_wall=35, wall=1604
2020-10-12 06:37:45 | INFO | train_inner | epoch 005:    120 / 1120 loss=6.585, nll_loss=5.316, ppl=39.85, wps=21315.5, ups=2.81, wpb=7589.8, bsz=287.5, num_updates=4600, lr=0.000186501, gnorm=1.074, clip=0, train_wall=35, wall=1640
2020-10-12 06:38:20 | INFO | train_inner | epoch 005:    220 / 1120 loss=6.528, nll_loss=5.25, ppl=38.06, wps=21293.3, ups=2.81, wpb=7584.7, bsz=282.8, num_updates=4700, lr=0.000184506, gnorm=1.078, clip=0, train_wall=35, wall=1675
2020-10-12 06:38:57 | INFO | train_inner | epoch 005:    320 / 1120 loss=6.406, nll_loss=5.111, ppl=34.57, wps=21929.2, ups=2.78, wpb=7901.4, bsz=304.3, num_updates=4800, lr=0.000182574, gnorm=1.003, clip=0, train_wall=35, wall=1711
2020-10-12 06:39:32 | INFO | train_inner | epoch 005:    420 / 1120 loss=6.398, nll_loss=5.102, ppl=34.34, wps=21503.2, ups=2.78, wpb=7724.4, bsz=297.1, num_updates=4900, lr=0.000180702, gnorm=1.074, clip=0, train_wall=35, wall=1747
2020-10-12 06:40:08 | INFO | train_inner | epoch 005:    520 / 1120 loss=6.406, nll_loss=5.11, ppl=34.53, wps=21392.9, ups=2.8, wpb=7644.4, bsz=269.6, num_updates=5000, lr=0.000178885, gnorm=1.019, clip=0, train_wall=35, wall=1783
2020-10-12 06:40:44 | INFO | train_inner | epoch 005:    620 / 1120 loss=6.332, nll_loss=5.025, ppl=32.55, wps=21725, ups=2.77, wpb=7849.3, bsz=297.6, num_updates=5100, lr=0.000177123, gnorm=1.031, clip=0, train_wall=36, wall=1819
2020-10-12 06:41:20 | INFO | train_inner | epoch 005:    720 / 1120 loss=6.256, nll_loss=4.939, ppl=30.68, wps=21527.8, ups=2.8, wpb=7693.3, bsz=291.1, num_updates=5200, lr=0.000175412, gnorm=1.047, clip=0, train_wall=35, wall=1855
2020-10-12 06:41:55 | INFO | train_inner | epoch 005:    820 / 1120 loss=6.234, nll_loss=4.914, ppl=30.14, wps=21295.9, ups=2.83, wpb=7520.2, bsz=247.3, num_updates=5300, lr=0.000173749, gnorm=1.004, clip=0, train_wall=35, wall=1890
2020-10-12 06:42:31 | INFO | train_inner | epoch 005:    920 / 1120 loss=6.135, nll_loss=4.801, ppl=27.89, wps=21766.8, ups=2.78, wpb=7816.5, bsz=283, num_updates=5400, lr=0.000172133, gnorm=0.986, clip=0, train_wall=35, wall=1926
2020-10-12 06:43:07 | INFO | train_inner | epoch 005:   1020 / 1120 loss=6.09, nll_loss=4.749, ppl=26.9, wps=21654, ups=2.77, wpb=7808.3, bsz=302.6, num_updates=5500, lr=0.000170561, gnorm=1.034, clip=0, train_wall=35, wall=1962
2020-10-12 06:43:43 | INFO | train_inner | epoch 005:   1120 / 1120 loss=6.128, nll_loss=4.793, ppl=27.72, wps=21574.1, ups=2.81, wpb=7667.1, bsz=290.7, num_updates=5600, lr=0.000169031, gnorm=1.035, clip=0, train_wall=35, wall=1998
2020-10-12 06:43:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 06:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001069
2020-10-12 06:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068714
2020-10-12 06:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051313
2020-10-12 06:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121436
2020-10-12 06:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000920
2020-10-12 06:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068828
2020-10-12 06:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050848
2020-10-12 06:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120923
2020-10-12 06:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:43:48 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.84 | nll_loss 4.381 | ppl 20.83 | wps 44830.2 | wpb 2406.1 | bsz 90.2 | num_updates 5600 | best_loss 5.84
2020-10-12 06:43:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 06:43:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 5 @ 5600 updates, score 5.84) (writing took 1.5797650360036641 seconds)
2020-10-12 06:43:50 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 06:43:50 | INFO | train | epoch 005 | loss 6.322 | nll_loss 5.015 | ppl 32.32 | wps 21109.2 | ups 2.74 | wpb 7707.9 | bsz 286.9 | num_updates 5600 | lr 0.000169031 | gnorm 1.035 | clip 0 | train_wall 394 | wall 2004
2020-10-12 06:43:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 06:43:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 06:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:43:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003259
2020-10-12 06:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.047154
2020-10-12 06:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002190
2020-10-12 06:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.727636
2020-10-12 06:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.777532
2020-10-12 06:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034717
2020-10-12 06:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001938
2020-10-12 06:43:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:43:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.713287
2020-10-12 06:43:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.750482
2020-10-12 06:43:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:43:51 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 06:44:27 | INFO | train_inner | epoch 006:    100 / 1120 loss=6.043, nll_loss=4.697, ppl=25.94, wps=17608, ups=2.27, wpb=7741.7, bsz=284.1, num_updates=5700, lr=0.000167542, gnorm=1.038, clip=0, train_wall=35, wall=2041
2020-10-12 06:45:03 | INFO | train_inner | epoch 006:    200 / 1120 loss=5.995, nll_loss=4.641, ppl=24.96, wps=21787.7, ups=2.78, wpb=7836.9, bsz=295.5, num_updates=5800, lr=0.000166091, gnorm=1.022, clip=0, train_wall=35, wall=2077
2020-10-12 06:45:39 | INFO | train_inner | epoch 006:    300 / 1120 loss=5.951, nll_loss=4.59, ppl=24.08, wps=21542.1, ups=2.79, wpb=7711.4, bsz=273.4, num_updates=5900, lr=0.000164677, gnorm=0.969, clip=0, train_wall=35, wall=2113
2020-10-12 06:46:15 | INFO | train_inner | epoch 006:    400 / 1120 loss=5.964, nll_loss=4.604, ppl=24.32, wps=21309, ups=2.77, wpb=7694.4, bsz=290.9, num_updates=6000, lr=0.000163299, gnorm=1.011, clip=0, train_wall=36, wall=2149
2020-10-12 06:46:50 | INFO | train_inner | epoch 006:    500 / 1120 loss=5.887, nll_loss=4.516, ppl=22.88, wps=21375.2, ups=2.81, wpb=7619.3, bsz=277.4, num_updates=6100, lr=0.000161955, gnorm=0.967, clip=0, train_wall=35, wall=2185
2020-10-12 06:47:26 | INFO | train_inner | epoch 006:    600 / 1120 loss=5.883, nll_loss=4.511, ppl=22.8, wps=21630.4, ups=2.77, wpb=7811.9, bsz=286.1, num_updates=6200, lr=0.000160644, gnorm=1.016, clip=0, train_wall=35, wall=2221
2020-10-12 06:48:03 | INFO | train_inner | epoch 006:    700 / 1120 loss=5.857, nll_loss=4.482, ppl=22.34, wps=21169.4, ups=2.77, wpb=7656, bsz=305.1, num_updates=6300, lr=0.000159364, gnorm=1.04, clip=0, train_wall=36, wall=2257
2020-10-12 06:48:38 | INFO | train_inner | epoch 006:    800 / 1120 loss=5.797, nll_loss=4.414, ppl=21.31, wps=21427.3, ups=2.8, wpb=7658.3, bsz=292.2, num_updates=6400, lr=0.000158114, gnorm=1.006, clip=0, train_wall=35, wall=2293
2020-10-12 06:49:14 | INFO | train_inner | epoch 006:    900 / 1120 loss=5.802, nll_loss=4.419, ppl=21.39, wps=21582.3, ups=2.8, wpb=7708.2, bsz=273.1, num_updates=6500, lr=0.000156893, gnorm=0.986, clip=0, train_wall=35, wall=2329
2020-10-12 06:49:50 | INFO | train_inner | epoch 006:   1000 / 1120 loss=5.746, nll_loss=4.355, ppl=20.46, wps=21462.1, ups=2.78, wpb=7724.3, bsz=315.5, num_updates=6600, lr=0.0001557, gnorm=0.958, clip=0, train_wall=35, wall=2365
2020-10-12 06:50:26 | INFO | train_inner | epoch 006:   1100 / 1120 loss=5.783, nll_loss=4.396, ppl=21.05, wps=21643.1, ups=2.79, wpb=7755.5, bsz=268.5, num_updates=6700, lr=0.000154533, gnorm=0.975, clip=0, train_wall=35, wall=2401
2020-10-12 06:50:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 06:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000953
2020-10-12 06:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068710
2020-10-12 06:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051107
2020-10-12 06:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121108
2020-10-12 06:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000886
2020-10-12 06:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068461
2020-10-12 06:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051880
2020-10-12 06:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121555
2020-10-12 06:50:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:50:38 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.498 | nll_loss 3.973 | ppl 15.7 | wps 44997.8 | wpb 2406.1 | bsz 90.2 | num_updates 6720 | best_loss 5.498
2020-10-12 06:50:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 06:50:39 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 6 @ 6720 updates, score 5.498) (writing took 1.5787589899846353 seconds)
2020-10-12 06:50:39 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 06:50:39 | INFO | train | epoch 006 | loss 5.882 | nll_loss 4.51 | ppl 22.79 | wps 21059.8 | ups 2.73 | wpb 7707.9 | bsz 286.9 | num_updates 6720 | lr 0.000154303 | gnorm 1 | clip 0 | train_wall 395 | wall 2414
2020-10-12 06:50:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 06:50:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 06:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:50:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003245
2020-10-12 06:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.046866
2020-10-12 06:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002071
2020-10-12 06:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.725149
2020-10-12 06:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.774708
2020-10-12 06:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034928
2020-10-12 06:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001872
2020-10-12 06:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:50:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.714359
2020-10-12 06:50:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.751691
2020-10-12 06:50:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:50:41 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 06:51:09 | INFO | train_inner | epoch 007:     80 / 1120 loss=5.7, nll_loss=4.303, ppl=19.73, wps=17436.3, ups=2.3, wpb=7588.3, bsz=256.6, num_updates=6800, lr=0.000153393, gnorm=0.98, clip=0, train_wall=35, wall=2444
2020-10-12 06:51:45 | INFO | train_inner | epoch 007:    180 / 1120 loss=5.618, nll_loss=4.209, ppl=18.5, wps=21652.2, ups=2.78, wpb=7776.4, bsz=288.2, num_updates=6900, lr=0.000152277, gnorm=0.902, clip=0, train_wall=35, wall=2480
2020-10-12 06:52:21 | INFO | train_inner | epoch 007:    280 / 1120 loss=5.593, nll_loss=4.18, ppl=18.13, wps=21197.3, ups=2.79, wpb=7595.5, bsz=290.4, num_updates=7000, lr=0.000151186, gnorm=0.93, clip=0, train_wall=35, wall=2516
2020-10-12 06:52:57 | INFO | train_inner | epoch 007:    380 / 1120 loss=5.588, nll_loss=4.174, ppl=18.05, wps=21607.2, ups=2.77, wpb=7791, bsz=304.2, num_updates=7100, lr=0.000150117, gnorm=0.916, clip=0, train_wall=35, wall=2552
2020-10-12 06:53:33 | INFO | train_inner | epoch 007:    480 / 1120 loss=5.562, nll_loss=4.144, ppl=17.68, wps=21627.2, ups=2.81, wpb=7704.4, bsz=270.6, num_updates=7200, lr=0.000149071, gnorm=0.931, clip=0, train_wall=35, wall=2588
2020-10-12 06:54:09 | INFO | train_inner | epoch 007:    580 / 1120 loss=5.579, nll_loss=4.162, ppl=17.9, wps=21489.4, ups=2.8, wpb=7670.6, bsz=266.7, num_updates=7300, lr=0.000148047, gnorm=0.953, clip=0, train_wall=35, wall=2623
2020-10-12 06:54:44 | INFO | train_inner | epoch 007:    680 / 1120 loss=5.584, nll_loss=4.168, ppl=17.98, wps=21537.1, ups=2.78, wpb=7738, bsz=271.9, num_updates=7400, lr=0.000147043, gnorm=0.909, clip=0, train_wall=35, wall=2659
2020-10-12 06:55:20 | INFO | train_inner | epoch 007:    780 / 1120 loss=5.518, nll_loss=4.094, ppl=17.08, wps=21277.7, ups=2.78, wpb=7662.2, bsz=304.7, num_updates=7500, lr=0.000146059, gnorm=0.983, clip=0, train_wall=35, wall=2695
2020-10-12 06:55:57 | INFO | train_inner | epoch 007:    880 / 1120 loss=5.484, nll_loss=4.055, ppl=16.62, wps=21653.5, ups=2.76, wpb=7853.2, bsz=311, num_updates=7600, lr=0.000145095, gnorm=0.893, clip=0, train_wall=36, wall=2731
2020-10-12 06:56:33 | INFO | train_inner | epoch 007:    980 / 1120 loss=5.495, nll_loss=4.067, ppl=16.76, wps=21115.9, ups=2.78, wpb=7605.9, bsz=289, num_updates=7700, lr=0.00014415, gnorm=0.944, clip=0, train_wall=35, wall=2767
2020-10-12 06:57:09 | INFO | train_inner | epoch 007:   1080 / 1120 loss=5.487, nll_loss=4.057, ppl=16.65, wps=21414.8, ups=2.78, wpb=7703.4, bsz=297.4, num_updates=7800, lr=0.000143223, gnorm=0.914, clip=0, train_wall=35, wall=2803
2020-10-12 06:57:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 06:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001045
2020-10-12 06:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068880
2020-10-12 06:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052235
2020-10-12 06:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122508
2020-10-12 06:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000932
2020-10-12 06:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068312
2020-10-12 06:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052010
2020-10-12 06:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121577
2020-10-12 06:57:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:57:28 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.307 | nll_loss 3.743 | ppl 13.39 | wps 44937.9 | wpb 2406.1 | bsz 90.2 | num_updates 7840 | best_loss 5.307
2020-10-12 06:57:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 06:57:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 7 @ 7840 updates, score 5.307) (writing took 1.5954812199925072 seconds)
2020-10-12 06:57:30 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 06:57:30 | INFO | train | epoch 007 | loss 5.558 | nll_loss 4.139 | ppl 17.62 | wps 21039.6 | ups 2.73 | wpb 7707.9 | bsz 286.9 | num_updates 7840 | lr 0.000142857 | gnorm 0.932 | clip 0 | train_wall 395 | wall 2824
2020-10-12 06:57:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 06:57:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 06:57:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:57:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003231
2020-10-12 06:57:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.047953
2020-10-12 06:57:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:57:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002123
2020-10-12 06:57:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:57:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.722068
2020-10-12 06:57:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.772772
2020-10-12 06:57:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:57:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 06:57:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035534
2020-10-12 06:57:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:57:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001950
2020-10-12 06:57:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:57:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.712189
2020-10-12 06:57:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.750222
2020-10-12 06:57:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 06:57:31 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 06:57:53 | INFO | train_inner | epoch 008:     60 / 1120 loss=5.398, nll_loss=3.957, ppl=15.53, wps=17555.2, ups=2.26, wpb=7764.4, bsz=302.8, num_updates=7900, lr=0.000142314, gnorm=0.941, clip=0, train_wall=35, wall=2848
2020-10-12 06:58:29 | INFO | train_inner | epoch 008:    160 / 1120 loss=5.346, nll_loss=3.898, ppl=14.91, wps=21496.4, ups=2.8, wpb=7664.3, bsz=313.1, num_updates=8000, lr=0.000141421, gnorm=0.908, clip=0, train_wall=35, wall=2883
2020-10-12 06:59:05 | INFO | train_inner | epoch 008:    260 / 1120 loss=5.336, nll_loss=3.885, ppl=14.78, wps=21819.7, ups=2.79, wpb=7832.1, bsz=285.8, num_updates=8100, lr=0.000140546, gnorm=0.917, clip=0, train_wall=35, wall=2919
2020-10-12 06:59:40 | INFO | train_inner | epoch 008:    360 / 1120 loss=5.395, nll_loss=3.953, ppl=15.48, wps=21187, ups=2.81, wpb=7542.9, bsz=285.7, num_updates=8200, lr=0.000139686, gnorm=0.932, clip=0, train_wall=35, wall=2955
2020-10-12 07:00:16 | INFO | train_inner | epoch 008:    460 / 1120 loss=5.415, nll_loss=3.975, ppl=15.72, wps=21540.1, ups=2.76, wpb=7815.4, bsz=278.8, num_updates=8300, lr=0.000138842, gnorm=0.909, clip=0, train_wall=36, wall=2991
2020-10-12 07:00:52 | INFO | train_inner | epoch 008:    560 / 1120 loss=5.341, nll_loss=3.891, ppl=14.84, wps=21305.7, ups=2.81, wpb=7587.4, bsz=264.9, num_updates=8400, lr=0.000138013, gnorm=0.912, clip=0, train_wall=35, wall=3027
2020-10-12 07:01:28 | INFO | train_inner | epoch 008:    660 / 1120 loss=5.334, nll_loss=3.883, ppl=14.75, wps=21287.3, ups=2.78, wpb=7665.9, bsz=280.1, num_updates=8500, lr=0.000137199, gnorm=0.887, clip=0, train_wall=35, wall=3063
2020-10-12 07:02:04 | INFO | train_inner | epoch 008:    760 / 1120 loss=5.314, nll_loss=3.861, ppl=14.53, wps=21608.7, ups=2.77, wpb=7804.5, bsz=298, num_updates=8600, lr=0.000136399, gnorm=0.921, clip=0, train_wall=36, wall=3099
2020-10-12 07:02:40 | INFO | train_inner | epoch 008:    860 / 1120 loss=5.324, nll_loss=3.872, ppl=14.64, wps=21474.8, ups=2.79, wpb=7694, bsz=261.8, num_updates=8700, lr=0.000135613, gnorm=0.869, clip=0, train_wall=35, wall=3135
2020-10-12 07:03:16 | INFO | train_inner | epoch 008:    960 / 1120 loss=5.275, nll_loss=3.817, ppl=14.09, wps=21196.4, ups=2.79, wpb=7595.8, bsz=304.5, num_updates=8800, lr=0.00013484, gnorm=0.903, clip=0, train_wall=35, wall=3170
2020-10-12 07:03:52 | INFO | train_inner | epoch 008:   1060 / 1120 loss=5.296, nll_loss=3.84, ppl=14.32, wps=21559.5, ups=2.74, wpb=7869.5, bsz=287.8, num_updates=8900, lr=0.00013408, gnorm=0.86, clip=0, train_wall=36, wall=3207
2020-10-12 07:04:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000952
2020-10-12 07:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068780
2020-10-12 07:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052895
2020-10-12 07:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122967
2020-10-12 07:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000915
2020-10-12 07:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068352
2020-10-12 07:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051849
2020-10-12 07:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121451
2020-10-12 07:04:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:04:19 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.131 | nll_loss 3.549 | ppl 11.7 | wps 44820.9 | wpb 2406.1 | bsz 90.2 | num_updates 8960 | best_loss 5.131
2020-10-12 07:04:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:04:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 8 @ 8960 updates, score 5.131) (writing took 1.9454125310003292 seconds)
2020-10-12 07:04:21 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 07:04:21 | INFO | train | epoch 008 | loss 5.334 | nll_loss 3.884 | ppl 14.76 | wps 21010.9 | ups 2.73 | wpb 7707.9 | bsz 286.9 | num_updates 8960 | lr 0.000133631 | gnorm 0.902 | clip 0 | train_wall 395 | wall 3235
2020-10-12 07:04:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 07:04:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 07:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:04:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003309
2020-10-12 07:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044074
2020-10-12 07:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001897
2020-10-12 07:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.722247
2020-10-12 07:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.768760
2020-10-12 07:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034665
2020-10-12 07:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001861
2020-10-12 07:04:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.705506
2020-10-12 07:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.742562
2020-10-12 07:04:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:04:22 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 07:04:36 | INFO | train_inner | epoch 009:     40 / 1120 loss=5.241, nll_loss=3.778, ppl=13.72, wps=17348.4, ups=2.27, wpb=7650.3, bsz=290.5, num_updates=9000, lr=0.000133333, gnorm=0.885, clip=0, train_wall=35, wall=3251
2020-10-12 07:05:12 | INFO | train_inner | epoch 009:    140 / 1120 loss=5.227, nll_loss=3.761, ppl=13.55, wps=21644.3, ups=2.78, wpb=7784.2, bsz=284.3, num_updates=9100, lr=0.000132599, gnorm=0.881, clip=0, train_wall=35, wall=3287
2020-10-12 07:05:48 | INFO | train_inner | epoch 009:    240 / 1120 loss=5.204, nll_loss=3.735, ppl=13.32, wps=21517, ups=2.8, wpb=7675.7, bsz=280.4, num_updates=9200, lr=0.000131876, gnorm=0.888, clip=0, train_wall=35, wall=3323
2020-10-12 07:06:24 | INFO | train_inner | epoch 009:    340 / 1120 loss=5.178, nll_loss=3.705, ppl=13.05, wps=21317.9, ups=2.79, wpb=7640.8, bsz=292, num_updates=9300, lr=0.000131165, gnorm=0.861, clip=0, train_wall=35, wall=3359
2020-10-12 07:06:59 | INFO | train_inner | epoch 009:    440 / 1120 loss=5.176, nll_loss=3.704, ppl=13.03, wps=21184, ups=2.83, wpb=7491.4, bsz=263, num_updates=9400, lr=0.000130466, gnorm=0.874, clip=0, train_wall=35, wall=3394
2020-10-12 07:07:35 | INFO | train_inner | epoch 009:    540 / 1120 loss=5.207, nll_loss=3.737, ppl=13.34, wps=21197.2, ups=2.77, wpb=7653.1, bsz=288.4, num_updates=9500, lr=0.000129777, gnorm=0.867, clip=0, train_wall=35, wall=3430
2020-10-12 07:08:11 | INFO | train_inner | epoch 009:    640 / 1120 loss=5.138, nll_loss=3.661, ppl=12.65, wps=21438, ups=2.78, wpb=7720.5, bsz=269.2, num_updates=9600, lr=0.000129099, gnorm=0.854, clip=0, train_wall=35, wall=3466
2020-10-12 07:08:48 | INFO | train_inner | epoch 009:    740 / 1120 loss=5.19, nll_loss=3.718, ppl=13.16, wps=21473.1, ups=2.75, wpb=7796.8, bsz=293.2, num_updates=9700, lr=0.000128432, gnorm=0.874, clip=0, train_wall=36, wall=3502
2020-10-12 07:09:23 | INFO | train_inner | epoch 009:    840 / 1120 loss=5.131, nll_loss=3.651, ppl=12.56, wps=21691, ups=2.8, wpb=7737.7, bsz=272.4, num_updates=9800, lr=0.000127775, gnorm=0.868, clip=0, train_wall=35, wall=3538
2020-10-12 07:10:00 | INFO | train_inner | epoch 009:    940 / 1120 loss=5.08, nll_loss=3.594, ppl=12.08, wps=21442.7, ups=2.75, wpb=7811.4, bsz=334.7, num_updates=9900, lr=0.000127128, gnorm=0.854, clip=0, train_wall=36, wall=3574
2020-10-12 07:10:36 | INFO | train_inner | epoch 009:   1040 / 1120 loss=5.165, nll_loss=3.69, ppl=12.91, wps=21589.3, ups=2.78, wpb=7775.1, bsz=274.1, num_updates=10000, lr=0.000126491, gnorm=0.855, clip=0, train_wall=35, wall=3610
2020-10-12 07:11:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001067
2020-10-12 07:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068566
2020-10-12 07:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:11:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052097
2020-10-12 07:11:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122068
2020-10-12 07:11:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:11:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:11:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000931
2020-10-12 07:11:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:11:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068695
2020-10-12 07:11:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:11:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050703
2020-10-12 07:11:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120690
2020-10-12 07:11:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:11:10 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.011 | nll_loss 3.405 | ppl 10.59 | wps 44747.5 | wpb 2406.1 | bsz 90.2 | num_updates 10080 | best_loss 5.011
2020-10-12 07:11:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:11:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 9 @ 10080 updates, score 5.011) (writing took 1.5788786270131823 seconds)
2020-10-12 07:11:11 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 07:11:11 | INFO | train | epoch 009 | loss 5.166 | nll_loss 3.692 | ppl 12.92 | wps 21032.3 | ups 2.73 | wpb 7707.9 | bsz 286.9 | num_updates 10080 | lr 0.000125988 | gnorm 0.869 | clip 0 | train_wall 395 | wall 3646
2020-10-12 07:11:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 07:11:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:11:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003203
2020-10-12 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.046877
2020-10-12 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002146
2020-10-12 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:11:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.722326
2020-10-12 07:11:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.771896
2020-10-12 07:11:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:11:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:11:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034787
2020-10-12 07:11:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:11:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001862
2020-10-12 07:11:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:11:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.704968
2020-10-12 07:11:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.742153
2020-10-12 07:11:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:11:13 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 07:11:20 | INFO | train_inner | epoch 010:     20 / 1120 loss=5.106, nll_loss=3.623, ppl=12.32, wps=17579.4, ups=2.26, wpb=7763.5, bsz=300, num_updates=10100, lr=0.000125863, gnorm=0.867, clip=0, train_wall=35, wall=3655
2020-10-12 07:11:56 | INFO | train_inner | epoch 010:    120 / 1120 loss=5.059, nll_loss=3.57, ppl=11.88, wps=21487.3, ups=2.78, wpb=7717.9, bsz=293.9, num_updates=10200, lr=0.000125245, gnorm=0.896, clip=0, train_wall=35, wall=3691
2020-10-12 07:12:32 | INFO | train_inner | epoch 010:    220 / 1120 loss=5.08, nll_loss=3.594, ppl=12.07, wps=21242, ups=2.79, wpb=7626.7, bsz=266.6, num_updates=10300, lr=0.000124635, gnorm=0.855, clip=0, train_wall=35, wall=3726
2020-10-12 07:13:08 | INFO | train_inner | epoch 010:    320 / 1120 loss=5.016, nll_loss=3.522, ppl=11.49, wps=21259.6, ups=2.79, wpb=7606.6, bsz=313, num_updates=10400, lr=0.000124035, gnorm=0.871, clip=0, train_wall=35, wall=3762
2020-10-12 07:13:44 | INFO | train_inner | epoch 010:    420 / 1120 loss=5.062, nll_loss=3.573, ppl=11.9, wps=21601.1, ups=2.78, wpb=7771.8, bsz=286.1, num_updates=10500, lr=0.000123443, gnorm=0.868, clip=0, train_wall=35, wall=3798
2020-10-12 07:14:20 | INFO | train_inner | epoch 010:    520 / 1120 loss=5.041, nll_loss=3.55, ppl=11.71, wps=21504.9, ups=2.77, wpb=7764.5, bsz=304.5, num_updates=10600, lr=0.000122859, gnorm=0.844, clip=0, train_wall=35, wall=3834
2020-10-12 07:14:56 | INFO | train_inner | epoch 010:    620 / 1120 loss=5.02, nll_loss=3.526, ppl=11.52, wps=21833.2, ups=2.78, wpb=7856, bsz=278.6, num_updates=10700, lr=0.000122284, gnorm=0.848, clip=0, train_wall=35, wall=3870
2020-10-12 07:15:32 | INFO | train_inner | epoch 010:    720 / 1120 loss=5.046, nll_loss=3.556, ppl=11.76, wps=21429.1, ups=2.78, wpb=7696.2, bsz=269.5, num_updates=10800, lr=0.000121716, gnorm=0.858, clip=0, train_wall=35, wall=3906
2020-10-12 07:16:08 | INFO | train_inner | epoch 010:    820 / 1120 loss=5.034, nll_loss=3.541, ppl=11.64, wps=21523, ups=2.77, wpb=7762.1, bsz=290.6, num_updates=10900, lr=0.000121157, gnorm=0.866, clip=0, train_wall=35, wall=3942
2020-10-12 07:16:44 | INFO | train_inner | epoch 010:    920 / 1120 loss=5.062, nll_loss=3.572, ppl=11.89, wps=20934.4, ups=2.77, wpb=7566, bsz=281.3, num_updates=11000, lr=0.000120605, gnorm=0.889, clip=0, train_wall=36, wall=3978
2020-10-12 07:17:20 | INFO | train_inner | epoch 010:   1020 / 1120 loss=5.042, nll_loss=3.55, ppl=11.71, wps=21256.5, ups=2.78, wpb=7645.7, bsz=281.2, num_updates=11100, lr=0.00012006, gnorm=0.873, clip=0, train_wall=35, wall=4014
2020-10-12 07:17:56 | INFO | train_inner | epoch 010:   1120 / 1120 loss=4.973, nll_loss=3.473, ppl=11.1, wps=21470.5, ups=2.79, wpb=7683.1, bsz=285.2, num_updates=11200, lr=0.000119523, gnorm=0.857, clip=0, train_wall=35, wall=4050
2020-10-12 07:17:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000936
2020-10-12 07:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068539
2020-10-12 07:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051996
2020-10-12 07:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121805
2020-10-12 07:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000873
2020-10-12 07:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069453
2020-10-12 07:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051652
2020-10-12 07:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122303
2020-10-12 07:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:18:01 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.925 | nll_loss 3.319 | ppl 9.98 | wps 44881.8 | wpb 2406.1 | bsz 90.2 | num_updates 11200 | best_loss 4.925
2020-10-12 07:18:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:18:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 10 @ 11200 updates, score 4.925) (writing took 1.6718355499906465 seconds)
2020-10-12 07:18:02 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 07:18:02 | INFO | train | epoch 010 | loss 5.04 | nll_loss 3.548 | ppl 11.7 | wps 20994.6 | ups 2.72 | wpb 7707.9 | bsz 286.9 | num_updates 11200 | lr 0.000119523 | gnorm 0.865 | clip 0 | train_wall 396 | wall 4057
2020-10-12 07:18:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 07:18:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 07:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:18:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004705
2020-10-12 07:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.050144
2020-10-12 07:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001826
2020-10-12 07:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:18:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.724044
2020-10-12 07:18:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.776546
2020-10-12 07:18:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:18:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:18:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034675
2020-10-12 07:18:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:18:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001797
2020-10-12 07:18:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:18:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.703728
2020-10-12 07:18:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.740731
2020-10-12 07:18:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:18:04 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 07:18:39 | INFO | train_inner | epoch 011:    100 / 1120 loss=4.924, nll_loss=3.417, ppl=10.68, wps=17457.5, ups=2.28, wpb=7669.2, bsz=286.9, num_updates=11300, lr=0.000118993, gnorm=0.839, clip=0, train_wall=35, wall=4094
2020-10-12 07:19:15 | INFO | train_inner | epoch 011:    200 / 1120 loss=4.919, nll_loss=3.412, ppl=10.64, wps=21866, ups=2.79, wpb=7841.2, bsz=294.5, num_updates=11400, lr=0.00011847, gnorm=0.843, clip=0, train_wall=35, wall=4130
2020-10-12 07:19:52 | INFO | train_inner | epoch 011:    300 / 1120 loss=4.94, nll_loss=3.434, ppl=10.81, wps=21498.1, ups=2.75, wpb=7825.4, bsz=308, num_updates=11500, lr=0.000117954, gnorm=0.846, clip=0, train_wall=36, wall=4166
2020-10-12 07:20:28 | INFO | train_inner | epoch 011:    400 / 1120 loss=4.983, nll_loss=3.483, ppl=11.18, wps=21370.5, ups=2.77, wpb=7705.8, bsz=268.7, num_updates=11600, lr=0.000117444, gnorm=0.886, clip=0, train_wall=35, wall=4202
2020-10-12 07:21:04 | INFO | train_inner | epoch 011:    500 / 1120 loss=4.94, nll_loss=3.434, ppl=10.81, wps=21256.2, ups=2.76, wpb=7692.3, bsz=301.4, num_updates=11700, lr=0.000116941, gnorm=0.852, clip=0, train_wall=36, wall=4239
2020-10-12 07:21:40 | INFO | train_inner | epoch 011:    600 / 1120 loss=4.969, nll_loss=3.467, ppl=11.06, wps=21327.9, ups=2.77, wpb=7709.3, bsz=270.8, num_updates=11800, lr=0.000116445, gnorm=0.863, clip=0, train_wall=36, wall=4275
2020-10-12 07:22:16 | INFO | train_inner | epoch 011:    700 / 1120 loss=4.912, nll_loss=3.403, ppl=10.58, wps=21653.9, ups=2.77, wpb=7806, bsz=298.2, num_updates=11900, lr=0.000115954, gnorm=0.824, clip=0, train_wall=35, wall=4311
2020-10-12 07:22:52 | INFO | train_inner | epoch 011:    800 / 1120 loss=4.945, nll_loss=3.441, ppl=10.86, wps=20930.8, ups=2.78, wpb=7527.3, bsz=302.6, num_updates=12000, lr=0.00011547, gnorm=0.883, clip=0, train_wall=35, wall=4347
2020-10-12 07:23:28 | INFO | train_inner | epoch 011:    900 / 1120 loss=4.904, nll_loss=3.394, ppl=10.51, wps=21296.8, ups=2.81, wpb=7575.3, bsz=249.2, num_updates=12100, lr=0.000114992, gnorm=0.843, clip=0, train_wall=35, wall=4382
2020-10-12 07:24:04 | INFO | train_inner | epoch 011:   1000 / 1120 loss=4.924, nll_loss=3.417, ppl=10.68, wps=21559.4, ups=2.75, wpb=7843, bsz=301.8, num_updates=12200, lr=0.00011452, gnorm=0.832, clip=0, train_wall=36, wall=4419
2020-10-12 07:24:40 | INFO | train_inner | epoch 011:   1100 / 1120 loss=4.944, nll_loss=3.439, ppl=10.85, wps=21211.5, ups=2.81, wpb=7556.1, bsz=261.4, num_updates=12300, lr=0.000114053, gnorm=0.868, clip=0, train_wall=35, wall=4454
2020-10-12 07:24:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:24:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:24:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001066
2020-10-12 07:24:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:24:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069756
2020-10-12 07:24:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:24:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052408
2020-10-12 07:24:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123571
2020-10-12 07:24:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:24:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:24:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000927
2020-10-12 07:24:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:24:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068821
2020-10-12 07:24:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:24:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051216
2020-10-12 07:24:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121289
2020-10-12 07:24:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:24:52 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 4.848 | nll_loss 3.228 | ppl 9.37 | wps 44791.3 | wpb 2406.1 | bsz 90.2 | num_updates 12320 | best_loss 4.848
2020-10-12 07:24:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:24:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 11 @ 12320 updates, score 4.848) (writing took 1.7844067759870086 seconds)
2020-10-12 07:24:54 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 07:24:54 | INFO | train | epoch 011 | loss 4.934 | nll_loss 3.428 | ppl 10.77 | wps 20983.3 | ups 2.72 | wpb 7707.9 | bsz 286.9 | num_updates 12320 | lr 0.000113961 | gnorm 0.852 | clip 0 | train_wall 396 | wall 4468
2020-10-12 07:24:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 07:24:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 07:24:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:24:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003289
2020-10-12 07:24:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.045648
2020-10-12 07:24:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:24:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001893
2020-10-12 07:24:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:24:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.716678
2020-10-12 07:24:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.764755
2020-10-12 07:24:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035469
2020-10-12 07:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001874
2020-10-12 07:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.719204
2020-10-12 07:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.757080
2020-10-12 07:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:24:55 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 07:25:24 | INFO | train_inner | epoch 012:     80 / 1120 loss=4.84, nll_loss=3.322, ppl=10, wps=17589.3, ups=2.28, wpb=7724.2, bsz=307, num_updates=12400, lr=0.000113592, gnorm=0.838, clip=0, train_wall=35, wall=4498
2020-10-12 07:26:00 | INFO | train_inner | epoch 012:    180 / 1120 loss=4.815, nll_loss=3.294, ppl=9.81, wps=21341.7, ups=2.77, wpb=7715.9, bsz=315.8, num_updates=12500, lr=0.000113137, gnorm=0.838, clip=0, train_wall=36, wall=4534
2020-10-12 07:26:36 | INFO | train_inner | epoch 012:    280 / 1120 loss=4.873, nll_loss=3.358, ppl=10.26, wps=21636.6, ups=2.76, wpb=7837, bsz=273, num_updates=12600, lr=0.000112687, gnorm=0.827, clip=0, train_wall=36, wall=4571
2020-10-12 07:27:12 | INFO | train_inner | epoch 012:    380 / 1120 loss=4.88, nll_loss=3.367, ppl=10.32, wps=21437.1, ups=2.77, wpb=7733.1, bsz=283.8, num_updates=12700, lr=0.000112243, gnorm=0.846, clip=0, train_wall=35, wall=4607
2020-10-12 07:27:48 | INFO | train_inner | epoch 012:    480 / 1120 loss=4.913, nll_loss=3.403, ppl=10.58, wps=21178.1, ups=2.8, wpb=7565.5, bsz=259.9, num_updates=12800, lr=0.000111803, gnorm=0.894, clip=0, train_wall=35, wall=4642
2020-10-12 07:28:24 | INFO | train_inner | epoch 012:    580 / 1120 loss=4.852, nll_loss=3.335, ppl=10.09, wps=21520, ups=2.78, wpb=7743.6, bsz=284.4, num_updates=12900, lr=0.000111369, gnorm=0.831, clip=0, train_wall=35, wall=4678
2020-10-12 07:29:00 | INFO | train_inner | epoch 012:    680 / 1120 loss=4.832, nll_loss=3.312, ppl=9.93, wps=21381.9, ups=2.76, wpb=7759.2, bsz=280.6, num_updates=13000, lr=0.00011094, gnorm=0.853, clip=0, train_wall=36, wall=4715
2020-10-12 07:29:36 | INFO | train_inner | epoch 012:    780 / 1120 loss=4.815, nll_loss=3.294, ppl=9.81, wps=21342.2, ups=2.8, wpb=7630.5, bsz=308.5, num_updates=13100, lr=0.000110516, gnorm=0.891, clip=0, train_wall=35, wall=4750
2020-10-12 07:30:12 | INFO | train_inner | epoch 012:    880 / 1120 loss=4.814, nll_loss=3.292, ppl=9.8, wps=21369.6, ups=2.78, wpb=7692.5, bsz=290, num_updates=13200, lr=0.000110096, gnorm=0.848, clip=0, train_wall=35, wall=4786
2020-10-12 07:30:48 | INFO | train_inner | epoch 012:    980 / 1120 loss=4.824, nll_loss=3.303, ppl=9.87, wps=21462.2, ups=2.78, wpb=7734.1, bsz=286, num_updates=13300, lr=0.000109682, gnorm=0.823, clip=0, train_wall=35, wall=4823
2020-10-12 07:31:24 | INFO | train_inner | epoch 012:   1080 / 1120 loss=4.842, nll_loss=3.324, ppl=10.01, wps=21502.7, ups=2.78, wpb=7727.8, bsz=290.6, num_updates=13400, lr=0.000109272, gnorm=0.852, clip=0, train_wall=35, wall=4858
2020-10-12 07:31:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000948
2020-10-12 07:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069040
2020-10-12 07:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051802
2020-10-12 07:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122130
2020-10-12 07:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000877
2020-10-12 07:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068171
2020-10-12 07:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051562
2020-10-12 07:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120938
2020-10-12 07:31:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:31:43 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 4.8 | nll_loss 3.179 | ppl 9.06 | wps 44822.3 | wpb 2406.1 | bsz 90.2 | num_updates 13440 | best_loss 4.8
2020-10-12 07:31:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:31:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 12 @ 13440 updates, score 4.8) (writing took 1.9247211460024118 seconds)
2020-10-12 07:31:45 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 07:31:45 | INFO | train | epoch 012 | loss 4.847 | nll_loss 3.33 | ppl 10.05 | wps 20981.4 | ups 2.72 | wpb 7707.9 | bsz 286.9 | num_updates 13440 | lr 0.000109109 | gnorm 0.85 | clip 0 | train_wall 396 | wall 4880
2020-10-12 07:31:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 07:31:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 07:31:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:31:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003190
2020-10-12 07:31:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043488
2020-10-12 07:31:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:31:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001836
2020-10-12 07:31:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.719927
2020-10-12 07:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.765777
2020-10-12 07:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034286
2020-10-12 07:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001768
2020-10-12 07:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.703010
2020-10-12 07:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.739590
2020-10-12 07:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:31:47 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 07:32:08 | INFO | train_inner | epoch 013:     60 / 1120 loss=4.806, nll_loss=3.283, ppl=9.73, wps=17245.2, ups=2.26, wpb=7636.9, bsz=285, num_updates=13500, lr=0.000108866, gnorm=0.846, clip=0, train_wall=35, wall=4903
2020-10-12 07:32:44 | INFO | train_inner | epoch 013:    160 / 1120 loss=4.851, nll_loss=3.332, ppl=10.07, wps=21308.4, ups=2.79, wpb=7638.2, bsz=258.4, num_updates=13600, lr=0.000108465, gnorm=0.862, clip=0, train_wall=35, wall=4939
2020-10-12 07:33:20 | INFO | train_inner | epoch 013:    260 / 1120 loss=4.757, nll_loss=3.228, ppl=9.37, wps=21413.6, ups=2.78, wpb=7702.2, bsz=288.7, num_updates=13700, lr=0.000108069, gnorm=0.846, clip=0, train_wall=35, wall=4975
2020-10-12 07:33:56 | INFO | train_inner | epoch 013:    360 / 1120 loss=4.74, nll_loss=3.209, ppl=9.25, wps=21644, ups=2.8, wpb=7740.2, bsz=292.2, num_updates=13800, lr=0.000107676, gnorm=0.841, clip=0, train_wall=35, wall=5010
2020-10-12 07:34:32 | INFO | train_inner | epoch 013:    460 / 1120 loss=4.792, nll_loss=3.267, ppl=9.63, wps=21449.1, ups=2.78, wpb=7703, bsz=275.3, num_updates=13900, lr=0.000107288, gnorm=0.849, clip=0, train_wall=35, wall=5046
2020-10-12 07:35:08 | INFO | train_inner | epoch 013:    560 / 1120 loss=4.783, nll_loss=3.257, ppl=9.56, wps=21446.3, ups=2.77, wpb=7737.9, bsz=283, num_updates=14000, lr=0.000106904, gnorm=0.831, clip=0, train_wall=35, wall=5082
2020-10-12 07:35:44 | INFO | train_inner | epoch 013:    660 / 1120 loss=4.785, nll_loss=3.259, ppl=9.57, wps=21664.1, ups=2.77, wpb=7834.6, bsz=276.6, num_updates=14100, lr=0.000106525, gnorm=0.84, clip=0, train_wall=36, wall=5118
2020-10-12 07:36:20 | INFO | train_inner | epoch 013:    760 / 1120 loss=4.739, nll_loss=3.207, ppl=9.24, wps=21319.8, ups=2.78, wpb=7674.4, bsz=297.3, num_updates=14200, lr=0.000106149, gnorm=0.816, clip=0, train_wall=35, wall=5154
2020-10-12 07:36:56 | INFO | train_inner | epoch 013:    860 / 1120 loss=4.776, nll_loss=3.249, ppl=9.51, wps=21168.4, ups=2.8, wpb=7571.9, bsz=288.4, num_updates=14300, lr=0.000105777, gnorm=0.862, clip=0, train_wall=35, wall=5190
2020-10-12 07:37:32 | INFO | train_inner | epoch 013:    960 / 1120 loss=4.784, nll_loss=3.257, ppl=9.56, wps=21343.3, ups=2.75, wpb=7748.5, bsz=295.4, num_updates=14400, lr=0.000105409, gnorm=0.84, clip=0, train_wall=36, wall=5227
2020-10-12 07:38:08 | INFO | train_inner | epoch 013:   1060 / 1120 loss=4.763, nll_loss=3.235, ppl=9.42, wps=21516.5, ups=2.79, wpb=7706.4, bsz=291, num_updates=14500, lr=0.000105045, gnorm=0.869, clip=0, train_wall=35, wall=5262
2020-10-12 07:38:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001062
2020-10-12 07:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068881
2020-10-12 07:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051967
2020-10-12 07:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122249
2020-10-12 07:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000922
2020-10-12 07:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069787
2020-10-12 07:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050886
2020-10-12 07:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121919
2020-10-12 07:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:38:34 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 4.745 | nll_loss 3.114 | ppl 8.66 | wps 44711.1 | wpb 2406.1 | bsz 90.2 | num_updates 14560 | best_loss 4.745
2020-10-12 07:38:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:38:36 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 13 @ 14560 updates, score 4.745) (writing took 1.9301334740011953 seconds)
2020-10-12 07:38:36 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 07:38:36 | INFO | train | epoch 013 | loss 4.772 | nll_loss 3.245 | ppl 9.48 | wps 21000 | ups 2.72 | wpb 7707.9 | bsz 286.9 | num_updates 14560 | lr 0.000104828 | gnorm 0.844 | clip 0 | train_wall 396 | wall 5291
2020-10-12 07:38:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 07:38:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 07:38:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:38:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003284
2020-10-12 07:38:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.045096
2020-10-12 07:38:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:38:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001872
2020-10-12 07:38:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.719594
2020-10-12 07:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.767107
2020-10-12 07:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034716
2020-10-12 07:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001833
2020-10-12 07:38:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:38:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.717477
2020-10-12 07:38:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.754557
2020-10-12 07:38:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:38:38 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 07:38:52 | INFO | train_inner | epoch 014:     40 / 1120 loss=4.738, nll_loss=3.206, ppl=9.23, wps=17417.6, ups=2.25, wpb=7729.6, bsz=294, num_updates=14600, lr=0.000104685, gnorm=0.833, clip=0, train_wall=35, wall=5307
2020-10-12 07:39:28 | INFO | train_inner | epoch 014:    140 / 1120 loss=4.687, nll_loss=3.149, ppl=8.87, wps=21502.2, ups=2.79, wpb=7703.1, bsz=300.4, num_updates=14700, lr=0.000104328, gnorm=0.818, clip=0, train_wall=35, wall=5343
2020-10-12 07:40:04 | INFO | train_inner | epoch 014:    240 / 1120 loss=4.716, nll_loss=3.181, ppl=9.07, wps=21373.6, ups=2.77, wpb=7713.9, bsz=291.1, num_updates=14800, lr=0.000103975, gnorm=0.842, clip=0, train_wall=35, wall=5379
2020-10-12 07:40:40 | INFO | train_inner | epoch 014:    340 / 1120 loss=4.672, nll_loss=3.132, ppl=8.77, wps=21627.2, ups=2.78, wpb=7788.1, bsz=281.2, num_updates=14900, lr=0.000103626, gnorm=0.817, clip=0, train_wall=35, wall=5415
2020-10-12 07:41:16 | INFO | train_inner | epoch 014:    440 / 1120 loss=4.709, nll_loss=3.175, ppl=9.03, wps=21280.8, ups=2.8, wpb=7604.6, bsz=262.9, num_updates=15000, lr=0.00010328, gnorm=0.861, clip=0, train_wall=35, wall=5450
2020-10-12 07:41:52 | INFO | train_inner | epoch 014:    540 / 1120 loss=4.728, nll_loss=3.194, ppl=9.15, wps=21435.4, ups=2.79, wpb=7694.6, bsz=279, num_updates=15100, lr=0.000102937, gnorm=0.846, clip=0, train_wall=35, wall=5486
2020-10-12 07:42:27 | INFO | train_inner | epoch 014:    640 / 1120 loss=4.713, nll_loss=3.177, ppl=9.05, wps=21344.2, ups=2.79, wpb=7652.9, bsz=292.5, num_updates=15200, lr=0.000102598, gnorm=0.827, clip=0, train_wall=35, wall=5522
2020-10-12 07:43:04 | INFO | train_inner | epoch 014:    740 / 1120 loss=4.715, nll_loss=3.18, ppl=9.06, wps=21206.4, ups=2.75, wpb=7699.1, bsz=294.7, num_updates=15300, lr=0.000102262, gnorm=0.833, clip=0, train_wall=36, wall=5558
2020-10-12 07:43:40 | INFO | train_inner | epoch 014:    840 / 1120 loss=4.713, nll_loss=3.178, ppl=9.05, wps=21470.8, ups=2.76, wpb=7785.4, bsz=271.6, num_updates=15400, lr=0.000101929, gnorm=0.831, clip=0, train_wall=36, wall=5595
2020-10-12 07:44:16 | INFO | train_inner | epoch 014:    940 / 1120 loss=4.672, nll_loss=3.131, ppl=8.76, wps=21301.7, ups=2.78, wpb=7663, bsz=299.8, num_updates=15500, lr=0.0001016, gnorm=0.835, clip=0, train_wall=35, wall=5631
2020-10-12 07:44:53 | INFO | train_inner | epoch 014:   1040 / 1120 loss=4.73, nll_loss=3.197, ppl=9.17, wps=21284.3, ups=2.73, wpb=7784.8, bsz=297.3, num_updates=15600, lr=0.000101274, gnorm=0.844, clip=0, train_wall=36, wall=5667
2020-10-12 07:45:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001052
2020-10-12 07:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068007
2020-10-12 07:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052315
2020-10-12 07:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121717
2020-10-12 07:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000942
2020-10-12 07:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067349
2020-10-12 07:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051615
2020-10-12 07:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120239
2020-10-12 07:45:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:45:26 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 4.7 | nll_loss 3.062 | ppl 8.35 | wps 44859.2 | wpb 2406.1 | bsz 90.2 | num_updates 15680 | best_loss 4.7
2020-10-12 07:45:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:45:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 14 @ 15680 updates, score 4.7) (writing took 1.9588398519845214 seconds)
2020-10-12 07:45:28 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 07:45:28 | INFO | train | epoch 014 | loss 4.706 | nll_loss 3.169 | ppl 9 | wps 20950.5 | ups 2.72 | wpb 7707.9 | bsz 286.9 | num_updates 15680 | lr 0.000101015 | gnorm 0.837 | clip 0 | train_wall 397 | wall 5703
2020-10-12 07:45:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 07:45:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 07:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:45:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003098
2020-10-12 07:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044844
2020-10-12 07:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001848
2020-10-12 07:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.710162
2020-10-12 07:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.757391
2020-10-12 07:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034618
2020-10-12 07:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001839
2020-10-12 07:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.706424
2020-10-12 07:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.743429
2020-10-12 07:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:45:30 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 07:45:37 | INFO | train_inner | epoch 015:     20 / 1120 loss=4.639, nll_loss=3.096, ppl=8.55, wps=17466.1, ups=2.26, wpb=7729.7, bsz=308, num_updates=15700, lr=0.000100951, gnorm=0.84, clip=0, train_wall=35, wall=5712
2020-10-12 07:46:13 | INFO | train_inner | epoch 015:    120 / 1120 loss=4.672, nll_loss=3.131, ppl=8.76, wps=21606.5, ups=2.77, wpb=7800.2, bsz=280, num_updates=15800, lr=0.000100631, gnorm=0.806, clip=0, train_wall=35, wall=5748
2020-10-12 07:46:49 | INFO | train_inner | epoch 015:    220 / 1120 loss=4.67, nll_loss=3.129, ppl=8.75, wps=21282.9, ups=2.81, wpb=7581.4, bsz=264.2, num_updates=15900, lr=0.000100314, gnorm=0.842, clip=0, train_wall=35, wall=5783
2020-10-12 07:47:25 | INFO | train_inner | epoch 015:    320 / 1120 loss=4.621, nll_loss=3.074, ppl=8.42, wps=21311.3, ups=2.78, wpb=7661.8, bsz=317.4, num_updates=16000, lr=0.0001, gnorm=0.834, clip=0, train_wall=35, wall=5819
2020-10-12 07:48:00 | INFO | train_inner | epoch 015:    420 / 1120 loss=4.653, nll_loss=3.11, ppl=8.63, wps=21363.4, ups=2.81, wpb=7594.5, bsz=273.1, num_updates=16100, lr=9.9689e-05, gnorm=0.828, clip=0, train_wall=35, wall=5855
2020-10-12 07:48:36 | INFO | train_inner | epoch 015:    520 / 1120 loss=4.668, nll_loss=3.125, ppl=8.73, wps=21422.5, ups=2.75, wpb=7797.9, bsz=289.1, num_updates=16200, lr=9.93808e-05, gnorm=0.845, clip=0, train_wall=36, wall=5891
2020-10-12 07:49:12 | INFO | train_inner | epoch 015:    620 / 1120 loss=4.63, nll_loss=3.084, ppl=8.48, wps=21245.3, ups=2.78, wpb=7644.4, bsz=308.3, num_updates=16300, lr=9.90755e-05, gnorm=0.857, clip=0, train_wall=35, wall=5927
2020-10-12 07:49:48 | INFO | train_inner | epoch 015:    720 / 1120 loss=4.628, nll_loss=3.082, ppl=8.47, wps=21386.8, ups=2.78, wpb=7702.5, bsz=293.9, num_updates=16400, lr=9.8773e-05, gnorm=0.82, clip=0, train_wall=35, wall=5963
2020-10-12 07:50:25 | INFO | train_inner | epoch 015:    820 / 1120 loss=4.662, nll_loss=3.12, ppl=8.69, wps=21400.3, ups=2.74, wpb=7804.9, bsz=281.8, num_updates=16500, lr=9.84732e-05, gnorm=0.844, clip=0, train_wall=36, wall=6000
2020-10-12 07:51:01 | INFO | train_inner | epoch 015:    920 / 1120 loss=4.654, nll_loss=3.111, ppl=8.64, wps=21464.2, ups=2.76, wpb=7764.2, bsz=278.3, num_updates=16600, lr=9.81761e-05, gnorm=0.829, clip=0, train_wall=36, wall=6036
2020-10-12 07:51:37 | INFO | train_inner | epoch 015:   1020 / 1120 loss=4.637, nll_loss=3.092, ppl=8.52, wps=21830.4, ups=2.77, wpb=7872.1, bsz=274.6, num_updates=16700, lr=9.78818e-05, gnorm=0.815, clip=0, train_wall=35, wall=6072
2020-10-12 07:52:13 | INFO | train_inner | epoch 015:   1120 / 1120 loss=4.651, nll_loss=3.108, ppl=8.62, wps=21016.2, ups=2.78, wpb=7558.4, bsz=280.7, num_updates=16800, lr=9.759e-05, gnorm=0.851, clip=0, train_wall=35, wall=6108
2020-10-12 07:52:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:52:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:52:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001073
2020-10-12 07:52:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:52:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067956
2020-10-12 07:52:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:52:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052471
2020-10-12 07:52:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121846
2020-10-12 07:52:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:52:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:52:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000922
2020-10-12 07:52:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:52:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066774
2020-10-12 07:52:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:52:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051698
2020-10-12 07:52:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119731
2020-10-12 07:52:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:52:18 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 4.661 | nll_loss 3.015 | ppl 8.08 | wps 44701.2 | wpb 2406.1 | bsz 90.2 | num_updates 16800 | best_loss 4.661
2020-10-12 07:52:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:52:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 15 @ 16800 updates, score 4.661) (writing took 1.751742711989209 seconds)
2020-10-12 07:52:20 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 07:52:20 | INFO | train | epoch 015 | loss 4.647 | nll_loss 3.103 | ppl 8.59 | wps 20968.1 | ups 2.72 | wpb 7707.9 | bsz 286.9 | num_updates 16800 | lr 9.759e-05 | gnorm 0.833 | clip 0 | train_wall 397 | wall 6115
2020-10-12 07:52:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 07:52:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 07:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:52:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003363
2020-10-12 07:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044002
2020-10-12 07:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001817
2020-10-12 07:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.723412
2020-10-12 07:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.769764
2020-10-12 07:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034806
2020-10-12 07:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001812
2020-10-12 07:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:52:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.718297
2020-10-12 07:52:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.755461
2020-10-12 07:52:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:52:22 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 07:52:57 | INFO | train_inner | epoch 016:    100 / 1120 loss=4.602, nll_loss=3.052, ppl=8.29, wps=17263, ups=2.27, wpb=7619.7, bsz=289.9, num_updates=16900, lr=9.73009e-05, gnorm=0.838, clip=0, train_wall=35, wall=6152
2020-10-12 07:53:33 | INFO | train_inner | epoch 016:    200 / 1120 loss=4.605, nll_loss=3.056, ppl=8.31, wps=21318, ups=2.79, wpb=7629.5, bsz=288.2, num_updates=17000, lr=9.70143e-05, gnorm=0.837, clip=0, train_wall=35, wall=6188
2020-10-12 07:54:09 | INFO | train_inner | epoch 016:    300 / 1120 loss=4.644, nll_loss=3.099, ppl=8.57, wps=21391.1, ups=2.79, wpb=7664.4, bsz=268.1, num_updates=17100, lr=9.67302e-05, gnorm=0.855, clip=0, train_wall=35, wall=6224
2020-10-12 07:54:45 | INFO | train_inner | epoch 016:    400 / 1120 loss=4.584, nll_loss=3.032, ppl=8.18, wps=21217.3, ups=2.76, wpb=7689.3, bsz=309.4, num_updates=17200, lr=9.64486e-05, gnorm=0.835, clip=0, train_wall=36, wall=6260
2020-10-12 07:55:21 | INFO | train_inner | epoch 016:    500 / 1120 loss=4.58, nll_loss=3.027, ppl=8.15, wps=21393.2, ups=2.77, wpb=7711.4, bsz=285, num_updates=17300, lr=9.61694e-05, gnorm=0.818, clip=0, train_wall=35, wall=6296
2020-10-12 07:55:57 | INFO | train_inner | epoch 016:    600 / 1120 loss=4.612, nll_loss=3.063, ppl=8.35, wps=21298.2, ups=2.77, wpb=7697.3, bsz=280.9, num_updates=17400, lr=9.58927e-05, gnorm=0.84, clip=0, train_wall=36, wall=6332
2020-10-12 07:56:34 | INFO | train_inner | epoch 016:    700 / 1120 loss=4.574, nll_loss=3.02, ppl=8.11, wps=21527.8, ups=2.75, wpb=7840.3, bsz=300.8, num_updates=17500, lr=9.56183e-05, gnorm=0.828, clip=0, train_wall=36, wall=6368
2020-10-12 07:57:10 | INFO | train_inner | epoch 016:    800 / 1120 loss=4.614, nll_loss=3.065, ppl=8.37, wps=21643.9, ups=2.76, wpb=7840.2, bsz=282.3, num_updates=17600, lr=9.53463e-05, gnorm=0.847, clip=0, train_wall=36, wall=6405
2020-10-12 07:57:46 | INFO | train_inner | epoch 016:    900 / 1120 loss=4.565, nll_loss=3.011, ppl=8.06, wps=21547, ups=2.78, wpb=7750.6, bsz=281.2, num_updates=17700, lr=9.50765e-05, gnorm=0.821, clip=0, train_wall=35, wall=6441
2020-10-12 07:58:22 | INFO | train_inner | epoch 016:   1000 / 1120 loss=4.592, nll_loss=3.042, ppl=8.23, wps=21203, ups=2.77, wpb=7666.2, bsz=282.3, num_updates=17800, lr=9.48091e-05, gnorm=0.831, clip=0, train_wall=36, wall=6477
2020-10-12 07:58:58 | INFO | train_inner | epoch 016:   1100 / 1120 loss=4.602, nll_loss=3.053, ppl=8.3, wps=21404.5, ups=2.77, wpb=7734.4, bsz=290.7, num_updates=17900, lr=9.45439e-05, gnorm=0.838, clip=0, train_wall=36, wall=6513
2020-10-12 07:59:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:59:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:59:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000952
2020-10-12 07:59:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:59:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067624
2020-10-12 07:59:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:59:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051630
2020-10-12 07:59:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120545
2020-10-12 07:59:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:59:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:59:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000889
2020-10-12 07:59:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:59:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067535
2020-10-12 07:59:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:59:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051508
2020-10-12 07:59:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120258
2020-10-12 07:59:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:59:10 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.646 | nll_loss 3.001 | ppl 8.01 | wps 44912.9 | wpb 2406.1 | bsz 90.2 | num_updates 17920 | best_loss 4.646
2020-10-12 07:59:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:59:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 16 @ 17920 updates, score 4.646) (writing took 1.953396344993962 seconds)
2020-10-12 07:59:12 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 07:59:12 | INFO | train | epoch 016 | loss 4.597 | nll_loss 3.046 | ppl 8.26 | wps 20937.1 | ups 2.72 | wpb 7707.9 | bsz 286.9 | num_updates 17920 | lr 9.44911e-05 | gnorm 0.835 | clip 0 | train_wall 397 | wall 6527
2020-10-12 07:59:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 07:59:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 07:59:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:59:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003266
2020-10-12 07:59:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044318
2020-10-12 07:59:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:59:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001862
2020-10-12 07:59:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.721995
2020-10-12 07:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.768711
2020-10-12 07:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 07:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035388
2020-10-12 07:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001844
2020-10-12 07:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:59:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.710696
2020-10-12 07:59:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.748457
2020-10-12 07:59:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 07:59:14 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 07:59:42 | INFO | train_inner | epoch 017:     80 / 1120 loss=4.564, nll_loss=3.009, ppl=8.05, wps=17393.5, ups=2.26, wpb=7695.6, bsz=264.2, num_updates=18000, lr=9.42809e-05, gnorm=0.823, clip=0, train_wall=35, wall=6557
2020-10-12 08:00:18 | INFO | train_inner | epoch 017:    180 / 1120 loss=4.526, nll_loss=2.967, ppl=7.82, wps=21398.1, ups=2.8, wpb=7630.5, bsz=288.4, num_updates=18100, lr=9.40201e-05, gnorm=0.821, clip=0, train_wall=35, wall=6593
2020-10-12 08:00:54 | INFO | train_inner | epoch 017:    280 / 1120 loss=4.53, nll_loss=2.971, ppl=7.84, wps=21596.7, ups=2.77, wpb=7805.4, bsz=285.8, num_updates=18200, lr=9.37614e-05, gnorm=0.809, clip=0, train_wall=36, wall=6629
2020-10-12 08:01:30 | INFO | train_inner | epoch 017:    380 / 1120 loss=4.561, nll_loss=3.004, ppl=8.02, wps=21584.7, ups=2.77, wpb=7800.1, bsz=285, num_updates=18300, lr=9.35049e-05, gnorm=0.83, clip=0, train_wall=36, wall=6665
2020-10-12 08:02:06 | INFO | train_inner | epoch 017:    480 / 1120 loss=4.523, nll_loss=2.963, ppl=7.8, wps=21546.9, ups=2.77, wpb=7773.9, bsz=298.5, num_updates=18400, lr=9.32505e-05, gnorm=0.836, clip=0, train_wall=35, wall=6701
2020-10-12 08:02:43 | INFO | train_inner | epoch 017:    580 / 1120 loss=4.597, nll_loss=3.046, ppl=8.26, wps=21136.6, ups=2.77, wpb=7641.3, bsz=269.2, num_updates=18500, lr=9.29981e-05, gnorm=0.866, clip=0, train_wall=36, wall=6737
2020-10-12 08:03:19 | INFO | train_inner | epoch 017:    680 / 1120 loss=4.56, nll_loss=3.005, ppl=8.03, wps=21407.4, ups=2.78, wpb=7694.6, bsz=282.2, num_updates=18600, lr=9.27478e-05, gnorm=0.844, clip=0, train_wall=35, wall=6773
2020-10-12 08:03:55 | INFO | train_inner | epoch 017:    780 / 1120 loss=4.553, nll_loss=2.996, ppl=7.98, wps=21171.2, ups=2.76, wpb=7680.5, bsz=290.1, num_updates=18700, lr=9.24995e-05, gnorm=0.849, clip=0, train_wall=36, wall=6810
2020-10-12 08:04:31 | INFO | train_inner | epoch 017:    880 / 1120 loss=4.524, nll_loss=2.965, ppl=7.81, wps=21552.4, ups=2.77, wpb=7780.6, bsz=308.5, num_updates=18800, lr=9.22531e-05, gnorm=0.819, clip=0, train_wall=35, wall=6846
2020-10-12 08:05:07 | INFO | train_inner | epoch 017:    980 / 1120 loss=4.584, nll_loss=3.032, ppl=8.18, wps=21108.6, ups=2.8, wpb=7533.3, bsz=277.8, num_updates=18900, lr=9.20087e-05, gnorm=0.86, clip=0, train_wall=35, wall=6881
2020-10-12 08:05:43 | INFO | train_inner | epoch 017:   1080 / 1120 loss=4.52, nll_loss=2.96, ppl=7.78, wps=21298.9, ups=2.78, wpb=7667.2, bsz=302.5, num_updates=19000, lr=9.17663e-05, gnorm=0.82, clip=0, train_wall=35, wall=6917
2020-10-12 08:05:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001050
2020-10-12 08:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068068
2020-10-12 08:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051448
2020-10-12 08:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120903
2020-10-12 08:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000935
2020-10-12 08:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067292
2020-10-12 08:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050233
2020-10-12 08:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118791
2020-10-12 08:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:06:02 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.617 | nll_loss 2.967 | ppl 7.82 | wps 44829.2 | wpb 2406.1 | bsz 90.2 | num_updates 19040 | best_loss 4.617
2020-10-12 08:06:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:06:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 17 @ 19040 updates, score 4.617) (writing took 1.605522153986385 seconds)
2020-10-12 08:06:04 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 08:06:04 | INFO | train | epoch 017 | loss 4.55 | nll_loss 2.993 | ppl 7.96 | wps 20990.6 | ups 2.72 | wpb 7707.9 | bsz 286.9 | num_updates 19040 | lr 9.16698e-05 | gnorm 0.834 | clip 0 | train_wall 396 | wall 6938
2020-10-12 08:06:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 08:06:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 08:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:06:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003422
2020-10-12 08:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.046934
2020-10-12 08:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002013
2020-10-12 08:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.720811
2020-10-12 08:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.770459
2020-10-12 08:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034639
2020-10-12 08:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001778
2020-10-12 08:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:06:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.717947
2020-10-12 08:06:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.754895
2020-10-12 08:06:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:06:05 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 08:06:26 | INFO | train_inner | epoch 018:     60 / 1120 loss=4.515, nll_loss=2.955, ppl=7.75, wps=17639.2, ups=2.28, wpb=7734.4, bsz=276.5, num_updates=19100, lr=9.15258e-05, gnorm=0.828, clip=0, train_wall=35, wall=6961
2020-10-12 08:07:02 | INFO | train_inner | epoch 018:    160 / 1120 loss=4.495, nll_loss=2.931, ppl=7.63, wps=21806, ups=2.78, wpb=7834.5, bsz=291.7, num_updates=19200, lr=9.12871e-05, gnorm=0.835, clip=0, train_wall=35, wall=6997
2020-10-12 08:07:38 | INFO | train_inner | epoch 018:    260 / 1120 loss=4.471, nll_loss=2.904, ppl=7.49, wps=21377.1, ups=2.79, wpb=7654.1, bsz=311.6, num_updates=19300, lr=9.10503e-05, gnorm=0.852, clip=0, train_wall=35, wall=7033
2020-10-12 08:08:14 | INFO | train_inner | epoch 018:    360 / 1120 loss=4.503, nll_loss=2.939, ppl=7.67, wps=21501.8, ups=2.76, wpb=7794.1, bsz=290.4, num_updates=19400, lr=9.08153e-05, gnorm=0.821, clip=0, train_wall=36, wall=7069
2020-10-12 08:08:50 | INFO | train_inner | epoch 018:    460 / 1120 loss=4.528, nll_loss=2.968, ppl=7.82, wps=21510.1, ups=2.78, wpb=7737.9, bsz=279.1, num_updates=19500, lr=9.05822e-05, gnorm=0.873, clip=0, train_wall=35, wall=7105
2020-10-12 08:09:27 | INFO | train_inner | epoch 018:    560 / 1120 loss=4.494, nll_loss=2.929, ppl=7.62, wps=21195.8, ups=2.77, wpb=7653.4, bsz=304.1, num_updates=19600, lr=9.03508e-05, gnorm=0.827, clip=0, train_wall=36, wall=7141
2020-10-12 08:10:02 | INFO | train_inner | epoch 018:    660 / 1120 loss=4.509, nll_loss=2.947, ppl=7.71, wps=21345.2, ups=2.79, wpb=7662.5, bsz=288.6, num_updates=19700, lr=9.01212e-05, gnorm=0.834, clip=0, train_wall=35, wall=7177
2020-10-12 08:10:39 | INFO | train_inner | epoch 018:    760 / 1120 loss=4.543, nll_loss=2.984, ppl=7.91, wps=21410.5, ups=2.77, wpb=7729.5, bsz=286.5, num_updates=19800, lr=8.98933e-05, gnorm=0.838, clip=0, train_wall=36, wall=7213
2020-10-12 08:11:14 | INFO | train_inner | epoch 018:    860 / 1120 loss=4.496, nll_loss=2.933, ppl=7.64, wps=21663, ups=2.79, wpb=7773.1, bsz=297.9, num_updates=19900, lr=8.96672e-05, gnorm=0.815, clip=0, train_wall=35, wall=7249
2020-10-12 08:11:50 | INFO | train_inner | epoch 018:    960 / 1120 loss=4.544, nll_loss=2.987, ppl=7.93, wps=21260.9, ups=2.79, wpb=7632.7, bsz=254.5, num_updates=20000, lr=8.94427e-05, gnorm=0.851, clip=0, train_wall=35, wall=7285
2020-10-12 08:12:26 | INFO | train_inner | epoch 018:   1060 / 1120 loss=4.514, nll_loss=2.953, ppl=7.74, wps=21500, ups=2.79, wpb=7707.1, bsz=291, num_updates=20100, lr=8.92199e-05, gnorm=0.833, clip=0, train_wall=35, wall=7321
2020-10-12 08:12:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000939
2020-10-12 08:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069258
2020-10-12 08:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051611
2020-10-12 08:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122148
2020-10-12 08:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000882
2020-10-12 08:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066619
2020-10-12 08:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051286
2020-10-12 08:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119107
2020-10-12 08:12:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:12:53 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.587 | nll_loss 2.935 | ppl 7.65 | wps 44695.2 | wpb 2406.1 | bsz 90.2 | num_updates 20160 | best_loss 4.587
2020-10-12 08:12:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:12:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 18 @ 20160 updates, score 4.587) (writing took 1.577937402005773 seconds)
2020-10-12 08:12:54 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 08:12:54 | INFO | train | epoch 018 | loss 4.509 | nll_loss 2.946 | ppl 7.71 | wps 21022.8 | ups 2.73 | wpb 7707.9 | bsz 286.9 | num_updates 20160 | lr 8.90871e-05 | gnorm 0.838 | clip 0 | train_wall 396 | wall 7349
2020-10-12 08:12:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 08:12:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 08:12:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:12:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003114
2020-10-12 08:12:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.046354
2020-10-12 08:12:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:12:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001880
2020-10-12 08:12:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.725914
2020-10-12 08:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.774692
2020-10-12 08:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034293
2020-10-12 08:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001753
2020-10-12 08:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:12:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.715377
2020-10-12 08:12:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.751948
2020-10-12 08:12:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:12:56 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 08:13:10 | INFO | train_inner | epoch 019:     40 / 1120 loss=4.489, nll_loss=2.925, ppl=7.59, wps=17322.8, ups=2.27, wpb=7620.1, bsz=285.3, num_updates=20200, lr=8.89988e-05, gnorm=0.838, clip=0, train_wall=35, wall=7365
2020-10-12 08:13:46 | INFO | train_inner | epoch 019:    140 / 1120 loss=4.478, nll_loss=2.912, ppl=7.53, wps=21582, ups=2.83, wpb=7638, bsz=253.9, num_updates=20300, lr=8.87794e-05, gnorm=0.854, clip=0, train_wall=35, wall=7400
2020-10-12 08:14:22 | INFO | train_inner | epoch 019:    240 / 1120 loss=4.468, nll_loss=2.9, ppl=7.46, wps=21847.5, ups=2.77, wpb=7901.3, bsz=291.6, num_updates=20400, lr=8.85615e-05, gnorm=0.813, clip=0, train_wall=36, wall=7436
2020-10-12 08:14:58 | INFO | train_inner | epoch 019:    340 / 1120 loss=4.468, nll_loss=2.9, ppl=7.46, wps=21594.7, ups=2.79, wpb=7753.2, bsz=292, num_updates=20500, lr=8.83452e-05, gnorm=0.835, clip=0, train_wall=35, wall=7472
2020-10-12 08:15:34 | INFO | train_inner | epoch 019:    440 / 1120 loss=4.485, nll_loss=2.919, ppl=7.56, wps=21418, ups=2.78, wpb=7699.6, bsz=268.2, num_updates=20600, lr=8.81305e-05, gnorm=0.827, clip=0, train_wall=35, wall=7508
2020-10-12 08:16:10 | INFO | train_inner | epoch 019:    540 / 1120 loss=4.446, nll_loss=2.876, ppl=7.34, wps=21537.9, ups=2.78, wpb=7758.6, bsz=304.6, num_updates=20700, lr=8.79174e-05, gnorm=0.819, clip=0, train_wall=35, wall=7544
2020-10-12 08:16:45 | INFO | train_inner | epoch 019:    640 / 1120 loss=4.475, nll_loss=2.909, ppl=7.51, wps=21190.4, ups=2.81, wpb=7540.5, bsz=282.4, num_updates=20800, lr=8.77058e-05, gnorm=0.843, clip=0, train_wall=35, wall=7580
2020-10-12 08:17:21 | INFO | train_inner | epoch 019:    740 / 1120 loss=4.489, nll_loss=2.923, ppl=7.58, wps=21355.9, ups=2.79, wpb=7660.2, bsz=275.8, num_updates=20900, lr=8.74957e-05, gnorm=0.845, clip=0, train_wall=35, wall=7616
2020-10-12 08:17:57 | INFO | train_inner | epoch 019:    840 / 1120 loss=4.445, nll_loss=2.874, ppl=7.33, wps=21506.6, ups=2.78, wpb=7741.4, bsz=322.3, num_updates=21000, lr=8.72872e-05, gnorm=0.829, clip=0, train_wall=35, wall=7652
2020-10-12 08:18:33 | INFO | train_inner | epoch 019:    940 / 1120 loss=4.485, nll_loss=2.92, ppl=7.57, wps=21565, ups=2.78, wpb=7757.3, bsz=282.9, num_updates=21100, lr=8.70801e-05, gnorm=0.849, clip=0, train_wall=35, wall=7688
2020-10-12 08:19:09 | INFO | train_inner | epoch 019:   1040 / 1120 loss=4.471, nll_loss=2.905, ppl=7.49, wps=21457.2, ups=2.82, wpb=7615.7, bsz=270.7, num_updates=21200, lr=8.68744e-05, gnorm=0.841, clip=0, train_wall=35, wall=7723
2020-10-12 08:19:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001067
2020-10-12 08:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067545
2020-10-12 08:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052039
2020-10-12 08:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120993
2020-10-12 08:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000920
2020-10-12 08:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068801
2020-10-12 08:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051811
2020-10-12 08:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121884
2020-10-12 08:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:19:42 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.576 | nll_loss 2.923 | ppl 7.58 | wps 44718.5 | wpb 2406.1 | bsz 90.2 | num_updates 21280 | best_loss 4.576
2020-10-12 08:19:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:19:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 19 @ 21280 updates, score 4.576) (writing took 1.665407860011328 seconds)
2020-10-12 08:19:44 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 08:19:44 | INFO | train | epoch 019 | loss 4.47 | nll_loss 2.902 | ppl 7.48 | wps 21071.5 | ups 2.73 | wpb 7707.9 | bsz 286.9 | num_updates 21280 | lr 8.6711e-05 | gnorm 0.836 | clip 0 | train_wall 395 | wall 7759
2020-10-12 08:19:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 08:19:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 08:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:19:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004446
2020-10-12 08:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.050331
2020-10-12 08:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001844
2020-10-12 08:19:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:19:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.721650
2020-10-12 08:19:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.774367
2020-10-12 08:19:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:19:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:19:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035384
2020-10-12 08:19:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:19:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001846
2020-10-12 08:19:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:19:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.709323
2020-10-12 08:19:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.747087
2020-10-12 08:19:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:19:46 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 08:19:53 | INFO | train_inner | epoch 020:     20 / 1120 loss=4.428, nll_loss=2.856, ppl=7.24, wps=17555.8, ups=2.27, wpb=7731.2, bsz=305.3, num_updates=21300, lr=8.66703e-05, gnorm=0.846, clip=0, train_wall=35, wall=7767
2020-10-12 08:20:28 | INFO | train_inner | epoch 020:    120 / 1120 loss=4.432, nll_loss=2.859, ppl=7.25, wps=21676.2, ups=2.81, wpb=7714.8, bsz=292, num_updates=21400, lr=8.64675e-05, gnorm=0.834, clip=0, train_wall=35, wall=7803
2020-10-12 08:21:04 | INFO | train_inner | epoch 020:    220 / 1120 loss=4.409, nll_loss=2.834, ppl=7.13, wps=21652, ups=2.78, wpb=7795.9, bsz=293.9, num_updates=21500, lr=8.62662e-05, gnorm=0.811, clip=0, train_wall=35, wall=7839
2020-10-12 08:21:40 | INFO | train_inner | epoch 020:    320 / 1120 loss=4.437, nll_loss=2.865, ppl=7.28, wps=21375.6, ups=2.79, wpb=7648.7, bsz=290.7, num_updates=21600, lr=8.60663e-05, gnorm=0.847, clip=0, train_wall=35, wall=7875
2020-10-12 08:22:16 | INFO | train_inner | epoch 020:    420 / 1120 loss=4.42, nll_loss=2.846, ppl=7.19, wps=21659.5, ups=2.77, wpb=7805.8, bsz=296.1, num_updates=21700, lr=8.58678e-05, gnorm=0.823, clip=0, train_wall=35, wall=7911
2020-10-12 08:22:52 | INFO | train_inner | epoch 020:    520 / 1120 loss=4.438, nll_loss=2.865, ppl=7.29, wps=21331.7, ups=2.77, wpb=7700, bsz=286.3, num_updates=21800, lr=8.56706e-05, gnorm=0.835, clip=0, train_wall=36, wall=7947
2020-10-12 08:23:28 | INFO | train_inner | epoch 020:    620 / 1120 loss=4.428, nll_loss=2.856, ppl=7.24, wps=21391.9, ups=2.8, wpb=7645.5, bsz=284.4, num_updates=21900, lr=8.54748e-05, gnorm=0.875, clip=0, train_wall=35, wall=7983
2020-10-12 08:24:04 | INFO | train_inner | epoch 020:    720 / 1120 loss=4.442, nll_loss=2.87, ppl=7.31, wps=21496.3, ups=2.75, wpb=7806.1, bsz=295.4, num_updates=22000, lr=8.52803e-05, gnorm=0.808, clip=0, train_wall=36, wall=8019
2020-10-12 08:24:40 | INFO | train_inner | epoch 020:    820 / 1120 loss=4.485, nll_loss=2.919, ppl=7.56, wps=21336.3, ups=2.77, wpb=7697.5, bsz=256.7, num_updates=22100, lr=8.50871e-05, gnorm=0.856, clip=0, train_wall=36, wall=8055
2020-10-12 08:25:16 | INFO | train_inner | epoch 020:    920 / 1120 loss=4.417, nll_loss=2.844, ppl=7.18, wps=21342.3, ups=2.82, wpb=7570, bsz=302.2, num_updates=22200, lr=8.48953e-05, gnorm=0.847, clip=0, train_wall=35, wall=8090
2020-10-12 08:25:52 | INFO | train_inner | epoch 020:   1020 / 1120 loss=4.454, nll_loss=2.884, ppl=7.38, wps=21395.5, ups=2.77, wpb=7717.3, bsz=283.9, num_updates=22300, lr=8.47047e-05, gnorm=0.848, clip=0, train_wall=35, wall=8126
2020-10-12 08:26:27 | INFO | train_inner | epoch 020:   1120 / 1120 loss=4.447, nll_loss=2.877, ppl=7.34, wps=21490.9, ups=2.81, wpb=7661.2, bsz=268.8, num_updates=22400, lr=8.45154e-05, gnorm=0.852, clip=0, train_wall=35, wall=8162
2020-10-12 08:26:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:26:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:26:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000980
2020-10-12 08:26:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:26:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067611
2020-10-12 08:26:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:26:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051355
2020-10-12 08:26:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120277
2020-10-12 08:26:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:26:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:26:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000910
2020-10-12 08:26:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:26:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068609
2020-10-12 08:26:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:26:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050520
2020-10-12 08:26:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120361
2020-10-12 08:26:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:26:33 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.543 | nll_loss 2.883 | ppl 7.38 | wps 44873.3 | wpb 2406.1 | bsz 90.2 | num_updates 22400 | best_loss 4.543
2020-10-12 08:26:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:26:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 20 @ 22400 updates, score 4.543) (writing took 1.5767614580108784 seconds)
2020-10-12 08:26:34 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 08:26:34 | INFO | train | epoch 020 | loss 4.435 | nll_loss 2.863 | ppl 7.27 | wps 21047.1 | ups 2.73 | wpb 7707.9 | bsz 286.9 | num_updates 22400 | lr 8.45154e-05 | gnorm 0.839 | clip 0 | train_wall 395 | wall 8169
2020-10-12 08:26:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 08:26:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 08:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:26:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003360
2020-10-12 08:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.047024
2020-10-12 08:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002013
2020-10-12 08:26:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:26:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.721716
2020-10-12 08:26:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.771373
2020-10-12 08:26:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:26:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:26:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034695
2020-10-12 08:26:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:26:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001788
2020-10-12 08:26:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:26:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.713188
2020-10-12 08:26:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.750201
2020-10-12 08:26:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:26:36 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 08:27:11 | INFO | train_inner | epoch 021:    100 / 1120 loss=4.377, nll_loss=2.797, ppl=6.95, wps=17723.5, ups=2.27, wpb=7792.2, bsz=302.2, num_updates=22500, lr=8.43274e-05, gnorm=0.819, clip=0, train_wall=35, wall=8206
2020-10-12 08:27:47 | INFO | train_inner | epoch 021:    200 / 1120 loss=4.378, nll_loss=2.799, ppl=6.96, wps=21578.9, ups=2.81, wpb=7668.3, bsz=277.3, num_updates=22600, lr=8.41406e-05, gnorm=0.824, clip=0, train_wall=35, wall=8242
2020-10-12 08:28:23 | INFO | train_inner | epoch 021:    300 / 1120 loss=4.398, nll_loss=2.821, ppl=7.07, wps=21511.8, ups=2.77, wpb=7773.1, bsz=286.4, num_updates=22700, lr=8.39551e-05, gnorm=0.824, clip=0, train_wall=36, wall=8278
2020-10-12 08:28:59 | INFO | train_inner | epoch 021:    400 / 1120 loss=4.402, nll_loss=2.825, ppl=7.09, wps=21671.2, ups=2.8, wpb=7732.7, bsz=263.4, num_updates=22800, lr=8.37708e-05, gnorm=0.828, clip=0, train_wall=35, wall=8313
2020-10-12 08:29:35 | INFO | train_inner | epoch 021:    500 / 1120 loss=4.412, nll_loss=2.837, ppl=7.15, wps=21382.9, ups=2.79, wpb=7674.2, bsz=274.2, num_updates=22900, lr=8.35877e-05, gnorm=0.852, clip=0, train_wall=35, wall=8349
2020-10-12 08:30:11 | INFO | train_inner | epoch 021:    600 / 1120 loss=4.398, nll_loss=2.82, ppl=7.06, wps=21512.7, ups=2.79, wpb=7723.6, bsz=296.9, num_updates=23000, lr=8.34058e-05, gnorm=0.856, clip=0, train_wall=35, wall=8385
2020-10-12 08:30:47 | INFO | train_inner | epoch 021:    700 / 1120 loss=4.416, nll_loss=2.841, ppl=7.17, wps=21182.2, ups=2.77, wpb=7660, bsz=279.3, num_updates=23100, lr=8.3225e-05, gnorm=0.841, clip=0, train_wall=36, wall=8421
2020-10-12 08:31:23 | INFO | train_inner | epoch 021:    800 / 1120 loss=4.413, nll_loss=2.837, ppl=7.15, wps=21213.4, ups=2.77, wpb=7661.1, bsz=305.9, num_updates=23200, lr=8.30455e-05, gnorm=0.856, clip=0, train_wall=35, wall=8457
2020-10-12 08:31:59 | INFO | train_inner | epoch 021:    900 / 1120 loss=4.436, nll_loss=2.863, ppl=7.28, wps=21346.1, ups=2.77, wpb=7705.4, bsz=272.5, num_updates=23300, lr=8.28671e-05, gnorm=0.853, clip=0, train_wall=35, wall=8494
2020-10-12 08:32:35 | INFO | train_inner | epoch 021:   1000 / 1120 loss=4.401, nll_loss=2.825, ppl=7.08, wps=21194.3, ups=2.79, wpb=7598.7, bsz=293.7, num_updates=23400, lr=8.26898e-05, gnorm=0.841, clip=0, train_wall=35, wall=8529
2020-10-12 08:33:11 | INFO | train_inner | epoch 021:   1100 / 1120 loss=4.389, nll_loss=2.811, ppl=7.02, wps=21589.8, ups=2.76, wpb=7818.9, bsz=310.7, num_updates=23500, lr=8.25137e-05, gnorm=0.834, clip=0, train_wall=36, wall=8566
2020-10-12 08:33:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001069
2020-10-12 08:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069871
2020-10-12 08:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053432
2020-10-12 08:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124714
2020-10-12 08:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000925
2020-10-12 08:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068437
2020-10-12 08:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051714
2020-10-12 08:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121402
2020-10-12 08:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:33:23 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 4.531 | nll_loss 2.872 | ppl 7.32 | wps 44811.9 | wpb 2406.1 | bsz 90.2 | num_updates 23520 | best_loss 4.531
2020-10-12 08:33:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:33:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 21 @ 23520 updates, score 4.531) (writing took 1.6937785890186206 seconds)
2020-10-12 08:33:25 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 08:33:25 | INFO | train | epoch 021 | loss 4.402 | nll_loss 2.825 | ppl 7.09 | wps 21018 | ups 2.73 | wpb 7707.9 | bsz 286.9 | num_updates 23520 | lr 8.24786e-05 | gnorm 0.84 | clip 0 | train_wall 396 | wall 8580
2020-10-12 08:33:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 08:33:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 08:33:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:33:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003518
2020-10-12 08:33:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.047909
2020-10-12 08:33:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:33:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001873
2020-10-12 08:33:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.720698
2020-10-12 08:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.771028
2020-10-12 08:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034592
2020-10-12 08:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001784
2020-10-12 08:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.714437
2020-10-12 08:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.751346
2020-10-12 08:33:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:33:26 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 08:33:55 | INFO | train_inner | epoch 022:     80 / 1120 loss=4.374, nll_loss=2.794, ppl=6.94, wps=17436.8, ups=2.28, wpb=7659.4, bsz=280.3, num_updates=23600, lr=8.23387e-05, gnorm=0.846, clip=0, train_wall=35, wall=8610
2020-10-12 08:34:31 | INFO | train_inner | epoch 022:    180 / 1120 loss=4.357, nll_loss=2.774, ppl=6.84, wps=21601.7, ups=2.81, wpb=7696.5, bsz=295.4, num_updates=23700, lr=8.21648e-05, gnorm=0.835, clip=0, train_wall=35, wall=8645
2020-10-12 08:35:07 | INFO | train_inner | epoch 022:    280 / 1120 loss=4.362, nll_loss=2.78, ppl=6.87, wps=21392.3, ups=2.76, wpb=7745.6, bsz=286.7, num_updates=23800, lr=8.1992e-05, gnorm=0.83, clip=0, train_wall=36, wall=8681
2020-10-12 08:35:43 | INFO | train_inner | epoch 022:    380 / 1120 loss=4.381, nll_loss=2.8, ppl=6.96, wps=21486.5, ups=2.77, wpb=7747.9, bsz=278.1, num_updates=23900, lr=8.18203e-05, gnorm=0.836, clip=0, train_wall=35, wall=8717
2020-10-12 08:36:19 | INFO | train_inner | epoch 022:    480 / 1120 loss=4.407, nll_loss=2.83, ppl=7.11, wps=21171.7, ups=2.78, wpb=7623.5, bsz=284.7, num_updates=24000, lr=8.16497e-05, gnorm=0.864, clip=0, train_wall=35, wall=8753
2020-10-12 08:36:55 | INFO | train_inner | epoch 022:    580 / 1120 loss=4.375, nll_loss=2.795, ppl=6.94, wps=21393.7, ups=2.79, wpb=7670.2, bsz=292.6, num_updates=24100, lr=8.14801e-05, gnorm=0.865, clip=0, train_wall=35, wall=8789
2020-10-12 08:37:31 | INFO | train_inner | epoch 022:    680 / 1120 loss=4.401, nll_loss=2.824, ppl=7.08, wps=21215.6, ups=2.77, wpb=7646, bsz=262.1, num_updates=24200, lr=8.13116e-05, gnorm=0.85, clip=0, train_wall=35, wall=8825
2020-10-12 08:38:07 | INFO | train_inner | epoch 022:    780 / 1120 loss=4.356, nll_loss=2.773, ppl=6.84, wps=21761.1, ups=2.76, wpb=7871.6, bsz=303.2, num_updates=24300, lr=8.11441e-05, gnorm=0.819, clip=0, train_wall=36, wall=8862
2020-10-12 08:38:43 | INFO | train_inner | epoch 022:    880 / 1120 loss=4.348, nll_loss=2.766, ppl=6.8, wps=21486.7, ups=2.81, wpb=7658.2, bsz=293.6, num_updates=24400, lr=8.09776e-05, gnorm=0.846, clip=0, train_wall=35, wall=8897
2020-10-12 08:39:19 | INFO | train_inner | epoch 022:    980 / 1120 loss=4.368, nll_loss=2.787, ppl=6.9, wps=21433.1, ups=2.78, wpb=7720.2, bsz=278.8, num_updates=24500, lr=8.08122e-05, gnorm=0.828, clip=0, train_wall=35, wall=8933
2020-10-12 08:39:55 | INFO | train_inner | epoch 022:   1080 / 1120 loss=4.385, nll_loss=2.807, ppl=7, wps=21477.6, ups=2.78, wpb=7737.8, bsz=284, num_updates=24600, lr=8.06478e-05, gnorm=0.83, clip=0, train_wall=35, wall=8969
2020-10-12 08:40:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000944
2020-10-12 08:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068313
2020-10-12 08:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051332
2020-10-12 08:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120917
2020-10-12 08:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000942
2020-10-12 08:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067107
2020-10-12 08:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052814
2020-10-12 08:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121184
2020-10-12 08:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:40:14 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 4.523 | nll_loss 2.864 | ppl 7.28 | wps 44849.4 | wpb 2406.1 | bsz 90.2 | num_updates 24640 | best_loss 4.523
2020-10-12 08:40:14 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:40:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 22 @ 24640 updates, score 4.523) (writing took 1.9439618090109434 seconds)
2020-10-12 08:40:16 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 08:40:16 | INFO | train | epoch 022 | loss 4.373 | nll_loss 2.792 | ppl 6.93 | wps 20998.6 | ups 2.72 | wpb 7707.9 | bsz 286.9 | num_updates 24640 | lr 8.05823e-05 | gnorm 0.84 | clip 0 | train_wall 396 | wall 8991
2020-10-12 08:40:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 08:40:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 08:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:40:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003291
2020-10-12 08:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043836
2020-10-12 08:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001824
2020-10-12 08:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:40:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.720840
2020-10-12 08:40:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.767079
2020-10-12 08:40:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:40:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:40:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034542
2020-10-12 08:40:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:40:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001794
2020-10-12 08:40:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.718415
2020-10-12 08:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.755286
2020-10-12 08:40:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:40:18 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 08:40:39 | INFO | train_inner | epoch 023:     60 / 1120 loss=4.352, nll_loss=2.769, ppl=6.82, wps=17359.8, ups=2.26, wpb=7689.6, bsz=281.1, num_updates=24700, lr=8.04844e-05, gnorm=0.836, clip=0, train_wall=35, wall=9014
2020-10-12 08:41:15 | INFO | train_inner | epoch 023:    160 / 1120 loss=4.31, nll_loss=2.721, ppl=6.59, wps=21307.7, ups=2.78, wpb=7670.2, bsz=305.9, num_updates=24800, lr=8.03219e-05, gnorm=0.848, clip=0, train_wall=35, wall=9050
2020-10-12 08:41:51 | INFO | train_inner | epoch 023:    260 / 1120 loss=4.325, nll_loss=2.738, ppl=6.67, wps=21495.8, ups=2.79, wpb=7714.6, bsz=288.7, num_updates=24900, lr=8.01605e-05, gnorm=0.838, clip=0, train_wall=35, wall=9085
2020-10-12 08:42:27 | INFO | train_inner | epoch 023:    360 / 1120 loss=4.334, nll_loss=2.749, ppl=6.72, wps=21496.1, ups=2.77, wpb=7750.4, bsz=288.5, num_updates=25000, lr=8e-05, gnorm=0.859, clip=0, train_wall=35, wall=9121
2020-10-12 08:43:03 | INFO | train_inner | epoch 023:    460 / 1120 loss=4.333, nll_loss=2.747, ppl=6.71, wps=21292.4, ups=2.77, wpb=7684.9, bsz=300.2, num_updates=25100, lr=7.98405e-05, gnorm=0.835, clip=0, train_wall=36, wall=9158
2020-10-12 08:43:38 | INFO | train_inner | epoch 023:    560 / 1120 loss=4.387, nll_loss=2.807, ppl=7, wps=21219.4, ups=2.82, wpb=7531.8, bsz=249.9, num_updates=25200, lr=7.96819e-05, gnorm=0.874, clip=0, train_wall=35, wall=9193
2020-10-12 08:44:15 | INFO | train_inner | epoch 023:    660 / 1120 loss=4.358, nll_loss=2.775, ppl=6.84, wps=21476.4, ups=2.77, wpb=7762.9, bsz=267.1, num_updates=25300, lr=7.95243e-05, gnorm=0.833, clip=0, train_wall=36, wall=9229
2020-10-12 08:44:51 | INFO | train_inner | epoch 023:    760 / 1120 loss=4.347, nll_loss=2.763, ppl=6.79, wps=21461.1, ups=2.77, wpb=7752.9, bsz=289.7, num_updates=25400, lr=7.93676e-05, gnorm=0.836, clip=0, train_wall=36, wall=9265
2020-10-12 08:45:27 | INFO | train_inner | epoch 023:    860 / 1120 loss=4.347, nll_loss=2.764, ppl=6.79, wps=21506.7, ups=2.77, wpb=7766.8, bsz=286.1, num_updates=25500, lr=7.92118e-05, gnorm=0.838, clip=0, train_wall=36, wall=9301
2020-10-12 08:46:03 | INFO | train_inner | epoch 023:    960 / 1120 loss=4.362, nll_loss=2.78, ppl=6.87, wps=21501.1, ups=2.75, wpb=7804.4, bsz=299.8, num_updates=25600, lr=7.90569e-05, gnorm=0.834, clip=0, train_wall=36, wall=9338
2020-10-12 08:46:39 | INFO | train_inner | epoch 023:   1060 / 1120 loss=4.334, nll_loss=2.749, ppl=6.72, wps=21406.8, ups=2.8, wpb=7637.7, bsz=300.4, num_updates=25700, lr=7.8903e-05, gnorm=0.854, clip=0, train_wall=35, wall=9373
2020-10-12 08:47:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:47:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:47:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001058
2020-10-12 08:47:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:47:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069056
2020-10-12 08:47:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:47:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051660
2020-10-12 08:47:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122118
2020-10-12 08:47:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:47:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:47:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000934
2020-10-12 08:47:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:47:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068789
2020-10-12 08:47:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:47:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052121
2020-10-12 08:47:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122164
2020-10-12 08:47:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:47:05 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 4.508 | nll_loss 2.846 | ppl 7.19 | wps 45144.7 | wpb 2406.1 | bsz 90.2 | num_updates 25760 | best_loss 4.508
2020-10-12 08:47:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:47:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 23 @ 25760 updates, score 4.508) (writing took 1.8672352099965792 seconds)
2020-10-12 08:47:07 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 08:47:07 | INFO | train | epoch 023 | loss 4.344 | nll_loss 2.76 | ppl 6.77 | wps 20998.2 | ups 2.72 | wpb 7707.9 | bsz 286.9 | num_updates 25760 | lr 7.8811e-05 | gnorm 0.844 | clip 0 | train_wall 396 | wall 9402
2020-10-12 08:47:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 08:47:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 08:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:47:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003337
2020-10-12 08:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044835
2020-10-12 08:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001812
2020-10-12 08:47:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.720130
2020-10-12 08:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.767306
2020-10-12 08:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034665
2020-10-12 08:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001841
2020-10-12 08:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:47:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.709783
2020-10-12 08:47:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.746826
2020-10-12 08:47:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:47:09 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 08:47:23 | INFO | train_inner | epoch 024:     40 / 1120 loss=4.323, nll_loss=2.736, ppl=6.66, wps=17558.3, ups=2.25, wpb=7790.2, bsz=309.4, num_updates=25800, lr=7.87499e-05, gnorm=0.834, clip=0, train_wall=35, wall=9418
2020-10-12 08:47:59 | INFO | train_inner | epoch 024:    140 / 1120 loss=4.309, nll_loss=2.719, ppl=6.58, wps=21456.6, ups=2.79, wpb=7695.3, bsz=280.2, num_updates=25900, lr=7.85977e-05, gnorm=0.836, clip=0, train_wall=35, wall=9454
2020-10-12 08:48:35 | INFO | train_inner | epoch 024:    240 / 1120 loss=4.309, nll_loss=2.72, ppl=6.59, wps=21726.1, ups=2.77, wpb=7843.9, bsz=289, num_updates=26000, lr=7.84465e-05, gnorm=0.818, clip=0, train_wall=36, wall=9490
2020-10-12 08:49:11 | INFO | train_inner | epoch 024:    340 / 1120 loss=4.31, nll_loss=2.721, ppl=6.59, wps=21098.9, ups=2.79, wpb=7558, bsz=274.6, num_updates=26100, lr=7.8296e-05, gnorm=0.859, clip=0, train_wall=35, wall=9526
2020-10-12 08:49:47 | INFO | train_inner | epoch 024:    440 / 1120 loss=4.307, nll_loss=2.718, ppl=6.58, wps=21503.3, ups=2.77, wpb=7753.9, bsz=293.9, num_updates=26200, lr=7.81465e-05, gnorm=0.843, clip=0, train_wall=35, wall=9562
2020-10-12 08:50:23 | INFO | train_inner | epoch 024:    540 / 1120 loss=4.316, nll_loss=2.729, ppl=6.63, wps=21336.1, ups=2.8, wpb=7633.6, bsz=250.4, num_updates=26300, lr=7.79978e-05, gnorm=0.846, clip=0, train_wall=35, wall=9597
2020-10-12 08:50:59 | INFO | train_inner | epoch 024:    640 / 1120 loss=4.35, nll_loss=2.765, ppl=6.8, wps=21326.9, ups=2.76, wpb=7719.5, bsz=282.3, num_updates=26400, lr=7.78499e-05, gnorm=0.842, clip=0, train_wall=36, wall=9634
2020-10-12 08:51:35 | INFO | train_inner | epoch 024:    740 / 1120 loss=4.33, nll_loss=2.744, ppl=6.7, wps=21192.6, ups=2.8, wpb=7574.7, bsz=284.6, num_updates=26500, lr=7.77029e-05, gnorm=0.871, clip=0, train_wall=35, wall=9669
2020-10-12 08:52:11 | INFO | train_inner | epoch 024:    840 / 1120 loss=4.311, nll_loss=2.722, ppl=6.6, wps=21491.9, ups=2.77, wpb=7750.5, bsz=296.2, num_updates=26600, lr=7.75567e-05, gnorm=0.843, clip=0, train_wall=35, wall=9705
2020-10-12 08:52:46 | INFO | train_inner | epoch 024:    940 / 1120 loss=4.327, nll_loss=2.741, ppl=6.68, wps=21388.8, ups=2.8, wpb=7625.5, bsz=283.2, num_updates=26700, lr=7.74113e-05, gnorm=0.849, clip=0, train_wall=35, wall=9741
2020-10-12 08:53:23 | INFO | train_inner | epoch 024:   1040 / 1120 loss=4.317, nll_loss=2.728, ppl=6.63, wps=21452.1, ups=2.75, wpb=7799.3, bsz=313.4, num_updates=26800, lr=7.72667e-05, gnorm=0.835, clip=0, train_wall=36, wall=9777
2020-10-12 08:53:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000945
2020-10-12 08:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068863
2020-10-12 08:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053421
2020-10-12 08:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123561
2020-10-12 08:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000885
2020-10-12 08:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:53:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069291
2020-10-12 08:53:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:53:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051999
2020-10-12 08:53:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122498
2020-10-12 08:53:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:53:56 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 4.499 | nll_loss 2.839 | ppl 7.16 | wps 44861.4 | wpb 2406.1 | bsz 90.2 | num_updates 26880 | best_loss 4.499
2020-10-12 08:53:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:53:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 24 @ 26880 updates, score 4.499) (writing took 1.9273408820154145 seconds)
2020-10-12 08:53:58 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 08:53:58 | INFO | train | epoch 024 | loss 4.317 | nll_loss 2.729 | ppl 6.63 | wps 20987.5 | ups 2.72 | wpb 7707.9 | bsz 286.9 | num_updates 26880 | lr 7.71517e-05 | gnorm 0.844 | clip 0 | train_wall 396 | wall 9813
2020-10-12 08:53:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 08:53:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 08:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:53:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003287
2020-10-12 08:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044273
2020-10-12 08:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001868
2020-10-12 08:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.721992
2020-10-12 08:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.768673
2020-10-12 08:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 08:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035294
2020-10-12 08:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001868
2020-10-12 08:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.706716
2020-10-12 08:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.744424
2020-10-12 08:54:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 08:54:00 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 08:54:07 | INFO | train_inner | epoch 025:     20 / 1120 loss=4.306, nll_loss=2.718, ppl=6.58, wps=17451.8, ups=2.26, wpb=7728.8, bsz=292.7, num_updates=26900, lr=7.7123e-05, gnorm=0.843, clip=0, train_wall=35, wall=9822
2020-10-12 08:54:43 | INFO | train_inner | epoch 025:    120 / 1120 loss=4.243, nll_loss=2.646, ppl=6.26, wps=21665.6, ups=2.76, wpb=7836.9, bsz=319.1, num_updates=27000, lr=7.698e-05, gnorm=0.804, clip=0, train_wall=36, wall=9858
2020-10-12 08:55:19 | INFO | train_inner | epoch 025:    220 / 1120 loss=4.266, nll_loss=2.671, ppl=6.37, wps=21474.2, ups=2.78, wpb=7711.2, bsz=314.2, num_updates=27100, lr=7.68379e-05, gnorm=0.834, clip=0, train_wall=35, wall=9894
2020-10-12 08:55:55 | INFO | train_inner | epoch 025:    320 / 1120 loss=4.304, nll_loss=2.713, ppl=6.56, wps=21215.5, ups=2.8, wpb=7570.5, bsz=302.6, num_updates=27200, lr=7.66965e-05, gnorm=0.856, clip=0, train_wall=35, wall=9929
2020-10-12 08:56:31 | INFO | train_inner | epoch 025:    420 / 1120 loss=4.31, nll_loss=2.721, ppl=6.59, wps=21441.7, ups=2.75, wpb=7807.2, bsz=270.9, num_updates=27300, lr=7.65559e-05, gnorm=0.841, clip=0, train_wall=36, wall=9966
2020-10-12 08:57:07 | INFO | train_inner | epoch 025:    520 / 1120 loss=4.303, nll_loss=2.713, ppl=6.56, wps=21397.3, ups=2.79, wpb=7671.4, bsz=273, num_updates=27400, lr=7.64161e-05, gnorm=0.86, clip=0, train_wall=35, wall=10002
2020-10-12 08:57:43 | INFO | train_inner | epoch 025:    620 / 1120 loss=4.275, nll_loss=2.683, ppl=6.42, wps=21339.9, ups=2.81, wpb=7595, bsz=284.4, num_updates=27500, lr=7.6277e-05, gnorm=0.851, clip=0, train_wall=35, wall=10037
2020-10-12 08:58:18 | INFO | train_inner | epoch 025:    720 / 1120 loss=4.288, nll_loss=2.697, ppl=6.48, wps=21499.6, ups=2.8, wpb=7684.2, bsz=284.4, num_updates=27600, lr=7.61387e-05, gnorm=0.837, clip=0, train_wall=35, wall=10073
2020-10-12 08:58:54 | INFO | train_inner | epoch 025:    820 / 1120 loss=4.298, nll_loss=2.708, ppl=6.53, wps=21459.3, ups=2.79, wpb=7701.3, bsz=282.5, num_updates=27700, lr=7.60011e-05, gnorm=0.853, clip=0, train_wall=35, wall=10109
2020-10-12 08:59:30 | INFO | train_inner | epoch 025:    920 / 1120 loss=4.316, nll_loss=2.728, ppl=6.63, wps=21162.3, ups=2.78, wpb=7609.9, bsz=260.2, num_updates=27800, lr=7.58643e-05, gnorm=0.857, clip=0, train_wall=35, wall=10145
2020-10-12 09:00:06 | INFO | train_inner | epoch 025:   1020 / 1120 loss=4.3, nll_loss=2.71, ppl=6.54, wps=21635.6, ups=2.76, wpb=7835.4, bsz=286.6, num_updates=27900, lr=7.57282e-05, gnorm=0.829, clip=0, train_wall=36, wall=10181
2020-10-12 09:00:42 | INFO | train_inner | epoch 025:   1120 / 1120 loss=4.307, nll_loss=2.718, ppl=6.58, wps=21753.8, ups=2.79, wpb=7792.4, bsz=281.5, num_updates=28000, lr=7.55929e-05, gnorm=0.835, clip=0, train_wall=35, wall=10217
2020-10-12 09:00:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 09:00:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:00:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001050
2020-10-12 09:00:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:00:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067309
2020-10-12 09:00:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:00:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051255
2020-10-12 09:00:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126398
2020-10-12 09:00:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:00:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:00:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000922
2020-10-12 09:00:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:00:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067401
2020-10-12 09:00:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:00:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050183
2020-10-12 09:00:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118827
2020-10-12 09:00:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:00:47 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 4.494 | nll_loss 2.834 | ppl 7.13 | wps 45184.2 | wpb 2406.1 | bsz 90.2 | num_updates 28000 | best_loss 4.494
2020-10-12 09:00:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 09:00:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 25 @ 28000 updates, score 4.494) (writing took 2.035870909021469 seconds)
2020-10-12 09:00:49 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 09:00:49 | INFO | train | epoch 025 | loss 4.292 | nll_loss 2.701 | ppl 6.5 | wps 21004.3 | ups 2.73 | wpb 7707.9 | bsz 286.9 | num_updates 28000 | lr 7.55929e-05 | gnorm 0.842 | clip 0 | train_wall 396 | wall 10224
2020-10-12 09:00:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 09:00:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 09:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:00:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003485
2020-10-12 09:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044896
2020-10-12 09:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001781
2020-10-12 09:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.709824
2020-10-12 09:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.757034
2020-10-12 09:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034137
2020-10-12 09:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001755
2020-10-12 09:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.701737
2020-10-12 09:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.738157
2020-10-12 09:00:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:00:51 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 09:01:27 | INFO | train_inner | epoch 026:    100 / 1120 loss=4.231, nll_loss=2.632, ppl=6.2, wps=17519, ups=2.25, wpb=7770.8, bsz=291.7, num_updates=28100, lr=7.54583e-05, gnorm=0.835, clip=0, train_wall=35, wall=10261
2020-10-12 09:02:02 | INFO | train_inner | epoch 026:    200 / 1120 loss=4.277, nll_loss=2.684, ppl=6.42, wps=21182.3, ups=2.83, wpb=7493.3, bsz=264.2, num_updates=28200, lr=7.53244e-05, gnorm=0.896, clip=0, train_wall=35, wall=10297
2020-10-12 09:02:38 | INFO | train_inner | epoch 026:    300 / 1120 loss=4.264, nll_loss=2.668, ppl=6.36, wps=21763.5, ups=2.77, wpb=7852.4, bsz=311.5, num_updates=28300, lr=7.51912e-05, gnorm=0.826, clip=0, train_wall=35, wall=10333
2020-10-12 09:03:14 | INFO | train_inner | epoch 026:    400 / 1120 loss=4.25, nll_loss=2.653, ppl=6.29, wps=21480.5, ups=2.79, wpb=7700.1, bsz=306.6, num_updates=28400, lr=7.50587e-05, gnorm=0.845, clip=0, train_wall=35, wall=10369
2020-10-12 09:03:50 | INFO | train_inner | epoch 026:    500 / 1120 loss=4.293, nll_loss=2.7, ppl=6.5, wps=21172.8, ups=2.79, wpb=7600.4, bsz=280.1, num_updates=28500, lr=7.49269e-05, gnorm=0.852, clip=0, train_wall=35, wall=10405
2020-10-12 09:04:26 | INFO | train_inner | epoch 026:    600 / 1120 loss=4.269, nll_loss=2.675, ppl=6.38, wps=21558, ups=2.76, wpb=7822.7, bsz=287.3, num_updates=28600, lr=7.47958e-05, gnorm=0.827, clip=0, train_wall=36, wall=10441
2020-10-12 09:05:02 | INFO | train_inner | epoch 026:    700 / 1120 loss=4.266, nll_loss=2.671, ppl=6.37, wps=21672, ups=2.78, wpb=7796.2, bsz=297, num_updates=28700, lr=7.46653e-05, gnorm=0.836, clip=0, train_wall=35, wall=10477
2020-10-12 09:05:38 | INFO | train_inner | epoch 026:    800 / 1120 loss=4.272, nll_loss=2.678, ppl=6.4, wps=21520.1, ups=2.8, wpb=7693.5, bsz=281, num_updates=28800, lr=7.45356e-05, gnorm=0.834, clip=0, train_wall=35, wall=10513
2020-10-12 09:06:14 | INFO | train_inner | epoch 026:    900 / 1120 loss=4.284, nll_loss=2.691, ppl=6.46, wps=21425.4, ups=2.79, wpb=7691.3, bsz=276.2, num_updates=28900, lr=7.44065e-05, gnorm=0.849, clip=0, train_wall=35, wall=10548
2020-10-12 09:06:50 | INFO | train_inner | epoch 026:   1000 / 1120 loss=4.275, nll_loss=2.681, ppl=6.41, wps=21451, ups=2.75, wpb=7801.3, bsz=301.8, num_updates=29000, lr=7.42781e-05, gnorm=0.834, clip=0, train_wall=36, wall=10585
2020-10-12 09:07:26 | INFO | train_inner | epoch 026:   1100 / 1120 loss=4.28, nll_loss=2.688, ppl=6.44, wps=21075.5, ups=2.79, wpb=7546, bsz=257.3, num_updates=29100, lr=7.41504e-05, gnorm=0.861, clip=0, train_wall=35, wall=10621
2020-10-12 09:07:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 09:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000957
2020-10-12 09:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067641
2020-10-12 09:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051175
2020-10-12 09:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120108
2020-10-12 09:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000911
2020-10-12 09:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069308
2020-10-12 09:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050677
2020-10-12 09:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121228
2020-10-12 09:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:07:38 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 4.474 | nll_loss 2.81 | ppl 7.01 | wps 44809 | wpb 2406.1 | bsz 90.2 | num_updates 29120 | best_loss 4.474
2020-10-12 09:07:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 09:07:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 26 @ 29120 updates, score 4.474) (writing took 1.5838552140048705 seconds)
2020-10-12 09:07:40 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 09:07:40 | INFO | train | epoch 026 | loss 4.269 | nll_loss 2.674 | ppl 6.38 | wps 21042.3 | ups 2.73 | wpb 7707.9 | bsz 286.9 | num_updates 29120 | lr 7.41249e-05 | gnorm 0.845 | clip 0 | train_wall 395 | wall 10634
2020-10-12 09:07:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 09:07:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 09:07:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:07:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003304
2020-10-12 09:07:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.046777
2020-10-12 09:07:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:07:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002006
2020-10-12 09:07:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:07:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.735384
2020-10-12 09:07:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.784703
2020-10-12 09:07:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:07:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034470
2020-10-12 09:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001789
2020-10-12 09:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.713400
2020-10-12 09:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.750180
2020-10-12 09:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:07:41 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 09:08:09 | INFO | train_inner | epoch 027:     80 / 1120 loss=4.229, nll_loss=2.63, ppl=6.19, wps=17343, ups=2.3, wpb=7547.3, bsz=297.3, num_updates=29200, lr=7.40233e-05, gnorm=0.854, clip=0, train_wall=35, wall=10664
2020-10-12 09:08:46 | INFO | train_inner | epoch 027:    180 / 1120 loss=4.26, nll_loss=2.664, ppl=6.34, wps=21822.4, ups=2.77, wpb=7870.9, bsz=278.4, num_updates=29300, lr=7.38969e-05, gnorm=0.847, clip=0, train_wall=35, wall=10700
2020-10-12 09:09:22 | INFO | train_inner | epoch 027:    280 / 1120 loss=4.25, nll_loss=2.652, ppl=6.29, wps=21482.6, ups=2.75, wpb=7812.2, bsz=306, num_updates=29400, lr=7.37711e-05, gnorm=0.846, clip=0, train_wall=36, wall=10737
2020-10-12 09:09:58 | INFO | train_inner | epoch 027:    380 / 1120 loss=4.234, nll_loss=2.634, ppl=6.21, wps=21704.9, ups=2.78, wpb=7811.8, bsz=277.6, num_updates=29500, lr=7.3646e-05, gnorm=0.832, clip=0, train_wall=35, wall=10773
2020-10-12 09:10:34 | INFO | train_inner | epoch 027:    480 / 1120 loss=4.245, nll_loss=2.647, ppl=6.26, wps=21444.3, ups=2.77, wpb=7749.6, bsz=284.8, num_updates=29600, lr=7.35215e-05, gnorm=0.845, clip=0, train_wall=36, wall=10809
2020-10-12 09:11:10 | INFO | train_inner | epoch 027:    580 / 1120 loss=4.26, nll_loss=2.664, ppl=6.34, wps=21286.9, ups=2.79, wpb=7640.8, bsz=279.8, num_updates=29700, lr=7.33976e-05, gnorm=0.875, clip=0, train_wall=35, wall=10845
2020-10-12 09:11:46 | INFO | train_inner | epoch 027:    680 / 1120 loss=4.23, nll_loss=2.631, ppl=6.19, wps=21583.6, ups=2.77, wpb=7803.5, bsz=297.2, num_updates=29800, lr=7.32743e-05, gnorm=0.835, clip=0, train_wall=36, wall=10881
2020-10-12 09:12:22 | INFO | train_inner | epoch 027:    780 / 1120 loss=4.265, nll_loss=2.669, ppl=6.36, wps=21235.2, ups=2.77, wpb=7663, bsz=277, num_updates=29900, lr=7.31517e-05, gnorm=0.858, clip=0, train_wall=36, wall=10917
2020-10-12 09:12:58 | INFO | train_inner | epoch 027:    880 / 1120 loss=4.238, nll_loss=2.64, ppl=6.24, wps=21524.1, ups=2.78, wpb=7751.2, bsz=290.4, num_updates=30000, lr=7.30297e-05, gnorm=0.834, clip=0, train_wall=35, wall=10953
2020-10-12 09:13:34 | INFO | train_inner | epoch 027:    980 / 1120 loss=4.27, nll_loss=2.676, ppl=6.39, wps=21291, ups=2.79, wpb=7627.7, bsz=268.6, num_updates=30100, lr=7.29083e-05, gnorm=0.87, clip=0, train_wall=35, wall=10989
2020-10-12 09:14:10 | INFO | train_inner | epoch 027:   1080 / 1120 loss=4.235, nll_loss=2.637, ppl=6.22, wps=21304, ups=2.79, wpb=7640.2, bsz=300.2, num_updates=30200, lr=7.27875e-05, gnorm=0.851, clip=0, train_wall=35, wall=11025
2020-10-12 09:14:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 09:14:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:14:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001063
2020-10-12 09:14:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:14:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069275
2020-10-12 09:14:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:14:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050925
2020-10-12 09:14:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121604
2020-10-12 09:14:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:14:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:14:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000888
2020-10-12 09:14:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:14:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068537
2020-10-12 09:14:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:14:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051079
2020-10-12 09:14:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120827
2020-10-12 09:14:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:14:29 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 4.479 | nll_loss 2.811 | ppl 7.02 | wps 44542.7 | wpb 2406.1 | bsz 90.2 | num_updates 30240 | best_loss 4.474
2020-10-12 09:14:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 09:14:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_last.pt (epoch 27 @ 30240 updates, score 4.479) (writing took 1.1286685100058094 seconds)
2020-10-12 09:14:30 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 09:14:30 | INFO | train | epoch 027 | loss 4.247 | nll_loss 2.65 | ppl 6.28 | wps 21020.8 | ups 2.73 | wpb 7707.9 | bsz 286.9 | num_updates 30240 | lr 7.27393e-05 | gnorm 0.85 | clip 0 | train_wall 396 | wall 11045
2020-10-12 09:14:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 09:14:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 09:14:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:14:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003008
2020-10-12 09:14:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.042962
2020-10-12 09:14:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:14:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001830
2020-10-12 09:14:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:14:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.716556
2020-10-12 09:14:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.761878
2020-10-12 09:14:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:14:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:14:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034799
2020-10-12 09:14:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:14:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001811
2020-10-12 09:14:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:14:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.723044
2020-10-12 09:14:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.760180
2020-10-12 09:14:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:14:32 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 09:14:53 | INFO | train_inner | epoch 028:     60 / 1120 loss=4.236, nll_loss=2.637, ppl=6.22, wps=17621.1, ups=2.3, wpb=7658, bsz=276.1, num_updates=30300, lr=7.26672e-05, gnorm=0.863, clip=0, train_wall=35, wall=11068
2020-10-12 09:15:29 | INFO | train_inner | epoch 028:    160 / 1120 loss=4.236, nll_loss=2.636, ppl=6.22, wps=21354.2, ups=2.77, wpb=7701.4, bsz=293.4, num_updates=30400, lr=7.25476e-05, gnorm=0.843, clip=0, train_wall=35, wall=11104
2020-10-12 09:16:05 | INFO | train_inner | epoch 028:    260 / 1120 loss=4.24, nll_loss=2.641, ppl=6.24, wps=21705.1, ups=2.8, wpb=7738.7, bsz=258.6, num_updates=30500, lr=7.24286e-05, gnorm=0.851, clip=0, train_wall=35, wall=11140
2020-10-12 09:16:41 | INFO | train_inner | epoch 028:    360 / 1120 loss=4.187, nll_loss=2.582, ppl=5.99, wps=21813, ups=2.77, wpb=7885.2, bsz=300.2, num_updates=30600, lr=7.23102e-05, gnorm=0.817, clip=0, train_wall=36, wall=11176
2020-10-12 09:17:17 | INFO | train_inner | epoch 028:    460 / 1120 loss=4.242, nll_loss=2.643, ppl=6.25, wps=21399.7, ups=2.79, wpb=7661.3, bsz=276.2, num_updates=30700, lr=7.21923e-05, gnorm=0.861, clip=0, train_wall=35, wall=11212
2020-10-12 09:17:53 | INFO | train_inner | epoch 028:    560 / 1120 loss=4.217, nll_loss=2.615, ppl=6.13, wps=21251.7, ups=2.77, wpb=7682.6, bsz=299.1, num_updates=30800, lr=7.2075e-05, gnorm=0.862, clip=0, train_wall=36, wall=11248
2020-10-12 09:18:29 | INFO | train_inner | epoch 028:    660 / 1120 loss=4.247, nll_loss=2.649, ppl=6.27, wps=21044, ups=2.79, wpb=7546.5, bsz=265.8, num_updates=30900, lr=7.19583e-05, gnorm=0.877, clip=0, train_wall=35, wall=11284
2020-10-12 09:19:05 | INFO | train_inner | epoch 028:    760 / 1120 loss=4.229, nll_loss=2.63, ppl=6.19, wps=21593.3, ups=2.76, wpb=7812.4, bsz=292.5, num_updates=31000, lr=7.18421e-05, gnorm=0.834, clip=0, train_wall=36, wall=11320
2020-10-12 09:19:41 | INFO | train_inner | epoch 028:    860 / 1120 loss=4.239, nll_loss=2.64, ppl=6.23, wps=21334.5, ups=2.77, wpb=7691.2, bsz=293.9, num_updates=31100, lr=7.17265e-05, gnorm=0.871, clip=0, train_wall=35, wall=11356
2020-10-12 09:20:17 | INFO | train_inner | epoch 028:    960 / 1120 loss=4.228, nll_loss=2.629, ppl=6.18, wps=21121.9, ups=2.77, wpb=7630.9, bsz=290.6, num_updates=31200, lr=7.16115e-05, gnorm=0.874, clip=0, train_wall=36, wall=11392
2020-10-12 09:20:53 | INFO | train_inner | epoch 028:   1060 / 1120 loss=4.217, nll_loss=2.616, ppl=6.13, wps=21352.3, ups=2.79, wpb=7660.3, bsz=299.8, num_updates=31300, lr=7.1497e-05, gnorm=0.899, clip=0, train_wall=35, wall=11428
2020-10-12 09:21:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 09:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000950
2020-10-12 09:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069263
2020-10-12 09:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052395
2020-10-12 09:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122939
2020-10-12 09:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000914
2020-10-12 09:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068844
2020-10-12 09:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051745
2020-10-12 09:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121823
2020-10-12 09:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:21:20 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 4.461 | nll_loss 2.791 | ppl 6.92 | wps 44783.7 | wpb 2406.1 | bsz 90.2 | num_updates 31360 | best_loss 4.461
2020-10-12 09:21:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 09:21:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 28 @ 31360 updates, score 4.461) (writing took 1.7970806100056507 seconds)
2020-10-12 09:21:22 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 09:21:22 | INFO | train | epoch 028 | loss 4.227 | nll_loss 2.627 | ppl 6.18 | wps 20977.9 | ups 2.72 | wpb 7707.9 | bsz 286.9 | num_updates 31360 | lr 7.14286e-05 | gnorm 0.858 | clip 0 | train_wall 396 | wall 11457
2020-10-12 09:21:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 09:21:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 09:21:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:21:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003221
2020-10-12 09:21:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043673
2020-10-12 09:21:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:21:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001816
2020-10-12 09:21:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.720025
2020-10-12 09:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.766039
2020-10-12 09:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034482
2020-10-12 09:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001790
2020-10-12 09:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.702384
2020-10-12 09:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.739185
2020-10-12 09:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:21:23 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 09:21:38 | INFO | train_inner | epoch 029:     40 / 1120 loss=4.204, nll_loss=2.601, ppl=6.07, wps=17396.8, ups=2.24, wpb=7749.9, bsz=317.3, num_updates=31400, lr=7.13831e-05, gnorm=0.837, clip=0, train_wall=35, wall=11472
2020-10-12 09:22:14 | INFO | train_inner | epoch 029:    140 / 1120 loss=4.197, nll_loss=2.593, ppl=6.03, wps=21668.5, ups=2.79, wpb=7752.7, bsz=280.6, num_updates=31500, lr=7.12697e-05, gnorm=0.85, clip=0, train_wall=35, wall=11508
2020-10-12 09:22:49 | INFO | train_inner | epoch 029:    240 / 1120 loss=4.153, nll_loss=2.544, ppl=5.83, wps=21507.7, ups=2.79, wpb=7696.5, bsz=316.2, num_updates=31600, lr=7.11568e-05, gnorm=0.838, clip=0, train_wall=35, wall=11544
2020-10-12 09:23:26 | INFO | train_inner | epoch 029:    340 / 1120 loss=4.221, nll_loss=2.619, ppl=6.14, wps=21491.5, ups=2.74, wpb=7838.9, bsz=281.9, num_updates=31700, lr=7.10445e-05, gnorm=0.848, clip=0, train_wall=36, wall=11580
2020-10-12 09:24:02 | INFO | train_inner | epoch 029:    440 / 1120 loss=4.189, nll_loss=2.584, ppl=6, wps=21475.3, ups=2.76, wpb=7772.6, bsz=298.6, num_updates=31800, lr=7.09327e-05, gnorm=0.838, clip=0, train_wall=36, wall=11617
2020-10-12 09:24:38 | INFO | train_inner | epoch 029:    540 / 1120 loss=4.226, nll_loss=2.625, ppl=6.17, wps=21355, ups=2.79, wpb=7656.2, bsz=268.4, num_updates=31900, lr=7.08214e-05, gnorm=0.863, clip=0, train_wall=35, wall=11653
2020-10-12 09:25:14 | INFO | train_inner | epoch 029:    640 / 1120 loss=4.221, nll_loss=2.619, ppl=6.14, wps=21064.3, ups=2.8, wpb=7511.8, bsz=276.6, num_updates=32000, lr=7.07107e-05, gnorm=0.857, clip=0, train_wall=35, wall=11688
2020-10-12 09:25:49 | INFO | train_inner | epoch 029:    740 / 1120 loss=4.191, nll_loss=2.586, ppl=6.01, wps=21538.7, ups=2.78, wpb=7746.2, bsz=300.4, num_updates=32100, lr=7.06005e-05, gnorm=0.854, clip=0, train_wall=35, wall=11724
2020-10-12 09:26:26 | INFO | train_inner | epoch 029:    840 / 1120 loss=4.249, nll_loss=2.65, ppl=6.28, wps=21282.5, ups=2.76, wpb=7722.7, bsz=276.9, num_updates=32200, lr=7.04907e-05, gnorm=0.859, clip=0, train_wall=36, wall=11760
2020-10-12 09:27:02 | INFO | train_inner | epoch 029:    940 / 1120 loss=4.189, nll_loss=2.585, ppl=6, wps=21413.8, ups=2.76, wpb=7747.4, bsz=297.8, num_updates=32300, lr=7.03815e-05, gnorm=0.849, clip=0, train_wall=36, wall=11797
2020-10-12 09:27:38 | INFO | train_inner | epoch 029:   1040 / 1120 loss=4.226, nll_loss=2.626, ppl=6.17, wps=21549.7, ups=2.79, wpb=7711.9, bsz=264.6, num_updates=32400, lr=7.02728e-05, gnorm=0.867, clip=0, train_wall=35, wall=11832
2020-10-12 09:28:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 09:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001065
2020-10-12 09:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068631
2020-10-12 09:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051705
2020-10-12 09:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121738
2020-10-12 09:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000920
2020-10-12 09:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067329
2020-10-12 09:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051017
2020-10-12 09:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119584
2020-10-12 09:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:28:11 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 4.457 | nll_loss 2.79 | ppl 6.92 | wps 45046.8 | wpb 2406.1 | bsz 90.2 | num_updates 32480 | best_loss 4.457
2020-10-12 09:28:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 09:28:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 29 @ 32480 updates, score 4.457) (writing took 1.6627168120176066 seconds)
2020-10-12 09:28:13 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 09:28:13 | INFO | train | epoch 029 | loss 4.206 | nll_loss 2.603 | ppl 6.08 | wps 21013.2 | ups 2.73 | wpb 7707.9 | bsz 286.9 | num_updates 32480 | lr 7.01862e-05 | gnorm 0.852 | clip 0 | train_wall 396 | wall 11867
2020-10-12 09:28:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 09:28:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 09:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:28:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004600
2020-10-12 09:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.050964
2020-10-12 09:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001853
2020-10-12 09:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.727275
2020-10-12 09:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.780637
2020-10-12 09:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034544
2020-10-12 09:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001828
2020-10-12 09:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.729968
2020-10-12 09:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.766870
2020-10-12 09:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:28:14 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 09:28:21 | INFO | train_inner | epoch 030:     20 / 1120 loss=4.224, nll_loss=2.623, ppl=6.16, wps=17362.7, ups=2.29, wpb=7589.2, bsz=269.8, num_updates=32500, lr=7.01646e-05, gnorm=0.862, clip=0, train_wall=35, wall=11876
2020-10-12 09:28:57 | INFO | train_inner | epoch 030:    120 / 1120 loss=4.186, nll_loss=2.579, ppl=5.98, wps=21634.9, ups=2.8, wpb=7724.5, bsz=275.9, num_updates=32600, lr=7.00569e-05, gnorm=0.848, clip=0, train_wall=35, wall=11912
2020-10-12 09:29:33 | INFO | train_inner | epoch 030:    220 / 1120 loss=4.184, nll_loss=2.577, ppl=5.97, wps=21400.6, ups=2.81, wpb=7612.7, bsz=292.2, num_updates=32700, lr=6.99497e-05, gnorm=0.868, clip=0, train_wall=35, wall=11947
2020-10-12 09:30:09 | INFO | train_inner | epoch 030:    320 / 1120 loss=4.158, nll_loss=2.547, ppl=5.85, wps=21244.1, ups=2.79, wpb=7606.5, bsz=298.8, num_updates=32800, lr=6.9843e-05, gnorm=0.846, clip=0, train_wall=35, wall=11983
2020-10-12 09:30:45 | INFO | train_inner | epoch 030:    420 / 1120 loss=4.195, nll_loss=2.589, ppl=6.02, wps=21770.3, ups=2.76, wpb=7901.6, bsz=265.3, num_updates=32900, lr=6.97368e-05, gnorm=0.824, clip=0, train_wall=36, wall=12019
2020-10-12 09:31:21 | INFO | train_inner | epoch 030:    520 / 1120 loss=4.208, nll_loss=2.605, ppl=6.08, wps=21448.1, ups=2.77, wpb=7752.1, bsz=281.9, num_updates=33000, lr=6.96311e-05, gnorm=0.883, clip=0, train_wall=36, wall=12056
2020-10-12 09:31:57 | INFO | train_inner | epoch 030:    620 / 1120 loss=4.19, nll_loss=2.585, ppl=6, wps=21370.6, ups=2.77, wpb=7722.6, bsz=288.7, num_updates=33100, lr=6.95258e-05, gnorm=0.861, clip=0, train_wall=36, wall=12092
2020-10-12 09:32:33 | INFO | train_inner | epoch 030:    720 / 1120 loss=4.2, nll_loss=2.595, ppl=6.04, wps=21313.6, ups=2.78, wpb=7664.4, bsz=286.7, num_updates=33200, lr=6.9421e-05, gnorm=0.856, clip=0, train_wall=35, wall=12128
2020-10-12 09:33:09 | INFO | train_inner | epoch 030:    820 / 1120 loss=4.201, nll_loss=2.597, ppl=6.05, wps=21117.3, ups=2.78, wpb=7596.3, bsz=276.5, num_updates=33300, lr=6.93167e-05, gnorm=0.873, clip=0, train_wall=35, wall=12164
2020-10-12 09:33:45 | INFO | train_inner | epoch 030:    920 / 1120 loss=4.183, nll_loss=2.578, ppl=5.97, wps=21551.5, ups=2.8, wpb=7710.2, bsz=300.8, num_updates=33400, lr=6.92129e-05, gnorm=0.849, clip=0, train_wall=35, wall=12199
2020-10-12 09:34:21 | INFO | train_inner | epoch 030:   1020 / 1120 loss=4.179, nll_loss=2.573, ppl=5.95, wps=21374, ups=2.76, wpb=7740.1, bsz=293.4, num_updates=33500, lr=6.91095e-05, gnorm=0.853, clip=0, train_wall=36, wall=12236
2020-10-12 09:34:57 | INFO | train_inner | epoch 030:   1120 / 1120 loss=4.175, nll_loss=2.568, ppl=5.93, wps=21596.9, ups=2.78, wpb=7757.8, bsz=300.9, num_updates=33600, lr=6.90066e-05, gnorm=0.832, clip=0, train_wall=35, wall=12272
2020-10-12 09:34:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 09:34:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:34:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000951
2020-10-12 09:34:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:34:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068058
2020-10-12 09:34:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:34:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051843
2020-10-12 09:34:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121180
2020-10-12 09:34:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:34:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:34:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000877
2020-10-12 09:34:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:34:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067963
2020-10-12 09:34:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:34:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051507
2020-10-12 09:34:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120669
2020-10-12 09:34:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:35:02 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 4.457 | nll_loss 2.788 | ppl 6.91 | wps 44974.3 | wpb 2406.1 | bsz 90.2 | num_updates 33600 | best_loss 4.457
2020-10-12 09:35:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 09:35:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 30 @ 33600 updates, score 4.457) (writing took 1.6026483189780265 seconds)
2020-10-12 09:35:04 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 09:35:04 | INFO | train | epoch 030 | loss 4.188 | nll_loss 2.582 | ppl 5.99 | wps 21006.6 | ups 2.73 | wpb 7707.9 | bsz 286.9 | num_updates 33600 | lr 6.90066e-05 | gnorm 0.854 | clip 0 | train_wall 396 | wall 12278
2020-10-12 09:35:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 09:35:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 09:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:35:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003502
2020-10-12 09:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.047140
2020-10-12 09:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002124
2020-10-12 09:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.725521
2020-10-12 09:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.775329
2020-10-12 09:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034551
2020-10-12 09:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001789
2020-10-12 09:35:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:35:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.708852
2020-10-12 09:35:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.745718
2020-10-12 09:35:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:35:05 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 09:35:41 | INFO | train_inner | epoch 031:    100 / 1120 loss=4.156, nll_loss=2.545, ppl=5.84, wps=17613.4, ups=2.26, wpb=7810.5, bsz=301.4, num_updates=33700, lr=6.89041e-05, gnorm=0.855, clip=0, train_wall=35, wall=12316
2020-10-12 09:36:17 | INFO | train_inner | epoch 031:    200 / 1120 loss=4.156, nll_loss=2.547, ppl=5.84, wps=21567.7, ups=2.79, wpb=7739.6, bsz=298.4, num_updates=33800, lr=6.88021e-05, gnorm=0.855, clip=0, train_wall=35, wall=12352
2020-10-12 09:36:53 | INFO | train_inner | epoch 031:    300 / 1120 loss=4.176, nll_loss=2.568, ppl=5.93, wps=21428.7, ups=2.76, wpb=7764.7, bsz=285.8, num_updates=33900, lr=6.87005e-05, gnorm=0.851, clip=0, train_wall=36, wall=12388
2020-10-12 09:37:29 | INFO | train_inner | epoch 031:    400 / 1120 loss=4.157, nll_loss=2.547, ppl=5.84, wps=21295.1, ups=2.8, wpb=7608.6, bsz=299, num_updates=34000, lr=6.85994e-05, gnorm=0.855, clip=0, train_wall=35, wall=12424
2020-10-12 09:38:05 | INFO | train_inner | epoch 031:    500 / 1120 loss=4.174, nll_loss=2.566, ppl=5.92, wps=21561.5, ups=2.78, wpb=7761.1, bsz=294.2, num_updates=34100, lr=6.84988e-05, gnorm=0.855, clip=0, train_wall=35, wall=12460
2020-10-12 09:38:41 | INFO | train_inner | epoch 031:    600 / 1120 loss=4.172, nll_loss=2.564, ppl=5.91, wps=21469.9, ups=2.78, wpb=7721.3, bsz=256.1, num_updates=34200, lr=6.83986e-05, gnorm=0.847, clip=0, train_wall=35, wall=12496
2020-10-12 09:39:17 | INFO | train_inner | epoch 031:    700 / 1120 loss=4.185, nll_loss=2.578, ppl=5.97, wps=21332.5, ups=2.78, wpb=7679.2, bsz=263.8, num_updates=34300, lr=6.82988e-05, gnorm=0.867, clip=0, train_wall=35, wall=12532
2020-10-12 09:39:53 | INFO | train_inner | epoch 031:    800 / 1120 loss=4.162, nll_loss=2.553, ppl=5.87, wps=21624.5, ups=2.78, wpb=7784.1, bsz=303.7, num_updates=34400, lr=6.81994e-05, gnorm=0.85, clip=0, train_wall=35, wall=12568
2020-10-12 09:40:29 | INFO | train_inner | epoch 031:    900 / 1120 loss=4.188, nll_loss=2.581, ppl=5.98, wps=21141.6, ups=2.81, wpb=7526.3, bsz=250.9, num_updates=34500, lr=6.81005e-05, gnorm=0.869, clip=0, train_wall=35, wall=12603
2020-10-12 09:41:05 | INFO | train_inner | epoch 031:   1000 / 1120 loss=4.178, nll_loss=2.571, ppl=5.94, wps=21277.8, ups=2.79, wpb=7623.1, bsz=300.5, num_updates=34600, lr=6.8002e-05, gnorm=0.874, clip=0, train_wall=35, wall=12639
2020-10-12 09:41:41 | INFO | train_inner | epoch 031:   1100 / 1120 loss=4.177, nll_loss=2.57, ppl=5.94, wps=21640.4, ups=2.78, wpb=7792.9, bsz=300, num_updates=34700, lr=6.7904e-05, gnorm=0.852, clip=0, train_wall=35, wall=12675
2020-10-12 09:41:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 09:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001054
2020-10-12 09:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070161
2020-10-12 09:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052285
2020-10-12 09:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123843
2020-10-12 09:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000889
2020-10-12 09:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067234
2020-10-12 09:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051135
2020-10-12 09:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119575
2020-10-12 09:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:41:53 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 4.429 | nll_loss 2.762 | ppl 6.79 | wps 44785.5 | wpb 2406.1 | bsz 90.2 | num_updates 34720 | best_loss 4.429
2020-10-12 09:41:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 09:41:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 31 @ 34720 updates, score 4.429) (writing took 1.6721228160022292 seconds)
2020-10-12 09:41:54 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 09:41:54 | INFO | train | epoch 031 | loss 4.17 | nll_loss 2.562 | ppl 5.9 | wps 21019.5 | ups 2.73 | wpb 7707.9 | bsz 286.9 | num_updates 34720 | lr 6.78844e-05 | gnorm 0.856 | clip 0 | train_wall 396 | wall 12689
2020-10-12 09:41:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 09:41:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 09:41:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:41:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004119
2020-10-12 09:41:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.049091
2020-10-12 09:41:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:41:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001821
2020-10-12 09:41:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.712180
2020-10-12 09:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.763632
2020-10-12 09:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034658
2020-10-12 09:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001814
2020-10-12 09:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.702329
2020-10-12 09:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.739348
2020-10-12 09:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:41:56 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 09:42:25 | INFO | train_inner | epoch 032:     80 / 1120 loss=4.127, nll_loss=2.513, ppl=5.71, wps=17584.8, ups=2.27, wpb=7751.1, bsz=288.3, num_updates=34800, lr=6.78064e-05, gnorm=0.837, clip=0, train_wall=35, wall=12719
2020-10-12 09:43:01 | INFO | train_inner | epoch 032:    180 / 1120 loss=4.159, nll_loss=2.548, ppl=5.85, wps=21416.5, ups=2.77, wpb=7727.1, bsz=288.8, num_updates=34900, lr=6.77091e-05, gnorm=0.859, clip=0, train_wall=35, wall=12755
2020-10-12 09:43:37 | INFO | train_inner | epoch 032:    280 / 1120 loss=4.144, nll_loss=2.531, ppl=5.78, wps=21581.8, ups=2.77, wpb=7785, bsz=287.5, num_updates=35000, lr=6.76123e-05, gnorm=0.847, clip=0, train_wall=35, wall=12791
2020-10-12 09:44:13 | INFO | train_inner | epoch 032:    380 / 1120 loss=4.144, nll_loss=2.532, ppl=5.78, wps=21639.4, ups=2.76, wpb=7829.5, bsz=295.8, num_updates=35100, lr=6.7516e-05, gnorm=0.834, clip=0, train_wall=36, wall=12828
2020-10-12 09:44:49 | INFO | train_inner | epoch 032:    480 / 1120 loss=4.157, nll_loss=2.546, ppl=5.84, wps=21375.3, ups=2.78, wpb=7680.7, bsz=283.7, num_updates=35200, lr=6.742e-05, gnorm=0.866, clip=0, train_wall=35, wall=12864
2020-10-12 09:45:25 | INFO | train_inner | epoch 032:    580 / 1120 loss=4.154, nll_loss=2.543, ppl=5.83, wps=21282.5, ups=2.78, wpb=7665.6, bsz=302.2, num_updates=35300, lr=6.73244e-05, gnorm=0.863, clip=0, train_wall=35, wall=12900
2020-10-12 09:46:01 | INFO | train_inner | epoch 032:    680 / 1120 loss=4.142, nll_loss=2.531, ppl=5.78, wps=21638.4, ups=2.79, wpb=7766.1, bsz=296.6, num_updates=35400, lr=6.72293e-05, gnorm=0.848, clip=0, train_wall=35, wall=12935
2020-10-12 09:46:37 | INFO | train_inner | epoch 032:    780 / 1120 loss=4.164, nll_loss=2.554, ppl=5.87, wps=21498.5, ups=2.79, wpb=7698.1, bsz=275, num_updates=35500, lr=6.71345e-05, gnorm=0.859, clip=0, train_wall=35, wall=12971
2020-10-12 09:47:13 | INFO | train_inner | epoch 032:    880 / 1120 loss=4.179, nll_loss=2.571, ppl=5.94, wps=21050.3, ups=2.78, wpb=7572.1, bsz=257.4, num_updates=35600, lr=6.70402e-05, gnorm=0.876, clip=0, train_wall=35, wall=13007
2020-10-12 09:47:48 | INFO | train_inner | epoch 032:    980 / 1120 loss=4.155, nll_loss=2.545, ppl=5.84, wps=21075.6, ups=2.81, wpb=7506.2, bsz=276.8, num_updates=35700, lr=6.69462e-05, gnorm=0.879, clip=0, train_wall=35, wall=13043
2020-10-12 09:48:24 | INFO | train_inner | epoch 032:   1080 / 1120 loss=4.163, nll_loss=2.554, ppl=5.87, wps=21771.3, ups=2.77, wpb=7869.5, bsz=293, num_updates=35800, lr=6.68526e-05, gnorm=0.857, clip=0, train_wall=36, wall=13079
2020-10-12 09:48:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000948
2020-10-12 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069393
2020-10-12 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051558
2020-10-12 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122236
2020-10-12 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000907
2020-10-12 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069330
2020-10-12 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051092
2020-10-12 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121649
2020-10-12 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:48:44 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 4.437 | nll_loss 2.77 | ppl 6.82 | wps 44646.1 | wpb 2406.1 | bsz 90.2 | num_updates 35840 | best_loss 4.429
2020-10-12 09:48:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 09:48:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_last.pt (epoch 32 @ 35840 updates, score 4.437) (writing took 1.1128973029844929 seconds)
2020-10-12 09:48:45 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 09:48:45 | INFO | train | epoch 032 | loss 4.152 | nll_loss 2.542 | ppl 5.82 | wps 21034.9 | ups 2.73 | wpb 7707.9 | bsz 286.9 | num_updates 35840 | lr 6.68153e-05 | gnorm 0.859 | clip 0 | train_wall 396 | wall 13099
2020-10-12 09:48:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 09:48:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 09:48:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:48:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002974
2020-10-12 09:48:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.042034
2020-10-12 09:48:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:48:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001779
2020-10-12 09:48:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.716264
2020-10-12 09:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.760597
2020-10-12 09:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034258
2020-10-12 09:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001762
2020-10-12 09:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.707621
2020-10-12 09:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.744173
2020-10-12 09:48:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:48:46 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 09:49:07 | INFO | train_inner | epoch 033:     60 / 1120 loss=4.11, nll_loss=2.495, ppl=5.64, wps=17525.4, ups=2.32, wpb=7566.1, bsz=303.9, num_updates=35900, lr=6.67595e-05, gnorm=0.883, clip=0, train_wall=35, wall=13122
2020-10-12 09:49:43 | INFO | train_inner | epoch 033:    160 / 1120 loss=4.142, nll_loss=2.53, ppl=5.77, wps=21486.1, ups=2.79, wpb=7701.2, bsz=274.1, num_updates=36000, lr=6.66667e-05, gnorm=0.859, clip=0, train_wall=35, wall=13158
2020-10-12 09:50:19 | INFO | train_inner | epoch 033:    260 / 1120 loss=4.131, nll_loss=2.518, ppl=5.73, wps=21144.2, ups=2.79, wpb=7591.8, bsz=298.6, num_updates=36100, lr=6.65743e-05, gnorm=0.876, clip=0, train_wall=35, wall=13194
2020-10-12 09:50:55 | INFO | train_inner | epoch 033:    360 / 1120 loss=4.149, nll_loss=2.536, ppl=5.8, wps=21246.3, ups=2.78, wpb=7653, bsz=284.6, num_updates=36200, lr=6.64822e-05, gnorm=0.863, clip=0, train_wall=35, wall=13230
2020-10-12 09:51:32 | INFO | train_inner | epoch 033:    460 / 1120 loss=4.116, nll_loss=2.501, ppl=5.66, wps=21559.8, ups=2.76, wpb=7813.6, bsz=313, num_updates=36300, lr=6.63906e-05, gnorm=0.85, clip=0, train_wall=36, wall=13266
2020-10-12 09:52:08 | INFO | train_inner | epoch 033:    560 / 1120 loss=4.145, nll_loss=2.532, ppl=5.79, wps=21348.2, ups=2.77, wpb=7694.6, bsz=270, num_updates=36400, lr=6.62994e-05, gnorm=0.87, clip=0, train_wall=35, wall=13302
2020-10-12 09:52:44 | INFO | train_inner | epoch 033:    660 / 1120 loss=4.133, nll_loss=2.519, ppl=5.73, wps=21612.2, ups=2.76, wpb=7831.2, bsz=287.8, num_updates=36500, lr=6.62085e-05, gnorm=0.842, clip=0, train_wall=36, wall=13338
2020-10-12 09:53:20 | INFO | train_inner | epoch 033:    760 / 1120 loss=4.12, nll_loss=2.507, ppl=5.68, wps=21435.4, ups=2.77, wpb=7734.3, bsz=285.8, num_updates=36600, lr=6.6118e-05, gnorm=0.845, clip=0, train_wall=35, wall=13375
2020-10-12 09:53:56 | INFO | train_inner | epoch 033:    860 / 1120 loss=4.147, nll_loss=2.536, ppl=5.8, wps=21258.8, ups=2.76, wpb=7695.9, bsz=290.9, num_updates=36700, lr=6.60278e-05, gnorm=0.877, clip=0, train_wall=36, wall=13411
2020-10-12 09:54:32 | INFO | train_inner | epoch 033:    960 / 1120 loss=4.145, nll_loss=2.534, ppl=5.79, wps=21431.4, ups=2.77, wpb=7742.4, bsz=285.5, num_updates=36800, lr=6.5938e-05, gnorm=0.851, clip=0, train_wall=36, wall=13447
2020-10-12 09:55:08 | INFO | train_inner | epoch 033:   1060 / 1120 loss=4.143, nll_loss=2.532, ppl=5.78, wps=21425.8, ups=2.78, wpb=7701.9, bsz=282.2, num_updates=36900, lr=6.58486e-05, gnorm=0.872, clip=0, train_wall=35, wall=13483
2020-10-12 09:55:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 09:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001063
2020-10-12 09:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069733
2020-10-12 09:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051936
2020-10-12 09:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123068
2020-10-12 09:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000944
2020-10-12 09:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068739
2020-10-12 09:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051410
2020-10-12 09:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121425
2020-10-12 09:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:55:35 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 4.424 | nll_loss 2.749 | ppl 6.72 | wps 43964.2 | wpb 2406.1 | bsz 90.2 | num_updates 36960 | best_loss 4.424
2020-10-12 09:55:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 09:55:36 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 33 @ 36960 updates, score 4.424) (writing took 1.664076307992218 seconds)
2020-10-12 09:55:36 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 09:55:36 | INFO | train | epoch 033 | loss 4.137 | nll_loss 2.524 | ppl 5.75 | wps 20969.4 | ups 2.72 | wpb 7707.9 | bsz 286.9 | num_updates 36960 | lr 6.57952e-05 | gnorm 0.861 | clip 0 | train_wall 396 | wall 13511
2020-10-12 09:55:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 09:55:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 09:55:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:55:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004743
2020-10-12 09:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.051358
2020-10-12 09:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001839
2020-10-12 09:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.710544
2020-10-12 09:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.764271
2020-10-12 09:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 09:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034516
2020-10-12 09:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001799
2020-10-12 09:55:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:55:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.705679
2020-10-12 09:55:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.742520
2020-10-12 09:55:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 09:55:38 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 09:55:52 | INFO | train_inner | epoch 034:     40 / 1120 loss=4.133, nll_loss=2.519, ppl=5.73, wps=17412.2, ups=2.26, wpb=7702.3, bsz=281.9, num_updates=37000, lr=6.57596e-05, gnorm=0.847, clip=0, train_wall=35, wall=13527
2020-10-12 09:56:28 | INFO | train_inner | epoch 034:    140 / 1120 loss=4.118, nll_loss=2.503, ppl=5.67, wps=21474.7, ups=2.79, wpb=7703.7, bsz=262.7, num_updates=37100, lr=6.56709e-05, gnorm=0.862, clip=0, train_wall=35, wall=13563
2020-10-12 09:57:04 | INFO | train_inner | epoch 034:    240 / 1120 loss=4.1, nll_loss=2.482, ppl=5.59, wps=21624.3, ups=2.77, wpb=7814.4, bsz=320.6, num_updates=37200, lr=6.55826e-05, gnorm=0.853, clip=0, train_wall=36, wall=13599
2020-10-12 09:57:40 | INFO | train_inner | epoch 034:    340 / 1120 loss=4.109, nll_loss=2.492, ppl=5.63, wps=21374.7, ups=2.78, wpb=7694.2, bsz=287.3, num_updates=37300, lr=6.54946e-05, gnorm=0.856, clip=0, train_wall=35, wall=13635
2020-10-12 09:58:17 | INFO | train_inner | epoch 034:    440 / 1120 loss=4.118, nll_loss=2.502, ppl=5.67, wps=21291, ups=2.76, wpb=7712.5, bsz=309.8, num_updates=37400, lr=6.5407e-05, gnorm=0.87, clip=0, train_wall=36, wall=13671
2020-10-12 09:58:53 | INFO | train_inner | epoch 034:    540 / 1120 loss=4.125, nll_loss=2.51, ppl=5.7, wps=21602.8, ups=2.78, wpb=7784.4, bsz=286.3, num_updates=37500, lr=6.53197e-05, gnorm=0.86, clip=0, train_wall=35, wall=13707
2020-10-12 09:59:28 | INFO | train_inner | epoch 034:    640 / 1120 loss=4.132, nll_loss=2.518, ppl=5.73, wps=21471.8, ups=2.81, wpb=7653.5, bsz=262.8, num_updates=37600, lr=6.52328e-05, gnorm=0.884, clip=0, train_wall=35, wall=13743
2020-10-12 10:00:04 | INFO | train_inner | epoch 034:    740 / 1120 loss=4.128, nll_loss=2.514, ppl=5.71, wps=21247.3, ups=2.79, wpb=7617.3, bsz=286.8, num_updates=37700, lr=6.51462e-05, gnorm=0.875, clip=0, train_wall=35, wall=13779
2020-10-12 10:00:40 | INFO | train_inner | epoch 034:    840 / 1120 loss=4.147, nll_loss=2.534, ppl=5.79, wps=21633.2, ups=2.76, wpb=7851.9, bsz=281.3, num_updates=37800, lr=6.506e-05, gnorm=0.865, clip=0, train_wall=36, wall=13815
2020-10-12 10:01:16 | INFO | train_inner | epoch 034:    940 / 1120 loss=4.122, nll_loss=2.508, ppl=5.69, wps=21435.5, ups=2.78, wpb=7707, bsz=288.3, num_updates=37900, lr=6.49741e-05, gnorm=0.867, clip=0, train_wall=35, wall=13851
2020-10-12 10:01:52 | INFO | train_inner | epoch 034:   1040 / 1120 loss=4.131, nll_loss=2.518, ppl=5.73, wps=21182.5, ups=2.78, wpb=7619.9, bsz=283.7, num_updates=38000, lr=6.48886e-05, gnorm=0.877, clip=0, train_wall=35, wall=13887
2020-10-12 10:02:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 10:02:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:02:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000942
2020-10-12 10:02:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:02:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068214
2020-10-12 10:02:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:02:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050959
2020-10-12 10:02:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120447
2020-10-12 10:02:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:02:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:02:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000905
2020-10-12 10:02:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:02:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066761
2020-10-12 10:02:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:02:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051303
2020-10-12 10:02:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119285
2020-10-12 10:02:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:02:26 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.424 | nll_loss 2.753 | ppl 6.74 | wps 44671.8 | wpb 2406.1 | bsz 90.2 | num_updates 38080 | best_loss 4.424
2020-10-12 10:02:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 10:02:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 34 @ 38080 updates, score 4.424) (writing took 1.5719250490074046 seconds)
2020-10-12 10:02:27 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 10:02:27 | INFO | train | epoch 034 | loss 4.122 | nll_loss 2.507 | ppl 5.68 | wps 21002.6 | ups 2.72 | wpb 7707.9 | bsz 286.9 | num_updates 38080 | lr 6.48204e-05 | gnorm 0.866 | clip 0 | train_wall 396 | wall 13922
2020-10-12 10:02:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 10:02:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 10:02:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:02:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003278
2020-10-12 10:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.046426
2020-10-12 10:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002039
2020-10-12 10:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.718250
2020-10-12 10:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.767412
2020-10-12 10:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034141
2020-10-12 10:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001761
2020-10-12 10:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:02:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.703768
2020-10-12 10:02:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.740195
2020-10-12 10:02:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:02:29 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 10:02:36 | INFO | train_inner | epoch 035:     20 / 1120 loss=4.113, nll_loss=2.498, ppl=5.65, wps=17345.8, ups=2.29, wpb=7587.7, bsz=275.8, num_updates=38100, lr=6.48034e-05, gnorm=0.874, clip=0, train_wall=35, wall=13931
2020-10-12 10:03:12 | INFO | train_inner | epoch 035:    120 / 1120 loss=4.081, nll_loss=2.46, ppl=5.5, wps=21551.7, ups=2.8, wpb=7701.7, bsz=300.2, num_updates=38200, lr=6.47185e-05, gnorm=0.875, clip=0, train_wall=35, wall=13967
2020-10-12 10:03:48 | INFO | train_inner | epoch 035:    220 / 1120 loss=4.086, nll_loss=2.466, ppl=5.53, wps=21622.8, ups=2.8, wpb=7718.4, bsz=282.7, num_updates=38300, lr=6.46339e-05, gnorm=0.856, clip=0, train_wall=35, wall=14002
2020-10-12 10:04:23 | INFO | train_inner | epoch 035:    320 / 1120 loss=4.102, nll_loss=2.485, ppl=5.6, wps=21364.7, ups=2.79, wpb=7647.9, bsz=271.1, num_updates=38400, lr=6.45497e-05, gnorm=0.873, clip=0, train_wall=35, wall=14038
2020-10-12 10:04:59 | INFO | train_inner | epoch 035:    420 / 1120 loss=4.091, nll_loss=2.473, ppl=5.55, wps=21249.7, ups=2.81, wpb=7563, bsz=280.7, num_updates=38500, lr=6.44658e-05, gnorm=0.866, clip=0, train_wall=35, wall=14074
2020-10-12 10:05:35 | INFO | train_inner | epoch 035:    520 / 1120 loss=4.143, nll_loss=2.529, ppl=5.77, wps=21310.7, ups=2.78, wpb=7661.8, bsz=261.9, num_updates=38600, lr=6.43823e-05, gnorm=0.88, clip=0, train_wall=35, wall=14110
2020-10-12 10:06:11 | INFO | train_inner | epoch 035:    620 / 1120 loss=4.081, nll_loss=2.462, ppl=5.51, wps=21636.1, ups=2.78, wpb=7786.9, bsz=292.2, num_updates=38700, lr=6.4299e-05, gnorm=0.854, clip=0, train_wall=35, wall=14146
2020-10-12 10:06:47 | INFO | train_inner | epoch 035:    720 / 1120 loss=4.096, nll_loss=2.478, ppl=5.57, wps=21259.2, ups=2.75, wpb=7732.3, bsz=298.3, num_updates=38800, lr=6.42161e-05, gnorm=0.877, clip=0, train_wall=36, wall=14182
2020-10-12 10:07:23 | INFO | train_inner | epoch 035:    820 / 1120 loss=4.113, nll_loss=2.496, ppl=5.64, wps=21634.9, ups=2.77, wpb=7821.8, bsz=295.2, num_updates=38900, lr=6.41335e-05, gnorm=0.849, clip=0, train_wall=36, wall=14218
2020-10-12 10:07:59 | INFO | train_inner | epoch 035:    920 / 1120 loss=4.135, nll_loss=2.521, ppl=5.74, wps=21165.4, ups=2.79, wpb=7596.3, bsz=269.4, num_updates=39000, lr=6.40513e-05, gnorm=0.879, clip=0, train_wall=35, wall=14254
2020-10-12 10:08:36 | INFO | train_inner | epoch 035:   1020 / 1120 loss=4.118, nll_loss=2.502, ppl=5.67, wps=21588.5, ups=2.73, wpb=7904.1, bsz=320.6, num_updates=39100, lr=6.39693e-05, gnorm=0.849, clip=0, train_wall=36, wall=14291
2020-10-12 10:09:12 | INFO | train_inner | epoch 035:   1120 / 1120 loss=4.114, nll_loss=2.498, ppl=5.65, wps=21487.2, ups=2.8, wpb=7668.8, bsz=293.4, num_updates=39200, lr=6.38877e-05, gnorm=0.886, clip=0, train_wall=35, wall=14326
2020-10-12 10:09:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 10:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001069
2020-10-12 10:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069118
2020-10-12 10:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051527
2020-10-12 10:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128529
2020-10-12 10:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000911
2020-10-12 10:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069269
2020-10-12 10:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051258
2020-10-12 10:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121761
2020-10-12 10:09:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:09:17 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.417 | nll_loss 2.745 | ppl 6.71 | wps 44780.9 | wpb 2406.1 | bsz 90.2 | num_updates 39200 | best_loss 4.417
2020-10-12 10:09:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 10:09:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 35 @ 39200 updates, score 4.417) (writing took 2.040200838993769 seconds)
2020-10-12 10:09:19 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 10:09:19 | INFO | train | epoch 035 | loss 4.105 | nll_loss 2.488 | ppl 5.61 | wps 20989.6 | ups 2.72 | wpb 7707.9 | bsz 286.9 | num_updates 39200 | lr 6.38877e-05 | gnorm 0.868 | clip 0 | train_wall 396 | wall 14333
2020-10-12 10:09:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 10:09:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 10:09:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:09:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003246
2020-10-12 10:09:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044348
2020-10-12 10:09:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:09:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001836
2020-10-12 10:09:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:09:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.722654
2020-10-12 10:09:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.769366
2020-10-12 10:09:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:09:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:09:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034309
2020-10-12 10:09:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:09:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001767
2020-10-12 10:09:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:09:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.707203
2020-10-12 10:09:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.743807
2020-10-12 10:09:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:09:20 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 10:09:56 | INFO | train_inner | epoch 036:    100 / 1120 loss=4.047, nll_loss=2.423, ppl=5.36, wps=17301, ups=2.25, wpb=7684.8, bsz=297.3, num_updates=39300, lr=6.38063e-05, gnorm=0.87, clip=0, train_wall=35, wall=14371
2020-10-12 10:10:32 | INFO | train_inner | epoch 036:    200 / 1120 loss=4.075, nll_loss=2.454, ppl=5.48, wps=21650.1, ups=2.79, wpb=7758, bsz=283.4, num_updates=39400, lr=6.37253e-05, gnorm=0.86, clip=0, train_wall=35, wall=14407
2020-10-12 10:11:08 | INFO | train_inner | epoch 036:    300 / 1120 loss=4.075, nll_loss=2.454, ppl=5.48, wps=21485.5, ups=2.77, wpb=7759.4, bsz=308.1, num_updates=39500, lr=6.36446e-05, gnorm=0.857, clip=0, train_wall=36, wall=14443
2020-10-12 10:11:44 | INFO | train_inner | epoch 036:    400 / 1120 loss=4.078, nll_loss=2.458, ppl=5.49, wps=21436.3, ups=2.8, wpb=7652.8, bsz=283, num_updates=39600, lr=6.35642e-05, gnorm=0.865, clip=0, train_wall=35, wall=14478
2020-10-12 10:12:20 | INFO | train_inner | epoch 036:    500 / 1120 loss=4.122, nll_loss=2.505, ppl=5.68, wps=21247.7, ups=2.78, wpb=7652.3, bsz=267, num_updates=39700, lr=6.34841e-05, gnorm=0.879, clip=0, train_wall=35, wall=14514
2020-10-12 10:12:56 | INFO | train_inner | epoch 036:    600 / 1120 loss=4.097, nll_loss=2.479, ppl=5.57, wps=21452, ups=2.77, wpb=7730.7, bsz=291.5, num_updates=39800, lr=6.34043e-05, gnorm=0.863, clip=0, train_wall=35, wall=14550
2020-10-12 10:13:32 | INFO | train_inner | epoch 036:    700 / 1120 loss=4.1, nll_loss=2.481, ppl=5.58, wps=21123.8, ups=2.76, wpb=7662.3, bsz=294.2, num_updates=39900, lr=6.33248e-05, gnorm=0.875, clip=0, train_wall=36, wall=14587
2020-10-12 10:14:08 | INFO | train_inner | epoch 036:    800 / 1120 loss=4.097, nll_loss=2.479, ppl=5.58, wps=21287, ups=2.79, wpb=7633, bsz=278.2, num_updates=40000, lr=6.32456e-05, gnorm=0.886, clip=0, train_wall=35, wall=14623
2020-10-12 10:14:44 | INFO | train_inner | epoch 036:    900 / 1120 loss=4.096, nll_loss=2.477, ppl=5.57, wps=21396.4, ups=2.77, wpb=7716.8, bsz=277.6, num_updates=40100, lr=6.31666e-05, gnorm=0.867, clip=0, train_wall=35, wall=14659
2020-10-12 10:15:20 | INFO | train_inner | epoch 036:   1000 / 1120 loss=4.111, nll_loss=2.495, ppl=5.64, wps=21407.1, ups=2.76, wpb=7751.9, bsz=285.6, num_updates=40200, lr=6.3088e-05, gnorm=0.865, clip=0, train_wall=36, wall=14695
2020-10-12 10:15:56 | INFO | train_inner | epoch 036:   1100 / 1120 loss=4.093, nll_loss=2.476, ppl=5.56, wps=21607.2, ups=2.78, wpb=7785.9, bsz=297.4, num_updates=40300, lr=6.30097e-05, gnorm=0.868, clip=0, train_wall=35, wall=14731
2020-10-12 10:16:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 10:16:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:16:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000935
2020-10-12 10:16:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:16:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066719
2020-10-12 10:16:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:16:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051112
2020-10-12 10:16:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119098
2020-10-12 10:16:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:16:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:16:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000923
2020-10-12 10:16:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:16:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068325
2020-10-12 10:16:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:16:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050308
2020-10-12 10:16:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119876
2020-10-12 10:16:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:16:08 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 4.411 | nll_loss 2.739 | ppl 6.68 | wps 44771.6 | wpb 2406.1 | bsz 90.2 | num_updates 40320 | best_loss 4.411
2020-10-12 10:16:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 10:16:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 36 @ 40320 updates, score 4.411) (writing took 1.5973062810080592 seconds)
2020-10-12 10:16:10 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 10:16:10 | INFO | train | epoch 036 | loss 4.091 | nll_loss 2.472 | ppl 5.55 | wps 20996.3 | ups 2.72 | wpb 7707.9 | bsz 286.9 | num_updates 40320 | lr 6.29941e-05 | gnorm 0.869 | clip 0 | train_wall 396 | wall 14745
2020-10-12 10:16:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 10:16:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 10:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:16:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003051
2020-10-12 10:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.047507
2020-10-12 10:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002112
2020-10-12 10:16:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:16:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.723702
2020-10-12 10:16:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.773872
2020-10-12 10:16:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:16:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:16:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035202
2020-10-12 10:16:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:16:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001873
2020-10-12 10:16:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:16:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.701973
2020-10-12 10:16:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.739585
2020-10-12 10:16:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:16:11 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 10:16:40 | INFO | train_inner | epoch 037:     80 / 1120 loss=4.056, nll_loss=2.433, ppl=5.4, wps=17714.7, ups=2.27, wpb=7794.3, bsz=291.8, num_updates=40400, lr=6.29317e-05, gnorm=0.858, clip=0, train_wall=35, wall=14775
2020-10-12 10:17:16 | INFO | train_inner | epoch 037:    180 / 1120 loss=4.055, nll_loss=2.431, ppl=5.39, wps=21435.5, ups=2.78, wpb=7707.5, bsz=283.1, num_updates=40500, lr=6.28539e-05, gnorm=0.859, clip=0, train_wall=35, wall=14811
2020-10-12 10:17:52 | INFO | train_inner | epoch 037:    280 / 1120 loss=4.076, nll_loss=2.455, ppl=5.48, wps=21191.1, ups=2.79, wpb=7586.6, bsz=257.7, num_updates=40600, lr=6.27765e-05, gnorm=0.882, clip=0, train_wall=35, wall=14847
2020-10-12 10:18:28 | INFO | train_inner | epoch 037:    380 / 1120 loss=4.079, nll_loss=2.457, ppl=5.49, wps=21218.2, ups=2.79, wpb=7606.4, bsz=275.2, num_updates=40700, lr=6.26993e-05, gnorm=0.884, clip=0, train_wall=35, wall=14882
2020-10-12 10:19:04 | INFO | train_inner | epoch 037:    480 / 1120 loss=4.067, nll_loss=2.445, ppl=5.44, wps=21437.9, ups=2.78, wpb=7706.5, bsz=299.8, num_updates=40800, lr=6.26224e-05, gnorm=0.865, clip=0, train_wall=35, wall=14918
2020-10-12 10:19:40 | INFO | train_inner | epoch 037:    580 / 1120 loss=4.082, nll_loss=2.462, ppl=5.51, wps=21490.3, ups=2.77, wpb=7756.8, bsz=281.3, num_updates=40900, lr=6.25458e-05, gnorm=0.871, clip=0, train_wall=35, wall=14954
2020-10-12 10:20:16 | INFO | train_inner | epoch 037:    680 / 1120 loss=4.062, nll_loss=2.438, ppl=5.42, wps=21675.7, ups=2.76, wpb=7856.8, bsz=298.1, num_updates=41000, lr=6.24695e-05, gnorm=0.847, clip=0, train_wall=36, wall=14991
2020-10-12 10:20:52 | INFO | train_inner | epoch 037:    780 / 1120 loss=4.108, nll_loss=2.491, ppl=5.62, wps=21304.2, ups=2.79, wpb=7649.2, bsz=279, num_updates=41100, lr=6.23935e-05, gnorm=0.893, clip=0, train_wall=35, wall=15027
2020-10-12 10:21:28 | INFO | train_inner | epoch 037:    880 / 1120 loss=4.064, nll_loss=2.442, ppl=5.43, wps=21363.3, ups=2.75, wpb=7763.2, bsz=292.3, num_updates=41200, lr=6.23177e-05, gnorm=0.848, clip=0, train_wall=36, wall=15063
2020-10-12 10:22:05 | INFO | train_inner | epoch 037:    980 / 1120 loss=4.096, nll_loss=2.477, ppl=5.57, wps=21456, ups=2.76, wpb=7770.2, bsz=295.1, num_updates=41300, lr=6.22422e-05, gnorm=0.873, clip=0, train_wall=36, wall=15099
2020-10-12 10:22:40 | INFO | train_inner | epoch 037:   1080 / 1120 loss=4.107, nll_loss=2.489, ppl=5.61, wps=21277.5, ups=2.81, wpb=7565.5, bsz=283, num_updates=41400, lr=6.2167e-05, gnorm=0.897, clip=0, train_wall=35, wall=15135
2020-10-12 10:22:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 10:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001048
2020-10-12 10:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068498
2020-10-12 10:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051989
2020-10-12 10:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121862
2020-10-12 10:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000913
2020-10-12 10:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067791
2020-10-12 10:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050722
2020-10-12 10:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119738
2020-10-12 10:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:23:00 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 4.415 | nll_loss 2.74 | ppl 6.68 | wps 44732 | wpb 2406.1 | bsz 90.2 | num_updates 41440 | best_loss 4.411
2020-10-12 10:23:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 10:23:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_last.pt (epoch 37 @ 41440 updates, score 4.415) (writing took 0.9687119989830535 seconds)
2020-10-12 10:23:01 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 10:23:01 | INFO | train | epoch 037 | loss 4.077 | nll_loss 2.456 | ppl 5.49 | wps 21015.8 | ups 2.73 | wpb 7707.9 | bsz 286.9 | num_updates 41440 | lr 6.2137e-05 | gnorm 0.87 | clip 0 | train_wall 396 | wall 15155
2020-10-12 10:23:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 10:23:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 10:23:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:23:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002988
2020-10-12 10:23:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.042129
2020-10-12 10:23:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:23:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001774
2020-10-12 10:23:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:23:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.703345
2020-10-12 10:23:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.747777
2020-10-12 10:23:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:23:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:23:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034271
2020-10-12 10:23:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:23:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001772
2020-10-12 10:23:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:23:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.713139
2020-10-12 10:23:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.749701
2020-10-12 10:23:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:23:02 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 10:23:24 | INFO | train_inner | epoch 038:     60 / 1120 loss=4.087, nll_loss=2.465, ppl=5.52, wps=17671.3, ups=2.29, wpb=7730.3, bsz=284.2, num_updates=41500, lr=6.2092e-05, gnorm=0.868, clip=0, train_wall=35, wall=15178
2020-10-12 10:24:00 | INFO | train_inner | epoch 038:    160 / 1120 loss=4.061, nll_loss=2.437, ppl=5.42, wps=21230.3, ups=2.8, wpb=7582.7, bsz=256.2, num_updates=41600, lr=6.20174e-05, gnorm=0.901, clip=0, train_wall=35, wall=15214
2020-10-12 10:24:36 | INFO | train_inner | epoch 038:    260 / 1120 loss=4.048, nll_loss=2.422, ppl=5.36, wps=21348.2, ups=2.78, wpb=7688.4, bsz=293.9, num_updates=41700, lr=6.1943e-05, gnorm=0.865, clip=0, train_wall=35, wall=15250
2020-10-12 10:25:12 | INFO | train_inner | epoch 038:    360 / 1120 loss=4.054, nll_loss=2.429, ppl=5.38, wps=21731.1, ups=2.75, wpb=7903.1, bsz=311.8, num_updates=41800, lr=6.18688e-05, gnorm=0.854, clip=0, train_wall=36, wall=15287
2020-10-12 10:25:48 | INFO | train_inner | epoch 038:    460 / 1120 loss=4.058, nll_loss=2.434, ppl=5.41, wps=21485.1, ups=2.77, wpb=7752.2, bsz=301.3, num_updates=41900, lr=6.17949e-05, gnorm=0.865, clip=0, train_wall=35, wall=15323
2020-10-12 10:26:24 | INFO | train_inner | epoch 038:    560 / 1120 loss=4.037, nll_loss=2.41, ppl=5.32, wps=21812.1, ups=2.75, wpb=7940.9, bsz=314.5, num_updates=42000, lr=6.17213e-05, gnorm=0.837, clip=0, train_wall=36, wall=15359
2020-10-12 10:27:00 | INFO | train_inner | epoch 038:    660 / 1120 loss=4.098, nll_loss=2.48, ppl=5.58, wps=21129.6, ups=2.8, wpb=7549.4, bsz=278.5, num_updates=42100, lr=6.1648e-05, gnorm=0.92, clip=0, train_wall=35, wall=15395
2020-10-12 10:27:36 | INFO | train_inner | epoch 038:    760 / 1120 loss=4.086, nll_loss=2.465, ppl=5.52, wps=21373.9, ups=2.76, wpb=7753.2, bsz=275.1, num_updates=42200, lr=6.15749e-05, gnorm=0.865, clip=0, train_wall=36, wall=15431
2020-10-12 10:28:12 | INFO | train_inner | epoch 038:    860 / 1120 loss=4.057, nll_loss=2.433, ppl=5.4, wps=21543.4, ups=2.77, wpb=7773.1, bsz=310.5, num_updates=42300, lr=6.15021e-05, gnorm=0.872, clip=0, train_wall=35, wall=15467
2020-10-12 10:28:48 | INFO | train_inner | epoch 038:    960 / 1120 loss=4.068, nll_loss=2.446, ppl=5.45, wps=21579.3, ups=2.79, wpb=7746.5, bsz=282.4, num_updates=42400, lr=6.14295e-05, gnorm=0.877, clip=0, train_wall=35, wall=15503
2020-10-12 10:29:24 | INFO | train_inner | epoch 038:   1060 / 1120 loss=4.063, nll_loss=2.441, ppl=5.43, wps=21214.8, ups=2.81, wpb=7544.6, bsz=278.6, num_updates=42500, lr=6.13572e-05, gnorm=0.89, clip=0, train_wall=35, wall=15539
2020-10-12 10:29:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 10:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000959
2020-10-12 10:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067729
2020-10-12 10:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051083
2020-10-12 10:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120098
2020-10-12 10:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000902
2020-10-12 10:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068971
2020-10-12 10:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050949
2020-10-12 10:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121136
2020-10-12 10:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:29:50 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 4.415 | nll_loss 2.739 | ppl 6.68 | wps 44705.5 | wpb 2406.1 | bsz 90.2 | num_updates 42560 | best_loss 4.411
2020-10-12 10:29:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 10:29:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_last.pt (epoch 38 @ 42560 updates, score 4.415) (writing took 1.1184105549764354 seconds)
2020-10-12 10:29:51 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 10:29:51 | INFO | train | epoch 038 | loss 4.065 | nll_loss 2.442 | ppl 5.43 | wps 21023.8 | ups 2.73 | wpb 7707.9 | bsz 286.9 | num_updates 42560 | lr 6.13139e-05 | gnorm 0.875 | clip 0 | train_wall 396 | wall 15566
2020-10-12 10:29:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 10:29:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 10:29:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:29:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002943
2020-10-12 10:29:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.042400
2020-10-12 10:29:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:29:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001802
2020-10-12 10:29:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:29:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.721806
2020-10-12 10:29:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.766533
2020-10-12 10:29:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:29:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:29:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034534
2020-10-12 10:29:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:29:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001794
2020-10-12 10:29:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:29:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.714368
2020-10-12 10:29:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.751216
2020-10-12 10:29:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:29:53 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 10:30:07 | INFO | train_inner | epoch 039:     40 / 1120 loss=4.056, nll_loss=2.432, ppl=5.4, wps=17510.1, ups=2.31, wpb=7565.6, bsz=267, num_updates=42600, lr=6.12851e-05, gnorm=0.872, clip=0, train_wall=35, wall=15582
2020-10-12 10:30:43 | INFO | train_inner | epoch 039:    140 / 1120 loss=4.037, nll_loss=2.41, ppl=5.31, wps=21681.5, ups=2.78, wpb=7788.7, bsz=293.4, num_updates=42700, lr=6.12133e-05, gnorm=0.857, clip=0, train_wall=35, wall=15618
2020-10-12 10:31:19 | INFO | train_inner | epoch 039:    240 / 1120 loss=4.038, nll_loss=2.412, ppl=5.32, wps=21479.7, ups=2.78, wpb=7719.9, bsz=295.1, num_updates=42800, lr=6.11418e-05, gnorm=0.877, clip=0, train_wall=35, wall=15654
2020-10-12 10:31:55 | INFO | train_inner | epoch 039:    340 / 1120 loss=4.057, nll_loss=2.431, ppl=5.39, wps=21065, ups=2.8, wpb=7531.3, bsz=261.2, num_updates=42900, lr=6.10705e-05, gnorm=0.891, clip=0, train_wall=35, wall=15689
2020-10-12 10:32:31 | INFO | train_inner | epoch 039:    440 / 1120 loss=4.021, nll_loss=2.393, ppl=5.25, wps=21550.2, ups=2.76, wpb=7796.4, bsz=307.2, num_updates=43000, lr=6.09994e-05, gnorm=0.863, clip=0, train_wall=36, wall=15726
2020-10-12 10:33:07 | INFO | train_inner | epoch 039:    540 / 1120 loss=4.059, nll_loss=2.436, ppl=5.41, wps=21684.9, ups=2.79, wpb=7768.9, bsz=275.5, num_updates=43100, lr=6.09286e-05, gnorm=0.86, clip=0, train_wall=35, wall=15761
2020-10-12 10:33:43 | INFO | train_inner | epoch 039:    640 / 1120 loss=4.066, nll_loss=2.442, ppl=5.43, wps=21239.2, ups=2.79, wpb=7625.2, bsz=278.3, num_updates=43200, lr=6.08581e-05, gnorm=0.87, clip=0, train_wall=35, wall=15797
2020-10-12 10:34:19 | INFO | train_inner | epoch 039:    740 / 1120 loss=4.068, nll_loss=2.446, ppl=5.45, wps=21502.1, ups=2.77, wpb=7774.3, bsz=278.6, num_updates=43300, lr=6.07877e-05, gnorm=0.869, clip=0, train_wall=36, wall=15834
2020-10-12 10:34:55 | INFO | train_inner | epoch 039:    840 / 1120 loss=4.063, nll_loss=2.44, ppl=5.43, wps=21204.1, ups=2.79, wpb=7599.5, bsz=276.8, num_updates=43400, lr=6.07177e-05, gnorm=0.888, clip=0, train_wall=35, wall=15869
2020-10-12 10:35:31 | INFO | train_inner | epoch 039:    940 / 1120 loss=4.064, nll_loss=2.442, ppl=5.43, wps=21432.2, ups=2.76, wpb=7769.1, bsz=282.6, num_updates=43500, lr=6.06478e-05, gnorm=0.866, clip=0, train_wall=36, wall=15906
2020-10-12 10:36:07 | INFO | train_inner | epoch 039:   1040 / 1120 loss=4.048, nll_loss=2.423, ppl=5.36, wps=21297.5, ups=2.74, wpb=7768.6, bsz=320.9, num_updates=43600, lr=6.05783e-05, gnorm=0.872, clip=0, train_wall=36, wall=15942
2020-10-12 10:36:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 10:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001065
2020-10-12 10:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069151
2020-10-12 10:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051253
2020-10-12 10:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121808
2020-10-12 10:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000906
2020-10-12 10:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069079
2020-10-12 10:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051885
2020-10-12 10:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122188
2020-10-12 10:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:36:41 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 4.4 | nll_loss 2.73 | ppl 6.64 | wps 44777 | wpb 2406.1 | bsz 90.2 | num_updates 43680 | best_loss 4.4
2020-10-12 10:36:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 10:36:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_best.pt (epoch 39 @ 43680 updates, score 4.4) (writing took 1.6653360569907818 seconds)
2020-10-12 10:36:43 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 10:36:43 | INFO | train | epoch 039 | loss 4.051 | nll_loss 2.426 | ppl 5.37 | wps 20986.1 | ups 2.72 | wpb 7707.9 | bsz 286.9 | num_updates 43680 | lr 6.05228e-05 | gnorm 0.871 | clip 0 | train_wall 396 | wall 15977
2020-10-12 10:36:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 10:36:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 10:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:36:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004564
2020-10-12 10:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.050784
2020-10-12 10:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001795
2020-10-12 10:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.720276
2020-10-12 10:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.773400
2020-10-12 10:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:36:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:36:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034625
2020-10-12 10:36:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:36:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001774
2020-10-12 10:36:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:36:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.723251
2020-10-12 10:36:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.760185
2020-10-12 10:36:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:36:44 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 10:36:52 | INFO | train_inner | epoch 040:     20 / 1120 loss=4.042, nll_loss=2.417, ppl=5.34, wps=17416.2, ups=2.27, wpb=7685.7, bsz=285.1, num_updates=43700, lr=6.05089e-05, gnorm=0.865, clip=0, train_wall=35, wall=15986
2020-10-12 10:37:27 | INFO | train_inner | epoch 040:    120 / 1120 loss=4.006, nll_loss=2.375, ppl=5.19, wps=21361.8, ups=2.81, wpb=7601.7, bsz=319.6, num_updates=43800, lr=6.04398e-05, gnorm=0.882, clip=0, train_wall=35, wall=16022
2020-10-12 10:38:03 | INFO | train_inner | epoch 040:    220 / 1120 loss=4.043, nll_loss=2.415, ppl=5.33, wps=21593.6, ups=2.78, wpb=7764.9, bsz=273, num_updates=43900, lr=6.03709e-05, gnorm=0.873, clip=0, train_wall=35, wall=16058
2020-10-12 10:38:39 | INFO | train_inner | epoch 040:    320 / 1120 loss=4.032, nll_loss=2.404, ppl=5.29, wps=21671.7, ups=2.77, wpb=7837.5, bsz=289.7, num_updates=44000, lr=6.03023e-05, gnorm=0.866, clip=0, train_wall=36, wall=16094
2020-10-12 10:39:15 | INFO | train_inner | epoch 040:    420 / 1120 loss=4.043, nll_loss=2.417, ppl=5.34, wps=21125.8, ups=2.77, wpb=7634, bsz=269.3, num_updates=44100, lr=6.02339e-05, gnorm=0.878, clip=0, train_wall=36, wall=16130
2020-10-12 10:39:51 | INFO | train_inner | epoch 040:    520 / 1120 loss=4.046, nll_loss=2.42, ppl=5.35, wps=21267.4, ups=2.8, wpb=7589.2, bsz=263.4, num_updates=44200, lr=6.01657e-05, gnorm=0.88, clip=0, train_wall=35, wall=16166
2020-10-12 10:40:28 | INFO | train_inner | epoch 040:    620 / 1120 loss=4.042, nll_loss=2.415, ppl=5.33, wps=21526.7, ups=2.74, wpb=7844.4, bsz=292.2, num_updates=44300, lr=6.00977e-05, gnorm=0.865, clip=0, train_wall=36, wall=16202
2020-10-12 10:41:04 | INFO | train_inner | epoch 040:    720 / 1120 loss=4.003, nll_loss=2.374, ppl=5.18, wps=21275.2, ups=2.78, wpb=7660.2, bsz=342.2, num_updates=44400, lr=6.003e-05, gnorm=0.885, clip=0, train_wall=35, wall=16238
2020-10-12 10:41:40 | INFO | train_inner | epoch 040:    820 / 1120 loss=4.053, nll_loss=2.428, ppl=5.38, wps=21519.6, ups=2.76, wpb=7792.4, bsz=270.4, num_updates=44500, lr=5.99625e-05, gnorm=0.863, clip=0, train_wall=36, wall=16274
2020-10-12 10:42:16 | INFO | train_inner | epoch 040:    920 / 1120 loss=4.052, nll_loss=2.427, ppl=5.38, wps=21375.7, ups=2.78, wpb=7698.4, bsz=282.9, num_updates=44600, lr=5.98953e-05, gnorm=0.885, clip=0, train_wall=35, wall=16310
2020-10-12 10:42:52 | INFO | train_inner | epoch 040:   1020 / 1120 loss=4.047, nll_loss=2.421, ppl=5.36, wps=21331.6, ups=2.76, wpb=7734.8, bsz=291.4, num_updates=44700, lr=5.98282e-05, gnorm=0.89, clip=0, train_wall=36, wall=16347
2020-10-12 10:43:28 | INFO | train_inner | epoch 040:   1120 / 1120 loss=4.065, nll_loss=2.44, ppl=5.43, wps=21136.3, ups=2.79, wpb=7573.9, bsz=265.8, num_updates=44800, lr=5.97614e-05, gnorm=0.902, clip=0, train_wall=35, wall=16383
2020-10-12 10:43:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 10:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000953
2020-10-12 10:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068711
2020-10-12 10:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051568
2020-10-12 10:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121565
2020-10-12 10:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000903
2020-10-12 10:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068782
2020-10-12 10:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051397
2020-10-12 10:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121399
2020-10-12 10:43:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:43:33 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 4.402 | nll_loss 2.729 | ppl 6.63 | wps 44841.3 | wpb 2406.1 | bsz 90.2 | num_updates 44800 | best_loss 4.4
2020-10-12 10:43:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 10:43:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrusukr_sepspm8000/M2O/checkpoint_last.pt (epoch 40 @ 44800 updates, score 4.402) (writing took 1.2054854200105183 seconds)
2020-10-12 10:43:34 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 10:43:34 | INFO | train | epoch 040 | loss 4.039 | nll_loss 2.412 | ppl 5.32 | wps 20980 | ups 2.72 | wpb 7707.9 | bsz 286.9 | num_updates 44800 | lr 5.97614e-05 | gnorm 0.878 | clip 0 | train_wall 397 | wall 16389
2020-10-12 10:43:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 10:43:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 10:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 10:43:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003485
2020-10-12 10:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044240
2020-10-12 10:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001785
2020-10-12 10:43:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.726127
2020-10-12 10:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.772685
2020-10-12 10:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 10:43:35 | INFO | fairseq_cli.train | done training in 16388.8 seconds
