2020-10-12 03:07:56 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_beljpn_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='bel-eng,jpn-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 03:07:56 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 03:07:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['bel', 'eng', 'jpn']
2020-10-12 03:07:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bel] dictionary: 22891 types
2020-10-12 03:07:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 22891 types
2020-10-12 03:07:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | [jpn] dictionary: 22891 types
2020-10-12 03:07:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 03:07:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:07:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 03:07:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:bel-eng': 1, 'main:jpn-eng': 1}
2020-10-12 03:07:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-12 03:07:56 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_beljpn_sepspm8000/M2O/valid.bel-eng.bel
2020-10-12 03:07:56 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_beljpn_sepspm8000/M2O/valid.bel-eng.eng
2020-10-12 03:07:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_beljpn_sepspm8000/M2O/ valid bel-eng 248 examples
2020-10-12 03:07:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:jpn-eng src_langtok: None; tgt_langtok: None
2020-10-12 03:07:56 | INFO | fairseq.data.data_utils | loaded 4429 examples from: fairseq/data-bin/ted_beljpn_sepspm8000/M2O/valid.jpn-eng.jpn
2020-10-12 03:07:56 | INFO | fairseq.data.data_utils | loaded 4429 examples from: fairseq/data-bin/ted_beljpn_sepspm8000/M2O/valid.jpn-eng.eng
2020-10-12 03:07:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_beljpn_sepspm8000/M2O/ valid jpn-eng 4429 examples
2020-10-12 03:07:57 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22891, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22891, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=22891, bias=False)
  )
)
2020-10-12 03:07:57 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 03:07:57 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 03:07:57 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 03:07:57 | INFO | fairseq_cli.train | num. model params: 43263488 (num. trained: 43263488)
2020-10-12 03:07:59 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 03:07:59 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 03:07:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 03:07:59 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.732 GB ; name = GeForce RTX 2080 Ti                     
2020-10-12 03:07:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 03:07:59 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 03:07:59 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 03:07:59 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_last.pt
2020-10-12 03:07:59 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 03:07:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 03:07:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:07:59 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 03:07:59 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:bel-eng': 1, 'main:jpn-eng': 1}
2020-10-12 03:07:59 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-12 03:07:59 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_beljpn_sepspm8000/M2O/train.bel-eng.bel
2020-10-12 03:07:59 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_beljpn_sepspm8000/M2O/train.bel-eng.eng
2020-10-12 03:07:59 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_beljpn_sepspm8000/M2O/ train bel-eng 4509 examples
2020-10-12 03:07:59 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:jpn-eng src_langtok: None; tgt_langtok: None
2020-10-12 03:07:59 | INFO | fairseq.data.data_utils | loaded 39987 examples from: fairseq/data-bin/ted_beljpn_sepspm8000/M2O/train.jpn-eng.jpn
2020-10-12 03:07:59 | INFO | fairseq.data.data_utils | loaded 39987 examples from: fairseq/data-bin/ted_beljpn_sepspm8000/M2O/train.jpn-eng.eng
2020-10-12 03:07:59 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_beljpn_sepspm8000/M2O/ train jpn-eng 39987 examples
2020-10-12 03:07:59 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:bel-eng', 4509), ('main:jpn-eng', 39987)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 03:07:59 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 03:07:59 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 44496
2020-10-12 03:07:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 44496; virtual dataset size 44496
2020-10-12 03:07:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 03:07:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:bel-eng': 4509, 'main:jpn-eng': 39987}; raw total size: 44496
2020-10-12 03:07:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:bel-eng': 4509, 'main:jpn-eng': 39987}; resampled total size: 44496
2020-10-12 03:07:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 03:07:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.003011
2020-10-12 03:07:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:07:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000414
2020-10-12 03:07:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005206
2020-10-12 03:07:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:07:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000198
2020-10-12 03:07:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:07:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098579
2020-10-12 03:07:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104326
2020-10-12 03:07:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:07:59 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 03:07:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:07:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004256
2020-10-12 03:07:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:07:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 03:07:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:07:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097077
2020-10-12 03:07:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101856
2020-10-12 03:07:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:07:59 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 03:08:30 | INFO | train_inner | epoch 001:    100 / 180 loss=14.291, nll_loss=14.189, ppl=18672.1, wps=22345.7, ups=3.25, wpb=6847.9, bsz=252.2, num_updates=100, lr=5.0975e-06, gnorm=4.382, clip=0, train_wall=30, wall=31
2020-10-12 03:08:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000774
2020-10-12 03:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040277
2020-10-12 03:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030681
2020-10-12 03:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072077
2020-10-12 03:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000643
2020-10-12 03:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040211
2020-10-12 03:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030282
2020-10-12 03:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071475
2020-10-12 03:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
/home/han/Documents/11737-hw/assign2/fairseq/fairseq/utils.py:340: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2020-10-12 03:08:58 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.015 | nll_loss 11.644 | ppl 3200.27 | wps 46400 | wpb 2233.7 | bsz 85 | num_updates 180
2020-10-12 03:08:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:08:59 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 1 @ 180 updates, score 12.015) (writing took 0.9657544350047829 seconds)
2020-10-12 03:08:59 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 03:08:59 | INFO | train | epoch 001 | loss 13.576 | nll_loss 13.391 | ppl 10743.8 | wps 20487.5 | ups 3.03 | wpb 6748.6 | bsz 247.2 | num_updates 180 | lr 9.0955e-06 | gnorm 3.482 | clip 0 | train_wall 55 | wall 60
2020-10-12 03:08:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 03:08:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 03:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:08:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000570
2020-10-12 03:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005603
2020-10-12 03:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 03:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097562
2020-10-12 03:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103673
2020-10-12 03:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004352
2020-10-12 03:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 03:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096743
2020-10-12 03:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101602
2020-10-12 03:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:08:59 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 03:09:05 | INFO | train_inner | epoch 002:     20 / 180 loss=12.554, nll_loss=12.253, ppl=4879.62, wps=19058.6, ups=2.84, wpb=6704.2, bsz=247.4, num_updates=200, lr=1.0095e-05, gnorm=2.284, clip=0, train_wall=31, wall=66
2020-10-12 03:09:37 | INFO | train_inner | epoch 002:    120 / 180 loss=11.738, nll_loss=11.345, ppl=2600.42, wps=20969.6, ups=3.15, wpb=6658.3, bsz=244.9, num_updates=300, lr=1.50925e-05, gnorm=1.869, clip=0, train_wall=31, wall=98
2020-10-12 03:09:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:09:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:09:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000772
2020-10-12 03:09:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:09:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040475
2020-10-12 03:09:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:09:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030868
2020-10-12 03:09:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072454
2020-10-12 03:09:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:09:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:09:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000689
2020-10-12 03:09:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:09:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040712
2020-10-12 03:09:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:09:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030302
2020-10-12 03:09:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072030
2020-10-12 03:09:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:09:59 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.124 | nll_loss 9.488 | ppl 718.21 | wps 45922.1 | wpb 2233.7 | bsz 85 | num_updates 360 | best_loss 10.124
2020-10-12 03:09:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:10:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 2 @ 360 updates, score 10.124) (writing took 1.5633490960026393 seconds)
2020-10-12 03:10:00 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 03:10:00 | INFO | train | epoch 002 | loss 11.502 | nll_loss 11.077 | ppl 2160.6 | wps 19637.8 | ups 2.91 | wpb 6748.6 | bsz 247.2 | num_updates 360 | lr 1.8091e-05 | gnorm 1.861 | clip 0 | train_wall 56 | wall 122
2020-10-12 03:10:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 03:10:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 03:10:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:10:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000510
2020-10-12 03:10:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005816
2020-10-12 03:10:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:10:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000245
2020-10-12 03:10:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103954
2020-10-12 03:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110377
2020-10-12 03:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004244
2020-10-12 03:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 03:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097545
2020-10-12 03:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102284
2020-10-12 03:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:10:01 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 03:10:13 | INFO | train_inner | epoch 003:     40 / 180 loss=10.592, nll_loss=10.045, ppl=1056.11, wps=18857.8, ups=2.73, wpb=6897.3, bsz=244, num_updates=400, lr=2.009e-05, gnorm=1.719, clip=0, train_wall=31, wall=135
2020-10-12 03:10:45 | INFO | train_inner | epoch 003:    140 / 180 loss=9.723, nll_loss=9.014, ppl=517.12, wps=20970.8, ups=3.16, wpb=6643.7, bsz=238.9, num_updates=500, lr=2.50875e-05, gnorm=1.607, clip=0, train_wall=31, wall=166
2020-10-12 03:10:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000772
2020-10-12 03:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041133
2020-10-12 03:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031090
2020-10-12 03:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073338
2020-10-12 03:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000677
2020-10-12 03:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040940
2020-10-12 03:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030860
2020-10-12 03:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072805
2020-10-12 03:10:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:11:01 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.201 | nll_loss 8.328 | ppl 321.45 | wps 45463.3 | wpb 2233.7 | bsz 85 | num_updates 540 | best_loss 9.201
2020-10-12 03:11:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:11:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 3 @ 540 updates, score 9.201) (writing took 1.5046002370072529 seconds)
2020-10-12 03:11:02 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 03:11:02 | INFO | train | epoch 003 | loss 9.794 | nll_loss 9.096 | ppl 547.29 | wps 19581.8 | ups 2.9 | wpb 6748.6 | bsz 247.2 | num_updates 540 | lr 2.70865e-05 | gnorm 1.55 | clip 0 | train_wall 56 | wall 184
2020-10-12 03:11:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 03:11:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 03:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:11:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000524
2020-10-12 03:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006007
2020-10-12 03:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000246
2020-10-12 03:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102491
2020-10-12 03:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109096
2020-10-12 03:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004242
2020-10-12 03:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 03:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096991
2020-10-12 03:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101737
2020-10-12 03:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:11:03 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 03:11:22 | INFO | train_inner | epoch 004:     60 / 180 loss=9.438, nll_loss=8.647, ppl=400.97, wps=18123.1, ups=2.73, wpb=6635.2, bsz=243.6, num_updates=600, lr=3.0085e-05, gnorm=1.316, clip=0, train_wall=31, wall=203
2020-10-12 03:11:54 | INFO | train_inner | epoch 004:    160 / 180 loss=9.223, nll_loss=8.375, ppl=331.97, wps=21403.6, ups=3.1, wpb=6909.1, bsz=261.4, num_updates=700, lr=3.50825e-05, gnorm=1.346, clip=0, train_wall=32, wall=235
2020-10-12 03:12:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000771
2020-10-12 03:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039977
2020-10-12 03:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030160
2020-10-12 03:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071242
2020-10-12 03:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000668
2020-10-12 03:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040799
2020-10-12 03:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030245
2020-10-12 03:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072029
2020-10-12 03:12:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:12:03 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.932 | nll_loss 7.979 | ppl 252.32 | wps 45763.8 | wpb 2233.7 | bsz 85 | num_updates 720 | best_loss 8.932
2020-10-12 03:12:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:12:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 4 @ 720 updates, score 8.932) (writing took 1.502957304008305 seconds)
2020-10-12 03:12:05 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 03:12:05 | INFO | train | epoch 004 | loss 9.254 | nll_loss 8.415 | ppl 341.43 | wps 19539.1 | ups 2.9 | wpb 6748.6 | bsz 247.2 | num_updates 720 | lr 3.6082e-05 | gnorm 1.38 | clip 0 | train_wall 57 | wall 246
2020-10-12 03:12:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 03:12:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 03:12:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:12:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000629
2020-10-12 03:12:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006095
2020-10-12 03:12:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:12:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 03:12:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:12:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102224
2020-10-12 03:12:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108829
2020-10-12 03:12:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:12:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:12:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004349
2020-10-12 03:12:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:12:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 03:12:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:12:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099617
2020-10-12 03:12:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104470
2020-10-12 03:12:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:12:05 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 03:12:30 | INFO | train_inner | epoch 005:     80 / 180 loss=9.1, nll_loss=8.223, ppl=298.85, wps=18217.6, ups=2.77, wpb=6583.3, bsz=231.4, num_updates=800, lr=4.008e-05, gnorm=1.595, clip=0, train_wall=31, wall=272
2020-10-12 03:13:02 | INFO | train_inner | epoch 005:    180 / 180 loss=8.953, nll_loss=8.056, ppl=266.14, wps=21297.9, ups=3.11, wpb=6858.6, bsz=261, num_updates=900, lr=4.50775e-05, gnorm=1.442, clip=0, train_wall=32, wall=304
2020-10-12 03:13:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:13:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:13:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000770
2020-10-12 03:13:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:13:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040836
2020-10-12 03:13:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:13:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030570
2020-10-12 03:13:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072514
2020-10-12 03:13:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:13:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:13:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000623
2020-10-12 03:13:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:13:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041008
2020-10-12 03:13:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:13:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030466
2020-10-12 03:13:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072416
2020-10-12 03:13:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:13:05 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.648 | nll_loss 7.655 | ppl 201.59 | wps 45470.6 | wpb 2233.7 | bsz 85 | num_updates 900 | best_loss 8.648
2020-10-12 03:13:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:13:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 5 @ 900 updates, score 8.648) (writing took 1.507987389995833 seconds)
2020-10-12 03:13:07 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 03:13:07 | INFO | train | epoch 005 | loss 9.013 | nll_loss 8.124 | ppl 278.96 | wps 19557.7 | ups 2.9 | wpb 6748.6 | bsz 247.2 | num_updates 900 | lr 4.50775e-05 | gnorm 1.483 | clip 0 | train_wall 56 | wall 308
2020-10-12 03:13:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 03:13:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 03:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:13:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000547
2020-10-12 03:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006302
2020-10-12 03:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 03:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104564
2020-10-12 03:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111498
2020-10-12 03:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004267
2020-10-12 03:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 03:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095590
2020-10-12 03:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100358
2020-10-12 03:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:13:07 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 03:13:39 | INFO | train_inner | epoch 006:    100 / 180 loss=8.838, nll_loss=7.922, ppl=242.47, wps=18479.7, ups=2.72, wpb=6802.9, bsz=251.4, num_updates=1000, lr=5.0075e-05, gnorm=1.562, clip=0, train_wall=32, wall=341
2020-10-12 03:14:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:14:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:14:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000784
2020-10-12 03:14:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:14:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040501
2020-10-12 03:14:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:14:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030251
2020-10-12 03:14:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071873
2020-10-12 03:14:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:14:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:14:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000657
2020-10-12 03:14:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:14:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041192
2020-10-12 03:14:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:14:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030338
2020-10-12 03:14:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072507
2020-10-12 03:14:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:14:07 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.402 | nll_loss 7.383 | ppl 166.86 | wps 45357.3 | wpb 2233.7 | bsz 85 | num_updates 1080 | best_loss 8.402
2020-10-12 03:14:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:14:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 6 @ 1080 updates, score 8.402) (writing took 1.513566116002039 seconds)
2020-10-12 03:14:09 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 03:14:09 | INFO | train | epoch 006 | loss 8.757 | nll_loss 7.831 | ppl 227.63 | wps 19508.4 | ups 2.89 | wpb 6748.6 | bsz 247.2 | num_updates 1080 | lr 5.4073e-05 | gnorm 1.464 | clip 0 | train_wall 57 | wall 370
2020-10-12 03:14:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 03:14:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 03:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:14:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000440
2020-10-12 03:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005824
2020-10-12 03:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 03:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105208
2020-10-12 03:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111625
2020-10-12 03:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004279
2020-10-12 03:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 03:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094443
2020-10-12 03:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099211
2020-10-12 03:14:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:14:09 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 03:14:15 | INFO | train_inner | epoch 007:     20 / 180 loss=8.628, nll_loss=7.685, ppl=205.77, wps=18360.8, ups=2.76, wpb=6654.7, bsz=240.6, num_updates=1100, lr=5.50725e-05, gnorm=1.302, clip=0, train_wall=31, wall=377
2020-10-12 03:14:48 | INFO | train_inner | epoch 007:    120 / 180 loss=8.505, nll_loss=7.545, ppl=186.76, wps=21256.6, ups=3.11, wpb=6838.6, bsz=259.2, num_updates=1200, lr=6.007e-05, gnorm=1.33, clip=0, train_wall=32, wall=409
2020-10-12 03:15:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000779
2020-10-12 03:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040819
2020-10-12 03:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031609
2020-10-12 03:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073551
2020-10-12 03:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000675
2020-10-12 03:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040125
2020-10-12 03:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031036
2020-10-12 03:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072167
2020-10-12 03:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:15:10 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.194 | nll_loss 7.136 | ppl 140.68 | wps 45568.8 | wpb 2233.7 | bsz 85 | num_updates 1260 | best_loss 8.194
2020-10-12 03:15:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:15:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 7 @ 1260 updates, score 8.194) (writing took 1.6150544760021148 seconds)
2020-10-12 03:15:11 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 03:15:11 | INFO | train | epoch 007 | loss 8.528 | nll_loss 7.571 | ppl 190.21 | wps 19504.5 | ups 2.89 | wpb 6748.6 | bsz 247.2 | num_updates 1260 | lr 6.30685e-05 | gnorm 1.364 | clip 0 | train_wall 56 | wall 433
2020-10-12 03:15:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 03:15:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 03:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:15:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.002396
2020-10-12 03:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.020418
2020-10-12 03:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000367
2020-10-12 03:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105636
2020-10-12 03:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.127057
2020-10-12 03:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004340
2020-10-12 03:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 03:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:15:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097623
2020-10-12 03:15:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102478
2020-10-12 03:15:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:15:12 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 03:15:24 | INFO | train_inner | epoch 008:     40 / 180 loss=8.516, nll_loss=7.557, ppl=188.36, wps=18364.2, ups=2.72, wpb=6760.2, bsz=239.1, num_updates=1300, lr=6.50675e-05, gnorm=1.432, clip=0, train_wall=31, wall=446
2020-10-12 03:15:56 | INFO | train_inner | epoch 008:    140 / 180 loss=8.31, nll_loss=7.322, ppl=160.02, wps=21337.5, ups=3.12, wpb=6841.4, bsz=245.3, num_updates=1400, lr=7.0065e-05, gnorm=1.291, clip=0, train_wall=32, wall=478
2020-10-12 03:16:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000770
2020-10-12 03:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040872
2020-10-12 03:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031317
2020-10-12 03:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073298
2020-10-12 03:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000703
2020-10-12 03:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040698
2020-10-12 03:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031041
2020-10-12 03:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072767
2020-10-12 03:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:16:12 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.017 | nll_loss 6.937 | ppl 122.55 | wps 45197.9 | wpb 2233.7 | bsz 85 | num_updates 1440 | best_loss 8.017
2020-10-12 03:16:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:16:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 8 @ 1440 updates, score 8.017) (writing took 1.5094048189930618 seconds)
2020-10-12 03:16:14 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 03:16:14 | INFO | train | epoch 008 | loss 8.338 | nll_loss 7.354 | ppl 163.6 | wps 19499.1 | ups 2.89 | wpb 6748.6 | bsz 247.2 | num_updates 1440 | lr 7.2064e-05 | gnorm 1.326 | clip 0 | train_wall 57 | wall 495
2020-10-12 03:16:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 03:16:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 03:16:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:16:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000628
2020-10-12 03:16:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006224
2020-10-12 03:16:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:16:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 03:16:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:16:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102378
2020-10-12 03:16:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109128
2020-10-12 03:16:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:16:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:16:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004270
2020-10-12 03:16:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:16:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 03:16:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:16:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098213
2020-10-12 03:16:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102989
2020-10-12 03:16:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:16:14 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 03:16:33 | INFO | train_inner | epoch 009:     60 / 180 loss=8.227, nll_loss=7.228, ppl=149.89, wps=18161.5, ups=2.74, wpb=6625.8, bsz=245.5, num_updates=1500, lr=7.50625e-05, gnorm=1.38, clip=0, train_wall=31, wall=514
2020-10-12 03:17:05 | INFO | train_inner | epoch 009:    160 / 180 loss=8.153, nll_loss=7.143, ppl=141.35, wps=20953.7, ups=3.12, wpb=6710.4, bsz=251.5, num_updates=1600, lr=8.006e-05, gnorm=1.224, clip=0, train_wall=31, wall=546
2020-10-12 03:17:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:17:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:17:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000775
2020-10-12 03:17:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:17:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041070
2020-10-12 03:17:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:17:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031585
2020-10-12 03:17:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073775
2020-10-12 03:17:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:17:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:17:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000681
2020-10-12 03:17:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:17:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040779
2020-10-12 03:17:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:17:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031124
2020-10-12 03:17:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072913
2020-10-12 03:17:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:17:14 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.85 | nll_loss 6.751 | ppl 107.69 | wps 45507.6 | wpb 2233.7 | bsz 85 | num_updates 1620 | best_loss 7.85
2020-10-12 03:17:14 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:17:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 9 @ 1620 updates, score 7.85) (writing took 1.515271822005161 seconds)
2020-10-12 03:17:16 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 03:17:16 | INFO | train | epoch 009 | loss 8.168 | nll_loss 7.159 | ppl 142.91 | wps 19531.5 | ups 2.89 | wpb 6748.6 | bsz 247.2 | num_updates 1620 | lr 8.10595e-05 | gnorm 1.282 | clip 0 | train_wall 56 | wall 557
2020-10-12 03:17:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 03:17:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 03:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:17:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000721
2020-10-12 03:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007430
2020-10-12 03:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 03:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100309
2020-10-12 03:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108399
2020-10-12 03:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004245
2020-10-12 03:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 03:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097412
2020-10-12 03:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102165
2020-10-12 03:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:17:16 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 03:17:42 | INFO | train_inner | epoch 010:     80 / 180 loss=8.096, nll_loss=7.077, ppl=135.02, wps=18350.2, ups=2.73, wpb=6710.3, bsz=237.5, num_updates=1700, lr=8.50575e-05, gnorm=1.291, clip=0, train_wall=31, wall=583
2020-10-12 03:18:14 | INFO | train_inner | epoch 010:    180 / 180 loss=7.99, nll_loss=6.955, ppl=124.1, wps=21191.2, ups=3.12, wpb=6793.2, bsz=254.7, num_updates=1800, lr=9.0055e-05, gnorm=1.249, clip=0, train_wall=31, wall=615
2020-10-12 03:18:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000763
2020-10-12 03:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039901
2020-10-12 03:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030991
2020-10-12 03:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071997
2020-10-12 03:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000675
2020-10-12 03:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040833
2020-10-12 03:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030459
2020-10-12 03:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072290
2020-10-12 03:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:18:16 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.767 | nll_loss 6.649 | ppl 100.33 | wps 45669.4 | wpb 2233.7 | bsz 85 | num_updates 1800 | best_loss 7.767
2020-10-12 03:18:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:18:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 10 @ 1800 updates, score 7.767) (writing took 1.5239043420006055 seconds)
2020-10-12 03:18:18 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 03:18:18 | INFO | train | epoch 010 | loss 8.025 | nll_loss 6.996 | ppl 127.61 | wps 19529.8 | ups 2.89 | wpb 6748.6 | bsz 247.2 | num_updates 1800 | lr 9.0055e-05 | gnorm 1.264 | clip 0 | train_wall 57 | wall 619
2020-10-12 03:18:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 03:18:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 03:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:18:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000628
2020-10-12 03:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006358
2020-10-12 03:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 03:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099927
2020-10-12 03:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106793
2020-10-12 03:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004285
2020-10-12 03:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 03:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098269
2020-10-12 03:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103063
2020-10-12 03:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:18:18 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 03:18:51 | INFO | train_inner | epoch 011:    100 / 180 loss=7.943, nll_loss=6.902, ppl=119.56, wps=18525.1, ups=2.7, wpb=6855.2, bsz=259.9, num_updates=1900, lr=9.50525e-05, gnorm=1.31, clip=0, train_wall=32, wall=652
2020-10-12 03:19:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000775
2020-10-12 03:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041110
2020-10-12 03:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030651
2020-10-12 03:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072874
2020-10-12 03:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000671
2020-10-12 03:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039747
2020-10-12 03:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030208
2020-10-12 03:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070948
2020-10-12 03:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:19:19 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.682 | nll_loss 6.55 | ppl 93.69 | wps 45555.4 | wpb 2233.7 | bsz 85 | num_updates 1980 | best_loss 7.682
2020-10-12 03:19:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:19:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 11 @ 1980 updates, score 7.682) (writing took 1.507995709995157 seconds)
2020-10-12 03:19:20 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 03:19:20 | INFO | train | epoch 011 | loss 7.895 | nll_loss 6.847 | ppl 115.11 | wps 19522.4 | ups 2.89 | wpb 6748.6 | bsz 247.2 | num_updates 1980 | lr 9.90505e-05 | gnorm 1.255 | clip 0 | train_wall 57 | wall 682
2020-10-12 03:19:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 03:19:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 03:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:19:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000629
2020-10-12 03:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006127
2020-10-12 03:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-12 03:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100708
2020-10-12 03:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107349
2020-10-12 03:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004270
2020-10-12 03:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 03:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096857
2020-10-12 03:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101626
2020-10-12 03:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:19:20 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 03:19:27 | INFO | train_inner | epoch 012:     20 / 180 loss=7.849, nll_loss=6.793, ppl=110.89, wps=18407.1, ups=2.75, wpb=6682.9, bsz=227.8, num_updates=2000, lr=0.00010005, gnorm=1.18, clip=0, train_wall=31, wall=688
2020-10-12 03:19:59 | INFO | train_inner | epoch 012:    120 / 180 loss=7.822, nll_loss=6.762, ppl=108.54, wps=21090.4, ups=3.11, wpb=6779.7, bsz=258.4, num_updates=2100, lr=0.000105048, gnorm=1.223, clip=0, train_wall=32, wall=720
2020-10-12 03:20:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000768
2020-10-12 03:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041019
2020-10-12 03:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031008
2020-10-12 03:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073136
2020-10-12 03:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000684
2020-10-12 03:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040808
2020-10-12 03:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030326
2020-10-12 03:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072145
2020-10-12 03:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:20:21 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.593 | nll_loss 6.451 | ppl 87.47 | wps 45518.2 | wpb 2233.7 | bsz 85 | num_updates 2160 | best_loss 7.593
2020-10-12 03:20:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:20:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 12 @ 2160 updates, score 7.593) (writing took 1.503549735993147 seconds)
2020-10-12 03:20:22 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 03:20:22 | INFO | train | epoch 012 | loss 7.777 | nll_loss 6.711 | ppl 104.79 | wps 19530.3 | ups 2.89 | wpb 6748.6 | bsz 247.2 | num_updates 2160 | lr 0.000108046 | gnorm 1.207 | clip 0 | train_wall 56 | wall 744
2020-10-12 03:20:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 03:20:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 03:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:20:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000630
2020-10-12 03:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006155
2020-10-12 03:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000256
2020-10-12 03:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102488
2020-10-12 03:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109241
2020-10-12 03:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:20:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:20:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004290
2020-10-12 03:20:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:20:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 03:20:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:20:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095998
2020-10-12 03:20:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100828
2020-10-12 03:20:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:20:23 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 03:20:35 | INFO | train_inner | epoch 013:     40 / 180 loss=7.689, nll_loss=6.61, ppl=97.69, wps=18477.3, ups=2.75, wpb=6723.9, bsz=248.6, num_updates=2200, lr=0.000110045, gnorm=1.238, clip=0, train_wall=31, wall=757
2020-10-12 03:21:08 | INFO | train_inner | epoch 013:    140 / 180 loss=7.667, nll_loss=6.585, ppl=96, wps=21091.7, ups=3.11, wpb=6778.3, bsz=238.5, num_updates=2300, lr=0.000115043, gnorm=1.159, clip=0, train_wall=32, wall=789
2020-10-12 03:21:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:21:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:21:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000698
2020-10-12 03:21:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:21:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040346
2020-10-12 03:21:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:21:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030706
2020-10-12 03:21:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072085
2020-10-12 03:21:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:21:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:21:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000650
2020-10-12 03:21:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:21:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040511
2020-10-12 03:21:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:21:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030377
2020-10-12 03:21:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071856
2020-10-12 03:21:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:21:23 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.463 | nll_loss 6.301 | ppl 78.86 | wps 45529.2 | wpb 2233.7 | bsz 85 | num_updates 2340 | best_loss 7.463
2020-10-12 03:21:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:21:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 13 @ 2340 updates, score 7.463) (writing took 1.8421688870002981 seconds)
2020-10-12 03:21:25 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 03:21:25 | INFO | train | epoch 013 | loss 7.673 | nll_loss 6.591 | ppl 96.42 | wps 19424.7 | ups 2.88 | wpb 6748.6 | bsz 247.2 | num_updates 2340 | lr 0.000117042 | gnorm 1.184 | clip 0 | train_wall 57 | wall 806
2020-10-12 03:21:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 03:21:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 03:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:21:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000637
2020-10-12 03:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006478
2020-10-12 03:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 03:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094371
2020-10-12 03:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101362
2020-10-12 03:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004302
2020-10-12 03:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 03:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096372
2020-10-12 03:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101170
2020-10-12 03:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:21:25 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 03:21:44 | INFO | train_inner | epoch 014:     60 / 180 loss=7.616, nll_loss=6.528, ppl=92.27, wps=18130.5, ups=2.71, wpb=6689.1, bsz=256.1, num_updates=2400, lr=0.00012004, gnorm=1.191, clip=0, train_wall=31, wall=826
2020-10-12 03:22:17 | INFO | train_inner | epoch 014:    160 / 180 loss=7.552, nll_loss=6.451, ppl=87.51, wps=21187.7, ups=3.12, wpb=6797.5, bsz=247.9, num_updates=2500, lr=0.000125037, gnorm=1.134, clip=0, train_wall=31, wall=858
2020-10-12 03:22:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000804
2020-10-12 03:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039606
2020-10-12 03:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030555
2020-10-12 03:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071307
2020-10-12 03:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000618
2020-10-12 03:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039842
2020-10-12 03:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030243
2020-10-12 03:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071030
2020-10-12 03:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:22:26 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.373 | nll_loss 6.186 | ppl 72.78 | wps 45550 | wpb 2233.7 | bsz 85 | num_updates 2520 | best_loss 7.373
2020-10-12 03:22:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:22:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 14 @ 2520 updates, score 7.373) (writing took 1.770638368994696 seconds)
2020-10-12 03:22:27 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 03:22:27 | INFO | train | epoch 014 | loss 7.567 | nll_loss 6.47 | ppl 88.63 | wps 19451.2 | ups 2.88 | wpb 6748.6 | bsz 247.2 | num_updates 2520 | lr 0.000126037 | gnorm 1.176 | clip 0 | train_wall 56 | wall 869
2020-10-12 03:22:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 03:22:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 03:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:22:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000485
2020-10-12 03:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005974
2020-10-12 03:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 03:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097970
2020-10-12 03:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104464
2020-10-12 03:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004384
2020-10-12 03:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 03:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096831
2020-10-12 03:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101715
2020-10-12 03:22:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:22:28 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 03:22:53 | INFO | train_inner | epoch 015:     80 / 180 loss=7.48, nll_loss=6.371, ppl=82.78, wps=18146.7, ups=2.75, wpb=6607.2, bsz=227.4, num_updates=2600, lr=0.000130035, gnorm=1.122, clip=0, train_wall=31, wall=894
2020-10-12 03:23:25 | INFO | train_inner | epoch 015:    180 / 180 loss=7.449, nll_loss=6.333, ppl=80.61, wps=21292.1, ups=3.12, wpb=6823.7, bsz=260.3, num_updates=2700, lr=0.000135032, gnorm=1.073, clip=0, train_wall=31, wall=926
2020-10-12 03:23:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:23:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:23:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000797
2020-10-12 03:23:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:23:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040525
2020-10-12 03:23:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:23:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031049
2020-10-12 03:23:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072712
2020-10-12 03:23:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:23:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:23:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000645
2020-10-12 03:23:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:23:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041041
2020-10-12 03:23:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:23:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030786
2020-10-12 03:23:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072801
2020-10-12 03:23:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:23:28 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.275 | nll_loss 6.084 | ppl 67.82 | wps 45497.8 | wpb 2233.7 | bsz 85 | num_updates 2700 | best_loss 7.275
2020-10-12 03:23:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:23:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 15 @ 2700 updates, score 7.275) (writing took 1.5266618199966615 seconds)
2020-10-12 03:23:29 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 03:23:29 | INFO | train | epoch 015 | loss 7.46 | nll_loss 6.347 | ppl 81.42 | wps 19585.1 | ups 2.9 | wpb 6748.6 | bsz 247.2 | num_updates 2700 | lr 0.000135032 | gnorm 1.083 | clip 0 | train_wall 56 | wall 931
2020-10-12 03:23:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 03:23:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 03:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:23:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000649
2020-10-12 03:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006202
2020-10-12 03:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000246
2020-10-12 03:23:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:23:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100969
2020-10-12 03:23:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107831
2020-10-12 03:23:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:23:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:23:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004241
2020-10-12 03:23:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:23:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 03:23:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:23:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097971
2020-10-12 03:23:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102706
2020-10-12 03:23:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:23:30 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 03:24:02 | INFO | train_inner | epoch 016:    100 / 180 loss=7.366, nll_loss=6.239, ppl=75.55, wps=18684.6, ups=2.72, wpb=6857.6, bsz=256.8, num_updates=2800, lr=0.00014003, gnorm=1.161, clip=0, train_wall=31, wall=963
2020-10-12 03:24:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000790
2020-10-12 03:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041019
2020-10-12 03:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031417
2020-10-12 03:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073568
2020-10-12 03:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000651
2020-10-12 03:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040683
2020-10-12 03:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030791
2020-10-12 03:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072448
2020-10-12 03:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:24:30 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.205 | nll_loss 6 | ppl 64.01 | wps 45628.5 | wpb 2233.7 | bsz 85 | num_updates 2880 | best_loss 7.205
2020-10-12 03:24:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:24:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 16 @ 2880 updates, score 7.205) (writing took 1.5021403160062619 seconds)
2020-10-12 03:24:31 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 03:24:31 | INFO | train | epoch 016 | loss 7.373 | nll_loss 6.247 | ppl 75.95 | wps 19560 | ups 2.9 | wpb 6748.6 | bsz 247.2 | num_updates 2880 | lr 0.000144028 | gnorm 1.147 | clip 0 | train_wall 56 | wall 993
2020-10-12 03:24:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 03:24:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 03:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:24:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000539
2020-10-12 03:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006091
2020-10-12 03:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 03:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101000
2020-10-12 03:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107614
2020-10-12 03:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004301
2020-10-12 03:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 03:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097681
2020-10-12 03:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102481
2020-10-12 03:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:24:32 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 03:24:38 | INFO | train_inner | epoch 017:     20 / 180 loss=7.359, nll_loss=6.231, ppl=75.11, wps=18317.2, ups=2.74, wpb=6691.2, bsz=249.8, num_updates=2900, lr=0.000145028, gnorm=1.124, clip=0, train_wall=31, wall=1000
2020-10-12 03:25:10 | INFO | train_inner | epoch 017:    120 / 180 loss=7.265, nll_loss=6.124, ppl=69.74, wps=21310.3, ups=3.12, wpb=6820.3, bsz=257.3, num_updates=3000, lr=0.000150025, gnorm=1.06, clip=0, train_wall=31, wall=1032
2020-10-12 03:25:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000785
2020-10-12 03:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041013
2020-10-12 03:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030580
2020-10-12 03:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072710
2020-10-12 03:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000687
2020-10-12 03:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040278
2020-10-12 03:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030557
2020-10-12 03:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071842
2020-10-12 03:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:25:32 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.142 | nll_loss 5.934 | ppl 61.14 | wps 45648.4 | wpb 2233.7 | bsz 85 | num_updates 3060 | best_loss 7.142
2020-10-12 03:25:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:25:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 17 @ 3060 updates, score 7.142) (writing took 1.501579338000738 seconds)
2020-10-12 03:25:33 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 03:25:33 | INFO | train | epoch 017 | loss 7.27 | nll_loss 6.128 | ppl 69.94 | wps 19610.8 | ups 2.91 | wpb 6748.6 | bsz 247.2 | num_updates 3060 | lr 0.000153024 | gnorm 1.095 | clip 0 | train_wall 56 | wall 1055
2020-10-12 03:25:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 03:25:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 03:25:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:25:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000684
2020-10-12 03:25:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006682
2020-10-12 03:25:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:25:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 03:25:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102388
2020-10-12 03:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109586
2020-10-12 03:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008429
2020-10-12 03:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 03:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097018
2020-10-12 03:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105969
2020-10-12 03:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:25:34 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 03:25:47 | INFO | train_inner | epoch 018:     40 / 180 loss=7.259, nll_loss=6.115, ppl=69.31, wps=18182.1, ups=2.75, wpb=6603.4, bsz=224.2, num_updates=3100, lr=0.000155023, gnorm=1.101, clip=0, train_wall=31, wall=1068
2020-10-12 03:26:19 | INFO | train_inner | epoch 018:    140 / 180 loss=7.151, nll_loss=5.992, ppl=63.64, wps=21367.4, ups=3.12, wpb=6857, bsz=262.8, num_updates=3200, lr=0.00016002, gnorm=1.162, clip=0, train_wall=32, wall=1100
2020-10-12 03:26:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000791
2020-10-12 03:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041153
2020-10-12 03:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030462
2020-10-12 03:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072744
2020-10-12 03:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000664
2020-10-12 03:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039824
2020-10-12 03:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030190
2020-10-12 03:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070998
2020-10-12 03:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:26:34 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.126 | nll_loss 5.91 | ppl 60.14 | wps 45518.9 | wpb 2233.7 | bsz 85 | num_updates 3240 | best_loss 7.126
2020-10-12 03:26:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:26:36 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 18 @ 3240 updates, score 7.126) (writing took 1.5063455929921474 seconds)
2020-10-12 03:26:36 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 03:26:36 | INFO | train | epoch 018 | loss 7.183 | nll_loss 6.028 | ppl 65.27 | wps 19564.8 | ups 2.9 | wpb 6748.6 | bsz 247.2 | num_updates 3240 | lr 0.000162019 | gnorm 1.126 | clip 0 | train_wall 56 | wall 1117
2020-10-12 03:26:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 03:26:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 03:26:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:26:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000610
2020-10-12 03:26:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006558
2020-10-12 03:26:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:26:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000239
2020-10-12 03:26:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:26:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100662
2020-10-12 03:26:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107802
2020-10-12 03:26:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:26:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:26:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004280
2020-10-12 03:26:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:26:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 03:26:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:26:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098160
2020-10-12 03:26:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102936
2020-10-12 03:26:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:26:36 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 03:26:55 | INFO | train_inner | epoch 019:     60 / 180 loss=7.147, nll_loss=5.986, ppl=63.38, wps=18194.4, ups=2.75, wpb=6611.8, bsz=237.2, num_updates=3300, lr=0.000165018, gnorm=1.106, clip=0, train_wall=31, wall=1136
2020-10-12 03:27:27 | INFO | train_inner | epoch 019:    160 / 180 loss=7.055, nll_loss=5.881, ppl=58.93, wps=21415.2, ups=3.12, wpb=6854.1, bsz=257.4, num_updates=3400, lr=0.000170015, gnorm=1.095, clip=0, train_wall=31, wall=1168
2020-10-12 03:27:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:27:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:27:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000776
2020-10-12 03:27:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:27:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040687
2020-10-12 03:27:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:27:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030494
2020-10-12 03:27:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072295
2020-10-12 03:27:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:27:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:27:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000655
2020-10-12 03:27:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:27:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040257
2020-10-12 03:27:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:27:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030572
2020-10-12 03:27:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071801
2020-10-12 03:27:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:27:36 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.062 | nll_loss 5.825 | ppl 56.7 | wps 45300.6 | wpb 2233.7 | bsz 85 | num_updates 3420 | best_loss 7.062
2020-10-12 03:27:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:27:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 19 @ 3420 updates, score 7.062) (writing took 1.517267527000513 seconds)
2020-10-12 03:27:38 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 03:27:38 | INFO | train | epoch 019 | loss 7.088 | nll_loss 5.918 | ppl 60.48 | wps 19575.7 | ups 2.9 | wpb 6748.6 | bsz 247.2 | num_updates 3420 | lr 0.000171015 | gnorm 1.086 | clip 0 | train_wall 56 | wall 1179
2020-10-12 03:27:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 03:27:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 03:27:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:27:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000735
2020-10-12 03:27:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007499
2020-10-12 03:27:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:27:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000273
2020-10-12 03:27:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:27:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101502
2020-10-12 03:27:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109655
2020-10-12 03:27:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:27:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:27:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004293
2020-10-12 03:27:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:27:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-12 03:27:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:27:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096785
2020-10-12 03:27:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101579
2020-10-12 03:27:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:27:38 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 03:28:04 | INFO | train_inner | epoch 020:     80 / 180 loss=7.091, nll_loss=5.921, ppl=60.58, wps=18669.8, ups=2.73, wpb=6833.4, bsz=223.4, num_updates=3500, lr=0.000175013, gnorm=1.059, clip=0, train_wall=31, wall=1205
2020-10-12 03:28:35 | INFO | train_inner | epoch 020:    180 / 180 loss=6.932, nll_loss=5.739, ppl=53.41, wps=20785.9, ups=3.15, wpb=6609, bsz=256, num_updates=3600, lr=0.00018001, gnorm=1.108, clip=0, train_wall=31, wall=1237
2020-10-12 03:28:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:28:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:28:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000759
2020-10-12 03:28:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:28:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040326
2020-10-12 03:28:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:28:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030263
2020-10-12 03:28:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071685
2020-10-12 03:28:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:28:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:28:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000676
2020-10-12 03:28:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:28:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040734
2020-10-12 03:28:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029891
2020-10-12 03:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071630
2020-10-12 03:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:28:38 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.034 | nll_loss 5.791 | ppl 55.38 | wps 45700.6 | wpb 2233.7 | bsz 85 | num_updates 3600 | best_loss 7.034
2020-10-12 03:28:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:28:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 20 @ 3600 updates, score 7.034) (writing took 1.526620043994626 seconds)
2020-10-12 03:28:40 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 03:28:40 | INFO | train | epoch 020 | loss 6.991 | nll_loss 5.806 | ppl 55.96 | wps 19534.4 | ups 2.89 | wpb 6748.6 | bsz 247.2 | num_updates 3600 | lr 0.00018001 | gnorm 1.089 | clip 0 | train_wall 57 | wall 1241
2020-10-12 03:28:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 03:28:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 03:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:28:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000495
2020-10-12 03:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006377
2020-10-12 03:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 03:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102435
2020-10-12 03:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109421
2020-10-12 03:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004239
2020-10-12 03:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 03:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096768
2020-10-12 03:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101503
2020-10-12 03:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:28:40 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 03:29:12 | INFO | train_inner | epoch 021:    100 / 180 loss=6.882, nll_loss=5.682, ppl=51.32, wps=18567.7, ups=2.73, wpb=6795.7, bsz=234.6, num_updates=3700, lr=0.000185008, gnorm=1.092, clip=0, train_wall=31, wall=1273
2020-10-12 03:29:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000768
2020-10-12 03:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040835
2020-10-12 03:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030858
2020-10-12 03:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072795
2020-10-12 03:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000626
2020-10-12 03:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041365
2020-10-12 03:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030536
2020-10-12 03:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072850
2020-10-12 03:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:29:40 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.006 | nll_loss 5.744 | ppl 53.58 | wps 45806.7 | wpb 2233.7 | bsz 85 | num_updates 3780 | best_loss 7.006
2020-10-12 03:29:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:29:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 21 @ 3780 updates, score 7.006) (writing took 1.5154789780062856 seconds)
2020-10-12 03:29:42 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 03:29:42 | INFO | train | epoch 021 | loss 6.912 | nll_loss 5.714 | ppl 52.51 | wps 19534.8 | ups 2.89 | wpb 6748.6 | bsz 247.2 | num_updates 3780 | lr 0.000189006 | gnorm 1.166 | clip 0 | train_wall 57 | wall 1303
2020-10-12 03:29:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 03:29:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 03:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:29:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000558
2020-10-12 03:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007261
2020-10-12 03:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000210
2020-10-12 03:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101379
2020-10-12 03:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109335
2020-10-12 03:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004260
2020-10-12 03:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 03:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097511
2020-10-12 03:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102271
2020-10-12 03:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:29:42 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 03:29:49 | INFO | train_inner | epoch 022:     20 / 180 loss=6.931, nll_loss=5.736, ppl=53.29, wps=18287.5, ups=2.73, wpb=6710.3, bsz=266.8, num_updates=3800, lr=0.000190005, gnorm=1.23, clip=0, train_wall=31, wall=1310
2020-10-12 03:30:21 | INFO | train_inner | epoch 022:    120 / 180 loss=6.805, nll_loss=5.592, ppl=48.24, wps=20997.9, ups=3.13, wpb=6716.4, bsz=242.5, num_updates=3900, lr=0.000195003, gnorm=1.153, clip=0, train_wall=31, wall=1342
2020-10-12 03:30:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:30:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:30:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000811
2020-10-12 03:30:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:30:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040872
2020-10-12 03:30:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:30:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031052
2020-10-12 03:30:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073081
2020-10-12 03:30:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:30:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:30:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000653
2020-10-12 03:30:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:30:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041411
2020-10-12 03:30:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:30:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030884
2020-10-12 03:30:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073275
2020-10-12 03:30:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:30:43 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.899 | nll_loss 5.623 | ppl 49.29 | wps 45543.7 | wpb 2233.7 | bsz 85 | num_updates 3960 | best_loss 6.899
2020-10-12 03:30:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:30:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 22 @ 3960 updates, score 6.899) (writing took 1.5006056379934307 seconds)
2020-10-12 03:30:44 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 03:30:44 | INFO | train | epoch 022 | loss 6.816 | nll_loss 5.603 | ppl 48.62 | wps 19572.3 | ups 2.9 | wpb 6748.6 | bsz 247.2 | num_updates 3960 | lr 0.000198001 | gnorm 1.127 | clip 0 | train_wall 56 | wall 1365
2020-10-12 03:30:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 03:30:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 03:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:30:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000564
2020-10-12 03:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006165
2020-10-12 03:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 03:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098890
2020-10-12 03:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105567
2020-10-12 03:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004316
2020-10-12 03:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 03:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096758
2020-10-12 03:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101584
2020-10-12 03:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:30:44 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 03:30:57 | INFO | train_inner | epoch 023:     40 / 180 loss=6.813, nll_loss=5.599, ppl=48.47, wps=18679.5, ups=2.74, wpb=6807.4, bsz=244.6, num_updates=4000, lr=0.0002, gnorm=1.109, clip=0, train_wall=31, wall=1378
2020-10-12 03:31:29 | INFO | train_inner | epoch 023:    140 / 180 loss=6.698, nll_loss=5.469, ppl=44.28, wps=21083.2, ups=3.11, wpb=6782.1, bsz=257.6, num_updates=4100, lr=0.000197546, gnorm=1.161, clip=0, train_wall=32, wall=1411
2020-10-12 03:31:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:31:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:31:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000788
2020-10-12 03:31:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:31:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041177
2020-10-12 03:31:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:31:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030782
2020-10-12 03:31:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073102
2020-10-12 03:31:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:31:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:31:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000668
2020-10-12 03:31:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:31:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041162
2020-10-12 03:31:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:31:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030757
2020-10-12 03:31:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072913
2020-10-12 03:31:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:31:45 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.849 | nll_loss 5.562 | ppl 47.24 | wps 45423.1 | wpb 2233.7 | bsz 85 | num_updates 4140 | best_loss 6.849
2020-10-12 03:31:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:31:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 23 @ 4140 updates, score 6.849) (writing took 1.6508924719964853 seconds)
2020-10-12 03:31:46 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 03:31:46 | INFO | train | epoch 023 | loss 6.721 | nll_loss 5.494 | ppl 45.06 | wps 19494.8 | ups 2.89 | wpb 6748.6 | bsz 247.2 | num_updates 4140 | lr 0.000196589 | gnorm 1.134 | clip 0 | train_wall 56 | wall 1428
2020-10-12 03:31:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 03:31:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 03:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:31:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000573
2020-10-12 03:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006365
2020-10-12 03:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 03:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106106
2020-10-12 03:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113005
2020-10-12 03:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004342
2020-10-12 03:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 03:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096625
2020-10-12 03:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101459
2020-10-12 03:31:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:31:47 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 03:32:06 | INFO | train_inner | epoch 024:     60 / 180 loss=6.665, nll_loss=5.429, ppl=43.08, wps=18233.7, ups=2.74, wpb=6649, bsz=227, num_updates=4200, lr=0.00019518, gnorm=1.077, clip=0, train_wall=31, wall=1447
2020-10-12 03:32:38 | INFO | train_inner | epoch 024:    160 / 180 loss=6.59, nll_loss=5.342, ppl=40.57, wps=21144.2, ups=3.11, wpb=6792.6, bsz=261.4, num_updates=4300, lr=0.000192897, gnorm=1.123, clip=0, train_wall=32, wall=1479
2020-10-12 03:32:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000767
2020-10-12 03:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041736
2020-10-12 03:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030763
2020-10-12 03:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073607
2020-10-12 03:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000669
2020-10-12 03:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040229
2020-10-12 03:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030700
2020-10-12 03:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071932
2020-10-12 03:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:32:47 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.795 | nll_loss 5.494 | ppl 45.06 | wps 45704.5 | wpb 2233.7 | bsz 85 | num_updates 4320 | best_loss 6.795
2020-10-12 03:32:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:32:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 24 @ 4320 updates, score 6.795) (writing took 1.5096134269988397 seconds)
2020-10-12 03:32:48 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 03:32:48 | INFO | train | epoch 024 | loss 6.62 | nll_loss 5.377 | ppl 41.56 | wps 19539.3 | ups 2.9 | wpb 6748.6 | bsz 247.2 | num_updates 4320 | lr 0.00019245 | gnorm 1.106 | clip 0 | train_wall 56 | wall 1490
2020-10-12 03:32:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 03:32:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 03:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:32:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000817
2020-10-12 03:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008794
2020-10-12 03:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000211
2020-10-12 03:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103001
2020-10-12 03:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112424
2020-10-12 03:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004318
2020-10-12 03:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-12 03:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097181
2020-10-12 03:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101988
2020-10-12 03:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:32:49 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 03:33:14 | INFO | train_inner | epoch 025:     80 / 180 loss=6.552, nll_loss=5.299, ppl=39.38, wps=18430.6, ups=2.75, wpb=6693, bsz=244.3, num_updates=4400, lr=0.000190693, gnorm=1.093, clip=0, train_wall=31, wall=1516
2020-10-12 03:33:46 | INFO | train_inner | epoch 025:    180 / 180 loss=6.514, nll_loss=5.253, ppl=38.14, wps=21305.7, ups=3.14, wpb=6791, bsz=246.1, num_updates=4500, lr=0.000188562, gnorm=1.081, clip=0, train_wall=31, wall=1547
2020-10-12 03:33:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000765
2020-10-12 03:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041290
2020-10-12 03:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031142
2020-10-12 03:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073536
2020-10-12 03:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000669
2020-10-12 03:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041044
2020-10-12 03:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030433
2020-10-12 03:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072481
2020-10-12 03:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:33:49 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.773 | nll_loss 5.464 | ppl 44.15 | wps 45692.8 | wpb 2233.7 | bsz 85 | num_updates 4500 | best_loss 6.773
2020-10-12 03:33:49 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:33:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 25 @ 4500 updates, score 6.773) (writing took 1.5075516659999266 seconds)
2020-10-12 03:33:50 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 03:33:50 | INFO | train | epoch 025 | loss 6.516 | nll_loss 5.256 | ppl 38.22 | wps 19616.3 | ups 2.91 | wpb 6748.6 | bsz 247.2 | num_updates 4500 | lr 0.000188562 | gnorm 1.088 | clip 0 | train_wall 56 | wall 1552
2020-10-12 03:33:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 03:33:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 03:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:33:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000579
2020-10-12 03:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006559
2020-10-12 03:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000266
2020-10-12 03:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103206
2020-10-12 03:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110384
2020-10-12 03:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005872
2020-10-12 03:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000190
2020-10-12 03:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098236
2020-10-12 03:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104645
2020-10-12 03:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:33:51 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 03:34:23 | INFO | train_inner | epoch 026:    100 / 180 loss=6.424, nll_loss=5.15, ppl=35.51, wps=18594.2, ups=2.73, wpb=6821.3, bsz=244.3, num_updates=4600, lr=0.000186501, gnorm=1.113, clip=0, train_wall=31, wall=1584
2020-10-12 03:34:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000775
2020-10-12 03:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041532
2020-10-12 03:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030537
2020-10-12 03:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073176
2020-10-12 03:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000681
2020-10-12 03:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040938
2020-10-12 03:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030741
2020-10-12 03:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072679
2020-10-12 03:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:34:51 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.737 | nll_loss 5.412 | ppl 42.58 | wps 45282.1 | wpb 2233.7 | bsz 85 | num_updates 4680 | best_loss 6.737
2020-10-12 03:34:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:34:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 26 @ 4680 updates, score 6.737) (writing took 1.502559779008152 seconds)
2020-10-12 03:34:53 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 03:34:53 | INFO | train | epoch 026 | loss 6.431 | nll_loss 5.158 | ppl 35.7 | wps 19547.9 | ups 2.9 | wpb 6748.6 | bsz 247.2 | num_updates 4680 | lr 0.0001849 | gnorm 1.109 | clip 0 | train_wall 56 | wall 1614
2020-10-12 03:34:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 03:34:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 03:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:34:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000508
2020-10-12 03:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005905
2020-10-12 03:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-12 03:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099685
2020-10-12 03:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106098
2020-10-12 03:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004255
2020-10-12 03:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 03:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095547
2020-10-12 03:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100300
2020-10-12 03:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:34:53 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 03:34:59 | INFO | train_inner | epoch 027:     20 / 180 loss=6.411, nll_loss=5.134, ppl=35.13, wps=18138.9, ups=2.74, wpb=6611.9, bsz=253.6, num_updates=4700, lr=0.000184506, gnorm=1.13, clip=0, train_wall=31, wall=1621
2020-10-12 03:35:31 | INFO | train_inner | epoch 027:    120 / 180 loss=6.352, nll_loss=5.065, ppl=33.47, wps=21227.1, ups=3.12, wpb=6808.6, bsz=251.7, num_updates=4800, lr=0.000182574, gnorm=1.082, clip=0, train_wall=31, wall=1653
2020-10-12 03:35:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000689
2020-10-12 03:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039938
2020-10-12 03:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030630
2020-10-12 03:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071581
2020-10-12 03:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000616
2020-10-12 03:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039503
2020-10-12 03:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030431
2020-10-12 03:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070875
2020-10-12 03:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:35:53 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.715 | nll_loss 5.388 | ppl 41.87 | wps 45465.6 | wpb 2233.7 | bsz 85 | num_updates 4860 | best_loss 6.715
2020-10-12 03:35:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:35:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 27 @ 4860 updates, score 6.715) (writing took 1.5033296329929726 seconds)
2020-10-12 03:35:55 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 03:35:55 | INFO | train | epoch 027 | loss 6.34 | nll_loss 5.051 | ppl 33.15 | wps 19549.1 | ups 2.9 | wpb 6748.6 | bsz 247.2 | num_updates 4860 | lr 0.000181444 | gnorm 1.1 | clip 0 | train_wall 56 | wall 1676
2020-10-12 03:35:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 03:35:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 03:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:35:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000614
2020-10-12 03:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006054
2020-10-12 03:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 03:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098730
2020-10-12 03:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105303
2020-10-12 03:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004281
2020-10-12 03:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 03:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097459
2020-10-12 03:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102238
2020-10-12 03:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:35:55 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 03:36:08 | INFO | train_inner | epoch 028:     40 / 180 loss=6.261, nll_loss=4.961, ppl=31.15, wps=18604.4, ups=2.74, wpb=6783.3, bsz=247.5, num_updates=4900, lr=0.000180702, gnorm=1.086, clip=0, train_wall=31, wall=1689
2020-10-12 03:36:40 | INFO | train_inner | epoch 028:    140 / 180 loss=6.298, nll_loss=5.001, ppl=32.02, wps=21098.7, ups=3.12, wpb=6758, bsz=243, num_updates=5000, lr=0.000178885, gnorm=1.095, clip=0, train_wall=31, wall=1721
2020-10-12 03:36:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:36:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:36:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000796
2020-10-12 03:36:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:36:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040873
2020-10-12 03:36:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:36:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029936
2020-10-12 03:36:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071940
2020-10-12 03:36:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:36:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:36:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000653
2020-10-12 03:36:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040508
2020-10-12 03:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030451
2020-10-12 03:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071925
2020-10-12 03:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:36:55 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.704 | nll_loss 5.366 | ppl 41.24 | wps 45769.7 | wpb 2233.7 | bsz 85 | num_updates 5040 | best_loss 6.704
2020-10-12 03:36:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:36:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 28 @ 5040 updates, score 6.704) (writing took 1.5565918690117542 seconds)
2020-10-12 03:36:57 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 03:36:57 | INFO | train | epoch 028 | loss 6.261 | nll_loss 4.96 | ppl 31.12 | wps 19552.5 | ups 2.9 | wpb 6748.6 | bsz 247.2 | num_updates 5040 | lr 0.000178174 | gnorm 1.105 | clip 0 | train_wall 56 | wall 1738
2020-10-12 03:36:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 03:36:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 03:36:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:36:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000600
2020-10-12 03:36:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005936
2020-10-12 03:36:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:36:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 03:36:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:36:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103737
2020-10-12 03:36:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110259
2020-10-12 03:36:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:36:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:36:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004281
2020-10-12 03:36:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:36:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 03:36:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:36:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095715
2020-10-12 03:36:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100504
2020-10-12 03:36:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:36:57 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 03:37:16 | INFO | train_inner | epoch 029:     60 / 180 loss=6.223, nll_loss=4.915, ppl=30.18, wps=18415.1, ups=2.75, wpb=6703.5, bsz=241.8, num_updates=5100, lr=0.000177123, gnorm=1.103, clip=0, train_wall=31, wall=1758
2020-10-12 03:37:48 | INFO | train_inner | epoch 029:    160 / 180 loss=6.169, nll_loss=4.852, ppl=28.89, wps=20963.8, ups=3.13, wpb=6704.4, bsz=252.5, num_updates=5200, lr=0.000175412, gnorm=1.057, clip=0, train_wall=31, wall=1790
2020-10-12 03:37:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:37:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000787
2020-10-12 03:37:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:37:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040849
2020-10-12 03:37:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:37:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031017
2020-10-12 03:37:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072982
2020-10-12 03:37:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:37:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:37:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000657
2020-10-12 03:37:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:37:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040625
2020-10-12 03:37:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:37:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030805
2020-10-12 03:37:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072403
2020-10-12 03:37:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:37:57 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.655 | nll_loss 5.311 | ppl 39.69 | wps 45603.8 | wpb 2233.7 | bsz 85 | num_updates 5220 | best_loss 6.655
2020-10-12 03:37:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:37:59 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 29 @ 5220 updates, score 6.655) (writing took 1.8458514470112277 seconds)
2020-10-12 03:37:59 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 03:37:59 | INFO | train | epoch 029 | loss 6.174 | nll_loss 4.859 | ppl 29.01 | wps 19471.8 | ups 2.89 | wpb 6748.6 | bsz 247.2 | num_updates 5220 | lr 0.000175075 | gnorm 1.053 | clip 0 | train_wall 56 | wall 1801
2020-10-12 03:37:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 03:37:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 03:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:37:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000568
2020-10-12 03:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006373
2020-10-12 03:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 03:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098006
2020-10-12 03:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104883
2020-10-12 03:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004421
2020-10-12 03:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 03:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098024
2020-10-12 03:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102946
2020-10-12 03:37:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:37:59 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 03:38:25 | INFO | train_inner | epoch 030:     80 / 180 loss=6.127, nll_loss=4.803, ppl=27.92, wps=18179.7, ups=2.72, wpb=6689.2, bsz=248.3, num_updates=5300, lr=0.000173749, gnorm=1.132, clip=0, train_wall=31, wall=1826
2020-10-12 03:38:57 | INFO | train_inner | epoch 030:    180 / 180 loss=6.099, nll_loss=4.769, ppl=27.27, wps=21423.2, ups=3.12, wpb=6857.2, bsz=242.1, num_updates=5400, lr=0.000172133, gnorm=1.07, clip=0, train_wall=31, wall=1858
2020-10-12 03:38:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000778
2020-10-12 03:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041468
2020-10-12 03:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031617
2020-10-12 03:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.074200
2020-10-12 03:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000665
2020-10-12 03:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041374
2020-10-12 03:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031022
2020-10-12 03:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073393
2020-10-12 03:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:39:00 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.643 | nll_loss 5.304 | ppl 39.5 | wps 45266.2 | wpb 2233.7 | bsz 85 | num_updates 5400 | best_loss 6.643
2020-10-12 03:39:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:39:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 30 @ 5400 updates, score 6.643) (writing took 1.5246068949927576 seconds)
2020-10-12 03:39:01 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 03:39:01 | INFO | train | epoch 030 | loss 6.106 | nll_loss 4.778 | ppl 27.44 | wps 19547.9 | ups 2.9 | wpb 6748.6 | bsz 247.2 | num_updates 5400 | lr 0.000172133 | gnorm 1.115 | clip 0 | train_wall 56 | wall 1863
2020-10-12 03:39:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 03:39:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 03:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:39:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000717
2020-10-12 03:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008420
2020-10-12 03:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000303
2020-10-12 03:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100462
2020-10-12 03:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109603
2020-10-12 03:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004323
2020-10-12 03:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 03:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096766
2020-10-12 03:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101591
2020-10-12 03:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:39:02 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 03:39:33 | INFO | train_inner | epoch 031:    100 / 180 loss=6.039, nll_loss=4.701, ppl=26.01, wps=18393.3, ups=2.74, wpb=6718.4, bsz=229.2, num_updates=5500, lr=0.000170561, gnorm=1.083, clip=0, train_wall=31, wall=1895
2020-10-12 03:39:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000781
2020-10-12 03:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040903
2020-10-12 03:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030937
2020-10-12 03:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072970
2020-10-12 03:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000667
2020-10-12 03:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040222
2020-10-12 03:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030580
2020-10-12 03:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071791
2020-10-12 03:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:40:02 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.635 | nll_loss 5.269 | ppl 38.55 | wps 45577.5 | wpb 2233.7 | bsz 85 | num_updates 5580 | best_loss 6.635
2020-10-12 03:40:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:40:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 31 @ 5580 updates, score 6.635) (writing took 1.9598980489972746 seconds)
2020-10-12 03:40:04 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 03:40:04 | INFO | train | epoch 031 | loss 6.026 | nll_loss 4.686 | ppl 25.74 | wps 19393.8 | ups 2.87 | wpb 6748.6 | bsz 247.2 | num_updates 5580 | lr 0.000169334 | gnorm 1.086 | clip 0 | train_wall 56 | wall 1925
2020-10-12 03:40:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 03:40:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 03:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:40:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000887
2020-10-12 03:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008321
2020-10-12 03:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 03:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098798
2020-10-12 03:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107827
2020-10-12 03:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004323
2020-10-12 03:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 03:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097984
2020-10-12 03:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102824
2020-10-12 03:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:40:04 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 03:40:11 | INFO | train_inner | epoch 032:     20 / 180 loss=5.987, nll_loss=4.64, ppl=24.92, wps=18316.5, ups=2.68, wpb=6830.2, bsz=269.1, num_updates=5600, lr=0.000169031, gnorm=1.091, clip=0, train_wall=32, wall=1932
2020-10-12 03:40:43 | INFO | train_inner | epoch 032:    120 / 180 loss=5.964, nll_loss=4.613, ppl=24.47, wps=20842.1, ups=3.14, wpb=6637.5, bsz=240, num_updates=5700, lr=0.000167542, gnorm=1.113, clip=0, train_wall=31, wall=1964
2020-10-12 03:41:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:41:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:41:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000766
2020-10-12 03:41:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:41:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040442
2020-10-12 03:41:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:41:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031087
2020-10-12 03:41:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072627
2020-10-12 03:41:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:41:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:41:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000676
2020-10-12 03:41:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:41:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039573
2020-10-12 03:41:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:41:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030242
2020-10-12 03:41:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070814
2020-10-12 03:41:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:41:05 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.616 | nll_loss 5.24 | ppl 37.8 | wps 45437.8 | wpb 2233.7 | bsz 85 | num_updates 5760 | best_loss 6.616
2020-10-12 03:41:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:41:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 32 @ 5760 updates, score 6.616) (writing took 1.9692391360003967 seconds)
2020-10-12 03:41:07 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 03:41:07 | INFO | train | epoch 032 | loss 5.957 | nll_loss 4.604 | ppl 24.33 | wps 19400 | ups 2.87 | wpb 6748.6 | bsz 247.2 | num_updates 5760 | lr 0.000166667 | gnorm 1.091 | clip 0 | train_wall 56 | wall 1988
2020-10-12 03:41:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 03:41:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 03:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:41:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000706
2020-10-12 03:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008384
2020-10-12 03:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000225
2020-10-12 03:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099984
2020-10-12 03:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109000
2020-10-12 03:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004321
2020-10-12 03:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 03:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098174
2020-10-12 03:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102994
2020-10-12 03:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:41:07 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 03:41:20 | INFO | train_inner | epoch 033:     40 / 180 loss=5.913, nll_loss=4.554, ppl=23.49, wps=18654.3, ups=2.69, wpb=6942.1, bsz=264.6, num_updates=5800, lr=0.000166091, gnorm=1.054, clip=0, train_wall=32, wall=2001
2020-10-12 03:41:52 | INFO | train_inner | epoch 033:    140 / 180 loss=5.902, nll_loss=4.539, ppl=23.24, wps=21203.1, ups=3.15, wpb=6741.5, bsz=236.6, num_updates=5900, lr=0.000164677, gnorm=1.075, clip=0, train_wall=31, wall=2033
2020-10-12 03:42:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000768
2020-10-12 03:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041637
2020-10-12 03:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031072
2020-10-12 03:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073813
2020-10-12 03:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000675
2020-10-12 03:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041369
2020-10-12 03:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030961
2020-10-12 03:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073334
2020-10-12 03:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:42:07 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.602 | nll_loss 5.234 | ppl 37.63 | wps 45545.2 | wpb 2233.7 | bsz 85 | num_updates 5940 | best_loss 6.602
2020-10-12 03:42:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:42:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 33 @ 5940 updates, score 6.602) (writing took 1.5261896000010893 seconds)
2020-10-12 03:42:09 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 03:42:09 | INFO | train | epoch 033 | loss 5.888 | nll_loss 4.523 | ppl 23 | wps 19558.3 | ups 2.9 | wpb 6748.6 | bsz 247.2 | num_updates 5940 | lr 0.000164122 | gnorm 1.084 | clip 0 | train_wall 56 | wall 2050
2020-10-12 03:42:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 03:42:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 03:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:42:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000633
2020-10-12 03:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006586
2020-10-12 03:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 03:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101015
2020-10-12 03:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108204
2020-10-12 03:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004282
2020-10-12 03:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 03:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097177
2020-10-12 03:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101952
2020-10-12 03:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:42:09 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 03:42:28 | INFO | train_inner | epoch 034:     60 / 180 loss=5.824, nll_loss=4.45, ppl=21.86, wps=18392.2, ups=2.73, wpb=6739.6, bsz=254.9, num_updates=6000, lr=0.000163299, gnorm=1.105, clip=0, train_wall=31, wall=2070
2020-10-12 03:43:00 | INFO | train_inner | epoch 034:    160 / 180 loss=5.83, nll_loss=4.456, ppl=21.94, wps=20732.2, ups=3.13, wpb=6622.9, bsz=241.8, num_updates=6100, lr=0.000161955, gnorm=1.08, clip=0, train_wall=31, wall=2102
2020-10-12 03:43:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000777
2020-10-12 03:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041249
2020-10-12 03:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030512
2020-10-12 03:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072867
2020-10-12 03:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000656
2020-10-12 03:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040104
2020-10-12 03:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030582
2020-10-12 03:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071659
2020-10-12 03:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:43:09 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.603 | nll_loss 5.226 | ppl 37.44 | wps 45577 | wpb 2233.7 | bsz 85 | num_updates 6120 | best_loss 6.602
2020-10-12 03:43:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:43:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_last.pt (epoch 34 @ 6120 updates, score 6.603) (writing took 1.063790804008022 seconds)
2020-10-12 03:43:10 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 03:43:10 | INFO | train | epoch 034 | loss 5.82 | nll_loss 4.445 | ppl 21.77 | wps 19674 | ups 2.92 | wpb 6748.6 | bsz 247.2 | num_updates 6120 | lr 0.00016169 | gnorm 1.088 | clip 0 | train_wall 57 | wall 2112
2020-10-12 03:43:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 03:43:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 03:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:43:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000725
2020-10-12 03:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007050
2020-10-12 03:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 03:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:43:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096069
2020-10-12 03:43:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103659
2020-10-12 03:43:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:43:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:43:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004290
2020-10-12 03:43:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:43:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 03:43:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:43:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095246
2020-10-12 03:43:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100032
2020-10-12 03:43:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:43:11 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 03:43:37 | INFO | train_inner | epoch 035:     80 / 180 loss=5.802, nll_loss=4.422, ppl=21.44, wps=19085.1, ups=2.75, wpb=6943.1, bsz=247.4, num_updates=6200, lr=0.000160644, gnorm=1.067, clip=0, train_wall=32, wall=2138
2020-10-12 03:44:08 | INFO | train_inner | epoch 035:    180 / 180 loss=5.752, nll_loss=4.365, ppl=20.61, wps=20715, ups=3.16, wpb=6562.3, bsz=241.2, num_updates=6300, lr=0.000159364, gnorm=1.146, clip=0, train_wall=31, wall=2170
2020-10-12 03:44:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000789
2020-10-12 03:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041241
2020-10-12 03:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030717
2020-10-12 03:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073087
2020-10-12 03:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000671
2020-10-12 03:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041542
2020-10-12 03:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030476
2020-10-12 03:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073014
2020-10-12 03:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:44:11 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.602 | nll_loss 5.22 | ppl 37.28 | wps 45541.9 | wpb 2233.7 | bsz 85 | num_updates 6300 | best_loss 6.602
2020-10-12 03:44:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:44:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 35 @ 6300 updates, score 6.602) (writing took 1.5069147699978203 seconds)
2020-10-12 03:44:13 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 03:44:13 | INFO | train | epoch 035 | loss 5.762 | nll_loss 4.376 | ppl 20.77 | wps 19536.2 | ups 2.89 | wpb 6748.6 | bsz 247.2 | num_updates 6300 | lr 0.000159364 | gnorm 1.104 | clip 0 | train_wall 56 | wall 2174
2020-10-12 03:44:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 03:44:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 03:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:44:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000518
2020-10-12 03:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006001
2020-10-12 03:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 03:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100573
2020-10-12 03:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107168
2020-10-12 03:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004307
2020-10-12 03:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 03:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094283
2020-10-12 03:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099091
2020-10-12 03:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:44:13 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 03:44:45 | INFO | train_inner | epoch 036:    100 / 180 loss=5.699, nll_loss=4.302, ppl=19.73, wps=18769.4, ups=2.72, wpb=6900, bsz=254.6, num_updates=6400, lr=0.000158114, gnorm=1.107, clip=0, train_wall=32, wall=2206
2020-10-12 03:45:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000787
2020-10-12 03:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040530
2020-10-12 03:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030508
2020-10-12 03:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072160
2020-10-12 03:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000662
2020-10-12 03:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040509
2020-10-12 03:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:45:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030651
2020-10-12 03:45:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072148
2020-10-12 03:45:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:45:13 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.59 | nll_loss 5.221 | ppl 37.29 | wps 45255.9 | wpb 2233.7 | bsz 85 | num_updates 6480 | best_loss 6.59
2020-10-12 03:45:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:45:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 36 @ 6480 updates, score 6.59) (writing took 1.8583067619911162 seconds)
2020-10-12 03:45:15 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 03:45:15 | INFO | train | epoch 036 | loss 5.702 | nll_loss 4.305 | ppl 19.77 | wps 19451.1 | ups 2.88 | wpb 6748.6 | bsz 247.2 | num_updates 6480 | lr 0.000157135 | gnorm 1.107 | clip 0 | train_wall 56 | wall 2236
2020-10-12 03:45:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 03:45:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 03:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:45:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000711
2020-10-12 03:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007996
2020-10-12 03:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000220
2020-10-12 03:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098531
2020-10-12 03:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107155
2020-10-12 03:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004363
2020-10-12 03:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 03:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097329
2020-10-12 03:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102199
2020-10-12 03:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:45:15 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 03:45:22 | INFO | train_inner | epoch 037:     20 / 180 loss=5.678, nll_loss=4.278, ppl=19.4, wps=18047.9, ups=2.73, wpb=6620.1, bsz=241.8, num_updates=6500, lr=0.000156893, gnorm=1.112, clip=0, train_wall=31, wall=2243
2020-10-12 03:45:54 | INFO | train_inner | epoch 037:    120 / 180 loss=5.648, nll_loss=4.242, ppl=18.93, wps=21144.2, ups=3.13, wpb=6756.5, bsz=245, num_updates=6600, lr=0.0001557, gnorm=1.095, clip=0, train_wall=31, wall=2275
2020-10-12 03:46:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000788
2020-10-12 03:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040740
2020-10-12 03:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031083
2020-10-12 03:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072948
2020-10-12 03:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000673
2020-10-12 03:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039971
2020-10-12 03:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030871
2020-10-12 03:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071839
2020-10-12 03:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:46:16 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.57 | nll_loss 5.183 | ppl 36.32 | wps 45777.6 | wpb 2233.7 | bsz 85 | num_updates 6660 | best_loss 6.57
2020-10-12 03:46:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:46:17 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 37 @ 6660 updates, score 6.57) (writing took 1.898276685999008 seconds)
2020-10-12 03:46:17 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 03:46:17 | INFO | train | epoch 037 | loss 5.643 | nll_loss 4.237 | ppl 18.85 | wps 19470.5 | ups 2.89 | wpb 6748.6 | bsz 247.2 | num_updates 6660 | lr 0.000154997 | gnorm 1.092 | clip 0 | train_wall 56 | wall 2299
2020-10-12 03:46:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 03:46:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 03:46:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:46:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000769
2020-10-12 03:46:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008462
2020-10-12 03:46:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:46:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000235
2020-10-12 03:46:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098850
2020-10-12 03:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107975
2020-10-12 03:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006784
2020-10-12 03:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 03:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098837
2020-10-12 03:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106137
2020-10-12 03:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:46:18 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 03:46:30 | INFO | train_inner | epoch 038:     40 / 180 loss=5.608, nll_loss=4.196, ppl=18.32, wps=18244.4, ups=2.72, wpb=6701.5, bsz=253.8, num_updates=6700, lr=0.000154533, gnorm=1.09, clip=0, train_wall=31, wall=2312
2020-10-12 03:47:02 | INFO | train_inner | epoch 038:    140 / 180 loss=5.587, nll_loss=4.169, ppl=17.99, wps=21232.3, ups=3.12, wpb=6810.2, bsz=246.2, num_updates=6800, lr=0.000153393, gnorm=1.101, clip=0, train_wall=32, wall=2344
2020-10-12 03:47:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:47:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:47:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000779
2020-10-12 03:47:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:47:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041317
2020-10-12 03:47:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:47:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030757
2020-10-12 03:47:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073190
2020-10-12 03:47:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:47:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:47:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000655
2020-10-12 03:47:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:47:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040807
2020-10-12 03:47:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:47:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030745
2020-10-12 03:47:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072523
2020-10-12 03:47:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:47:18 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.569 | nll_loss 5.182 | ppl 36.31 | wps 45735.9 | wpb 2233.7 | bsz 85 | num_updates 6840 | best_loss 6.569
2020-10-12 03:47:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:47:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 38 @ 6840 updates, score 6.569) (writing took 1.4968444140104111 seconds)
2020-10-12 03:47:19 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 03:47:19 | INFO | train | epoch 038 | loss 5.586 | nll_loss 4.169 | ppl 17.98 | wps 19590.3 | ups 2.9 | wpb 6748.6 | bsz 247.2 | num_updates 6840 | lr 0.000152944 | gnorm 1.109 | clip 0 | train_wall 56 | wall 2361
2020-10-12 03:47:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 03:47:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 03:47:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:47:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000978
2020-10-12 03:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007850
2020-10-12 03:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 03:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102826
2020-10-12 03:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111298
2020-10-12 03:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004229
2020-10-12 03:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 03:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097578
2020-10-12 03:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102306
2020-10-12 03:47:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:47:20 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 03:47:39 | INFO | train_inner | epoch 039:     60 / 180 loss=5.604, nll_loss=4.188, ppl=18.23, wps=18592.8, ups=2.73, wpb=6801.2, bsz=240.4, num_updates=6900, lr=0.000152277, gnorm=1.138, clip=0, train_wall=31, wall=2380
2020-10-12 03:48:11 | INFO | train_inner | epoch 039:    160 / 180 loss=5.501, nll_loss=4.07, ppl=16.8, wps=20898.4, ups=3.13, wpb=6670.1, bsz=251, num_updates=7000, lr=0.000151186, gnorm=1.109, clip=0, train_wall=31, wall=2412
2020-10-12 03:48:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000773
2020-10-12 03:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041217
2020-10-12 03:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031244
2020-10-12 03:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073570
2020-10-12 03:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000688
2020-10-12 03:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041217
2020-10-12 03:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031414
2020-10-12 03:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073653
2020-10-12 03:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:48:20 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.596 | nll_loss 5.213 | ppl 37.1 | wps 45515.3 | wpb 2233.7 | bsz 85 | num_updates 7020 | best_loss 6.569
2020-10-12 03:48:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:48:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_last.pt (epoch 39 @ 7020 updates, score 6.596) (writing took 1.048135007993551 seconds)
2020-10-12 03:48:21 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 03:48:21 | INFO | train | epoch 039 | loss 5.532 | nll_loss 4.105 | ppl 17.21 | wps 19691.1 | ups 2.92 | wpb 6748.6 | bsz 247.2 | num_updates 7020 | lr 0.00015097 | gnorm 1.124 | clip 0 | train_wall 56 | wall 2423
2020-10-12 03:48:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 03:48:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 03:48:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:48:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000604
2020-10-12 03:48:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006483
2020-10-12 03:48:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:48:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000198
2020-10-12 03:48:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:48:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097688
2020-10-12 03:48:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104731
2020-10-12 03:48:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:48:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:48:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004285
2020-10-12 03:48:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:48:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 03:48:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:48:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095424
2020-10-12 03:48:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100222
2020-10-12 03:48:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:48:21 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 03:48:47 | INFO | train_inner | epoch 040:     80 / 180 loss=5.446, nll_loss=4.007, ppl=16.08, wps=18745.2, ups=2.76, wpb=6787.1, bsz=253.8, num_updates=7100, lr=0.000150117, gnorm=1.127, clip=0, train_wall=31, wall=2449
2020-10-12 03:49:19 | INFO | train_inner | epoch 040:    180 / 180 loss=5.526, nll_loss=4.096, ppl=17.1, wps=20953.8, ups=3.13, wpb=6690.8, bsz=238.1, num_updates=7200, lr=0.000149071, gnorm=1.141, clip=0, train_wall=31, wall=2481
2020-10-12 03:49:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000764
2020-10-12 03:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041223
2020-10-12 03:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031041
2020-10-12 03:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073369
2020-10-12 03:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000679
2020-10-12 03:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040633
2020-10-12 03:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030470
2020-10-12 03:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072109
2020-10-12 03:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:49:22 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.567 | nll_loss 5.173 | ppl 36.09 | wps 45621.6 | wpb 2233.7 | bsz 85 | num_updates 7200 | best_loss 6.567
2020-10-12 03:49:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:49:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/M2O/checkpoint_best.pt (epoch 40 @ 7200 updates, score 6.567) (writing took 1.8565010489983251 seconds)
2020-10-12 03:49:24 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 03:49:24 | INFO | train | epoch 040 | loss 5.481 | nll_loss 4.045 | ppl 16.51 | wps 19390.3 | ups 2.87 | wpb 6748.6 | bsz 247.2 | num_updates 7200 | lr 0.000149071 | gnorm 1.135 | clip 0 | train_wall 57 | wall 2485
2020-10-12 03:49:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 03:49:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 03:49:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:49:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000743
2020-10-12 03:49:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008020
2020-10-12 03:49:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:49:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000222
2020-10-12 03:49:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:49:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097669
2020-10-12 03:49:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106433
2020-10-12 03:49:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:49:24 | INFO | fairseq_cli.train | done training in 2485.2 seconds
