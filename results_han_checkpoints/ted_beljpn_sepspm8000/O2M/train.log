2020-10-12 03:49:55 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_beljpn_sepspm8000/O2M/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok='tgt', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='eng-bel,eng-jpn', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 03:49:55 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 03:49:55 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['bel', 'eng', 'jpn']
2020-10-12 03:49:55 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bel] dictionary: 22891 types
2020-10-12 03:49:55 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 22891 types
2020-10-12 03:49:55 | INFO | fairseq.data.multilingual.multilingual_data_manager | [jpn] dictionary: 22891 types
2020-10-12 03:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 03:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:49:55 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 03:49:55 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:eng-bel': 1, 'main:eng-jpn': 1}
2020-10-12 03:49:55 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-bel src_langtok: 22888; tgt_langtok: None
2020-10-12 03:49:55 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_beljpn_sepspm8000/O2M/valid.eng-bel.eng
2020-10-12 03:49:55 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_beljpn_sepspm8000/O2M/valid.eng-bel.bel
2020-10-12 03:49:55 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_beljpn_sepspm8000/O2M/ valid eng-bel 248 examples
2020-10-12 03:49:55 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-jpn src_langtok: 22890; tgt_langtok: None
2020-10-12 03:49:55 | INFO | fairseq.data.data_utils | loaded 4429 examples from: fairseq/data-bin/ted_beljpn_sepspm8000/O2M/valid.eng-jpn.eng
2020-10-12 03:49:55 | INFO | fairseq.data.data_utils | loaded 4429 examples from: fairseq/data-bin/ted_beljpn_sepspm8000/O2M/valid.eng-jpn.jpn
2020-10-12 03:49:55 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_beljpn_sepspm8000/O2M/ valid eng-jpn 4429 examples
2020-10-12 03:49:55 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22891, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(22891, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=22891, bias=False)
  )
)
2020-10-12 03:49:55 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 03:49:55 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 03:49:55 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 03:49:55 | INFO | fairseq_cli.train | num. model params: 43263488 (num. trained: 43263488)
2020-10-12 03:49:57 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 03:49:57 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 03:49:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 03:49:57 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.732 GB ; name = GeForce RTX 2080 Ti                     
2020-10-12 03:49:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 03:49:57 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 03:49:57 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 03:49:57 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_last.pt
2020-10-12 03:49:57 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 03:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 03:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:49:57 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 03:49:57 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:eng-bel': 1, 'main:eng-jpn': 1}
2020-10-12 03:49:57 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-bel src_langtok: 22888; tgt_langtok: None
2020-10-12 03:49:57 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_beljpn_sepspm8000/O2M/train.eng-bel.eng
2020-10-12 03:49:57 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_beljpn_sepspm8000/O2M/train.eng-bel.bel
2020-10-12 03:49:57 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_beljpn_sepspm8000/O2M/ train eng-bel 4509 examples
2020-10-12 03:49:57 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-jpn src_langtok: 22890; tgt_langtok: None
2020-10-12 03:49:57 | INFO | fairseq.data.data_utils | loaded 39987 examples from: fairseq/data-bin/ted_beljpn_sepspm8000/O2M/train.eng-jpn.eng
2020-10-12 03:49:57 | INFO | fairseq.data.data_utils | loaded 39987 examples from: fairseq/data-bin/ted_beljpn_sepspm8000/O2M/train.eng-jpn.jpn
2020-10-12 03:49:57 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_beljpn_sepspm8000/O2M/ train eng-jpn 39987 examples
2020-10-12 03:49:57 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:eng-bel', 4509), ('main:eng-jpn', 39987)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 03:49:57 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 03:49:57 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 44496
2020-10-12 03:49:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 44496; virtual dataset size 44496
2020-10-12 03:49:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 03:49:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:eng-bel': 4509, 'main:eng-jpn': 39987}; raw total size: 44496
2020-10-12 03:49:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:eng-bel': 4509, 'main:eng-jpn': 39987}; resampled total size: 44496
2020-10-12 03:49:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 03:49:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.003024
2020-10-12 03:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:49:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000413
2020-10-12 03:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005214
2020-10-12 03:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 03:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096405
2020-10-12 03:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102145
2020-10-12 03:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:49:57 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 03:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004258
2020-10-12 03:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 03:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094906
2020-10-12 03:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099685
2020-10-12 03:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:49:58 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 03:50:30 | INFO | train_inner | epoch 001:    100 / 171 loss=14.694, nll_loss=14.636, ppl=25453.3, wps=18211.3, ups=3.15, wpb=5764.9, bsz=271.5, num_updates=100, lr=5.0975e-06, gnorm=2.752, clip=0, train_wall=32, wall=32
2020-10-12 03:50:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000734
2020-10-12 03:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041027
2020-10-12 03:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030486
2020-10-12 03:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072583
2020-10-12 03:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000662
2020-10-12 03:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039980
2020-10-12 03:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030343
2020-10-12 03:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071316
2020-10-12 03:50:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
/home/han/Documents/11737-hw/assign2/fairseq/fairseq/utils.py:340: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2020-10-12 03:50:55 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.543 | nll_loss 13.346 | ppl 10413.3 | wps 39914.1 | wpb 2120.5 | bsz 101.7 | num_updates 171
2020-10-12 03:50:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:50:56 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 1 @ 171 updates, score 13.543) (writing took 0.9636037180025596 seconds)
2020-10-12 03:50:56 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 03:50:56 | INFO | train | epoch 001 | loss 14.386 | nll_loss 14.293 | ppl 20067.6 | wps 16682.5 | ups 2.95 | wpb 5644.5 | bsz 260.2 | num_updates 171 | lr 8.64573e-06 | gnorm 2.18 | clip 0 | train_wall 54 | wall 58
2020-10-12 03:50:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 03:50:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 03:50:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:50:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000553
2020-10-12 03:50:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005621
2020-10-12 03:50:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:50:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-12 03:50:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:50:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097880
2020-10-12 03:50:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104010
2020-10-12 03:50:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:50:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:50:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004272
2020-10-12 03:50:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:50:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-12 03:50:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:50:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097868
2020-10-12 03:50:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102653
2020-10-12 03:50:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:50:56 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 03:51:05 | INFO | train_inner | epoch 002:     29 / 171 loss=13.833, nll_loss=13.677, ppl=13096, wps=15362.4, ups=2.79, wpb=5499.2, bsz=255, num_updates=200, lr=1.0095e-05, gnorm=1.328, clip=0, train_wall=31, wall=68
2020-10-12 03:51:38 | INFO | train_inner | epoch 002:    129 / 171 loss=13.288, nll_loss=13.075, ppl=8626.63, wps=17194.7, ups=3.06, wpb=5627.2, bsz=250.9, num_updates=300, lr=1.50925e-05, gnorm=0.997, clip=0, train_wall=32, wall=101
2020-10-12 03:51:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000772
2020-10-12 03:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040178
2020-10-12 03:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030365
2020-10-12 03:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071648
2020-10-12 03:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000679
2020-10-12 03:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039200
2020-10-12 03:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030517
2020-10-12 03:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070723
2020-10-12 03:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:51:54 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 12.524 | nll_loss 12.213 | ppl 4748.14 | wps 39290.2 | wpb 2120.5 | bsz 101.7 | num_updates 342 | best_loss 12.524
2020-10-12 03:51:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:51:56 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 2 @ 342 updates, score 12.524) (writing took 1.5069702839973615 seconds)
2020-10-12 03:51:56 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 03:51:56 | INFO | train | epoch 002 | loss 13.229 | nll_loss 13.008 | ppl 8239.23 | wps 16061.4 | ups 2.85 | wpb 5644.5 | bsz 260.2 | num_updates 342 | lr 1.71914e-05 | gnorm 1.015 | clip 0 | train_wall 55 | wall 119
2020-10-12 03:51:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 03:51:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 03:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:51:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000537
2020-10-12 03:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006143
2020-10-12 03:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 03:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104592
2020-10-12 03:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111268
2020-10-12 03:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004260
2020-10-12 03:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 03:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100350
2020-10-12 03:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105114
2020-10-12 03:51:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:51:56 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 03:52:15 | INFO | train_inner | epoch 003:     58 / 171 loss=12.673, nll_loss=12.382, ppl=5336.75, wps=15554.5, ups=2.7, wpb=5768.5, bsz=263.8, num_updates=400, lr=2.009e-05, gnorm=1.138, clip=0, train_wall=32, wall=138
2020-10-12 03:52:48 | INFO | train_inner | epoch 003:    158 / 171 loss=12.183, nll_loss=11.808, ppl=3584.54, wps=16854.9, ups=3.06, wpb=5501.1, bsz=251.3, num_updates=500, lr=2.50875e-05, gnorm=1.205, clip=0, train_wall=32, wall=171
2020-10-12 03:52:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:52:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:52:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000759
2020-10-12 03:52:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:52:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040949
2020-10-12 03:52:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:52:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029628
2020-10-12 03:52:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071671
2020-10-12 03:52:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:52:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:52:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000674
2020-10-12 03:52:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:52:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039713
2020-10-12 03:52:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:52:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029498
2020-10-12 03:52:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070213
2020-10-12 03:52:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:52:55 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 11.841 | nll_loss 11.382 | ppl 2668.68 | wps 39275.9 | wpb 2120.5 | bsz 101.7 | num_updates 513 | best_loss 11.841
2020-10-12 03:52:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:52:56 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 3 @ 513 updates, score 11.841) (writing took 1.5100977990077808 seconds)
2020-10-12 03:52:56 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 03:52:56 | INFO | train | epoch 003 | loss 12.295 | nll_loss 11.94 | ppl 3928.12 | wps 15974.4 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 513 | lr 2.57372e-05 | gnorm 1.236 | clip 0 | train_wall 55 | wall 179
2020-10-12 03:52:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 03:52:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 03:52:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:52:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000492
2020-10-12 03:52:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005980
2020-10-12 03:52:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:52:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-12 03:52:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:52:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102790
2020-10-12 03:52:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109357
2020-10-12 03:52:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:52:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:52:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004253
2020-10-12 03:52:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:52:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 03:52:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097549
2020-10-12 03:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102295
2020-10-12 03:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:52:57 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 03:53:25 | INFO | train_inner | epoch 004:     87 / 171 loss=11.904, nll_loss=11.461, ppl=2818.95, wps=15342, ups=2.69, wpb=5713.9, bsz=269.3, num_updates=600, lr=3.0085e-05, gnorm=1.362, clip=0, train_wall=32, wall=208
2020-10-12 03:53:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000779
2020-10-12 03:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039839
2020-10-12 03:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030650
2020-10-12 03:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071602
2020-10-12 03:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000693
2020-10-12 03:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041000
2020-10-12 03:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030271
2020-10-12 03:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072297
2020-10-12 03:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:53:55 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 11.589 | nll_loss 11.053 | ppl 2124.36 | wps 39234.7 | wpb 2120.5 | bsz 101.7 | num_updates 684 | best_loss 11.589
2020-10-12 03:53:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:53:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 4 @ 684 updates, score 11.589) (writing took 1.5181716780061834 seconds)
2020-10-12 03:53:57 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 03:53:57 | INFO | train | epoch 004 | loss 11.841 | nll_loss 11.377 | ppl 2659.23 | wps 15951.6 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 684 | lr 3.42829e-05 | gnorm 1.349 | clip 0 | train_wall 55 | wall 239
2020-10-12 03:53:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 03:53:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 03:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:53:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000734
2020-10-12 03:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007539
2020-10-12 03:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000206
2020-10-12 03:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102800
2020-10-12 03:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111012
2020-10-12 03:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004350
2020-10-12 03:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-12 03:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098211
2020-10-12 03:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103074
2020-10-12 03:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:53:57 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 03:54:02 | INFO | train_inner | epoch 005:     16 / 171 loss=11.786, nll_loss=11.302, ppl=2525.04, wps=15190.6, ups=2.68, wpb=5667.5, bsz=255.8, num_updates=700, lr=3.50825e-05, gnorm=1.243, clip=0, train_wall=32, wall=245
2020-10-12 03:54:35 | INFO | train_inner | epoch 005:    116 / 171 loss=11.671, nll_loss=11.157, ppl=2282.77, wps=17523.9, ups=3.05, wpb=5738.7, bsz=265.4, num_updates=800, lr=4.008e-05, gnorm=1.356, clip=0, train_wall=32, wall=278
2020-10-12 03:54:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000690
2020-10-12 03:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039942
2020-10-12 03:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030624
2020-10-12 03:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071584
2020-10-12 03:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000630
2020-10-12 03:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039789
2020-10-12 03:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029819
2020-10-12 03:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070557
2020-10-12 03:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:54:56 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 11.517 | nll_loss 10.967 | ppl 2001.76 | wps 39777.2 | wpb 2120.5 | bsz 101.7 | num_updates 855 | best_loss 11.517
2020-10-12 03:54:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:54:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 5 @ 855 updates, score 11.517) (writing took 1.5105158550140914 seconds)
2020-10-12 03:54:57 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 03:54:57 | INFO | train | epoch 005 | loss 11.652 | nll_loss 11.134 | ppl 2247.57 | wps 15979.9 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 855 | lr 4.28286e-05 | gnorm 1.319 | clip 0 | train_wall 55 | wall 300
2020-10-12 03:54:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 03:54:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 03:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:54:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000588
2020-10-12 03:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006115
2020-10-12 03:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 03:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098695
2020-10-12 03:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105407
2020-10-12 03:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004314
2020-10-12 03:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 03:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093861
2020-10-12 03:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098669
2020-10-12 03:54:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:54:57 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 03:55:12 | INFO | train_inner | epoch 006:     45 / 171 loss=11.576, nll_loss=11.041, ppl=2106.35, wps=14948.4, ups=2.7, wpb=5538.8, bsz=259.2, num_updates=900, lr=4.50775e-05, gnorm=1.293, clip=0, train_wall=32, wall=315
2020-10-12 03:55:45 | INFO | train_inner | epoch 006:    145 / 171 loss=11.508, nll_loss=10.959, ppl=1990.39, wps=17155.6, ups=3.03, wpb=5662.3, bsz=253.6, num_updates=1000, lr=5.0075e-05, gnorm=1.184, clip=0, train_wall=33, wall=348
2020-10-12 03:55:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:55:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:55:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000833
2020-10-12 03:55:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040381
2020-10-12 03:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030469
2020-10-12 03:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072012
2020-10-12 03:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000638
2020-10-12 03:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040456
2020-10-12 03:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030714
2020-10-12 03:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072133
2020-10-12 03:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:55:56 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.29 | nll_loss 10.689 | ppl 1651.41 | wps 39269.5 | wpb 2120.5 | bsz 101.7 | num_updates 1026 | best_loss 11.29
2020-10-12 03:55:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:55:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 6 @ 1026 updates, score 11.29) (writing took 1.5061445379978977 seconds)
2020-10-12 03:55:58 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 03:55:58 | INFO | train | epoch 006 | loss 11.506 | nll_loss 10.956 | ppl 1987.01 | wps 15971.2 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 1026 | lr 5.13744e-05 | gnorm 1.188 | clip 0 | train_wall 55 | wall 360
2020-10-12 03:55:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 03:55:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 03:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:55:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000849
2020-10-12 03:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008374
2020-10-12 03:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000216
2020-10-12 03:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103704
2020-10-12 03:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112799
2020-10-12 03:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004262
2020-10-12 03:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 03:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096617
2020-10-12 03:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101372
2020-10-12 03:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:55:58 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 03:56:22 | INFO | train_inner | epoch 007:     74 / 171 loss=11.363, nll_loss=10.791, ppl=1771.76, wps=15186.8, ups=2.71, wpb=5601.2, bsz=284.7, num_updates=1100, lr=5.50725e-05, gnorm=1.201, clip=0, train_wall=32, wall=385
2020-10-12 03:56:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:56:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:56:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000785
2020-10-12 03:56:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:56:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040343
2020-10-12 03:56:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:56:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029643
2020-10-12 03:56:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071101
2020-10-12 03:56:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:56:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:56:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000650
2020-10-12 03:56:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:56:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039971
2020-10-12 03:56:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:56:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030347
2020-10-12 03:56:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071294
2020-10-12 03:56:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:56:57 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 11.125 | nll_loss 10.5 | ppl 1448.14 | wps 39715.2 | wpb 2120.5 | bsz 101.7 | num_updates 1197 | best_loss 11.125
2020-10-12 03:56:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:56:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 7 @ 1197 updates, score 11.125) (writing took 1.6001813629991375 seconds)
2020-10-12 03:56:58 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 03:56:58 | INFO | train | epoch 007 | loss 11.367 | nll_loss 10.794 | ppl 1776.06 | wps 15940.8 | ups 2.82 | wpb 5644.5 | bsz 260.2 | num_updates 1197 | lr 5.99201e-05 | gnorm 1.128 | clip 0 | train_wall 55 | wall 421
2020-10-12 03:56:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 03:56:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 03:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:56:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000676
2020-10-12 03:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007661
2020-10-12 03:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000222
2020-10-12 03:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106594
2020-10-12 03:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114994
2020-10-12 03:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004265
2020-10-12 03:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 03:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096844
2020-10-12 03:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101635
2020-10-12 03:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:56:58 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 03:56:59 | INFO | train_inner | epoch 008:      3 / 171 loss=11.378, nll_loss=10.805, ppl=1789.46, wps=15133.3, ups=2.68, wpb=5653.2, bsz=239.8, num_updates=1200, lr=6.007e-05, gnorm=1.073, clip=0, train_wall=32, wall=422
2020-10-12 03:57:32 | INFO | train_inner | epoch 008:    103 / 171 loss=11.263, nll_loss=10.673, ppl=1632.14, wps=17245.1, ups=3.06, wpb=5639.3, bsz=257.3, num_updates=1300, lr=6.50675e-05, gnorm=1.259, clip=0, train_wall=32, wall=455
2020-10-12 03:57:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000780
2020-10-12 03:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039595
2020-10-12 03:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030080
2020-10-12 03:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070783
2020-10-12 03:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000645
2020-10-12 03:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039657
2020-10-12 03:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030057
2020-10-12 03:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070679
2020-10-12 03:57:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:57:57 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.981 | nll_loss 10.335 | ppl 1291.27 | wps 39431.5 | wpb 2120.5 | bsz 101.7 | num_updates 1368 | best_loss 10.981
2020-10-12 03:57:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:57:59 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 8 @ 1368 updates, score 10.981) (writing took 1.5202195899910294 seconds)
2020-10-12 03:57:59 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 03:57:59 | INFO | train | epoch 008 | loss 11.227 | nll_loss 10.632 | ppl 1587.42 | wps 15955.1 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 1368 | lr 6.84658e-05 | gnorm 1.19 | clip 0 | train_wall 55 | wall 481
2020-10-12 03:57:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 03:57:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 03:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:57:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000584
2020-10-12 03:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006123
2020-10-12 03:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000272
2020-10-12 03:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100933
2020-10-12 03:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107677
2020-10-12 03:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004277
2020-10-12 03:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 03:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096812
2020-10-12 03:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101589
2020-10-12 03:57:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:57:59 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 03:58:10 | INFO | train_inner | epoch 009:     32 / 171 loss=11.163, nll_loss=10.559, ppl=1508.61, wps=15239.3, ups=2.68, wpb=5694.9, bsz=259.8, num_updates=1400, lr=7.0065e-05, gnorm=1.079, clip=0, train_wall=32, wall=492
2020-10-12 03:58:42 | INFO | train_inner | epoch 009:    132 / 171 loss=11.047, nll_loss=10.426, ppl=1375.96, wps=17283.7, ups=3.04, wpb=5692.6, bsz=272.9, num_updates=1500, lr=7.50625e-05, gnorm=1.201, clip=0, train_wall=33, wall=525
2020-10-12 03:58:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000768
2020-10-12 03:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039572
2020-10-12 03:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030684
2020-10-12 03:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071355
2020-10-12 03:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000681
2020-10-12 03:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040252
2020-10-12 03:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030872
2020-10-12 03:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072122
2020-10-12 03:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:58:58 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 10.782 | nll_loss 10.108 | ppl 1103.81 | wps 39471.1 | wpb 2120.5 | bsz 101.7 | num_updates 1539 | best_loss 10.782
2020-10-12 03:58:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 03:58:59 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 9 @ 1539 updates, score 10.782) (writing took 1.5307352789968718 seconds)
2020-10-12 03:58:59 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 03:58:59 | INFO | train | epoch 009 | loss 11.056 | nll_loss 10.437 | ppl 1386.25 | wps 15974.1 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 1539 | lr 7.70115e-05 | gnorm 1.146 | clip 0 | train_wall 55 | wall 542
2020-10-12 03:58:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 03:58:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 03:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:58:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000632
2020-10-12 03:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006484
2020-10-12 03:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 03:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099371
2020-10-12 03:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106377
2020-10-12 03:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004289
2020-10-12 03:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 03:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096433
2020-10-12 03:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101245
2020-10-12 03:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:58:59 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 03:59:19 | INFO | train_inner | epoch 010:     61 / 171 loss=10.925, nll_loss=10.287, ppl=1249.56, wps=14766.7, ups=2.71, wpb=5453.9, bsz=249.8, num_updates=1600, lr=8.006e-05, gnorm=1.158, clip=0, train_wall=32, wall=562
2020-10-12 03:59:52 | INFO | train_inner | epoch 010:    161 / 171 loss=10.855, nll_loss=10.205, ppl=1180.73, wps=17562.3, ups=3.04, wpb=5779, bsz=269, num_updates=1700, lr=8.50575e-05, gnorm=1.192, clip=0, train_wall=32, wall=595
2020-10-12 03:59:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 03:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000759
2020-10-12 03:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040787
2020-10-12 03:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:59:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030418
2020-10-12 03:59:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072298
2020-10-12 03:59:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:59:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 03:59:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000692
2020-10-12 03:59:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:59:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039865
2020-10-12 03:59:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:59:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030258
2020-10-12 03:59:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071140
2020-10-12 03:59:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 03:59:58 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 10.568 | nll_loss 9.861 | ppl 930.11 | wps 39171.2 | wpb 2120.5 | bsz 101.7 | num_updates 1710 | best_loss 10.568
2020-10-12 03:59:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:00:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 10 @ 1710 updates, score 10.568) (writing took 1.5067549660016084 seconds)
2020-10-12 04:00:00 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 04:00:00 | INFO | train | epoch 010 | loss 10.866 | nll_loss 10.219 | ppl 1191.52 | wps 15949.8 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 1710 | lr 8.55573e-05 | gnorm 1.192 | clip 0 | train_wall 55 | wall 602
2020-10-12 04:00:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 04:00:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 04:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:00:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000565
2020-10-12 04:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006006
2020-10-12 04:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 04:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099582
2020-10-12 04:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106114
2020-10-12 04:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004328
2020-10-12 04:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 04:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096987
2020-10-12 04:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101822
2020-10-12 04:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:00:00 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 04:00:29 | INFO | train_inner | epoch 011:     90 / 171 loss=10.729, nll_loss=10.06, ppl=1067.83, wps=15071, ups=2.69, wpb=5594.2, bsz=254.3, num_updates=1800, lr=9.0055e-05, gnorm=1.257, clip=0, train_wall=32, wall=632
2020-10-12 04:00:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000762
2020-10-12 04:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039719
2020-10-12 04:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030391
2020-10-12 04:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071203
2020-10-12 04:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000684
2020-10-12 04:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039139
2020-10-12 04:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029732
2020-10-12 04:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.069876
2020-10-12 04:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:00:59 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 10.363 | nll_loss 9.614 | ppl 783.57 | wps 39332.9 | wpb 2120.5 | bsz 101.7 | num_updates 1881 | best_loss 10.363
2020-10-12 04:00:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:01:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 11 @ 1881 updates, score 10.363) (writing took 1.511409056998673 seconds)
2020-10-12 04:01:00 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 04:01:00 | INFO | train | epoch 011 | loss 10.675 | nll_loss 9.998 | ppl 1022.65 | wps 15952.9 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 1881 | lr 9.4103e-05 | gnorm 1.233 | clip 0 | train_wall 55 | wall 663
2020-10-12 04:01:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 04:01:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 04:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:01:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000498
2020-10-12 04:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005891
2020-10-12 04:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 04:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102710
2020-10-12 04:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109118
2020-10-12 04:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004260
2020-10-12 04:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 04:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100214
2020-10-12 04:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104979
2020-10-12 04:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:01:00 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 04:01:06 | INFO | train_inner | epoch 012:     19 / 171 loss=10.618, nll_loss=9.931, ppl=976.2, wps=15220.4, ups=2.7, wpb=5641.3, bsz=259.9, num_updates=1900, lr=9.50525e-05, gnorm=1.172, clip=0, train_wall=32, wall=669
2020-10-12 04:01:39 | INFO | train_inner | epoch 012:    119 / 171 loss=10.506, nll_loss=9.803, ppl=893.47, wps=17483.2, ups=3.04, wpb=5757.9, bsz=260.8, num_updates=2000, lr=0.00010005, gnorm=1.126, clip=0, train_wall=33, wall=702
2020-10-12 04:01:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:01:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:01:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000768
2020-10-12 04:01:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:01:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040325
2020-10-12 04:01:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:01:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030059
2020-10-12 04:01:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071482
2020-10-12 04:01:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:01:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:01:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000684
2020-10-12 04:01:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:01:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040319
2020-10-12 04:01:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:01:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029920
2020-10-12 04:01:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071246
2020-10-12 04:01:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:01:59 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 10.202 | nll_loss 9.429 | ppl 689.51 | wps 39525.8 | wpb 2120.5 | bsz 101.7 | num_updates 2052 | best_loss 10.202
2020-10-12 04:01:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:02:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 12 @ 2052 updates, score 10.202) (writing took 2.022936766996281 seconds)
2020-10-12 04:02:01 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 04:02:01 | INFO | train | epoch 012 | loss 10.473 | nll_loss 9.765 | ppl 870.17 | wps 15819.4 | ups 2.8 | wpb 5644.5 | bsz 260.2 | num_updates 2052 | lr 0.000102649 | gnorm 1.169 | clip 0 | train_wall 55 | wall 724
2020-10-12 04:02:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 04:02:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:02:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000497
2020-10-12 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005867
2020-10-12 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-12 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104503
2020-10-12 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110899
2020-10-12 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004251
2020-10-12 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095009
2020-10-12 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099763
2020-10-12 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:02:01 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 04:02:17 | INFO | train_inner | epoch 013:     48 / 171 loss=10.374, nll_loss=9.651, ppl=803.87, wps=14467.4, ups=2.65, wpb=5456, bsz=249.4, num_updates=2100, lr=0.000105048, gnorm=1.202, clip=0, train_wall=32, wall=740
2020-10-12 04:02:50 | INFO | train_inner | epoch 013:    148 / 171 loss=10.273, nll_loss=9.534, ppl=741.43, wps=17468.5, ups=3.04, wpb=5750.4, bsz=270.3, num_updates=2200, lr=0.000110045, gnorm=1.197, clip=0, train_wall=32, wall=773
2020-10-12 04:02:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000704
2020-10-12 04:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039343
2020-10-12 04:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:02:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029462
2020-10-12 04:02:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.069848
2020-10-12 04:02:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:02:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:02:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000628
2020-10-12 04:02:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:02:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040608
2020-10-12 04:02:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:02:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029546
2020-10-12 04:02:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071100
2020-10-12 04:02:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:03:00 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 10.024 | nll_loss 9.232 | ppl 601.45 | wps 39283.4 | wpb 2120.5 | bsz 101.7 | num_updates 2223 | best_loss 10.024
2020-10-12 04:03:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:03:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 13 @ 2223 updates, score 10.024) (writing took 1.5540021860069828 seconds)
2020-10-12 04:03:02 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 04:03:02 | INFO | train | epoch 013 | loss 10.292 | nll_loss 9.556 | ppl 752.61 | wps 15949.9 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 2223 | lr 0.000111194 | gnorm 1.188 | clip 0 | train_wall 55 | wall 784
2020-10-12 04:03:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 04:03:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 04:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:03:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000685
2020-10-12 04:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006093
2020-10-12 04:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000206
2020-10-12 04:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103314
2020-10-12 04:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109978
2020-10-12 04:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004422
2020-10-12 04:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 04:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097205
2020-10-12 04:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102144
2020-10-12 04:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:03:02 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 04:03:27 | INFO | train_inner | epoch 014:     77 / 171 loss=10.141, nll_loss=9.382, ppl=667.07, wps=15247.9, ups=2.7, wpb=5657.1, bsz=274.8, num_updates=2300, lr=0.000115043, gnorm=1.198, clip=0, train_wall=32, wall=810
2020-10-12 04:03:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000787
2020-10-12 04:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039807
2020-10-12 04:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030401
2020-10-12 04:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071324
2020-10-12 04:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000662
2020-10-12 04:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039298
2020-10-12 04:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029843
2020-10-12 04:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070120
2020-10-12 04:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:04:01 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 9.858 | nll_loss 9.038 | ppl 525.49 | wps 39490.2 | wpb 2120.5 | bsz 101.7 | num_updates 2394 | best_loss 9.858
2020-10-12 04:04:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:04:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 14 @ 2394 updates, score 9.858) (writing took 2.0545910810033092 seconds)
2020-10-12 04:04:03 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 04:04:03 | INFO | train | epoch 014 | loss 10.121 | nll_loss 9.358 | ppl 656.26 | wps 15844.8 | ups 2.81 | wpb 5644.5 | bsz 260.2 | num_updates 2394 | lr 0.00011974 | gnorm 1.193 | clip 0 | train_wall 55 | wall 845
2020-10-12 04:04:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 04:04:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 04:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:04:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000441
2020-10-12 04:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005955
2020-10-12 04:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000251
2020-10-12 04:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.107928
2020-10-12 04:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114498
2020-10-12 04:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004429
2020-10-12 04:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 04:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096889
2020-10-12 04:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101819
2020-10-12 04:04:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:04:03 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 04:04:05 | INFO | train_inner | epoch 015:      6 / 171 loss=10.13, nll_loss=9.366, ppl=659.85, wps=14973.6, ups=2.65, wpb=5647.8, bsz=241.9, num_updates=2400, lr=0.00012004, gnorm=1.209, clip=0, train_wall=32, wall=848
2020-10-12 04:04:38 | INFO | train_inner | epoch 015:    106 / 171 loss=9.994, nll_loss=9.211, ppl=592.68, wps=17439.3, ups=3.06, wpb=5694, bsz=259.8, num_updates=2500, lr=0.000125037, gnorm=1.259, clip=0, train_wall=32, wall=880
2020-10-12 04:04:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000794
2020-10-12 04:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040144
2020-10-12 04:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030840
2020-10-12 04:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072113
2020-10-12 04:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000671
2020-10-12 04:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039726
2020-10-12 04:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030303
2020-10-12 04:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071025
2020-10-12 04:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:05:02 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 9.764 | nll_loss 8.919 | ppl 483.88 | wps 38760.4 | wpb 2120.5 | bsz 101.7 | num_updates 2565 | best_loss 9.764
2020-10-12 04:05:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:05:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 15 @ 2565 updates, score 9.764) (writing took 1.5256848959979834 seconds)
2020-10-12 04:05:03 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 04:05:03 | INFO | train | epoch 015 | loss 9.963 | nll_loss 9.176 | ppl 578.36 | wps 15966.4 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 2565 | lr 0.000128286 | gnorm 1.216 | clip 0 | train_wall 55 | wall 906
2020-10-12 04:05:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 04:05:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 04:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:05:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000726
2020-10-12 04:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007004
2020-10-12 04:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-12 04:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100524
2020-10-12 04:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108090
2020-10-12 04:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004286
2020-10-12 04:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 04:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097317
2020-10-12 04:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102095
2020-10-12 04:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:05:03 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 04:05:15 | INFO | train_inner | epoch 016:     35 / 171 loss=9.85, nll_loss=9.046, ppl=528.77, wps=14866.4, ups=2.68, wpb=5539, bsz=269.9, num_updates=2600, lr=0.000130035, gnorm=1.179, clip=0, train_wall=32, wall=917
2020-10-12 04:05:47 | INFO | train_inner | epoch 016:    135 / 171 loss=9.798, nll_loss=8.985, ppl=506.63, wps=17208.3, ups=3.07, wpb=5609.9, bsz=261.2, num_updates=2700, lr=0.000135032, gnorm=1.205, clip=0, train_wall=32, wall=950
2020-10-12 04:05:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:05:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:05:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000779
2020-10-12 04:05:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:05:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040163
2020-10-12 04:05:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:05:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030002
2020-10-12 04:05:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071273
2020-10-12 04:05:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:05:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:05:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000667
2020-10-12 04:05:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:05:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039477
2020-10-12 04:05:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:05:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030366
2020-10-12 04:05:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070825
2020-10-12 04:05:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:06:02 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 9.599 | nll_loss 8.725 | ppl 423.17 | wps 39280 | wpb 2120.5 | bsz 101.7 | num_updates 2736 | best_loss 9.599
2020-10-12 04:06:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:06:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 16 @ 2736 updates, score 9.599) (writing took 1.5090851109998766 seconds)
2020-10-12 04:06:03 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 04:06:03 | INFO | train | epoch 016 | loss 9.797 | nll_loss 8.984 | ppl 506.34 | wps 15970.8 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 2736 | lr 0.000136832 | gnorm 1.211 | clip 0 | train_wall 55 | wall 966
2020-10-12 04:06:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 04:06:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 04:06:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:06:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000597
2020-10-12 04:06:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006125
2020-10-12 04:06:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:06:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 04:06:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102305
2020-10-12 04:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109020
2020-10-12 04:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004267
2020-10-12 04:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 04:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096219
2020-10-12 04:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100973
2020-10-12 04:06:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:06:04 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 04:06:25 | INFO | train_inner | epoch 017:     64 / 171 loss=9.705, nll_loss=8.879, ppl=470.78, wps=15317.8, ups=2.67, wpb=5729.4, bsz=262, num_updates=2800, lr=0.00014003, gnorm=1.23, clip=0, train_wall=32, wall=987
2020-10-12 04:06:58 | INFO | train_inner | epoch 017:    164 / 171 loss=9.642, nll_loss=8.803, ppl=446.63, wps=17313.3, ups=3.05, wpb=5678.9, bsz=255, num_updates=2900, lr=0.000145028, gnorm=1.276, clip=0, train_wall=32, wall=1020
2020-10-12 04:07:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000812
2020-10-12 04:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040986
2020-10-12 04:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030692
2020-10-12 04:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072822
2020-10-12 04:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000672
2020-10-12 04:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040712
2020-10-12 04:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030448
2020-10-12 04:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072147
2020-10-12 04:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:07:02 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 9.456 | nll_loss 8.569 | ppl 379.82 | wps 39409.3 | wpb 2120.5 | bsz 101.7 | num_updates 2907 | best_loss 9.456
2020-10-12 04:07:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:07:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 17 @ 2907 updates, score 9.456) (writing took 1.4006555200030562 seconds)
2020-10-12 04:07:04 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 04:07:04 | INFO | train | epoch 017 | loss 9.639 | nll_loss 8.801 | ppl 446 | wps 16014.6 | ups 2.84 | wpb 5644.5 | bsz 260.2 | num_updates 2907 | lr 0.000145377 | gnorm 1.257 | clip 0 | train_wall 55 | wall 1026
2020-10-12 04:07:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 04:07:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 04:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:07:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000788
2020-10-12 04:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008085
2020-10-12 04:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000210
2020-10-12 04:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097551
2020-10-12 04:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106348
2020-10-12 04:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004279
2020-10-12 04:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-12 04:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097837
2020-10-12 04:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102606
2020-10-12 04:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:07:04 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 04:07:34 | INFO | train_inner | epoch 018:     93 / 171 loss=9.478, nll_loss=8.618, ppl=392.84, wps=15010, ups=2.71, wpb=5534.9, bsz=259.8, num_updates=3000, lr=0.000150025, gnorm=1.226, clip=0, train_wall=32, wall=1057
2020-10-12 04:08:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:08:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:08:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000779
2020-10-12 04:08:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:08:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039630
2020-10-12 04:08:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:08:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030151
2020-10-12 04:08:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070892
2020-10-12 04:08:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:08:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:08:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000679
2020-10-12 04:08:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:08:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039679
2020-10-12 04:08:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:08:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029515
2020-10-12 04:08:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070190
2020-10-12 04:08:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:08:03 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 9.352 | nll_loss 8.429 | ppl 344.69 | wps 39198.7 | wpb 2120.5 | bsz 101.7 | num_updates 3078 | best_loss 9.352
2020-10-12 04:08:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:08:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 18 @ 3078 updates, score 9.352) (writing took 1.9422223449946614 seconds)
2020-10-12 04:08:05 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 04:08:05 | INFO | train | epoch 018 | loss 9.481 | nll_loss 8.619 | ppl 393.09 | wps 15851.4 | ups 2.81 | wpb 5644.5 | bsz 260.2 | num_updates 3078 | lr 0.000153923 | gnorm 1.253 | clip 0 | train_wall 55 | wall 1087
2020-10-12 04:08:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 04:08:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 04:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:08:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000761
2020-10-12 04:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008126
2020-10-12 04:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000209
2020-10-12 04:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095826
2020-10-12 04:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104631
2020-10-12 04:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004280
2020-10-12 04:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 04:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096241
2020-10-12 04:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101014
2020-10-12 04:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:08:05 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 04:08:12 | INFO | train_inner | epoch 019:     22 / 171 loss=9.445, nll_loss=8.576, ppl=381.75, wps=15146.9, ups=2.66, wpb=5689.7, bsz=265.4, num_updates=3100, lr=0.000155023, gnorm=1.255, clip=0, train_wall=32, wall=1095
2020-10-12 04:08:45 | INFO | train_inner | epoch 019:    122 / 171 loss=9.316, nll_loss=8.43, ppl=344.79, wps=17208.1, ups=3.05, wpb=5636.4, bsz=263.8, num_updates=3200, lr=0.00016002, gnorm=1.257, clip=0, train_wall=32, wall=1127
2020-10-12 04:09:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000788
2020-10-12 04:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040245
2020-10-12 04:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030159
2020-10-12 04:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071533
2020-10-12 04:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000688
2020-10-12 04:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040083
2020-10-12 04:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030586
2020-10-12 04:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071708
2020-10-12 04:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:09:04 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 9.215 | nll_loss 8.281 | ppl 310.99 | wps 39097.2 | wpb 2120.5 | bsz 101.7 | num_updates 3249 | best_loss 9.215
2020-10-12 04:09:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:09:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 19 @ 3249 updates, score 9.215) (writing took 1.549496719992021 seconds)
2020-10-12 04:09:05 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 04:09:05 | INFO | train | epoch 019 | loss 9.318 | nll_loss 8.431 | ppl 345.08 | wps 15970.9 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 3249 | lr 0.000162469 | gnorm 1.249 | clip 0 | train_wall 55 | wall 1148
2020-10-12 04:09:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 04:09:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 04:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:09:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000761
2020-10-12 04:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007794
2020-10-12 04:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000214
2020-10-12 04:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100559
2020-10-12 04:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108971
2020-10-12 04:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004364
2020-10-12 04:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-12 04:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095574
2020-10-12 04:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100434
2020-10-12 04:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:09:05 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 04:09:22 | INFO | train_inner | epoch 020:     51 / 171 loss=9.282, nll_loss=8.388, ppl=334.95, wps=15326.3, ups=2.67, wpb=5741.2, bsz=238.3, num_updates=3300, lr=0.000165018, gnorm=1.288, clip=0, train_wall=33, wall=1165
2020-10-12 04:09:55 | INFO | train_inner | epoch 020:    151 / 171 loss=9.142, nll_loss=8.227, ppl=299.53, wps=17079.6, ups=3.06, wpb=5584.6, bsz=268.8, num_updates=3400, lr=0.000170015, gnorm=1.305, clip=0, train_wall=32, wall=1198
2020-10-12 04:10:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000762
2020-10-12 04:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041293
2020-10-12 04:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030324
2020-10-12 04:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072710
2020-10-12 04:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000641
2020-10-12 04:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040642
2020-10-12 04:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029786
2020-10-12 04:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071387
2020-10-12 04:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:10:04 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 9.112 | nll_loss 8.151 | ppl 284.24 | wps 39265.4 | wpb 2120.5 | bsz 101.7 | num_updates 3420 | best_loss 9.112
2020-10-12 04:10:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:10:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 20 @ 3420 updates, score 9.112) (writing took 1.6442307139950572 seconds)
2020-10-12 04:10:06 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 04:10:06 | INFO | train | epoch 020 | loss 9.163 | nll_loss 8.252 | ppl 304.78 | wps 15910.8 | ups 2.82 | wpb 5644.5 | bsz 260.2 | num_updates 3420 | lr 0.000171015 | gnorm 1.314 | clip 0 | train_wall 55 | wall 1208
2020-10-12 04:10:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 04:10:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 04:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:10:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000644
2020-10-12 04:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006810
2020-10-12 04:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000190
2020-10-12 04:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098447
2020-10-12 04:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.105864
2020-10-12 04:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004340
2020-10-12 04:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 04:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095496
2020-10-12 04:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100339
2020-10-12 04:10:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:10:06 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 04:10:32 | INFO | train_inner | epoch 021:     80 / 171 loss=9.054, nll_loss=8.126, ppl=279.41, wps=15291.8, ups=2.67, wpb=5724.2, bsz=256.5, num_updates=3500, lr=0.000175013, gnorm=1.318, clip=0, train_wall=32, wall=1235
2020-10-12 04:11:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000791
2020-10-12 04:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040154
2020-10-12 04:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030039
2020-10-12 04:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071317
2020-10-12 04:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000676
2020-10-12 04:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039992
2020-10-12 04:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030433
2020-10-12 04:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071422
2020-10-12 04:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:11:05 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 9.013 | nll_loss 8.022 | ppl 259.91 | wps 39415.1 | wpb 2120.5 | bsz 101.7 | num_updates 3591 | best_loss 9.013
2020-10-12 04:11:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:11:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 21 @ 3591 updates, score 9.013) (writing took 1.5158422300009988 seconds)
2020-10-12 04:11:06 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 04:11:06 | INFO | train | epoch 021 | loss 9.01 | nll_loss 8.074 | ppl 269.47 | wps 15968.7 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 3591 | lr 0.00017956 | gnorm 1.327 | clip 0 | train_wall 55 | wall 1269
2020-10-12 04:11:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 04:11:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 04:11:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:11:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000559
2020-10-12 04:11:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006070
2020-10-12 04:11:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:11:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-12 04:11:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:11:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101529
2020-10-12 04:11:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108115
2020-10-12 04:11:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:11:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:11:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004322
2020-10-12 04:11:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:11:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 04:11:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:11:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097557
2020-10-12 04:11:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102377
2020-10-12 04:11:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:11:06 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 04:11:09 | INFO | train_inner | epoch 022:      9 / 171 loss=8.969, nll_loss=8.025, ppl=260.54, wps=14992.2, ups=2.7, wpb=5546.1, bsz=265.2, num_updates=3600, lr=0.00018001, gnorm=1.344, clip=0, train_wall=32, wall=1272
2020-10-12 04:11:42 | INFO | train_inner | epoch 022:    109 / 171 loss=8.841, nll_loss=7.88, ppl=235.52, wps=17058.1, ups=3.06, wpb=5574.1, bsz=254.3, num_updates=3700, lr=0.000185008, gnorm=1.282, clip=0, train_wall=32, wall=1305
2020-10-12 04:12:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000788
2020-10-12 04:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041086
2020-10-12 04:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030647
2020-10-12 04:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072866
2020-10-12 04:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:12:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:12:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000679
2020-10-12 04:12:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:12:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040942
2020-10-12 04:12:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:12:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030945
2020-10-12 04:12:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072890
2020-10-12 04:12:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:12:05 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 8.907 | nll_loss 7.894 | ppl 237.86 | wps 39398.3 | wpb 2120.5 | bsz 101.7 | num_updates 3762 | best_loss 8.907
2020-10-12 04:12:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:12:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 22 @ 3762 updates, score 8.907) (writing took 1.9431381210015388 seconds)
2020-10-12 04:12:07 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 04:12:07 | INFO | train | epoch 022 | loss 8.84 | nll_loss 7.876 | ppl 234.94 | wps 15867 | ups 2.81 | wpb 5644.5 | bsz 260.2 | num_updates 3762 | lr 0.000188106 | gnorm 1.289 | clip 0 | train_wall 55 | wall 1330
2020-10-12 04:12:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 04:12:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 04:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:12:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000607
2020-10-12 04:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006023
2020-10-12 04:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-12 04:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101578
2020-10-12 04:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108198
2020-10-12 04:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004341
2020-10-12 04:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 04:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097221
2020-10-12 04:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102097
2020-10-12 04:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:12:07 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 04:12:20 | INFO | train_inner | epoch 023:     38 / 171 loss=8.775, nll_loss=7.801, ppl=222.94, wps=15118.5, ups=2.66, wpb=5693.2, bsz=267.4, num_updates=3800, lr=0.000190005, gnorm=1.314, clip=0, train_wall=32, wall=1342
2020-10-12 04:12:52 | INFO | train_inner | epoch 023:    138 / 171 loss=8.675, nll_loss=7.686, ppl=205.9, wps=17434.4, ups=3.05, wpb=5716.6, bsz=269, num_updates=3900, lr=0.000195003, gnorm=1.34, clip=0, train_wall=32, wall=1375
2020-10-12 04:13:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:13:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:13:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000781
2020-10-12 04:13:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:13:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040162
2020-10-12 04:13:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:13:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029943
2020-10-12 04:13:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071210
2020-10-12 04:13:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:13:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:13:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000671
2020-10-12 04:13:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:13:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039955
2020-10-12 04:13:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:13:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030291
2020-10-12 04:13:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071232
2020-10-12 04:13:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:13:06 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 8.838 | nll_loss 7.816 | ppl 225.3 | wps 39273.1 | wpb 2120.5 | bsz 101.7 | num_updates 3933 | best_loss 8.838
2020-10-12 04:13:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:13:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 23 @ 3933 updates, score 8.838) (writing took 1.5188321110035758 seconds)
2020-10-12 04:13:07 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 04:13:07 | INFO | train | epoch 023 | loss 8.688 | nll_loss 7.701 | ppl 208.05 | wps 16004.5 | ups 2.84 | wpb 5644.5 | bsz 260.2 | num_updates 3933 | lr 0.000196652 | gnorm 1.339 | clip 0 | train_wall 55 | wall 1390
2020-10-12 04:13:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 04:13:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 04:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:13:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000645
2020-10-12 04:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006992
2020-10-12 04:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000203
2020-10-12 04:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104215
2020-10-12 04:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111785
2020-10-12 04:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004340
2020-10-12 04:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-12 04:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:13:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098423
2020-10-12 04:13:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103248
2020-10-12 04:13:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:13:08 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 04:13:30 | INFO | train_inner | epoch 024:     67 / 171 loss=8.609, nll_loss=7.609, ppl=195.17, wps=15097.1, ups=2.7, wpb=5593.7, bsz=243.6, num_updates=4000, lr=0.0002, gnorm=1.306, clip=0, train_wall=32, wall=1412
2020-10-12 04:14:02 | INFO | train_inner | epoch 024:    167 / 171 loss=8.51, nll_loss=7.492, ppl=180.04, wps=17321.9, ups=3.04, wpb=5691.8, bsz=271.6, num_updates=4100, lr=0.000197546, gnorm=1.337, clip=0, train_wall=32, wall=1445
2020-10-12 04:14:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000775
2020-10-12 04:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040032
2020-10-12 04:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029844
2020-10-12 04:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070982
2020-10-12 04:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000694
2020-10-12 04:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039520
2020-10-12 04:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030332
2020-10-12 04:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070861
2020-10-12 04:14:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:14:06 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.731 | nll_loss 7.674 | ppl 204.17 | wps 39178.3 | wpb 2120.5 | bsz 101.7 | num_updates 4104 | best_loss 8.731
2020-10-12 04:14:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:14:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 24 @ 4104 updates, score 8.731) (writing took 1.5390546650014585 seconds)
2020-10-12 04:14:08 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 04:14:08 | INFO | train | epoch 024 | loss 8.527 | nll_loss 7.514 | ppl 182.72 | wps 15981.8 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 4104 | lr 0.00019745 | gnorm 1.32 | clip 0 | train_wall 55 | wall 1450
2020-10-12 04:14:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 04:14:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 04:14:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:14:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000574
2020-10-12 04:14:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005872
2020-10-12 04:14:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:14:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-12 04:14:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:14:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106824
2020-10-12 04:14:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113294
2020-10-12 04:14:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:14:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:14:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004870
2020-10-12 04:14:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:14:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000168
2020-10-12 04:14:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:14:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096460
2020-10-12 04:14:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101828
2020-10-12 04:14:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:14:08 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 04:14:39 | INFO | train_inner | epoch 025:     96 / 171 loss=8.387, nll_loss=7.353, ppl=163.44, wps=14965.6, ups=2.7, wpb=5540.5, bsz=250.9, num_updates=4200, lr=0.00019518, gnorm=1.349, clip=0, train_wall=32, wall=1482
2020-10-12 04:15:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000765
2020-10-12 04:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040693
2020-10-12 04:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031223
2020-10-12 04:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.073013
2020-10-12 04:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000679
2020-10-12 04:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039909
2020-10-12 04:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029996
2020-10-12 04:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070897
2020-10-12 04:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:15:07 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.704 | nll_loss 7.641 | ppl 199.57 | wps 39278.9 | wpb 2120.5 | bsz 101.7 | num_updates 4275 | best_loss 8.704
2020-10-12 04:15:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:15:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 25 @ 4275 updates, score 8.704) (writing took 1.4988935900037177 seconds)
2020-10-12 04:15:08 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 04:15:08 | INFO | train | epoch 025 | loss 8.37 | nll_loss 7.332 | ppl 161.08 | wps 15971.6 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 4275 | lr 0.00019346 | gnorm 1.324 | clip 0 | train_wall 55 | wall 1511
2020-10-12 04:15:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 04:15:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 04:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:15:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000620
2020-10-12 04:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006418
2020-10-12 04:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-12 04:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104810
2020-10-12 04:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111756
2020-10-12 04:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004303
2020-10-12 04:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000165
2020-10-12 04:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099656
2020-10-12 04:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104452
2020-10-12 04:15:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:15:08 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 04:15:17 | INFO | train_inner | epoch 026:     25 / 171 loss=8.332, nll_loss=7.286, ppl=156.05, wps=15161.1, ups=2.69, wpb=5635.5, bsz=259, num_updates=4300, lr=0.000192897, gnorm=1.323, clip=0, train_wall=32, wall=1519
2020-10-12 04:15:49 | INFO | train_inner | epoch 026:    125 / 171 loss=8.197, nll_loss=7.132, ppl=140.27, wps=17505.2, ups=3.04, wpb=5754.4, bsz=281.5, num_updates=4400, lr=0.000190693, gnorm=1.278, clip=0, train_wall=32, wall=1552
2020-10-12 04:16:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:16:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:16:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000777
2020-10-12 04:16:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:16:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041033
2020-10-12 04:16:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:16:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030374
2020-10-12 04:16:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072518
2020-10-12 04:16:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:16:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:16:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000723
2020-10-12 04:16:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:16:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040351
2020-10-12 04:16:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:16:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030261
2020-10-12 04:16:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071655
2020-10-12 04:16:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:16:07 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.626 | nll_loss 7.545 | ppl 186.79 | wps 39352.2 | wpb 2120.5 | bsz 101.7 | num_updates 4446 | best_loss 8.626
2020-10-12 04:16:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:16:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 26 @ 4446 updates, score 8.626) (writing took 1.525081105006393 seconds)
2020-10-12 04:16:09 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 04:16:09 | INFO | train | epoch 026 | loss 8.223 | nll_loss 7.16 | ppl 142.99 | wps 15986.5 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 4446 | lr 0.000189703 | gnorm 1.314 | clip 0 | train_wall 55 | wall 1571
2020-10-12 04:16:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 04:16:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 04:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:16:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000738
2020-10-12 04:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007331
2020-10-12 04:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000197
2020-10-12 04:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103081
2020-10-12 04:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111088
2020-10-12 04:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004250
2020-10-12 04:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000169
2020-10-12 04:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097802
2020-10-12 04:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102544
2020-10-12 04:16:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:16:09 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 04:16:26 | INFO | train_inner | epoch 027:     54 / 171 loss=8.133, nll_loss=7.056, ppl=133.07, wps=14927.7, ups=2.71, wpb=5514.1, bsz=246.9, num_updates=4500, lr=0.000188562, gnorm=1.316, clip=0, train_wall=32, wall=1589
2020-10-12 04:16:59 | INFO | train_inner | epoch 027:    154 / 171 loss=8.112, nll_loss=7.029, ppl=130.63, wps=17433.6, ups=3.03, wpb=5750.2, bsz=258.4, num_updates=4600, lr=0.000186501, gnorm=1.286, clip=0, train_wall=33, wall=1622
2020-10-12 04:17:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:17:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:17:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000699
2020-10-12 04:17:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:17:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040440
2020-10-12 04:17:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:17:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030152
2020-10-12 04:17:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071615
2020-10-12 04:17:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:17:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:17:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000625
2020-10-12 04:17:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:17:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039547
2020-10-12 04:17:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:17:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029807
2020-10-12 04:17:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070298
2020-10-12 04:17:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:17:07 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.56 | nll_loss 7.457 | ppl 175.67 | wps 39069.9 | wpb 2120.5 | bsz 101.7 | num_updates 4617 | best_loss 8.56
2020-10-12 04:17:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:17:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 27 @ 4617 updates, score 8.56) (writing took 1.5273037110018777 seconds)
2020-10-12 04:17:09 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 04:17:09 | INFO | train | epoch 027 | loss 8.083 | nll_loss 6.998 | ppl 127.79 | wps 15964.4 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 4617 | lr 0.000186157 | gnorm 1.298 | clip 0 | train_wall 55 | wall 1632
2020-10-12 04:17:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 04:17:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 04:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:17:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000615
2020-10-12 04:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006247
2020-10-12 04:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-12 04:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101668
2020-10-12 04:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108538
2020-10-12 04:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004303
2020-10-12 04:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 04:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095922
2020-10-12 04:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100724
2020-10-12 04:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:17:09 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 04:17:36 | INFO | train_inner | epoch 028:     83 / 171 loss=7.979, nll_loss=6.877, ppl=117.52, wps=15228.3, ups=2.7, wpb=5632.7, bsz=258.6, num_updates=4700, lr=0.000184506, gnorm=1.309, clip=0, train_wall=32, wall=1659
2020-10-12 04:18:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000789
2020-10-12 04:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041154
2020-10-12 04:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030707
2020-10-12 04:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072985
2020-10-12 04:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000666
2020-10-12 04:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040112
2020-10-12 04:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031088
2020-10-12 04:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072182
2020-10-12 04:18:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:18:08 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.51 | nll_loss 7.388 | ppl 167.53 | wps 39522.4 | wpb 2120.5 | bsz 101.7 | num_updates 4788 | best_loss 8.51
2020-10-12 04:18:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:18:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 28 @ 4788 updates, score 8.51) (writing took 1.5038161849952303 seconds)
2020-10-12 04:18:09 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 04:18:09 | INFO | train | epoch 028 | loss 7.956 | nll_loss 6.849 | ppl 115.29 | wps 15954.9 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 4788 | lr 0.000182803 | gnorm 1.3 | clip 0 | train_wall 55 | wall 1692
2020-10-12 04:18:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 04:18:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 04:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:18:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000509
2020-10-12 04:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005785
2020-10-12 04:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 04:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100894
2020-10-12 04:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107283
2020-10-12 04:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004332
2020-10-12 04:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 04:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096290
2020-10-12 04:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101123
2020-10-12 04:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:18:10 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 04:18:14 | INFO | train_inner | epoch 029:     12 / 171 loss=7.938, nll_loss=6.828, ppl=113.59, wps=15047.6, ups=2.69, wpb=5590.5, bsz=261.6, num_updates=4800, lr=0.000182574, gnorm=1.303, clip=0, train_wall=32, wall=1696
2020-10-12 04:18:46 | INFO | train_inner | epoch 029:    112 / 171 loss=7.818, nll_loss=6.69, ppl=103.28, wps=17110, ups=3.06, wpb=5593.8, bsz=258.7, num_updates=4900, lr=0.000180702, gnorm=1.296, clip=0, train_wall=32, wall=1729
2020-10-12 04:19:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000793
2020-10-12 04:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040317
2020-10-12 04:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030968
2020-10-12 04:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072407
2020-10-12 04:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000675
2020-10-12 04:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041034
2020-10-12 04:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030106
2020-10-12 04:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072137
2020-10-12 04:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:19:08 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.458 | nll_loss 7.325 | ppl 160.37 | wps 39845.7 | wpb 2120.5 | bsz 101.7 | num_updates 4959 | best_loss 8.458
2020-10-12 04:19:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:19:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 29 @ 4959 updates, score 8.458) (writing took 1.5098059919982916 seconds)
2020-10-12 04:19:10 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 04:19:10 | INFO | train | epoch 029 | loss 7.823 | nll_loss 6.694 | ppl 103.55 | wps 15978.5 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 4959 | lr 0.000179623 | gnorm 1.275 | clip 0 | train_wall 55 | wall 1753
2020-10-12 04:19:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 04:19:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 04:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:19:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000620
2020-10-12 04:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006110
2020-10-12 04:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000264
2020-10-12 04:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103485
2020-10-12 04:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110213
2020-10-12 04:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004319
2020-10-12 04:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 04:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095735
2020-10-12 04:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100548
2020-10-12 04:19:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:19:10 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 04:19:24 | INFO | train_inner | epoch 030:     41 / 171 loss=7.775, nll_loss=6.638, ppl=99.63, wps=15243.7, ups=2.67, wpb=5715.1, bsz=256.6, num_updates=5000, lr=0.000178885, gnorm=1.266, clip=0, train_wall=33, wall=1766
2020-10-12 04:19:56 | INFO | train_inner | epoch 030:    141 / 171 loss=7.707, nll_loss=6.558, ppl=94.22, wps=17438.5, ups=3.06, wpb=5707.9, bsz=263.8, num_updates=5100, lr=0.000177123, gnorm=1.282, clip=0, train_wall=32, wall=1799
2020-10-12 04:20:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000782
2020-10-12 04:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040680
2020-10-12 04:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030206
2020-10-12 04:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072002
2020-10-12 04:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000666
2020-10-12 04:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040675
2020-10-12 04:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030454
2020-10-12 04:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072112
2020-10-12 04:20:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:20:09 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.424 | nll_loss 7.272 | ppl 154.56 | wps 39210.4 | wpb 2120.5 | bsz 101.7 | num_updates 5130 | best_loss 8.424
2020-10-12 04:20:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:20:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 30 @ 5130 updates, score 8.424) (writing took 1.5003429500065977 seconds)
2020-10-12 04:20:10 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 04:20:10 | INFO | train | epoch 030 | loss 7.702 | nll_loss 6.554 | ppl 93.93 | wps 15951.8 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 5130 | lr 0.000176604 | gnorm 1.288 | clip 0 | train_wall 55 | wall 1813
2020-10-12 04:20:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 04:20:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 04:20:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:20:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000478
2020-10-12 04:20:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005941
2020-10-12 04:20:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:20:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 04:20:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:20:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100010
2020-10-12 04:20:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106456
2020-10-12 04:20:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:20:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:20:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004280
2020-10-12 04:20:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:20:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 04:20:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:20:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095683
2020-10-12 04:20:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100448
2020-10-12 04:20:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:20:11 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 04:20:34 | INFO | train_inner | epoch 031:     70 / 171 loss=7.63, nll_loss=6.47, ppl=88.63, wps=14932.6, ups=2.69, wpb=5549.2, bsz=252.3, num_updates=5200, lr=0.000175412, gnorm=1.328, clip=0, train_wall=32, wall=1836
2020-10-12 04:21:06 | INFO | train_inner | epoch 031:    170 / 171 loss=7.593, nll_loss=6.425, ppl=85.92, wps=17410.9, ups=3.04, wpb=5723.8, bsz=269.3, num_updates=5300, lr=0.000173749, gnorm=1.288, clip=0, train_wall=32, wall=1869
2020-10-12 04:21:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:21:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:21:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000774
2020-10-12 04:21:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:21:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040190
2020-10-12 04:21:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:21:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030341
2020-10-12 04:21:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071637
2020-10-12 04:21:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:21:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:21:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000681
2020-10-12 04:21:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:21:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040398
2020-10-12 04:21:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:21:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030073
2020-10-12 04:21:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071470
2020-10-12 04:21:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:21:09 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.389 | nll_loss 7.23 | ppl 150.08 | wps 39192.6 | wpb 2120.5 | bsz 101.7 | num_updates 5301 | best_loss 8.389
2020-10-12 04:21:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:21:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 31 @ 5301 updates, score 8.389) (writing took 1.5088488570036134 seconds)
2020-10-12 04:21:11 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 04:21:11 | INFO | train | epoch 031 | loss 7.591 | nll_loss 6.424 | ppl 85.84 | wps 15966.3 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 5301 | lr 0.000173733 | gnorm 1.315 | clip 0 | train_wall 55 | wall 1874
2020-10-12 04:21:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 04:21:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 04:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:21:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000554
2020-10-12 04:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006069
2020-10-12 04:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 04:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.100305
2020-10-12 04:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106882
2020-10-12 04:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004327
2020-10-12 04:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 04:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096607
2020-10-12 04:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101432
2020-10-12 04:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:21:11 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 04:21:43 | INFO | train_inner | epoch 032:     99 / 171 loss=7.447, nll_loss=6.259, ppl=76.58, wps=15198.2, ups=2.7, wpb=5623.5, bsz=261.4, num_updates=5400, lr=0.000172133, gnorm=1.272, clip=0, train_wall=32, wall=1906
2020-10-12 04:22:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000777
2020-10-12 04:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040713
2020-10-12 04:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029840
2020-10-12 04:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071656
2020-10-12 04:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000685
2020-10-12 04:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040449
2020-10-12 04:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030261
2020-10-12 04:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071718
2020-10-12 04:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:22:10 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.375 | nll_loss 7.21 | ppl 148.03 | wps 39303.1 | wpb 2120.5 | bsz 101.7 | num_updates 5472 | best_loss 8.375
2020-10-12 04:22:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:22:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 32 @ 5472 updates, score 8.375) (writing took 1.4996418850059854 seconds)
2020-10-12 04:22:11 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 04:22:11 | INFO | train | epoch 032 | loss 7.473 | nll_loss 6.286 | ppl 78.01 | wps 15983 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 5472 | lr 0.000170996 | gnorm 1.274 | clip 0 | train_wall 55 | wall 1934
2020-10-12 04:22:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 04:22:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 04:22:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:22:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000596
2020-10-12 04:22:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006408
2020-10-12 04:22:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:22:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000254
2020-10-12 04:22:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:22:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.106354
2020-10-12 04:22:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113371
2020-10-12 04:22:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:22:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:22:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004300
2020-10-12 04:22:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:22:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 04:22:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:22:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096045
2020-10-12 04:22:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100837
2020-10-12 04:22:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:22:11 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 04:22:21 | INFO | train_inner | epoch 033:     28 / 171 loss=7.453, nll_loss=6.261, ppl=76.68, wps=15582.8, ups=2.67, wpb=5829.8, bsz=274.7, num_updates=5500, lr=0.000170561, gnorm=1.28, clip=0, train_wall=33, wall=1944
2020-10-12 04:22:54 | INFO | train_inner | epoch 033:    128 / 171 loss=7.387, nll_loss=6.184, ppl=72.7, wps=16979.9, ups=3.06, wpb=5555.4, bsz=247.5, num_updates=5600, lr=0.000169031, gnorm=1.322, clip=0, train_wall=32, wall=1976
2020-10-12 04:23:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000797
2020-10-12 04:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040908
2020-10-12 04:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030451
2020-10-12 04:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072488
2020-10-12 04:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000682
2020-10-12 04:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039893
2020-10-12 04:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030769
2020-10-12 04:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071660
2020-10-12 04:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:23:10 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.378 | nll_loss 7.197 | ppl 146.69 | wps 39387.9 | wpb 2120.5 | bsz 101.7 | num_updates 5643 | best_loss 8.375
2020-10-12 04:23:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:23:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_last.pt (epoch 33 @ 5643 updates, score 8.378) (writing took 1.476924575996236 seconds)
2020-10-12 04:23:12 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 04:23:12 | INFO | train | epoch 033 | loss 7.375 | nll_loss 6.171 | ppl 72.06 | wps 15980.9 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 5643 | lr 0.000168386 | gnorm 1.306 | clip 0 | train_wall 55 | wall 1994
2020-10-12 04:23:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 04:23:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 04:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:23:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000517
2020-10-12 04:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005648
2020-10-12 04:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 04:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098488
2020-10-12 04:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104724
2020-10-12 04:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004399
2020-10-12 04:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-12 04:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101211
2020-10-12 04:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106111
2020-10-12 04:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:23:12 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 04:23:31 | INFO | train_inner | epoch 034:     57 / 171 loss=7.278, nll_loss=6.059, ppl=66.68, wps=15010.4, ups=2.71, wpb=5540.1, bsz=270.2, num_updates=5700, lr=0.000167542, gnorm=1.298, clip=0, train_wall=32, wall=2013
2020-10-12 04:24:03 | INFO | train_inner | epoch 034:    157 / 171 loss=7.299, nll_loss=6.08, ppl=67.63, wps=17190.7, ups=3.04, wpb=5657.8, bsz=253, num_updates=5800, lr=0.000166091, gnorm=1.317, clip=0, train_wall=32, wall=2046
2020-10-12 04:24:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000776
2020-10-12 04:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041047
2020-10-12 04:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030314
2020-10-12 04:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072478
2020-10-12 04:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000629
2020-10-12 04:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.041193
2020-10-12 04:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030615
2020-10-12 04:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072755
2020-10-12 04:24:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:24:10 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.336 | nll_loss 7.148 | ppl 141.84 | wps 39447.3 | wpb 2120.5 | bsz 101.7 | num_updates 5814 | best_loss 8.336
2020-10-12 04:24:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:24:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 34 @ 5814 updates, score 8.336) (writing took 1.5166212159965653 seconds)
2020-10-12 04:24:12 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 04:24:12 | INFO | train | epoch 034 | loss 7.274 | nll_loss 6.053 | ppl 66.4 | wps 15989 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 5814 | lr 0.000165891 | gnorm 1.308 | clip 0 | train_wall 55 | wall 2055
2020-10-12 04:24:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 04:24:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 04:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:24:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000748
2020-10-12 04:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.007202
2020-10-12 04:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000208
2020-10-12 04:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104224
2020-10-12 04:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112102
2020-10-12 04:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004312
2020-10-12 04:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 04:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096948
2020-10-12 04:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101791
2020-10-12 04:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:24:12 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 04:24:40 | INFO | train_inner | epoch 035:     86 / 171 loss=7.197, nll_loss=5.964, ppl=62.41, wps=15345.7, ups=2.7, wpb=5674.3, bsz=244.2, num_updates=5900, lr=0.000164677, gnorm=1.268, clip=0, train_wall=32, wall=2083
2020-10-12 04:25:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000790
2020-10-12 04:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039602
2020-10-12 04:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029836
2020-10-12 04:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070554
2020-10-12 04:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000661
2020-10-12 04:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039659
2020-10-12 04:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029823
2020-10-12 04:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070455
2020-10-12 04:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:25:11 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.336 | nll_loss 7.138 | ppl 140.8 | wps 39497.3 | wpb 2120.5 | bsz 101.7 | num_updates 5985 | best_loss 8.336
2020-10-12 04:25:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:25:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 35 @ 5985 updates, score 8.336) (writing took 1.518239501005155 seconds)
2020-10-12 04:25:12 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 04:25:12 | INFO | train | epoch 035 | loss 7.176 | nll_loss 5.938 | ppl 61.29 | wps 16002.7 | ups 2.84 | wpb 5644.5 | bsz 260.2 | num_updates 5985 | lr 0.000163504 | gnorm 1.303 | clip 0 | train_wall 55 | wall 2115
2020-10-12 04:25:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 04:25:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 04:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:25:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000883
2020-10-12 04:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.008320
2020-10-12 04:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000304
2020-10-12 04:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098506
2020-10-12 04:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107534
2020-10-12 04:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004295
2020-10-12 04:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 04:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:25:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095551
2020-10-12 04:25:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100342
2020-10-12 04:25:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:25:13 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 04:25:17 | INFO | train_inner | epoch 036:     15 / 171 loss=7.174, nll_loss=5.933, ppl=61.11, wps=15086, ups=2.7, wpb=5590.1, bsz=267.3, num_updates=6000, lr=0.000163299, gnorm=1.329, clip=0, train_wall=32, wall=2120
2020-10-12 04:25:50 | INFO | train_inner | epoch 036:    115 / 171 loss=7.061, nll_loss=5.804, ppl=55.87, wps=17207.6, ups=3.05, wpb=5650.4, bsz=264, num_updates=6100, lr=0.000161955, gnorm=1.307, clip=0, train_wall=32, wall=2153
2020-10-12 04:26:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000794
2020-10-12 04:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039973
2020-10-12 04:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.031138
2020-10-12 04:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072238
2020-10-12 04:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000657
2020-10-12 04:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040135
2020-10-12 04:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030292
2020-10-12 04:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071404
2020-10-12 04:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:26:11 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.315 | nll_loss 7.104 | ppl 137.56 | wps 39864.8 | wpb 2120.5 | bsz 101.7 | num_updates 6156 | best_loss 8.315
2020-10-12 04:26:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:26:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 36 @ 6156 updates, score 8.315) (writing took 1.5109546419989783 seconds)
2020-10-12 04:26:13 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 04:26:13 | INFO | train | epoch 036 | loss 7.083 | nll_loss 5.828 | ppl 56.81 | wps 15961.2 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 6156 | lr 0.000161217 | gnorm 1.309 | clip 0 | train_wall 55 | wall 2175
2020-10-12 04:26:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 04:26:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 04:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:26:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000716
2020-10-12 04:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006385
2020-10-12 04:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000256
2020-10-12 04:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103783
2020-10-12 04:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110771
2020-10-12 04:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004330
2020-10-12 04:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 04:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.098325
2020-10-12 04:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103154
2020-10-12 04:26:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:26:13 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 04:26:27 | INFO | train_inner | epoch 037:     44 / 171 loss=7.035, nll_loss=5.773, ppl=54.7, wps=15001.2, ups=2.7, wpb=5554.4, bsz=260.4, num_updates=6200, lr=0.000160644, gnorm=1.331, clip=0, train_wall=32, wall=2190
2020-10-12 04:27:00 | INFO | train_inner | epoch 037:    144 / 171 loss=7.028, nll_loss=5.761, ppl=54.23, wps=17376.2, ups=3.05, wpb=5704.8, bsz=245.8, num_updates=6300, lr=0.000159364, gnorm=1.312, clip=0, train_wall=32, wall=2223
2020-10-12 04:27:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:27:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:27:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000782
2020-10-12 04:27:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:27:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039714
2020-10-12 04:27:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:27:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030074
2020-10-12 04:27:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070938
2020-10-12 04:27:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:27:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:27:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000665
2020-10-12 04:27:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:27:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040261
2020-10-12 04:27:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:27:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030549
2020-10-12 04:27:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071794
2020-10-12 04:27:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:27:12 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.319 | nll_loss 7.104 | ppl 137.59 | wps 39640.2 | wpb 2120.5 | bsz 101.7 | num_updates 6327 | best_loss 8.315
2020-10-12 04:27:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:27:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_last.pt (epoch 37 @ 6327 updates, score 8.319) (writing took 1.0452205470064655 seconds)
2020-10-12 04:27:13 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 04:27:13 | INFO | train | epoch 037 | loss 6.993 | nll_loss 5.723 | ppl 52.84 | wps 16100.3 | ups 2.85 | wpb 5644.5 | bsz 260.2 | num_updates 6327 | lr 0.000159023 | gnorm 1.327 | clip 0 | train_wall 55 | wall 2235
2020-10-12 04:27:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 04:27:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 04:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:27:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000502
2020-10-12 04:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005484
2020-10-12 04:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 04:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097607
2020-10-12 04:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103587
2020-10-12 04:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004306
2020-10-12 04:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000167
2020-10-12 04:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097099
2020-10-12 04:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101902
2020-10-12 04:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:27:13 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 04:27:37 | INFO | train_inner | epoch 038:     73 / 171 loss=6.868, nll_loss=5.58, ppl=47.82, wps=15497.4, ups=2.72, wpb=5699.9, bsz=294.2, num_updates=6400, lr=0.000158114, gnorm=1.323, clip=0, train_wall=32, wall=2260
2020-10-12 04:28:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000769
2020-10-12 04:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040410
2020-10-12 04:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030818
2020-10-12 04:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.072328
2020-10-12 04:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000691
2020-10-12 04:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040123
2020-10-12 04:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029441
2020-10-12 04:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070569
2020-10-12 04:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:28:12 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.344 | nll_loss 7.137 | ppl 140.79 | wps 39263.7 | wpb 2120.5 | bsz 101.7 | num_updates 6498 | best_loss 8.315
2020-10-12 04:28:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:28:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_last.pt (epoch 38 @ 6498 updates, score 8.344) (writing took 1.056465504996595 seconds)
2020-10-12 04:28:13 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 04:28:13 | INFO | train | epoch 038 | loss 6.907 | nll_loss 5.621 | ppl 49.21 | wps 16107.1 | ups 2.85 | wpb 5644.5 | bsz 260.2 | num_updates 6498 | lr 0.000156917 | gnorm 1.327 | clip 0 | train_wall 55 | wall 2295
2020-10-12 04:28:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 04:28:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 04:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:28:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000537
2020-10-12 04:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006302
2020-10-12 04:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000188
2020-10-12 04:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096578
2020-10-12 04:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103420
2020-10-12 04:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004292
2020-10-12 04:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 04:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096970
2020-10-12 04:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101757
2020-10-12 04:28:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:28:13 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 04:28:14 | INFO | train_inner | epoch 039:      2 / 171 loss=6.962, nll_loss=5.682, ppl=51.34, wps=15342.8, ups=2.73, wpb=5625.7, bsz=239.8, num_updates=6500, lr=0.000156893, gnorm=1.339, clip=0, train_wall=32, wall=2296
2020-10-12 04:28:46 | INFO | train_inner | epoch 039:    102 / 171 loss=6.816, nll_loss=5.516, ppl=45.77, wps=17138.2, ups=3.05, wpb=5616.2, bsz=255, num_updates=6600, lr=0.0001557, gnorm=1.364, clip=0, train_wall=32, wall=2329
2020-10-12 04:29:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000764
2020-10-12 04:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040466
2020-10-12 04:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030192
2020-10-12 04:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071745
2020-10-12 04:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000688
2020-10-12 04:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.040600
2020-10-12 04:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.030357
2020-10-12 04:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.071962
2020-10-12 04:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:29:12 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.325 | nll_loss 7.108 | ppl 137.93 | wps 39314.6 | wpb 2120.5 | bsz 101.7 | num_updates 6669 | best_loss 8.315
2020-10-12 04:29:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:29:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_last.pt (epoch 39 @ 6669 updates, score 8.325) (writing took 1.054026313999202 seconds)
2020-10-12 04:29:13 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 04:29:13 | INFO | train | epoch 039 | loss 6.83 | nll_loss 5.53 | ppl 46.21 | wps 16087.8 | ups 2.85 | wpb 5644.5 | bsz 260.2 | num_updates 6669 | lr 0.000154892 | gnorm 1.359 | clip 0 | train_wall 55 | wall 2355
2020-10-12 04:29:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 04:29:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 04:29:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:29:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000516
2020-10-12 04:29:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005883
2020-10-12 04:29:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:29:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000166
2020-10-12 04:29:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:29:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097718
2020-10-12 04:29:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104087
2020-10-12 04:29:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:29:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:29:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004322
2020-10-12 04:29:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:29:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 04:29:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:29:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099298
2020-10-12 04:29:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104114
2020-10-12 04:29:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:29:13 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 04:29:23 | INFO | train_inner | epoch 040:     31 / 171 loss=6.802, nll_loss=5.497, ppl=45.16, wps=15485.2, ups=2.73, wpb=5671.4, bsz=271, num_updates=6700, lr=0.000154533, gnorm=1.364, clip=0, train_wall=32, wall=2366
2020-10-12 04:29:56 | INFO | train_inner | epoch 040:    131 / 171 loss=6.737, nll_loss=5.421, ppl=42.85, wps=17042.5, ups=3.03, wpb=5620.8, bsz=258.6, num_updates=6800, lr=0.000153393, gnorm=1.341, clip=0, train_wall=33, wall=2399
2020-10-12 04:30:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 04:30:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:30:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000772
2020-10-12 04:30:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:30:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039402
2020-10-12 04:30:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:30:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029733
2020-10-12 04:30:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070242
2020-10-12 04:30:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:30:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:30:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000645
2020-10-12 04:30:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:30:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.039616
2020-10-12 04:30:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:30:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.029853
2020-10-12 04:30:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.070429
2020-10-12 04:30:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:30:12 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.312 | nll_loss 7.078 | ppl 135.1 | wps 39080.6 | wpb 2120.5 | bsz 101.7 | num_updates 6840 | best_loss 8.312
2020-10-12 04:30:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 04:30:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_beljpn_sepspm8000/O2M/checkpoint_best.pt (epoch 40 @ 6840 updates, score 8.312) (writing took 1.5120825140038505 seconds)
2020-10-12 04:30:13 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 04:30:13 | INFO | train | epoch 040 | loss 6.746 | nll_loss 5.432 | ppl 43.17 | wps 15955.5 | ups 2.83 | wpb 5644.5 | bsz 260.2 | num_updates 6840 | lr 0.000152944 | gnorm 1.346 | clip 0 | train_wall 55 | wall 2416
2020-10-12 04:30:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 04:30:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 04:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A
2020-10-12 04:30:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000832
2020-10-12 04:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.009043
2020-10-12 04:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000217
2020-10-12 04:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101527
2020-10-12 04:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111189
2020-10-12 04:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A
2020-10-12 04:30:13 | INFO | fairseq_cli.train | done training in 2415.8 seconds
