2020-10-12 19:48:04 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_azefas_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='aze-eng,fas-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_azefas_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 19:48:04 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 19:48:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['aze', 'eng', 'fas']
2020-10-12 19:48:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | [aze] dictionary: 21987 types
2020-10-12 19:48:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 21987 types
2020-10-12 19:48:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | [fas] dictionary: 21987 types
2020-10-12 19:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 19:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6041.90234375Mb; avail=482810.4375Mb
2020-10-12 19:48:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 19:48:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:aze-eng': 1, 'main:fas-eng': 1}
2020-10-12 19:48:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:04 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azefas_sepspm8000/M2O/valid.aze-eng.aze
2020-10-12 19:48:04 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azefas_sepspm8000/M2O/valid.aze-eng.eng
2020-10-12 19:48:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azefas_sepspm8000/M2O/ valid aze-eng 671 examples
2020-10-12 19:48:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:fas-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:04 | INFO | fairseq.data.data_utils | loaded 3930 examples from: fairseq/data-bin/ted_azefas_sepspm8000/M2O/valid.fas-eng.fas
2020-10-12 19:48:04 | INFO | fairseq.data.data_utils | loaded 3930 examples from: fairseq/data-bin/ted_azefas_sepspm8000/M2O/valid.fas-eng.eng
2020-10-12 19:48:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azefas_sepspm8000/M2O/ valid fas-eng 3930 examples
2020-10-12 19:48:05 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21987, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21987, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=21987, bias=False)
  )
)
2020-10-12 19:48:05 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 19:48:05 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 19:48:05 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 19:48:05 | INFO | fairseq_cli.train | num. model params: 42800640 (num. trained: 42800640)
2020-10-12 19:48:09 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 19:48:09 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 19:48:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 19:48:09 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 19:48:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 19:48:09 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 19:48:09 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 19:48:09 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_last.pt
2020-10-12 19:48:09 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 19:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 19:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=8184.34765625Mb; avail=480656.9609375Mb
2020-10-12 19:48:09 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 19:48:09 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:aze-eng': 1, 'main:fas-eng': 1}
2020-10-12 19:48:09 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:09 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azefas_sepspm8000/M2O/train.aze-eng.aze
2020-10-12 19:48:09 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azefas_sepspm8000/M2O/train.aze-eng.eng
2020-10-12 19:48:09 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azefas_sepspm8000/M2O/ train aze-eng 5946 examples
2020-10-12 19:48:09 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:fas-eng src_langtok: None; tgt_langtok: None
2020-10-12 19:48:09 | INFO | fairseq.data.data_utils | loaded 19981 examples from: fairseq/data-bin/ted_azefas_sepspm8000/M2O/train.fas-eng.fas
2020-10-12 19:48:09 | INFO | fairseq.data.data_utils | loaded 19981 examples from: fairseq/data-bin/ted_azefas_sepspm8000/M2O/train.fas-eng.eng
2020-10-12 19:48:09 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azefas_sepspm8000/M2O/ train fas-eng 19981 examples
2020-10-12 19:48:09 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:aze-eng', 5946), ('main:fas-eng', 19981)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 19:48:09 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 19:48:09 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 25927
2020-10-12 19:48:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 25927; virtual dataset size 25927
2020-10-12 19:48:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 19:48:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:aze-eng': 5946, 'main:fas-eng': 19981}; raw total size: 25927
2020-10-12 19:48:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:aze-eng': 5946, 'main:fas-eng': 19981}; resampled total size: 25927
2020-10-12 19:48:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 19:48:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.004563
2020-10-12 19:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=8185.02734375Mb; avail=480656.88671875Mb
2020-10-12 19:48:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000615
2020-10-12 19:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005086
2020-10-12 19:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=8185.02734375Mb; avail=480656.88671875Mb
2020-10-12 19:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000207
2020-10-12 19:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=8185.02734375Mb; avail=480656.88671875Mb
2020-10-12 19:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093414
2020-10-12 19:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099471
2020-10-12 19:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=8185.4609375Mb; avail=480656.2890625Mb
2020-10-12 19:48:09 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 19:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=8185.45703125Mb; avail=480656.29296875Mb
2020-10-12 19:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003837
2020-10-12 19:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=8185.45703125Mb; avail=480656.29296875Mb
2020-10-12 19:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 19:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=8185.45703125Mb; avail=480656.29296875Mb
2020-10-12 19:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089349
2020-10-12 19:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094083
2020-10-12 19:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=8185.5703125Mb; avail=480656.77734375Mb
2020-10-12 19:48:09 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 19:48:37 | INFO | train_inner | epoch 001:    100 / 120 loss=14.094, nll_loss=13.976, ppl=16116.8, wps=20910.8, ups=3.61, wpb=5811.8, bsz=216.7, num_updates=100, lr=5.0975e-06, gnorm=4.596, clip=0, train_wall=27, wall=29
2020-10-12 19:48:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15698.58203125Mb; avail=473086.78125Mb
2020-10-12 19:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001689
2020-10-12 19:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15698.58203125Mb; avail=473086.78125Mb
2020-10-12 19:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068335
2020-10-12 19:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15698.578125Mb; avail=473086.78125Mb
2020-10-12 19:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049025
2020-10-12 19:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119848
2020-10-12 19:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15698.62109375Mb; avail=473086.5390625Mb
2020-10-12 19:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15698.625Mb; avail=473086.90234375Mb
2020-10-12 19:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001126
2020-10-12 19:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15698.625Mb; avail=473086.90234375Mb
2020-10-12 19:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068251
2020-10-12 19:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15698.64453125Mb; avail=473086.78125Mb
2020-10-12 19:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048859
2020-10-12 19:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118980
2020-10-12 19:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15698.4921875Mb; avail=473087.0234375Mb
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2020-10-12 19:48:45 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.295 | nll_loss 11.964 | ppl 3994.12 | wps 57736 | wpb 2252.3 | bsz 85.2 | num_updates 120
2020-10-12 19:48:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:48:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 1 @ 120 updates, score 12.295) (writing took 1.5491077749993565 seconds)
2020-10-12 19:48:47 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 19:48:47 | INFO | train | epoch 001 | loss 13.894 | nll_loss 13.753 | ppl 13806.4 | wps 18511.2 | ups 3.23 | wpb 5750.2 | bsz 216.1 | num_updates 120 | lr 6.097e-06 | gnorm 4.254 | clip 0 | train_wall 32 | wall 38
2020-10-12 19:48:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 19:48:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15717.0703125Mb; avail=473052.30859375Mb
2020-10-12 19:48:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000676
2020-10-12 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005049
2020-10-12 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15717.0703125Mb; avail=473052.30859375Mb
2020-10-12 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15717.0703125Mb; avail=473052.30859375Mb
2020-10-12 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090840
2020-10-12 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096843
2020-10-12 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15717.796875Mb; avail=473051.9453125Mb
2020-10-12 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15717.7265625Mb; avail=473052.4296875Mb
2020-10-12 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003650
2020-10-12 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15717.7265625Mb; avail=473052.4296875Mb
2020-10-12 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-12 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15717.7265625Mb; avail=473052.4296875Mb
2020-10-12 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091578
2020-10-12 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096146
2020-10-12 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15717.6328125Mb; avail=473052.4296875Mb
2020-10-12 19:48:47 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 19:49:10 | INFO | train_inner | epoch 002:     80 / 120 loss=12.411, nll_loss=12.101, ppl=4392.6, wps=17569.9, ups=3.1, wpb=5666.4, bsz=218.5, num_updates=200, lr=1.0095e-05, gnorm=2.13, clip=0, train_wall=27, wall=61
2020-10-12 19:49:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:49:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17877.10546875Mb; avail=470812.109375Mb
2020-10-12 19:49:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001551
2020-10-12 19:49:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.19921875Mb; avail=470812.23046875Mb
2020-10-12 19:49:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066470
2020-10-12 19:49:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.29296875Mb; avail=470812.23046875Mb
2020-10-12 19:49:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047923
2020-10-12 19:49:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116728
2020-10-12 19:49:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.8125Mb; avail=470812.23046875Mb
2020-10-12 19:49:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17876.73828125Mb; avail=470812.109375Mb
2020-10-12 19:49:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001164
2020-10-12 19:49:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17876.73828125Mb; avail=470812.109375Mb
2020-10-12 19:49:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067656
2020-10-12 19:49:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.22265625Mb; avail=470811.71484375Mb
2020-10-12 19:49:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047642
2020-10-12 19:49:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117216
2020-10-12 19:49:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.22265625Mb; avail=470811.71484375Mb
2020-10-12 19:49:23 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.35 | nll_loss 10.907 | ppl 1920.16 | wps 57769.1 | wpb 2252.3 | bsz 85.2 | num_updates 240 | best_loss 11.35
2020-10-12 19:49:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:49:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 2 @ 240 updates, score 11.35) (writing took 19.57497562499975 seconds)
2020-10-12 19:49:42 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 19:49:42 | INFO | train | epoch 002 | loss 12.148 | nll_loss 11.808 | ppl 3586.8 | wps 12382.3 | ups 2.15 | wpb 5750.2 | bsz 216.1 | num_updates 240 | lr 1.2094e-05 | gnorm 1.934 | clip 0 | train_wall 32 | wall 94
2020-10-12 19:49:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 19:49:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 19:49:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18254.71875Mb; avail=470419.0859375Mb
2020-10-12 19:49:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000846
2020-10-12 19:49:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005670
2020-10-12 19:49:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18261.37890625Mb; avail=470413.03125Mb
2020-10-12 19:49:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000216
2020-10-12 19:49:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18261.984375Mb; avail=470412.42578125Mb
2020-10-12 19:49:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093435
2020-10-12 19:49:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100194
2020-10-12 19:49:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18344.05078125Mb; avail=470330.453125Mb
2020-10-12 19:49:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18345.859375Mb; avail=470328.421875Mb
2020-10-12 19:49:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003838
2020-10-12 19:49:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18345.859375Mb; avail=470328.421875Mb
2020-10-12 19:49:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000218
2020-10-12 19:49:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18345.3671875Mb; avail=470328.9140625Mb
2020-10-12 19:49:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092362
2020-10-12 19:49:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097183
2020-10-12 19:49:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18345.90234375Mb; avail=470328.359375Mb
2020-10-12 19:49:43 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 19:49:59 | INFO | train_inner | epoch 003:     60 / 120 loss=11.555, nll_loss=11.146, ppl=2266.73, wps=11691.5, ups=2.02, wpb=5791.2, bsz=210, num_updates=300, lr=1.50925e-05, gnorm=1.677, clip=0, train_wall=26, wall=110
2020-10-12 19:50:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17830.91796875Mb; avail=470843.6015625Mb
2020-10-12 19:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001616
2020-10-12 19:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17830.91796875Mb; avail=470843.6015625Mb
2020-10-12 19:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067863
2020-10-12 19:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17830.34765625Mb; avail=470844.08984375Mb
2020-10-12 19:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049248
2020-10-12 19:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119557
2020-10-12 19:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17830.11328125Mb; avail=470844.32421875Mb
2020-10-12 19:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17830.09375Mb; avail=470844.56640625Mb
2020-10-12 19:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001161
2020-10-12 19:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17830.09375Mb; avail=470844.56640625Mb
2020-10-12 19:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068296
2020-10-12 19:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17829.19921875Mb; avail=470845.30078125Mb
2020-10-12 19:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049098
2020-10-12 19:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119402
2020-10-12 19:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17829.40625Mb; avail=470844.62890625Mb
2020-10-12 19:50:18 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.983 | nll_loss 9.325 | ppl 641.46 | wps 57351.9 | wpb 2252.3 | bsz 85.2 | num_updates 360 | best_loss 9.983
2020-10-12 19:50:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:50:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 3 @ 360 updates, score 9.983) (writing took 6.687995235999551 seconds)
2020-10-12 19:50:25 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 19:50:25 | INFO | train | epoch 003 | loss 11.039 | nll_loss 10.564 | ppl 1514.05 | wps 16288.7 | ups 2.83 | wpb 5750.2 | bsz 216.1 | num_updates 360 | lr 1.8091e-05 | gnorm 1.69 | clip 0 | train_wall 32 | wall 136
2020-10-12 19:50:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 19:50:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 19:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17812.7109375Mb; avail=470862.50390625Mb
2020-10-12 19:50:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001031
2020-10-12 19:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005690
2020-10-12 19:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17812.21875Mb; avail=470863.2421875Mb
2020-10-12 19:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000200
2020-10-12 19:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.7265625Mb; avail=470863.734375Mb
2020-10-12 19:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091746
2020-10-12 19:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098502
2020-10-12 19:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.33203125Mb; avail=470870.5859375Mb
2020-10-12 19:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17810.9921875Mb; avail=470870.32421875Mb
2020-10-12 19:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003991
2020-10-12 19:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17810.953125Mb; avail=470870.13671875Mb
2020-10-12 19:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 19:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.55859375Mb; avail=470870.0234375Mb
2020-10-12 19:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090495
2020-10-12 19:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095539
2020-10-12 19:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.2421875Mb; avail=470868.92578125Mb
2020-10-12 19:50:25 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 19:50:36 | INFO | train_inner | epoch 004:     40 / 120 loss=10.451, nll_loss=9.89, ppl=948.62, wps=15626.5, ups=2.7, wpb=5790.3, bsz=227.6, num_updates=400, lr=2.009e-05, gnorm=1.671, clip=0, train_wall=27, wall=147
2020-10-12 19:50:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17813.20703125Mb; avail=470860.62890625Mb
2020-10-12 19:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002148
2020-10-12 19:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.20703125Mb; avail=470860.62890625Mb
2020-10-12 19:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073416
2020-10-12 19:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.1484375Mb; avail=470860.9921875Mb
2020-10-12 19:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054515
2020-10-12 19:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130932
2020-10-12 19:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17810.75Mb; avail=470863.57421875Mb
2020-10-12 19:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17809.6484375Mb; avail=470864.0703125Mb
2020-10-12 19:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001155
2020-10-12 19:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.46484375Mb; avail=470862.859375Mb
2020-10-12 19:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070053
2020-10-12 19:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.078125Mb; avail=470780.5390625Mb
2020-10-12 19:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050154
2020-10-12 19:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122147
2020-10-12 19:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17958.453125Mb; avail=470715.875Mb
2020-10-12 19:51:00 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.267 | nll_loss 8.442 | ppl 347.73 | wps 57233.8 | wpb 2252.3 | bsz 85.2 | num_updates 480 | best_loss 9.267
2020-10-12 19:51:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:51:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 4 @ 480 updates, score 9.267) (writing took 6.6014850179999485 seconds)
2020-10-12 19:51:07 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 19:51:07 | INFO | train | epoch 004 | loss 9.82 | nll_loss 9.146 | ppl 566.42 | wps 16338.9 | ups 2.84 | wpb 5750.2 | bsz 216.1 | num_updates 480 | lr 2.4088e-05 | gnorm 1.332 | clip 0 | train_wall 32 | wall 178
2020-10-12 19:51:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 19:51:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 19:51:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17834.38671875Mb; avail=470839.16015625Mb
2020-10-12 19:51:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000929
2020-10-12 19:51:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005686
2020-10-12 19:51:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.9921875Mb; avail=470838.5546875Mb
2020-10-12 19:51:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000216
2020-10-12 19:51:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.9921875Mb; avail=470838.5546875Mb
2020-10-12 19:51:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090405
2020-10-12 19:51:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097047
2020-10-12 19:51:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.94921875Mb; avail=470838.91796875Mb
2020-10-12 19:51:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17835.00390625Mb; avail=470838.796875Mb
2020-10-12 19:51:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003632
2020-10-12 19:51:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.00390625Mb; avail=470838.796875Mb
2020-10-12 19:51:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 19:51:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.00390625Mb; avail=470838.796875Mb
2020-10-12 19:51:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095233
2020-10-12 19:51:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099783
2020-10-12 19:51:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.06640625Mb; avail=470838.5546875Mb
2020-10-12 19:51:07 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 19:51:13 | INFO | train_inner | epoch 005:     20 / 120 loss=9.629, nll_loss=8.915, ppl=482.81, wps=15590.8, ups=2.72, wpb=5732.9, bsz=210.8, num_updates=500, lr=2.50875e-05, gnorm=1.214, clip=0, train_wall=26, wall=184
2020-10-12 19:51:40 | INFO | train_inner | epoch 005:    120 / 120 loss=9.292, nll_loss=8.491, ppl=359.7, wps=20848.5, ups=3.65, wpb=5708.6, bsz=212.7, num_updates=600, lr=3.0085e-05, gnorm=1.435, clip=0, train_wall=26, wall=212
2020-10-12 19:51:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17745.20703125Mb; avail=470928.75390625Mb
2020-10-12 19:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001603
2020-10-12 19:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17745.8125Mb; avail=470928.1484375Mb
2020-10-12 19:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066912
2020-10-12 19:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17760.57421875Mb; avail=470913.3359375Mb
2020-10-12 19:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048287
2020-10-12 19:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117723
2020-10-12 19:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17762.88671875Mb; avail=470910.5546875Mb
2020-10-12 19:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17762.92578125Mb; avail=470910.19140625Mb
2020-10-12 19:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001149
2020-10-12 19:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17762.890625Mb; avail=470910.19140625Mb
2020-10-12 19:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067384
2020-10-12 19:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17763.07421875Mb; avail=470910.43359375Mb
2020-10-12 19:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047905
2020-10-12 19:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117200
2020-10-12 19:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17763.5234375Mb; avail=470910.23828125Mb
2020-10-12 19:51:43 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.016 | nll_loss 8.108 | ppl 275.98 | wps 57153.6 | wpb 2252.3 | bsz 85.2 | num_updates 600 | best_loss 9.016
2020-10-12 19:51:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:51:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 5 @ 600 updates, score 9.016) (writing took 6.860882799000137 seconds)
2020-10-12 19:51:50 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 19:51:50 | INFO | train | epoch 005 | loss 9.316 | nll_loss 8.523 | ppl 367.73 | wps 16203.9 | ups 2.82 | wpb 5750.2 | bsz 216.1 | num_updates 600 | lr 3.0085e-05 | gnorm 1.393 | clip 0 | train_wall 32 | wall 221
2020-10-12 19:51:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 19:51:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 19:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17750.03515625Mb; avail=470926.1328125Mb
2020-10-12 19:51:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000740
2020-10-12 19:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005487
2020-10-12 19:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17750.47265625Mb; avail=470925.60546875Mb
2020-10-12 19:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000205
2020-10-12 19:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17750.34375Mb; avail=470926.09765625Mb
2020-10-12 19:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090293
2020-10-12 19:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096840
2020-10-12 19:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17750.7421875Mb; avail=470924.6875Mb
2020-10-12 19:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17749.90234375Mb; avail=470923.8828125Mb
2020-10-12 19:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003913
2020-10-12 19:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17749.86328125Mb; avail=470923.6953125Mb
2020-10-12 19:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000210
2020-10-12 19:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17750.46875Mb; avail=470924.1875Mb
2020-10-12 19:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090340
2020-10-12 19:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095316
2020-10-12 19:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17750.640625Mb; avail=470923.3671875Mb
2020-10-12 19:51:50 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 19:52:18 | INFO | train_inner | epoch 006:    100 / 120 loss=9.106, nll_loss=8.253, ppl=305.05, wps=15371, ups=2.68, wpb=5733.2, bsz=216.1, num_updates=700, lr=3.50825e-05, gnorm=1.517, clip=0, train_wall=27, wall=249
2020-10-12 19:52:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17752.4296875Mb; avail=470921.36328125Mb
2020-10-12 19:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001612
2020-10-12 19:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17752.4296875Mb; avail=470921.36328125Mb
2020-10-12 19:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068438
2020-10-12 19:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17752.4296875Mb; avail=470921.36328125Mb
2020-10-12 19:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059118
2020-10-12 19:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129958
2020-10-12 19:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17752.28125Mb; avail=470921.12109375Mb
2020-10-12 19:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17752.359375Mb; avail=470921.2421875Mb
2020-10-12 19:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001104
2020-10-12 19:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17752.359375Mb; avail=470921.2421875Mb
2020-10-12 19:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067350
2020-10-12 19:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17752.25390625Mb; avail=470921.453125Mb
2020-10-12 19:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047836
2020-10-12 19:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117046
2020-10-12 19:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17752.359375Mb; avail=470921.4375Mb
2020-10-12 19:52:26 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.808 | nll_loss 7.857 | ppl 231.88 | wps 56898.1 | wpb 2252.3 | bsz 85.2 | num_updates 720 | best_loss 8.808
2020-10-12 19:52:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:52:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 6 @ 720 updates, score 8.808) (writing took 4.428831055000046 seconds)
2020-10-12 19:52:30 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 19:52:30 | INFO | train | epoch 006 | loss 9.105 | nll_loss 8.251 | ppl 304.58 | wps 17068.1 | ups 2.97 | wpb 5750.2 | bsz 216.1 | num_updates 720 | lr 3.6082e-05 | gnorm 1.473 | clip 0 | train_wall 32 | wall 261
2020-10-12 19:52:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 19:52:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 19:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17754.7421875Mb; avail=470918.5546875Mb
2020-10-12 19:52:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000798
2020-10-12 19:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005575
2020-10-12 19:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17754.7421875Mb; avail=470918.5546875Mb
2020-10-12 19:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 19:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17754.7421875Mb; avail=470918.5546875Mb
2020-10-12 19:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091896
2020-10-12 19:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098454
2020-10-12 19:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17754.7109375Mb; avail=470918.5546875Mb
2020-10-12 19:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17754.84765625Mb; avail=470918.43359375Mb
2020-10-12 19:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003772
2020-10-12 19:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17754.84765625Mb; avail=470918.43359375Mb
2020-10-12 19:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000174
2020-10-12 19:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17754.84765625Mb; avail=470918.43359375Mb
2020-10-12 19:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090542
2020-10-12 19:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095227
2020-10-12 19:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17754.84765625Mb; avail=470918.43359375Mb
2020-10-12 19:52:30 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 19:52:53 | INFO | train_inner | epoch 007:     80 / 120 loss=8.994, nll_loss=8.116, ppl=277.47, wps=16669.2, ups=2.85, wpb=5839.1, bsz=228.7, num_updates=800, lr=4.008e-05, gnorm=1.438, clip=0, train_wall=27, wall=284
2020-10-12 19:53:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17945.03125Mb; avail=470728.796875Mb
2020-10-12 19:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001627
2020-10-12 19:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17945.03125Mb; avail=470728.796875Mb
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069125
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17944.90625Mb; avail=470728.91796875Mb
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048827
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120408
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17945.14453125Mb; avail=470728.67578125Mb
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17945.14453125Mb; avail=470728.67578125Mb
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001148
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17945.1875Mb; avail=470728.5546875Mb
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068717
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17945.12890625Mb; avail=470728.67578125Mb
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048930
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119615
2020-10-12 19:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17945.171875Mb; avail=470728.43359375Mb
2020-10-12 19:53:06 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.65 | nll_loss 7.673 | ppl 204.07 | wps 56594.5 | wpb 2252.3 | bsz 85.2 | num_updates 840 | best_loss 8.65
2020-10-12 19:53:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:53:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 7 @ 840 updates, score 8.65) (writing took 4.448433447999378 seconds)
2020-10-12 19:53:10 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 19:53:10 | INFO | train | epoch 007 | loss 8.94 | nll_loss 8.054 | ppl 265.75 | wps 17118.8 | ups 2.98 | wpb 5750.2 | bsz 216.1 | num_updates 840 | lr 4.2079e-05 | gnorm 1.467 | clip 0 | train_wall 32 | wall 302
2020-10-12 19:53:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 19:53:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 19:53:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17962.02734375Mb; avail=470711.24609375Mb
2020-10-12 19:53:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000821
2020-10-12 19:53:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005449
2020-10-12 19:53:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17962.02734375Mb; avail=470711.24609375Mb
2020-10-12 19:53:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000190
2020-10-12 19:53:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17962.02734375Mb; avail=470711.24609375Mb
2020-10-12 19:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091070
2020-10-12 19:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097462
2020-10-12 19:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17961.9375Mb; avail=470711.125Mb
2020-10-12 19:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17962.11328125Mb; avail=470711.24609375Mb
2020-10-12 19:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003733
2020-10-12 19:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17962.015625Mb; avail=470711.3671875Mb
2020-10-12 19:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 19:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17962.015625Mb; avail=470711.3671875Mb
2020-10-12 19:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089609
2020-10-12 19:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094268
2020-10-12 19:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17962.3984375Mb; avail=470710.8828125Mb
2020-10-12 19:53:11 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 19:53:27 | INFO | train_inner | epoch 008:     60 / 120 loss=8.877, nll_loss=7.98, ppl=252.54, wps=16441.6, ups=2.88, wpb=5703.8, bsz=197.4, num_updates=900, lr=4.50775e-05, gnorm=1.473, clip=0, train_wall=27, wall=319
2020-10-12 19:53:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17741.4609375Mb; avail=470931.98046875Mb
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001560
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17741.4609375Mb; avail=470931.98046875Mb
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068590
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17741.39453125Mb; avail=470931.859375Mb
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047163
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118136
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17741.46875Mb; avail=470931.98046875Mb
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17741.515625Mb; avail=470931.73828125Mb
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001206
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17741.515625Mb; avail=470931.73828125Mb
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066329
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17741.3984375Mb; avail=470932.1015625Mb
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046206
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.114518
2020-10-12 19:53:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17741.3984375Mb; avail=470932.1015625Mb
2020-10-12 19:53:47 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.479 | nll_loss 7.478 | ppl 178.32 | wps 56601 | wpb 2252.3 | bsz 85.2 | num_updates 960 | best_loss 8.479
2020-10-12 19:53:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:53:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 8 @ 960 updates, score 8.479) (writing took 4.445340487000067 seconds)
2020-10-12 19:53:51 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 19:53:51 | INFO | train | epoch 008 | loss 8.768 | nll_loss 7.857 | ppl 231.78 | wps 16960.6 | ups 2.95 | wpb 5750.2 | bsz 216.1 | num_updates 960 | lr 4.8076e-05 | gnorm 1.483 | clip 0 | train_wall 32 | wall 342
2020-10-12 19:53:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 19:53:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 19:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17762.64453125Mb; avail=470910.046875Mb
2020-10-12 19:53:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000777
2020-10-12 19:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005231
2020-10-12 19:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17762.64453125Mb; avail=470910.046875Mb
2020-10-12 19:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 19:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17762.64453125Mb; avail=470910.046875Mb
2020-10-12 19:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091332
2020-10-12 19:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097513
2020-10-12 19:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17762.60546875Mb; avail=470910.16796875Mb
2020-10-12 19:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17762.60546875Mb; avail=470910.16796875Mb
2020-10-12 19:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003829
2020-10-12 19:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17762.60546875Mb; avail=470910.16796875Mb
2020-10-12 19:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 19:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17762.60546875Mb; avail=470910.16796875Mb
2020-10-12 19:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091115
2020-10-12 19:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095870
2020-10-12 19:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17762.63671875Mb; avail=470910.046875Mb
2020-10-12 19:53:51 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 19:54:03 | INFO | train_inner | epoch 009:     40 / 120 loss=8.655, nll_loss=7.729, ppl=212.15, wps=16395.2, ups=2.84, wpb=5778.4, bsz=234.9, num_updates=1000, lr=5.0075e-05, gnorm=1.434, clip=0, train_wall=27, wall=354
2020-10-12 19:54:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:54:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17750.6015625Mb; avail=470923.22265625Mb
2020-10-12 19:54:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001626
2020-10-12 19:54:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17750.6015625Mb; avail=470923.22265625Mb
2020-10-12 19:54:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069018
2020-10-12 19:54:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17750.6875Mb; avail=470923.328125Mb
2020-10-12 19:54:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048383
2020-10-12 19:54:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119821
2020-10-12 19:54:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17750.625Mb; avail=470923.20703125Mb
2020-10-12 19:54:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17750.61328125Mb; avail=470923.19140625Mb
2020-10-12 19:54:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001202
2020-10-12 19:54:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17750.61328125Mb; avail=470923.19140625Mb
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067088
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17750.6171875Mb; avail=470923.40234375Mb
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047721
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116833
2020-10-12 19:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17750.58984375Mb; avail=470923.40234375Mb
2020-10-12 19:54:27 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.305 | nll_loss 7.276 | ppl 154.94 | wps 57043.7 | wpb 2252.3 | bsz 85.2 | num_updates 1080 | best_loss 8.305
2020-10-12 19:54:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:54:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 9 @ 1080 updates, score 8.305) (writing took 4.413865384000019 seconds)
2020-10-12 19:54:31 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 19:54:31 | INFO | train | epoch 009 | loss 8.592 | nll_loss 7.655 | ppl 201.62 | wps 17199.6 | ups 2.99 | wpb 5750.2 | bsz 216.1 | num_updates 1080 | lr 5.4073e-05 | gnorm 1.411 | clip 0 | train_wall 32 | wall 382
2020-10-12 19:54:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 19:54:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 19:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17769.51171875Mb; avail=470903.65625Mb
2020-10-12 19:54:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000821
2020-10-12 19:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005478
2020-10-12 19:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17769.51171875Mb; avail=470903.65625Mb
2020-10-12 19:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000223
2020-10-12 19:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17769.51171875Mb; avail=470903.65625Mb
2020-10-12 19:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.097526
2020-10-12 19:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.104049
2020-10-12 19:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17770.1171875Mb; avail=470903.05078125Mb
2020-10-12 19:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17770.265625Mb; avail=470903.30859375Mb
2020-10-12 19:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003806
2020-10-12 19:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17770.265625Mb; avail=470903.30859375Mb
2020-10-12 19:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 19:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17770.265625Mb; avail=470903.30859375Mb
2020-10-12 19:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090710
2020-10-12 19:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095446
2020-10-12 19:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17770.15625Mb; avail=470903.4296875Mb
2020-10-12 19:54:31 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 19:54:37 | INFO | train_inner | epoch 010:     20 / 120 loss=8.538, nll_loss=7.592, ppl=192.97, wps=16469.3, ups=2.9, wpb=5672.7, bsz=202.1, num_updates=1100, lr=5.50725e-05, gnorm=1.449, clip=0, train_wall=26, wall=388
2020-10-12 19:55:05 | INFO | train_inner | epoch 010:    120 / 120 loss=8.439, nll_loss=7.481, ppl=178.61, wps=20724.5, ups=3.59, wpb=5774.2, bsz=217.2, num_updates=1200, lr=6.007e-05, gnorm=1.365, clip=0, train_wall=27, wall=416
2020-10-12 19:55:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17773.76171875Mb; avail=470900.0Mb
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001538
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17773.76171875Mb; avail=470900.0Mb
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067340
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17776.0703125Mb; avail=470897.69140625Mb
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047415
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117083
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17787.0625Mb; avail=470886.6484375Mb
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17791.0546875Mb; avail=470882.2890625Mb
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001176
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17791.0546875Mb; avail=470882.2890625Mb
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070583
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17791.921875Mb; avail=470881.3203125Mb
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046924
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119480
2020-10-12 19:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17791.58984375Mb; avail=470881.5625Mb
2020-10-12 19:55:07 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.161 | nll_loss 7.104 | ppl 137.6 | wps 56753.8 | wpb 2252.3 | bsz 85.2 | num_updates 1200 | best_loss 8.161
2020-10-12 19:55:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:55:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 10 @ 1200 updates, score 8.161) (writing took 4.453991695999321 seconds)
2020-10-12 19:55:12 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 19:55:12 | INFO | train | epoch 010 | loss 8.438 | nll_loss 7.479 | ppl 178.38 | wps 16984.4 | ups 2.95 | wpb 5750.2 | bsz 216.1 | num_updates 1200 | lr 6.007e-05 | gnorm 1.396 | clip 0 | train_wall 32 | wall 423
2020-10-12 19:55:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 19:55:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 19:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17815.53125Mb; avail=470857.9453125Mb
2020-10-12 19:55:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000801
2020-10-12 19:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005591
2020-10-12 19:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17815.53125Mb; avail=470857.9453125Mb
2020-10-12 19:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000208
2020-10-12 19:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17815.53125Mb; avail=470857.9453125Mb
2020-10-12 19:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092330
2020-10-12 19:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098975
2020-10-12 19:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.50390625Mb; avail=470857.08984375Mb
2020-10-12 19:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17816.46875Mb; avail=470857.33203125Mb
2020-10-12 19:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003776
2020-10-12 19:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.48046875Mb; avail=470857.2109375Mb
2020-10-12 19:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 19:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.48046875Mb; avail=470857.2109375Mb
2020-10-12 19:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089767
2020-10-12 19:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094466
2020-10-12 19:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.6015625Mb; avail=470857.08984375Mb
2020-10-12 19:55:12 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 19:55:40 | INFO | train_inner | epoch 011:    100 / 120 loss=8.306, nll_loss=7.328, ppl=160.68, wps=16522, ups=2.85, wpb=5796, bsz=217.5, num_updates=1300, lr=6.50675e-05, gnorm=1.422, clip=0, train_wall=27, wall=451
2020-10-12 19:55:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17840.9140625Mb; avail=470831.53125Mb
2020-10-12 19:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001743
2020-10-12 19:55:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17840.9140625Mb; avail=470831.53125Mb
2020-10-12 19:55:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067474
2020-10-12 19:55:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17840.65234375Mb; avail=470831.89453125Mb
2020-10-12 19:55:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049450
2020-10-12 19:55:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119498
2020-10-12 19:55:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17840.99609375Mb; avail=470831.65234375Mb
2020-10-12 19:55:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17841.73828125Mb; avail=470830.8046875Mb
2020-10-12 19:55:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001177
2020-10-12 19:55:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.7578125Mb; avail=470830.68359375Mb
2020-10-12 19:55:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067510
2020-10-12 19:55:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.4921875Mb; avail=470830.44921875Mb
2020-10-12 19:55:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048701
2020-10-12 19:55:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118212
2020-10-12 19:55:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.8515625Mb; avail=470830.0859375Mb
2020-10-12 19:55:48 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.044 | nll_loss 6.98 | ppl 126.23 | wps 55054.9 | wpb 2252.3 | bsz 85.2 | num_updates 1320 | best_loss 8.044
2020-10-12 19:55:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:55:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 11 @ 1320 updates, score 8.044) (writing took 7.188396890000149 seconds)
2020-10-12 19:55:55 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 19:55:55 | INFO | train | epoch 011 | loss 8.301 | nll_loss 7.323 | ppl 160.08 | wps 15919.4 | ups 2.77 | wpb 5750.2 | bsz 216.1 | num_updates 1320 | lr 6.6067e-05 | gnorm 1.394 | clip 0 | train_wall 32 | wall 466
2020-10-12 19:55:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 19:55:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 19:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17850.3203125Mb; avail=470826.02734375Mb
2020-10-12 19:55:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000756
2020-10-12 19:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005326
2020-10-12 19:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.91796875Mb; avail=470826.10546875Mb
2020-10-12 19:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 19:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.5234375Mb; avail=470825.9921875Mb
2020-10-12 19:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090608
2020-10-12 19:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096954
2020-10-12 19:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.1640625Mb; avail=470824.21484375Mb
2020-10-12 19:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17850.421875Mb; avail=470824.26171875Mb
2020-10-12 19:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003922
2020-10-12 19:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.3828125Mb; avail=470824.07421875Mb
2020-10-12 19:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 19:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.98828125Mb; avail=470824.20703125Mb
2020-10-12 19:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091686
2020-10-12 19:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096647
2020-10-12 19:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.859375Mb; avail=470822.72265625Mb
2020-10-12 19:55:55 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 19:56:17 | INFO | train_inner | epoch 012:     80 / 120 loss=8.182, nll_loss=7.187, ppl=145.69, wps=15295.1, ups=2.68, wpb=5710.6, bsz=213.5, num_updates=1400, lr=7.0065e-05, gnorm=1.314, clip=0, train_wall=26, wall=489
2020-10-12 19:56:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17852.05859375Mb; avail=470821.1640625Mb
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001588
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.05859375Mb; avail=470821.1640625Mb
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067125
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.44140625Mb; avail=470821.1640625Mb
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048460
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117951
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.44140625Mb; avail=470821.1640625Mb
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17853.01171875Mb; avail=470820.50390625Mb
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001148
2020-10-12 19:56:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.01171875Mb; avail=470820.50390625Mb
2020-10-12 19:56:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068340
2020-10-12 19:56:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.7265625Mb; avail=470819.65625Mb
2020-10-12 19:56:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048060
2020-10-12 19:56:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118338
2020-10-12 19:56:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.69921875Mb; avail=470819.50390625Mb
2020-10-12 19:56:31 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.968 | nll_loss 6.9 | ppl 119.45 | wps 57151.7 | wpb 2252.3 | bsz 85.2 | num_updates 1440 | best_loss 7.968
2020-10-12 19:56:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:56:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 12 @ 1440 updates, score 7.968) (writing took 4.429940240999713 seconds)
2020-10-12 19:56:35 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 19:56:35 | INFO | train | epoch 012 | loss 8.165 | nll_loss 7.168 | ppl 143.79 | wps 17233.9 | ups 3 | wpb 5750.2 | bsz 216.1 | num_updates 1440 | lr 7.2064e-05 | gnorm 1.347 | clip 0 | train_wall 32 | wall 507
2020-10-12 19:56:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 19:56:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 19:56:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18432.328125Mb; avail=470241.03515625Mb
2020-10-12 19:56:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000776
2020-10-12 19:56:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005576
2020-10-12 19:56:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18432.328125Mb; avail=470241.52734375Mb
2020-10-12 19:56:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000327
2020-10-12 19:56:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18432.328125Mb; avail=470241.7734375Mb
2020-10-12 19:56:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.135925
2020-10-12 19:56:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.143015
2020-10-12 19:56:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18431.84765625Mb; avail=470247.3828125Mb
2020-10-12 19:56:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18432.41796875Mb; avail=470247.25Mb
2020-10-12 19:56:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003707
2020-10-12 19:56:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18432.85546875Mb; avail=470246.94921875Mb
2020-10-12 19:56:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000176
2020-10-12 19:56:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18432.7265625Mb; avail=470246.8359375Mb
2020-10-12 19:56:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089364
2020-10-12 19:56:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.093999
2020-10-12 19:56:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18431.6640625Mb; avail=470246.0703125Mb
2020-10-12 19:56:36 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 19:56:52 | INFO | train_inner | epoch 013:     60 / 120 loss=8.107, nll_loss=7.1, ppl=137.19, wps=16440.4, ups=2.89, wpb=5697.9, bsz=212.3, num_updates=1500, lr=7.50625e-05, gnorm=1.374, clip=0, train_wall=26, wall=523
2020-10-12 19:57:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17815.73828125Mb; avail=470864.296875Mb
2020-10-12 19:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001688
2020-10-12 19:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.0859375Mb; avail=470864.5625Mb
2020-10-12 19:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068029
2020-10-12 19:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.19140625Mb; avail=470863.96875Mb
2020-10-12 19:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050189
2020-10-12 19:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120755
2020-10-12 19:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.203125Mb; avail=470861.69140625Mb
2020-10-12 19:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17817.7265625Mb; avail=470861.625Mb
2020-10-12 19:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001180
2020-10-12 19:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.9765625Mb; avail=470861.64453125Mb
2020-10-12 19:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068643
2020-10-12 19:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.08203125Mb; avail=470859.671875Mb
2020-10-12 19:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049719
2020-10-12 19:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120325
2020-10-12 19:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17818.671875Mb; avail=470858.83203125Mb
2020-10-12 19:57:11 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.817 | nll_loss 6.711 | ppl 104.75 | wps 56999.9 | wpb 2252.3 | bsz 85.2 | num_updates 1560 | best_loss 7.817
2020-10-12 19:57:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:57:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 13 @ 1560 updates, score 7.817) (writing took 9.236800222000056 seconds)
2020-10-12 19:57:21 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 19:57:21 | INFO | train | epoch 013 | loss 8.051 | nll_loss 7.036 | ppl 131.25 | wps 15235.5 | ups 2.65 | wpb 5750.2 | bsz 216.1 | num_updates 1560 | lr 7.8061e-05 | gnorm 1.421 | clip 0 | train_wall 32 | wall 552
2020-10-12 19:57:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 19:57:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 19:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18377.0078125Mb; avail=470296.8125Mb
2020-10-12 19:57:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000970
2020-10-12 19:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006704
2020-10-12 19:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18377.12109375Mb; avail=470296.69921875Mb
2020-10-12 19:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000309
2020-10-12 19:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18377.12109375Mb; avail=470296.69921875Mb
2020-10-12 19:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093049
2020-10-12 19:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101170
2020-10-12 19:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18377.40234375Mb; avail=470296.546875Mb
2020-10-12 19:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18377.40234375Mb; avail=470296.546875Mb
2020-10-12 19:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003848
2020-10-12 19:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18377.40234375Mb; avail=470296.546875Mb
2020-10-12 19:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 19:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18377.40234375Mb; avail=470296.546875Mb
2020-10-12 19:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090227
2020-10-12 19:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095014
2020-10-12 19:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18377.3046875Mb; avail=470296.65234375Mb
2020-10-12 19:57:21 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 19:57:32 | INFO | train_inner | epoch 014:     40 / 120 loss=8.002, nll_loss=6.98, ppl=126.26, wps=14688.8, ups=2.51, wpb=5852.1, bsz=232.1, num_updates=1600, lr=8.006e-05, gnorm=1.456, clip=0, train_wall=27, wall=563
2020-10-12 19:57:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17861.35546875Mb; avail=470811.203125Mb
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002021
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.35546875Mb; avail=470811.203125Mb
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067560
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.87109375Mb; avail=470811.12890625Mb
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047941
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118345
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.76953125Mb; avail=470811.12890625Mb
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17861.6875Mb; avail=470811.0078125Mb
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001063
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.6875Mb; avail=470811.0078125Mb
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066804
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.62890625Mb; avail=470811.25Mb
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047920
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116546
2020-10-12 19:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.7265625Mb; avail=470811.37109375Mb
2020-10-12 19:57:56 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.709 | nll_loss 6.598 | ppl 96.88 | wps 56763.7 | wpb 2252.3 | bsz 85.2 | num_updates 1680 | best_loss 7.709
2020-10-12 19:57:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:58:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 14 @ 1680 updates, score 7.709) (writing took 11.663926017999984 seconds)
2020-10-12 19:58:08 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 19:58:08 | INFO | train | epoch 014 | loss 7.935 | nll_loss 6.903 | ppl 119.68 | wps 14536.3 | ups 2.53 | wpb 5750.2 | bsz 216.1 | num_updates 1680 | lr 8.4058e-05 | gnorm 1.333 | clip 0 | train_wall 32 | wall 599
2020-10-12 19:58:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 19:58:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17832.69921875Mb; avail=470840.20703125Mb
2020-10-12 19:58:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001082
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005950
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17833.2890625Mb; avail=470839.72265625Mb
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000270
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17833.2890625Mb; avail=470839.72265625Mb
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092062
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099214
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17833.96875Mb; avail=470838.51171875Mb
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17834.1015625Mb; avail=470838.75390625Mb
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004823
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.0234375Mb; avail=470838.640625Mb
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000225
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.0234375Mb; avail=470838.640625Mb
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090577
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096397
2020-10-12 19:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.12890625Mb; avail=470838.8828125Mb
2020-10-12 19:58:08 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 19:58:14 | INFO | train_inner | epoch 015:     20 / 120 loss=7.916, nll_loss=6.881, ppl=117.83, wps=13387.4, ups=2.39, wpb=5602.5, bsz=199.8, num_updates=1700, lr=8.50575e-05, gnorm=1.309, clip=0, train_wall=26, wall=605
2020-10-12 19:58:41 | INFO | train_inner | epoch 015:    120 / 120 loss=7.823, nll_loss=6.774, ppl=109.45, wps=21198.3, ups=3.63, wpb=5842.1, bsz=221.1, num_updates=1800, lr=9.0055e-05, gnorm=1.336, clip=0, train_wall=27, wall=633
2020-10-12 19:58:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18369.3515625Mb; avail=470305.05859375Mb
2020-10-12 19:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001627
2020-10-12 19:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18369.78125Mb; avail=470304.953125Mb
2020-10-12 19:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067665
2020-10-12 19:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18369.16796875Mb; avail=470304.13671875Mb
2020-10-12 19:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047304
2020-10-12 19:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117396
2020-10-12 19:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18369.3984375Mb; avail=470304.12890625Mb
2020-10-12 19:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18369.390625Mb; avail=470304.25Mb
2020-10-12 19:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001126
2020-10-12 19:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18369.390625Mb; avail=470304.25Mb
2020-10-12 19:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066804
2020-10-12 19:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18369.37890625Mb; avail=470304.25Mb
2020-10-12 19:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046976
2020-10-12 19:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115689
2020-10-12 19:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18369.359375Mb; avail=470304.25Mb
2020-10-12 19:58:44 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.624 | nll_loss 6.495 | ppl 90.17 | wps 56790.9 | wpb 2252.3 | bsz 85.2 | num_updates 1800 | best_loss 7.624
2020-10-12 19:58:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:58:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 15 @ 1800 updates, score 7.624) (writing took 7.444906737000565 seconds)
2020-10-12 19:58:51 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 19:58:51 | INFO | train | epoch 015 | loss 7.826 | nll_loss 6.778 | ppl 109.74 | wps 15980.5 | ups 2.78 | wpb 5750.2 | bsz 216.1 | num_updates 1800 | lr 9.0055e-05 | gnorm 1.348 | clip 0 | train_wall 32 | wall 642
2020-10-12 19:58:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 19:58:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 19:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17812.83203125Mb; avail=470859.86328125Mb
2020-10-12 19:58:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000907
2020-10-12 19:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005695
2020-10-12 19:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.4375Mb; avail=470859.2578125Mb
2020-10-12 19:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000222
2020-10-12 19:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.4375Mb; avail=470859.2578125Mb
2020-10-12 19:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093643
2020-10-12 19:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100307
2020-10-12 19:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.41015625Mb; avail=470859.5Mb
2020-10-12 19:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17813.4375Mb; avail=470859.2578125Mb
2020-10-12 19:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003801
2020-10-12 19:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.1953125Mb; avail=470859.62109375Mb
2020-10-12 19:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 19:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.1953125Mb; avail=470859.62109375Mb
2020-10-12 19:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090944
2020-10-12 19:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095709
2020-10-12 19:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.3359375Mb; avail=470859.13671875Mb
2020-10-12 19:58:51 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 19:59:19 | INFO | train_inner | epoch 016:    100 / 120 loss=7.715, nll_loss=6.652, ppl=100.56, wps=15276.5, ups=2.64, wpb=5779.6, bsz=219.8, num_updates=1900, lr=9.50525e-05, gnorm=1.28, clip=0, train_wall=27, wall=670
2020-10-12 19:59:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 19:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19066.7265625Mb; avail=469606.83984375Mb
2020-10-12 19:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001614
2020-10-12 19:59:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19066.7265625Mb; avail=469606.83984375Mb
2020-10-12 19:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067158
2020-10-12 19:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19066.75Mb; avail=469607.203125Mb
2020-10-12 19:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047781
2020-10-12 19:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117349
2020-10-12 19:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19066.75Mb; avail=469607.203125Mb
2020-10-12 19:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19066.75Mb; avail=469607.203125Mb
2020-10-12 19:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001101
2020-10-12 19:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19066.75Mb; avail=469607.203125Mb
2020-10-12 19:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068491
2020-10-12 19:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19066.74609375Mb; avail=469607.203125Mb
2020-10-12 19:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047141
2020-10-12 19:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117644
2020-10-12 19:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19066.66796875Mb; avail=469607.08203125Mb
2020-10-12 19:59:27 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.542 | nll_loss 6.399 | ppl 84.41 | wps 57014.2 | wpb 2252.3 | bsz 85.2 | num_updates 1920 | best_loss 7.542
2020-10-12 19:59:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 19:59:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 16 @ 1920 updates, score 7.542) (writing took 9.573683293000613 seconds)
2020-10-12 19:59:37 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 19:59:37 | INFO | train | epoch 016 | loss 7.716 | nll_loss 6.652 | ppl 100.58 | wps 15215.6 | ups 2.65 | wpb 5750.2 | bsz 216.1 | num_updates 1920 | lr 9.6052e-05 | gnorm 1.287 | clip 0 | train_wall 32 | wall 688
2020-10-12 19:59:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 19:59:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 19:59:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17829.67578125Mb; avail=470842.75Mb
2020-10-12 19:59:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001108
2020-10-12 19:59:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005716
2020-10-12 19:59:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17829.65625Mb; avail=470842.87109375Mb
2020-10-12 19:59:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000222
2020-10-12 19:59:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17829.77734375Mb; avail=470842.75Mb
2020-10-12 19:59:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091945
2020-10-12 19:59:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098629
2020-10-12 19:59:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17830.00390625Mb; avail=470842.265625Mb
2020-10-12 19:59:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17830.02734375Mb; avail=470842.62890625Mb
2020-10-12 19:59:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003744
2020-10-12 19:59:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17830.0546875Mb; avail=470842.5078125Mb
2020-10-12 19:59:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 19:59:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17830.0546875Mb; avail=470842.5078125Mb
2020-10-12 19:59:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090332
2020-10-12 19:59:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095046
2020-10-12 19:59:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.98046875Mb; avail=470845.58203125Mb
2020-10-12 19:59:37 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 19:59:59 | INFO | train_inner | epoch 017:     80 / 120 loss=7.642, nll_loss=6.567, ppl=94.82, wps=14299.2, ups=2.52, wpb=5671.1, bsz=212.2, num_updates=2000, lr=0.00010005, gnorm=1.351, clip=0, train_wall=26, wall=710
2020-10-12 20:00:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17788.16796875Mb; avail=470884.9609375Mb
2020-10-12 20:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001594
2020-10-12 20:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17788.16796875Mb; avail=470884.9609375Mb
2020-10-12 20:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067370
2020-10-12 20:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17788.30859375Mb; avail=470884.83984375Mb
2020-10-12 20:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049181
2020-10-12 20:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118970
2020-10-12 20:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17788.30859375Mb; avail=470884.83984375Mb
2020-10-12 20:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17788.20703125Mb; avail=470884.90234375Mb
2020-10-12 20:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001158
2020-10-12 20:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17788.20703125Mb; avail=470884.90234375Mb
2020-10-12 20:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066755
2020-10-12 20:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17788.0859375Mb; avail=470885.0234375Mb
2020-10-12 20:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048700
2020-10-12 20:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117406
2020-10-12 20:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17788.3828125Mb; avail=470884.75390625Mb
2020-10-12 20:00:12 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.487 | nll_loss 6.342 | ppl 81.1 | wps 56619 | wpb 2252.3 | bsz 85.2 | num_updates 2040 | best_loss 7.487
2020-10-12 20:00:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:00:17 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 17 @ 2040 updates, score 7.487) (writing took 4.455117960999814 seconds)
2020-10-12 20:00:17 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 20:00:17 | INFO | train | epoch 017 | loss 7.63 | nll_loss 6.553 | ppl 93.91 | wps 17153.3 | ups 2.98 | wpb 5750.2 | bsz 216.1 | num_updates 2040 | lr 0.000102049 | gnorm 1.381 | clip 0 | train_wall 32 | wall 728
2020-10-12 20:00:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 20:00:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 20:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17788.69921875Mb; avail=470884.8984375Mb
2020-10-12 20:00:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001368
2020-10-12 20:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006991
2020-10-12 20:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17789.41796875Mb; avail=470884.1796875Mb
2020-10-12 20:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000237
2020-10-12 20:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17789.41796875Mb; avail=470884.1796875Mb
2020-10-12 20:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091867
2020-10-12 20:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100061
2020-10-12 20:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17789.71484375Mb; avail=470884.0Mb
2020-10-12 20:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17789.80078125Mb; avail=470883.7265625Mb
2020-10-12 20:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003695
2020-10-12 20:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17789.80078125Mb; avail=470883.7265625Mb
2020-10-12 20:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 20:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17789.80078125Mb; avail=470883.7265625Mb
2020-10-12 20:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092255
2020-10-12 20:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096911
2020-10-12 20:00:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17789.84375Mb; avail=470883.2421875Mb
2020-10-12 20:00:17 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 20:00:34 | INFO | train_inner | epoch 018:     60 / 120 loss=7.584, nll_loss=6.5, ppl=90.51, wps=16377.3, ups=2.87, wpb=5705, bsz=207.3, num_updates=2100, lr=0.000105048, gnorm=1.335, clip=0, train_wall=27, wall=745
2020-10-12 20:00:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18408.64453125Mb; avail=470265.0859375Mb
2020-10-12 20:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001927
2020-10-12 20:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18408.64453125Mb; avail=470265.0859375Mb
2020-10-12 20:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067968
2020-10-12 20:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18408.64453125Mb; avail=470265.0859375Mb
2020-10-12 20:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046695
2020-10-12 20:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117400
2020-10-12 20:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18408.7109375Mb; avail=470264.84375Mb
2020-10-12 20:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18408.66796875Mb; avail=470265.0859375Mb
2020-10-12 20:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001149
2020-10-12 20:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18408.66796875Mb; avail=470265.0859375Mb
2020-10-12 20:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066287
2020-10-12 20:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18408.66796875Mb; avail=470265.0859375Mb
2020-10-12 20:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047118
2020-10-12 20:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115308
2020-10-12 20:00:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18408.68359375Mb; avail=470265.0859375Mb
2020-10-12 20:00:53 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.427 | nll_loss 6.265 | ppl 76.91 | wps 57504.8 | wpb 2252.3 | bsz 85.2 | num_updates 2160 | best_loss 7.427
2020-10-12 20:00:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:01:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 18 @ 2160 updates, score 7.427) (writing took 8.16049887600002 seconds)
2020-10-12 20:01:01 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 20:01:01 | INFO | train | epoch 018 | loss 7.53 | nll_loss 6.439 | ppl 86.75 | wps 15673.6 | ups 2.73 | wpb 5750.2 | bsz 216.1 | num_updates 2160 | lr 0.000108046 | gnorm 1.292 | clip 0 | train_wall 32 | wall 772
2020-10-12 20:01:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 20:01:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 20:01:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17877.7890625Mb; avail=470795.24609375Mb
2020-10-12 20:01:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000964
2020-10-12 20:01:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005902
2020-10-12 20:01:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.7890625Mb; avail=470795.24609375Mb
2020-10-12 20:01:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000207
2020-10-12 20:01:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.7890625Mb; avail=470795.24609375Mb
2020-10-12 20:01:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090472
2020-10-12 20:01:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097337
2020-10-12 20:01:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17878.30078125Mb; avail=470794.640625Mb
2020-10-12 20:01:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17878.17578125Mb; avail=470794.76171875Mb
2020-10-12 20:01:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003763
2020-10-12 20:01:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17878.17578125Mb; avail=470794.76171875Mb
2020-10-12 20:01:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 20:01:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17878.17578125Mb; avail=470794.76171875Mb
2020-10-12 20:01:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089703
2020-10-12 20:01:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094400
2020-10-12 20:01:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17877.98828125Mb; avail=470794.84765625Mb
2020-10-12 20:01:01 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 20:01:13 | INFO | train_inner | epoch 019:     40 / 120 loss=7.52, nll_loss=6.427, ppl=86.04, wps=14839.3, ups=2.56, wpb=5803.3, bsz=221, num_updates=2200, lr=0.000110045, gnorm=1.296, clip=0, train_wall=27, wall=784
2020-10-12 20:01:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:01:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17841.75Mb; avail=470830.12109375Mb
2020-10-12 20:01:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001540
2020-10-12 20:01:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.75Mb; avail=470830.12109375Mb
2020-10-12 20:01:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067408
2020-10-12 20:01:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.75Mb; avail=470830.12109375Mb
2020-10-12 20:01:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047492
2020-10-12 20:01:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117229
2020-10-12 20:01:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.1484375Mb; avail=470830.96484375Mb
2020-10-12 20:01:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17842.02734375Mb; avail=470831.0859375Mb
2020-10-12 20:01:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001071
2020-10-12 20:01:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.02734375Mb; avail=470831.0859375Mb
2020-10-12 20:01:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066977
2020-10-12 20:01:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.046875Mb; avail=470830.96484375Mb
2020-10-12 20:01:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053631
2020-10-12 20:01:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122432
2020-10-12 20:01:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.21484375Mb; avail=470830.84375Mb
2020-10-12 20:01:37 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.336 | nll_loss 6.17 | ppl 71.98 | wps 57294 | wpb 2252.3 | bsz 85.2 | num_updates 2280 | best_loss 7.336
2020-10-12 20:01:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:01:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 19 @ 2280 updates, score 7.336) (writing took 4.4359599429999435 seconds)
2020-10-12 20:01:42 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 20:01:42 | INFO | train | epoch 019 | loss 7.437 | nll_loss 6.331 | ppl 80.53 | wps 16837.8 | ups 2.93 | wpb 5750.2 | bsz 216.1 | num_updates 2280 | lr 0.000114043 | gnorm 1.286 | clip 0 | train_wall 32 | wall 813
2020-10-12 20:01:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 20:01:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 20:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17859.49609375Mb; avail=470812.76171875Mb
2020-10-12 20:01:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000821
2020-10-12 20:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005374
2020-10-12 20:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17859.49609375Mb; avail=470812.76171875Mb
2020-10-12 20:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000207
2020-10-12 20:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17859.49609375Mb; avail=470812.76171875Mb
2020-10-12 20:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091839
2020-10-12 20:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098170
2020-10-12 20:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.05859375Mb; avail=470814.48046875Mb
2020-10-12 20:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17858.12109375Mb; avail=470814.1171875Mb
2020-10-12 20:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003722
2020-10-12 20:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.12109375Mb; avail=470814.1171875Mb
2020-10-12 20:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-12 20:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.12109375Mb; avail=470814.1171875Mb
2020-10-12 20:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094038
2020-10-12 20:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098975
2020-10-12 20:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.29296875Mb; avail=470813.875Mb
2020-10-12 20:01:42 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 20:01:47 | INFO | train_inner | epoch 020:     20 / 120 loss=7.382, nll_loss=6.268, ppl=77.08, wps=16427, ups=2.88, wpb=5712.9, bsz=215.8, num_updates=2300, lr=0.000115043, gnorm=1.306, clip=0, train_wall=27, wall=819
2020-10-12 20:02:15 | INFO | train_inner | epoch 020:    120 / 120 loss=7.36, nll_loss=6.243, ppl=75.73, wps=21138.3, ups=3.63, wpb=5829.4, bsz=220.2, num_updates=2400, lr=0.00012004, gnorm=1.349, clip=0, train_wall=27, wall=846
2020-10-12 20:02:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17772.27734375Mb; avail=470901.0703125Mb
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001593
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17772.27734375Mb; avail=470901.0703125Mb
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067501
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17772.27734375Mb; avail=470901.0703125Mb
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046246
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116180
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17772.3203125Mb; avail=470900.828125Mb
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17772.359375Mb; avail=470900.5859375Mb
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001134
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17772.359375Mb; avail=470900.5859375Mb
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067256
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17772.3359375Mb; avail=470900.94921875Mb
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046456
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115667
2020-10-12 20:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17779.734375Mb; avail=470893.6640625Mb
2020-10-12 20:02:17 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.307 | nll_loss 6.115 | ppl 69.29 | wps 57244.8 | wpb 2252.3 | bsz 85.2 | num_updates 2400 | best_loss 7.307
2020-10-12 20:02:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:02:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 20 @ 2400 updates, score 7.307) (writing took 4.433198368999911 seconds)
2020-10-12 20:02:22 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 20:02:22 | INFO | train | epoch 020 | loss 7.354 | nll_loss 6.236 | ppl 75.39 | wps 17180.6 | ups 2.99 | wpb 5750.2 | bsz 216.1 | num_updates 2400 | lr 0.00012004 | gnorm 1.352 | clip 0 | train_wall 32 | wall 853
2020-10-12 20:02:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 20:02:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 20:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17834.87109375Mb; avail=470837.640625Mb
2020-10-12 20:02:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000916
2020-10-12 20:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005789
2020-10-12 20:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.87109375Mb; avail=470837.640625Mb
2020-10-12 20:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000239
2020-10-12 20:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.96484375Mb; avail=470837.640625Mb
2020-10-12 20:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093491
2020-10-12 20:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100536
2020-10-12 20:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.21875Mb; avail=470837.890625Mb
2020-10-12 20:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17834.32421875Mb; avail=470838.25390625Mb
2020-10-12 20:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003797
2020-10-12 20:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.359375Mb; avail=470838.1328125Mb
2020-10-12 20:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000216
2020-10-12 20:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.359375Mb; avail=470838.1328125Mb
2020-10-12 20:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090059
2020-10-12 20:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094846
2020-10-12 20:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17834.50390625Mb; avail=470838.01171875Mb
2020-10-12 20:02:22 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 20:02:50 | INFO | train_inner | epoch 021:    100 / 120 loss=7.265, nll_loss=6.135, ppl=70.26, wps=16645.7, ups=2.87, wpb=5798.8, bsz=220.9, num_updates=2500, lr=0.000125037, gnorm=1.297, clip=0, train_wall=27, wall=881
2020-10-12 20:02:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:02:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17817.6015625Mb; avail=470854.7265625Mb
2020-10-12 20:02:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001649
2020-10-12 20:02:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.6015625Mb; avail=470854.7265625Mb
2020-10-12 20:02:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067025
2020-10-12 20:02:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.484375Mb; avail=470854.84765625Mb
2020-10-12 20:02:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047394
2020-10-12 20:02:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116854
2020-10-12 20:02:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.5078125Mb; avail=470854.84765625Mb
2020-10-12 20:02:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17817.5078125Mb; avail=470854.84765625Mb
2020-10-12 20:02:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001090
2020-10-12 20:02:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.5078125Mb; avail=470854.84765625Mb
2020-10-12 20:02:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068900
2020-10-12 20:02:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.5859375Mb; avail=470854.72265625Mb
2020-10-12 20:02:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047129
2020-10-12 20:02:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117913
2020-10-12 20:02:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17817.56640625Mb; avail=470854.48046875Mb
2020-10-12 20:02:58 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.254 | nll_loss 6.052 | ppl 66.36 | wps 57265.2 | wpb 2252.3 | bsz 85.2 | num_updates 2520 | best_loss 7.254
2020-10-12 20:02:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:03:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 21 @ 2520 updates, score 7.254) (writing took 4.435665756000162 seconds)
2020-10-12 20:03:02 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 20:03:02 | INFO | train | epoch 021 | loss 7.261 | nll_loss 6.129 | ppl 69.97 | wps 17110.3 | ups 2.98 | wpb 5750.2 | bsz 216.1 | num_updates 2520 | lr 0.000126037 | gnorm 1.314 | clip 0 | train_wall 32 | wall 894
2020-10-12 20:03:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 20:03:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 20:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17836.4375Mb; avail=470836.3203125Mb
2020-10-12 20:03:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001147
2020-10-12 20:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006559
2020-10-12 20:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.55078125Mb; avail=470836.20703125Mb
2020-10-12 20:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000265
2020-10-12 20:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.55078125Mb; avail=470836.20703125Mb
2020-10-12 20:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095707
2020-10-12 20:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103461
2020-10-12 20:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.41796875Mb; avail=470836.203125Mb
2020-10-12 20:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17836.53515625Mb; avail=470836.08203125Mb
2020-10-12 20:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003853
2020-10-12 20:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.53515625Mb; avail=470836.08203125Mb
2020-10-12 20:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000227
2020-10-12 20:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.53515625Mb; avail=470836.08203125Mb
2020-10-12 20:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090643
2020-10-12 20:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095481
2020-10-12 20:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.44140625Mb; avail=470836.32421875Mb
2020-10-12 20:03:02 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 20:03:25 | INFO | train_inner | epoch 022:     80 / 120 loss=7.179, nll_loss=6.035, ppl=65.56, wps=16606.6, ups=2.88, wpb=5763.1, bsz=211.1, num_updates=2600, lr=0.000130035, gnorm=1.284, clip=0, train_wall=27, wall=916
2020-10-12 20:03:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17794.96875Mb; avail=470877.59375Mb
2020-10-12 20:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001677
2020-10-12 20:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17794.96875Mb; avail=470877.59375Mb
2020-10-12 20:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066564
2020-10-12 20:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17794.83984375Mb; avail=470877.59375Mb
2020-10-12 20:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046578
2020-10-12 20:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115608
2020-10-12 20:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17794.8828125Mb; avail=470877.3515625Mb
2020-10-12 20:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17794.8359375Mb; avail=470877.59375Mb
2020-10-12 20:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001144
2020-10-12 20:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17794.8359375Mb; avail=470877.59375Mb
2020-10-12 20:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.114631
2020-10-12 20:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17794.41796875Mb; avail=470878.36328125Mb
2020-10-12 20:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046391
2020-10-12 20:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.162986
2020-10-12 20:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17794.390625Mb; avail=470878.484375Mb
2020-10-12 20:03:38 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.216 | nll_loss 6.005 | ppl 64.22 | wps 56517.7 | wpb 2252.3 | bsz 85.2 | num_updates 2640 | best_loss 7.216
2020-10-12 20:03:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:03:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 22 @ 2640 updates, score 7.216) (writing took 4.43415908299994 seconds)
2020-10-12 20:03:43 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 20:03:43 | INFO | train | epoch 022 | loss 7.175 | nll_loss 6.03 | ppl 65.33 | wps 17043.5 | ups 2.96 | wpb 5750.2 | bsz 216.1 | num_updates 2640 | lr 0.000132034 | gnorm 1.297 | clip 0 | train_wall 32 | wall 934
2020-10-12 20:03:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 20:03:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 20:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17837.328125Mb; avail=470835.0859375Mb
2020-10-12 20:03:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000691
2020-10-12 20:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005453
2020-10-12 20:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.328125Mb; avail=470835.0859375Mb
2020-10-12 20:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000198
2020-10-12 20:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.328125Mb; avail=470835.0859375Mb
2020-10-12 20:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091688
2020-10-12 20:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098101
2020-10-12 20:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.4296875Mb; avail=470835.0859375Mb
2020-10-12 20:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17837.45703125Mb; avail=470834.96484375Mb
2020-10-12 20:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003682
2020-10-12 20:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.4296875Mb; avail=470835.0859375Mb
2020-10-12 20:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000173
2020-10-12 20:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.4296875Mb; avail=470835.0859375Mb
2020-10-12 20:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091386
2020-10-12 20:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095987
2020-10-12 20:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.3828125Mb; avail=470835.44921875Mb
2020-10-12 20:03:43 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 20:03:59 | INFO | train_inner | epoch 023:     60 / 120 loss=7.139, nll_loss=5.988, ppl=63.47, wps=16190.2, ups=2.87, wpb=5648.3, bsz=209.9, num_updates=2700, lr=0.000135032, gnorm=1.394, clip=0, train_wall=27, wall=951
2020-10-12 20:04:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17816.0078125Mb; avail=470856.5859375Mb
2020-10-12 20:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001700
2020-10-12 20:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.0078125Mb; avail=470856.5859375Mb
2020-10-12 20:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074484
2020-10-12 20:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17815.91796875Mb; avail=470856.5859375Mb
2020-10-12 20:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049013
2020-10-12 20:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126007
2020-10-12 20:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17815.93359375Mb; avail=470856.46484375Mb
2020-10-12 20:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17815.93359375Mb; avail=470856.46484375Mb
2020-10-12 20:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001140
2020-10-12 20:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17815.93359375Mb; avail=470856.46484375Mb
2020-10-12 20:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072418
2020-10-12 20:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.0078125Mb; avail=470856.22265625Mb
2020-10-12 20:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049385
2020-10-12 20:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123774
2020-10-12 20:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.140625Mb; avail=470856.1015625Mb
2020-10-12 20:04:19 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.146 | nll_loss 5.922 | ppl 60.61 | wps 56784.9 | wpb 2252.3 | bsz 85.2 | num_updates 2760 | best_loss 7.146
2020-10-12 20:04:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:04:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 23 @ 2760 updates, score 7.146) (writing took 5.37872019400038 seconds)
2020-10-12 20:04:24 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 20:04:24 | INFO | train | epoch 023 | loss 7.094 | nll_loss 5.936 | ppl 61.21 | wps 16710.9 | ups 2.91 | wpb 5750.2 | bsz 216.1 | num_updates 2760 | lr 0.000138031 | gnorm 1.356 | clip 0 | train_wall 32 | wall 975
2020-10-12 20:04:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 20:04:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 20:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18806.3359375Mb; avail=469866.7421875Mb
2020-10-12 20:04:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000916
2020-10-12 20:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005614
2020-10-12 20:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18812.45703125Mb; avail=469860.203125Mb
2020-10-12 20:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 20:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18813.0625Mb; avail=469859.59765625Mb
2020-10-12 20:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092771
2020-10-12 20:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099425
2020-10-12 20:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18920.2421875Mb; avail=469752.53515625Mb
2020-10-12 20:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18957.21484375Mb; avail=469715.46484375Mb
2020-10-12 20:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003786
2020-10-12 20:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18961.453125Mb; avail=469711.83203125Mb
2020-10-12 20:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18962.05859375Mb; avail=469711.2265625Mb
2020-10-12 20:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090100
2020-10-12 20:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094813
2020-10-12 20:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19049.13671875Mb; avail=469624.62109375Mb
2020-10-12 20:04:24 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 20:04:35 | INFO | train_inner | epoch 024:     40 / 120 loss=7.035, nll_loss=5.869, ppl=58.43, wps=16091.1, ups=2.79, wpb=5765.4, bsz=224.6, num_updates=2800, lr=0.00014003, gnorm=1.29, clip=0, train_wall=27, wall=987
2020-10-12 20:04:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:04:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18093.37109375Mb; avail=470579.3984375Mb
2020-10-12 20:04:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001656
2020-10-12 20:04:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18093.37109375Mb; avail=470579.3984375Mb
2020-10-12 20:04:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066526
2020-10-12 20:04:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18093.37109375Mb; avail=470579.3984375Mb
2020-10-12 20:04:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047970
2020-10-12 20:04:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116951
2020-10-12 20:04:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18093.25Mb; avail=470579.51953125Mb
2020-10-12 20:04:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18093.33203125Mb; avail=470579.640625Mb
2020-10-12 20:04:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001112
2020-10-12 20:04:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18093.33203125Mb; avail=470579.640625Mb
2020-10-12 20:04:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066967
2020-10-12 20:04:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18093.0703125Mb; avail=470579.3984375Mb
2020-10-12 20:04:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047059
2020-10-12 20:04:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115903
2020-10-12 20:04:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18092.94921875Mb; avail=470579.640625Mb
2020-10-12 20:05:00 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.151 | nll_loss 5.932 | ppl 61.05 | wps 57097 | wpb 2252.3 | bsz 85.2 | num_updates 2880 | best_loss 7.146
2020-10-12 20:05:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:05:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_last.pt (epoch 24 @ 2880 updates, score 7.151) (writing took 2.245696320999741 seconds)
2020-10-12 20:05:02 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 20:05:02 | INFO | train | epoch 024 | loss 6.993 | nll_loss 5.82 | ppl 56.49 | wps 18146.3 | ups 3.16 | wpb 5750.2 | bsz 216.1 | num_updates 2880 | lr 0.000144028 | gnorm 1.308 | clip 0 | train_wall 32 | wall 1013
2020-10-12 20:05:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 20:05:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 20:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18094.2421875Mb; avail=470578.32421875Mb
2020-10-12 20:05:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001070
2020-10-12 20:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006342
2020-10-12 20:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18094.20703125Mb; avail=470578.56640625Mb
2020-10-12 20:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000214
2020-10-12 20:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18094.20703125Mb; avail=470578.56640625Mb
2020-10-12 20:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091087
2020-10-12 20:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098551
2020-10-12 20:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18093.2890625Mb; avail=470579.85546875Mb
2020-10-12 20:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18093.2890625Mb; avail=470579.85546875Mb
2020-10-12 20:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003773
2020-10-12 20:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18093.2890625Mb; avail=470579.85546875Mb
2020-10-12 20:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18093.2890625Mb; avail=470579.85546875Mb
2020-10-12 20:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090168
2020-10-12 20:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094883
2020-10-12 20:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18093.37890625Mb; avail=470579.4921875Mb
2020-10-12 20:05:02 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 20:05:08 | INFO | train_inner | epoch 025:     20 / 120 loss=6.99, nll_loss=5.817, ppl=56.36, wps=17899.7, ups=3.07, wpb=5832.3, bsz=217.1, num_updates=2900, lr=0.000145028, gnorm=1.329, clip=0, train_wall=27, wall=1019
2020-10-12 20:05:35 | INFO | train_inner | epoch 025:    120 / 120 loss=6.915, nll_loss=5.729, ppl=53.04, wps=20794, ups=3.65, wpb=5693.3, bsz=212.6, num_updates=3000, lr=0.000150025, gnorm=1.371, clip=0, train_wall=26, wall=1047
2020-10-12 20:05:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:05:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18467.16796875Mb; avail=470205.89453125Mb
2020-10-12 20:05:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001652
2020-10-12 20:05:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18467.7734375Mb; avail=470205.2890625Mb
2020-10-12 20:05:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066906
2020-10-12 20:05:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18467.6328125Mb; avail=470205.43359375Mb
2020-10-12 20:05:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047882
2020-10-12 20:05:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117219
2020-10-12 20:05:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18467.6328125Mb; avail=470205.43359375Mb
2020-10-12 20:05:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18467.6328125Mb; avail=470205.43359375Mb
2020-10-12 20:05:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001061
2020-10-12 20:05:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18467.6328125Mb; avail=470205.43359375Mb
2020-10-12 20:05:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066287
2020-10-12 20:05:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18467.56640625Mb; avail=470205.3125Mb
2020-10-12 20:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046865
2020-10-12 20:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.114979
2020-10-12 20:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18467.53125Mb; avail=470205.5546875Mb
2020-10-12 20:05:38 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.114 | nll_loss 5.894 | ppl 59.48 | wps 56917.3 | wpb 2252.3 | bsz 85.2 | num_updates 3000 | best_loss 7.114
2020-10-12 20:05:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:05:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 25 @ 3000 updates, score 7.114) (writing took 5.467092942999443 seconds)
2020-10-12 20:05:43 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 20:05:43 | INFO | train | epoch 025 | loss 6.915 | nll_loss 5.73 | ppl 53.07 | wps 16766.1 | ups 2.92 | wpb 5750.2 | bsz 216.1 | num_updates 3000 | lr 0.000150025 | gnorm 1.362 | clip 0 | train_wall 32 | wall 1054
2020-10-12 20:05:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 20:05:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 20:05:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17835.49609375Mb; avail=470837.5625Mb
2020-10-12 20:05:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000756
2020-10-12 20:05:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005475
2020-10-12 20:05:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.1015625Mb; avail=470836.95703125Mb
2020-10-12 20:05:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000190
2020-10-12 20:05:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.1015625Mb; avail=470836.95703125Mb
2020-10-12 20:05:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090164
2020-10-12 20:05:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096607
2020-10-12 20:05:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.23046875Mb; avail=470836.95703125Mb
2020-10-12 20:05:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17836.109375Mb; avail=470837.5625Mb
2020-10-12 20:05:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003770
2020-10-12 20:05:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.12109375Mb; avail=470837.8046875Mb
2020-10-12 20:05:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 20:05:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.12109375Mb; avail=470837.8046875Mb
2020-10-12 20:05:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090677
2020-10-12 20:05:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095384
2020-10-12 20:05:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.1796875Mb; avail=470837.5625Mb
2020-10-12 20:05:43 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 20:06:11 | INFO | train_inner | epoch 026:    100 / 120 loss=6.834, nll_loss=5.636, ppl=49.74, wps=16113.2, ups=2.78, wpb=5798.4, bsz=212.2, num_updates=3100, lr=0.000155023, gnorm=1.298, clip=0, train_wall=27, wall=1083
2020-10-12 20:06:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17799.05859375Mb; avail=470874.13671875Mb
2020-10-12 20:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001576
2020-10-12 20:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17799.05859375Mb; avail=470874.13671875Mb
2020-10-12 20:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069524
2020-10-12 20:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17799.21484375Mb; avail=470873.7734375Mb
2020-10-12 20:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047277
2020-10-12 20:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119189
2020-10-12 20:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17799.1796875Mb; avail=470874.015625Mb
2020-10-12 20:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17799.1796875Mb; avail=470874.015625Mb
2020-10-12 20:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001148
2020-10-12 20:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17799.1796875Mb; avail=470874.015625Mb
2020-10-12 20:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073937
2020-10-12 20:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17799.40625Mb; avail=470873.89453125Mb
2020-10-12 20:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049513
2020-10-12 20:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125389
2020-10-12 20:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17799.12890625Mb; avail=470873.86328125Mb
2020-10-12 20:06:19 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.008 | nll_loss 5.75 | ppl 53.81 | wps 57257.1 | wpb 2252.3 | bsz 85.2 | num_updates 3120 | best_loss 7.008
2020-10-12 20:06:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:06:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 26 @ 3120 updates, score 7.008) (writing took 4.490446382999835 seconds)
2020-10-12 20:06:24 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 20:06:24 | INFO | train | epoch 026 | loss 6.806 | nll_loss 5.604 | ppl 48.64 | wps 17056.7 | ups 2.97 | wpb 5750.2 | bsz 216.1 | num_updates 3120 | lr 0.000156022 | gnorm 1.29 | clip 0 | train_wall 32 | wall 1095
2020-10-12 20:06:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 20:06:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 20:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17839.78125Mb; avail=470833.3359375Mb
2020-10-12 20:06:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000899
2020-10-12 20:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005464
2020-10-12 20:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.78125Mb; avail=470833.3359375Mb
2020-10-12 20:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000197
2020-10-12 20:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.78125Mb; avail=470833.3359375Mb
2020-10-12 20:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091508
2020-10-12 20:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097934
2020-10-12 20:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.125Mb; avail=470833.94140625Mb
2020-10-12 20:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17839.2265625Mb; avail=470834.0625Mb
2020-10-12 20:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003780
2020-10-12 20:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.3046875Mb; avail=470834.18359375Mb
2020-10-12 20:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 20:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.3046875Mb; avail=470834.18359375Mb
2020-10-12 20:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090278
2020-10-12 20:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094983
2020-10-12 20:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.19140625Mb; avail=470834.18359375Mb
2020-10-12 20:06:24 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 20:06:46 | INFO | train_inner | epoch 027:     80 / 120 loss=6.729, nll_loss=5.515, ppl=45.73, wps=16610.9, ups=2.86, wpb=5801, bsz=218.9, num_updates=3200, lr=0.00016002, gnorm=1.353, clip=0, train_wall=27, wall=1117
2020-10-12 20:06:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17892.828125Mb; avail=470788.84375Mb
2020-10-12 20:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001684
2020-10-12 20:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.828125Mb; avail=470788.59765625Mb
2020-10-12 20:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067784
2020-10-12 20:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.3828125Mb; avail=470788.65625Mb
2020-10-12 20:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047993
2020-10-12 20:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118493
2020-10-12 20:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17893.06640625Mb; avail=470788.38671875Mb
2020-10-12 20:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17892.9453125Mb; avail=470788.21484375Mb
2020-10-12 20:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001202
2020-10-12 20:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.81640625Mb; avail=470787.49609375Mb
2020-10-12 20:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067948
2020-10-12 20:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.72265625Mb; avail=470787.89453125Mb
2020-10-12 20:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047513
2020-10-12 20:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117673
2020-10-12 20:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.9921875Mb; avail=470787.015625Mb
2020-10-12 20:06:59 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.959 | nll_loss 5.687 | ppl 51.52 | wps 57102.1 | wpb 2252.3 | bsz 85.2 | num_updates 3240 | best_loss 6.959
2020-10-12 20:06:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:07:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 27 @ 3240 updates, score 6.959) (writing took 4.54586958299933 seconds)
2020-10-12 20:07:04 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 20:07:04 | INFO | train | epoch 027 | loss 6.722 | nll_loss 5.507 | ppl 45.47 | wps 17088.8 | ups 2.97 | wpb 5750.2 | bsz 216.1 | num_updates 3240 | lr 0.000162019 | gnorm 1.357 | clip 0 | train_wall 32 | wall 1135
2020-10-12 20:07:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 20:07:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 20:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17869.85546875Mb; avail=470803.03515625Mb
2020-10-12 20:07:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000776
2020-10-12 20:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005508
2020-10-12 20:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17869.85546875Mb; avail=470803.03515625Mb
2020-10-12 20:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000226
2020-10-12 20:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17869.85546875Mb; avail=470803.03515625Mb
2020-10-12 20:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090393
2020-10-12 20:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096910
2020-10-12 20:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17869.94921875Mb; avail=470803.12890625Mb
2020-10-12 20:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17869.94921875Mb; avail=470803.12890625Mb
2020-10-12 20:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003727
2020-10-12 20:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17869.94921875Mb; avail=470803.12890625Mb
2020-10-12 20:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 20:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17869.94921875Mb; avail=470803.12890625Mb
2020-10-12 20:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090470
2020-10-12 20:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095148
2020-10-12 20:07:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17870.04296875Mb; avail=470802.88671875Mb
2020-10-12 20:07:04 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 20:07:21 | INFO | train_inner | epoch 028:     60 / 120 loss=6.596, nll_loss=5.363, ppl=41.17, wps=16111.4, ups=2.89, wpb=5576.9, bsz=218.9, num_updates=3300, lr=0.000165018, gnorm=1.293, clip=0, train_wall=26, wall=1152
2020-10-12 20:07:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17853.2734375Mb; avail=470819.75Mb
2020-10-12 20:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001797
2020-10-12 20:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.2734375Mb; avail=470819.75Mb
2020-10-12 20:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066905
2020-10-12 20:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.25Mb; avail=470819.87109375Mb
2020-10-12 20:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048238
2020-10-12 20:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117727
2020-10-12 20:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.12890625Mb; avail=470819.9921875Mb
2020-10-12 20:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17853.12890625Mb; avail=470819.9921875Mb
2020-10-12 20:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001111
2020-10-12 20:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.12890625Mb; avail=470819.9921875Mb
2020-10-12 20:07:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066854
2020-10-12 20:07:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.12890625Mb; avail=470819.9921875Mb
2020-10-12 20:07:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048813
2020-10-12 20:07:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117560
2020-10-12 20:07:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17853.26171875Mb; avail=470819.87109375Mb
2020-10-12 20:07:40 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.898 | nll_loss 5.619 | ppl 49.16 | wps 57036.4 | wpb 2252.3 | bsz 85.2 | num_updates 3360 | best_loss 6.898
2020-10-12 20:07:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:07:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 28 @ 3360 updates, score 6.898) (writing took 4.4357986459999665 seconds)
2020-10-12 20:07:44 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 20:07:44 | INFO | train | epoch 028 | loss 6.613 | nll_loss 5.382 | ppl 41.7 | wps 17174.6 | ups 2.99 | wpb 5750.2 | bsz 216.1 | num_updates 3360 | lr 0.000168016 | gnorm 1.295 | clip 0 | train_wall 32 | wall 1175
2020-10-12 20:07:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 20:07:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 20:07:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17852.18359375Mb; avail=470820.765625Mb
2020-10-12 20:07:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001544
2020-10-12 20:07:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.009003
2020-10-12 20:07:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.18359375Mb; avail=470820.765625Mb
2020-10-12 20:07:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000206
2020-10-12 20:07:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.18359375Mb; avail=470820.765625Mb
2020-10-12 20:07:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.136249
2020-10-12 20:07:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.146276
2020-10-12 20:07:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.03515625Mb; avail=470821.0078125Mb
2020-10-12 20:07:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17852.03515625Mb; avail=470821.0078125Mb
2020-10-12 20:07:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003763
2020-10-12 20:07:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.03515625Mb; avail=470821.0078125Mb
2020-10-12 20:07:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-12 20:07:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.03515625Mb; avail=470821.0078125Mb
2020-10-12 20:07:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.132125
2020-10-12 20:07:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.136817
2020-10-12 20:07:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.046875Mb; avail=470821.0078125Mb
2020-10-12 20:07:45 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 20:07:56 | INFO | train_inner | epoch 029:     40 / 120 loss=6.598, nll_loss=5.364, ppl=41.18, wps=17016.5, ups=2.87, wpb=5939.3, bsz=221.7, num_updates=3400, lr=0.000170015, gnorm=1.296, clip=0, train_wall=27, wall=1187
2020-10-12 20:08:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18526.3359375Mb; avail=470147.21875Mb
2020-10-12 20:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001687
2020-10-12 20:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18526.30859375Mb; avail=470147.4609375Mb
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066353
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18526.49609375Mb; avail=470146.9765625Mb
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047048
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115903
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18526.49609375Mb; avail=470146.9765625Mb
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18526.49609375Mb; avail=470146.9765625Mb
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001120
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18526.49609375Mb; avail=470146.9765625Mb
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066887
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18526.671875Mb; avail=470146.78125Mb
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.045782
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.114582
2020-10-12 20:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18526.67578125Mb; avail=470146.78125Mb
2020-10-12 20:08:20 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.837 | nll_loss 5.55 | ppl 46.84 | wps 56327.6 | wpb 2252.3 | bsz 85.2 | num_updates 3480 | best_loss 6.837
2020-10-12 20:08:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:08:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 29 @ 3480 updates, score 6.837) (writing took 4.458699801999501 seconds)
2020-10-12 20:08:24 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 20:08:24 | INFO | train | epoch 029 | loss 6.517 | nll_loss 5.27 | ppl 38.59 | wps 17150.9 | ups 2.98 | wpb 5750.2 | bsz 216.1 | num_updates 3480 | lr 0.000174013 | gnorm 1.304 | clip 0 | train_wall 32 | wall 1216
2020-10-12 20:08:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 20:08:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 20:08:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17823.3046875Mb; avail=470849.7578125Mb
2020-10-12 20:08:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000697
2020-10-12 20:08:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005415
2020-10-12 20:08:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.3046875Mb; avail=470849.7578125Mb
2020-10-12 20:08:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000227
2020-10-12 20:08:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.3046875Mb; avail=470849.7578125Mb
2020-10-12 20:08:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090971
2020-10-12 20:08:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097480
2020-10-12 20:08:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.0546875Mb; avail=470849.42578125Mb
2020-10-12 20:08:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17823.296875Mb; avail=470849.19140625Mb
2020-10-12 20:08:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003816
2020-10-12 20:08:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.296875Mb; avail=470849.19140625Mb
2020-10-12 20:08:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000201
2020-10-12 20:08:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.296875Mb; avail=470849.19140625Mb
2020-10-12 20:08:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089649
2020-10-12 20:08:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095114
2020-10-12 20:08:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17823.234375Mb; avail=470849.66796875Mb
2020-10-12 20:08:25 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 20:08:30 | INFO | train_inner | epoch 030:     20 / 120 loss=6.507, nll_loss=5.258, ppl=38.27, wps=16181.2, ups=2.89, wpb=5604.7, bsz=208.2, num_updates=3500, lr=0.000175013, gnorm=1.325, clip=0, train_wall=26, wall=1222
2020-10-12 20:08:58 | INFO | train_inner | epoch 030:    120 / 120 loss=6.425, nll_loss=5.163, ppl=35.82, wps=21098.9, ups=3.65, wpb=5780.9, bsz=216.5, num_updates=3600, lr=0.00018001, gnorm=1.346, clip=0, train_wall=26, wall=1249
2020-10-12 20:08:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17807.4375Mb; avail=470865.82421875Mb
2020-10-12 20:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001643
2020-10-12 20:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17807.4375Mb; avail=470865.82421875Mb
2020-10-12 20:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067057
2020-10-12 20:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17807.33203125Mb; avail=470866.06640625Mb
2020-10-12 20:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047237
2020-10-12 20:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116725
2020-10-12 20:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17807.3046875Mb; avail=470866.30859375Mb
2020-10-12 20:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17806.8125Mb; avail=470866.30859375Mb
2020-10-12 20:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001103
2020-10-12 20:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17806.8125Mb; avail=470866.30859375Mb
2020-10-12 20:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066538
2020-10-12 20:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17806.8125Mb; avail=470866.30859375Mb
2020-10-12 20:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047773
2020-10-12 20:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116161
2020-10-12 20:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17806.72265625Mb; avail=470866.3515625Mb
2020-10-12 20:09:00 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.831 | nll_loss 5.514 | ppl 45.69 | wps 55297.2 | wpb 2252.3 | bsz 85.2 | num_updates 3600 | best_loss 6.831
2020-10-12 20:09:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:09:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 30 @ 3600 updates, score 6.831) (writing took 8.166849543000353 seconds)
2020-10-12 20:09:08 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 20:09:08 | INFO | train | epoch 030 | loss 6.42 | nll_loss 5.158 | ppl 35.7 | wps 15687.8 | ups 2.73 | wpb 5750.2 | bsz 216.1 | num_updates 3600 | lr 0.00018001 | gnorm 1.346 | clip 0 | train_wall 32 | wall 1260
2020-10-12 20:09:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 20:09:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 20:09:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17826.6328125Mb; avail=470845.90234375Mb
2020-10-12 20:09:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000660
2020-10-12 20:09:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005401
2020-10-12 20:09:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.6328125Mb; avail=470845.90234375Mb
2020-10-12 20:09:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000203
2020-10-12 20:09:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.6328125Mb; avail=470845.90234375Mb
2020-10-12 20:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090781
2020-10-12 20:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097144
2020-10-12 20:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.5859375Mb; avail=470845.78125Mb
2020-10-12 20:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17826.50390625Mb; avail=470845.66015625Mb
2020-10-12 20:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003724
2020-10-12 20:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.50390625Mb; avail=470845.78125Mb
2020-10-12 20:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000182
2020-10-12 20:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.50390625Mb; avail=470845.78125Mb
2020-10-12 20:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090313
2020-10-12 20:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094959
2020-10-12 20:09:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17826.96875Mb; avail=470845.90234375Mb
2020-10-12 20:09:09 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 20:09:36 | INFO | train_inner | epoch 031:    100 / 120 loss=6.318, nll_loss=5.039, ppl=32.88, wps=14909.6, ups=2.59, wpb=5757.7, bsz=216.9, num_updates=3700, lr=0.000185008, gnorm=1.342, clip=0, train_wall=27, wall=1288
2020-10-12 20:09:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:09:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17827.62890625Mb; avail=470844.81640625Mb
2020-10-12 20:09:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001610
2020-10-12 20:09:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.62890625Mb; avail=470844.81640625Mb
2020-10-12 20:09:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067373
2020-10-12 20:09:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.47265625Mb; avail=470845.1796875Mb
2020-10-12 20:09:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047073
2020-10-12 20:09:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116864
2020-10-12 20:09:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.08203125Mb; avail=470845.54296875Mb
2020-10-12 20:09:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17827.08203125Mb; avail=470845.54296875Mb
2020-10-12 20:09:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001091
2020-10-12 20:09:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.08203125Mb; avail=470845.54296875Mb
2020-10-12 20:09:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066017
2020-10-12 20:09:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.0625Mb; avail=470845.54296875Mb
2020-10-12 20:09:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046255
2020-10-12 20:09:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.114157
2020-10-12 20:09:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.0625Mb; avail=470845.54296875Mb
2020-10-12 20:09:44 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.764 | nll_loss 5.443 | ppl 43.51 | wps 57147.9 | wpb 2252.3 | bsz 85.2 | num_updates 3720 | best_loss 6.764
2020-10-12 20:09:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:09:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 31 @ 3720 updates, score 6.764) (writing took 4.44095784000001 seconds)
2020-10-12 20:09:49 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 20:09:49 | INFO | train | epoch 031 | loss 6.323 | nll_loss 5.044 | ppl 33 | wps 17139.6 | ups 2.98 | wpb 5750.2 | bsz 216.1 | num_updates 3720 | lr 0.000186007 | gnorm 1.347 | clip 0 | train_wall 32 | wall 1300
2020-10-12 20:09:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 20:09:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18244.74609375Mb; avail=470428.32421875Mb
2020-10-12 20:09:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000859
2020-10-12 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005837
2020-10-12 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18251.01171875Mb; avail=470422.1484375Mb
2020-10-12 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000217
2020-10-12 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18251.6171875Mb; avail=470421.54296875Mb
2020-10-12 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093033
2020-10-12 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099859
2020-10-12 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18358.62890625Mb; avail=470314.6015625Mb
2020-10-12 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18358.63671875Mb; avail=470317.3125Mb
2020-10-12 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003815
2020-10-12 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18358.6484375Mb; avail=470318.05078125Mb
2020-10-12 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000219
2020-10-12 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18358.15625Mb; avail=470318.05078125Mb
2020-10-12 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091544
2020-10-12 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096421
2020-10-12 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18360.21484375Mb; avail=470320.43359375Mb
2020-10-12 20:09:49 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 20:10:11 | INFO | train_inner | epoch 032:     80 / 120 loss=6.236, nll_loss=4.943, ppl=30.77, wps=16315.8, ups=2.89, wpb=5649.2, bsz=208.4, num_updates=3800, lr=0.000190005, gnorm=1.335, clip=0, train_wall=26, wall=1322
2020-10-12 20:10:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18103.203125Mb; avail=470569.0234375Mb
2020-10-12 20:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001669
2020-10-12 20:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18103.203125Mb; avail=470569.0234375Mb
2020-10-12 20:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066948
2020-10-12 20:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18103.1640625Mb; avail=470569.14453125Mb
2020-10-12 20:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047306
2020-10-12 20:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116710
2020-10-12 20:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18103.1640625Mb; avail=470569.14453125Mb
2020-10-12 20:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18103.2265625Mb; avail=470568.78125Mb
2020-10-12 20:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001129
2020-10-12 20:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18103.2265625Mb; avail=470568.78125Mb
2020-10-12 20:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067195
2020-10-12 20:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18103.39453125Mb; avail=470569.0234375Mb
2020-10-12 20:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046672
2020-10-12 20:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115754
2020-10-12 20:10:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18103.109375Mb; avail=470569.03125Mb
2020-10-12 20:10:25 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.728 | nll_loss 5.39 | ppl 41.92 | wps 57216.9 | wpb 2252.3 | bsz 85.2 | num_updates 3840 | best_loss 6.728
2020-10-12 20:10:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:10:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 32 @ 3840 updates, score 6.728) (writing took 10.128196519000085 seconds)
2020-10-12 20:10:35 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 20:10:35 | INFO | train | epoch 032 | loss 6.217 | nll_loss 4.921 | ppl 30.3 | wps 15012.1 | ups 2.61 | wpb 5750.2 | bsz 216.1 | num_updates 3840 | lr 0.000192004 | gnorm 1.328 | clip 0 | train_wall 32 | wall 1346
2020-10-12 20:10:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 20:10:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 20:10:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18422.09375Mb; avail=470251.21484375Mb
2020-10-12 20:10:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000656
2020-10-12 20:10:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005333
2020-10-12 20:10:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18422.09375Mb; avail=470251.21484375Mb
2020-10-12 20:10:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000197
2020-10-12 20:10:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18422.09375Mb; avail=470251.21484375Mb
2020-10-12 20:10:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.150593
2020-10-12 20:10:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.156937
2020-10-12 20:10:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18422.2734375Mb; avail=470251.34765625Mb
2020-10-12 20:10:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18422.04296875Mb; avail=470251.58984375Mb
2020-10-12 20:10:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003664
2020-10-12 20:10:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18422.04296875Mb; avail=470251.58984375Mb
2020-10-12 20:10:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 20:10:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18422.04296875Mb; avail=470251.58984375Mb
2020-10-12 20:10:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099233
2020-10-12 20:10:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103970
2020-10-12 20:10:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18421.9765625Mb; avail=470250.85546875Mb
2020-10-12 20:10:35 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 20:10:52 | INFO | train_inner | epoch 033:     60 / 120 loss=6.158, nll_loss=4.853, ppl=28.9, wps=14696.7, ups=2.46, wpb=5981.4, bsz=232.9, num_updates=3900, lr=0.000195003, gnorm=1.317, clip=0, train_wall=27, wall=1363
2020-10-12 20:11:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17828.46875Mb; avail=470843.7890625Mb
2020-10-12 20:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001625
2020-10-12 20:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17828.46875Mb; avail=470843.7890625Mb
2020-10-12 20:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066889
2020-10-12 20:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17828.7109375Mb; avail=470843.546875Mb
2020-10-12 20:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046527
2020-10-12 20:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115905
2020-10-12 20:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17828.31640625Mb; avail=470844.03125Mb
2020-10-12 20:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17828.33203125Mb; avail=470843.91015625Mb
2020-10-12 20:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001161
2020-10-12 20:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17828.33203125Mb; avail=470843.91015625Mb
2020-10-12 20:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067197
2020-10-12 20:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17828.41796875Mb; avail=470844.03125Mb
2020-10-12 20:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046961
2020-10-12 20:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116109
2020-10-12 20:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17828.41796875Mb; avail=470844.03125Mb
2020-10-12 20:11:10 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.718 | nll_loss 5.379 | ppl 41.6 | wps 56868.9 | wpb 2252.3 | bsz 85.2 | num_updates 3960 | best_loss 6.718
2020-10-12 20:11:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:11:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 33 @ 3960 updates, score 6.718) (writing took 5.033110729000327 seconds)
2020-10-12 20:11:15 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 20:11:15 | INFO | train | epoch 033 | loss 6.117 | nll_loss 4.804 | ppl 27.93 | wps 16895.9 | ups 2.94 | wpb 5750.2 | bsz 216.1 | num_updates 3960 | lr 0.000198001 | gnorm 1.339 | clip 0 | train_wall 32 | wall 1387
2020-10-12 20:11:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 20:11:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 20:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18475.87890625Mb; avail=470196.61328125Mb
2020-10-12 20:11:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000679
2020-10-12 20:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005737
2020-10-12 20:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18475.87890625Mb; avail=470196.61328125Mb
2020-10-12 20:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18475.87890625Mb; avail=470196.61328125Mb
2020-10-12 20:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090679
2020-10-12 20:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097346
2020-10-12 20:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18475.89453125Mb; avail=470196.59765625Mb
2020-10-12 20:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18475.84765625Mb; avail=470196.64453125Mb
2020-10-12 20:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004388
2020-10-12 20:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18475.84765625Mb; avail=470196.5234375Mb
2020-10-12 20:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000200
2020-10-12 20:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18475.875Mb; avail=470196.5234375Mb
2020-10-12 20:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089913
2020-10-12 20:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095300
2020-10-12 20:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18475.57421875Mb; avail=470196.64453125Mb
2020-10-12 20:11:16 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 20:11:27 | INFO | train_inner | epoch 034:     40 / 120 loss=6.087, nll_loss=4.768, ppl=27.25, wps=16191.4, ups=2.85, wpb=5679.9, bsz=204.6, num_updates=4000, lr=0.0002, gnorm=1.392, clip=0, train_wall=26, wall=1398
2020-10-12 20:11:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17852.36328125Mb; avail=470820.65625Mb
2020-10-12 20:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001550
2020-10-12 20:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.36328125Mb; avail=470820.65625Mb
2020-10-12 20:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076659
2020-10-12 20:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.36328125Mb; avail=470820.65625Mb
2020-10-12 20:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049816
2020-10-12 20:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128863
2020-10-12 20:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.390625Mb; avail=470820.4140625Mb
2020-10-12 20:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17852.37109375Mb; avail=470820.65625Mb
2020-10-12 20:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001165
2020-10-12 20:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17852.37109375Mb; avail=470820.65625Mb
2020-10-12 20:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071520
2020-10-12 20:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.8125Mb; avail=470820.90625Mb
2020-10-12 20:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048838
2020-10-12 20:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122324
2020-10-12 20:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.703125Mb; avail=470821.02734375Mb
2020-10-12 20:11:51 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.657 | nll_loss 5.3 | ppl 39.39 | wps 56512.9 | wpb 2252.3 | bsz 85.2 | num_updates 4080 | best_loss 6.657
2020-10-12 20:11:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:11:56 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 34 @ 4080 updates, score 6.657) (writing took 4.523196092999569 seconds)
2020-10-12 20:11:56 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 20:11:56 | INFO | train | epoch 034 | loss 6.03 | nll_loss 4.702 | ppl 26.02 | wps 17161.3 | ups 2.98 | wpb 5750.2 | bsz 216.1 | num_updates 4080 | lr 0.00019803 | gnorm 1.395 | clip 0 | train_wall 32 | wall 1427
2020-10-12 20:11:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 20:11:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 20:11:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17874.9609375Mb; avail=470797.375Mb
2020-10-12 20:11:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000635
2020-10-12 20:11:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005020
2020-10-12 20:11:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.9609375Mb; avail=470797.375Mb
2020-10-12 20:11:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000196
2020-10-12 20:11:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.9609375Mb; avail=470797.375Mb
2020-10-12 20:11:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092260
2020-10-12 20:11:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098229
2020-10-12 20:11:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.99609375Mb; avail=470798.1171875Mb
2020-10-12 20:11:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17873.97265625Mb; avail=470798.359375Mb
2020-10-12 20:11:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003682
2020-10-12 20:11:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.97265625Mb; avail=470798.359375Mb
2020-10-12 20:11:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000172
2020-10-12 20:11:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.97265625Mb; avail=470798.359375Mb
2020-10-12 20:11:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090846
2020-10-12 20:11:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095515
2020-10-12 20:11:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.07421875Mb; avail=470798.578125Mb
2020-10-12 20:11:56 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 20:12:02 | INFO | train_inner | epoch 035:     20 / 120 loss=6.008, nll_loss=4.677, ppl=25.58, wps=16395.8, ups=2.87, wpb=5720.9, bsz=217.6, num_updates=4100, lr=0.000197546, gnorm=1.345, clip=0, train_wall=27, wall=1433
2020-10-12 20:12:29 | INFO | train_inner | epoch 035:    120 / 120 loss=5.901, nll_loss=4.553, ppl=23.47, wps=20732.8, ups=3.63, wpb=5712, bsz=216, num_updates=4200, lr=0.00019518, gnorm=1.337, clip=0, train_wall=27, wall=1460
2020-10-12 20:12:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17873.79296875Mb; avail=470798.78125Mb
2020-10-12 20:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001592
2020-10-12 20:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.79296875Mb; avail=470798.78125Mb
2020-10-12 20:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068472
2020-10-12 20:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.2734375Mb; avail=470799.0234375Mb
2020-10-12 20:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046718
2020-10-12 20:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117572
2020-10-12 20:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.15625Mb; avail=470798.328125Mb
2020-10-12 20:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17874.1171875Mb; avail=470798.5703125Mb
2020-10-12 20:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001082
2020-10-12 20:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.1171875Mb; avail=470798.5703125Mb
2020-10-12 20:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066773
2020-10-12 20:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.08984375Mb; avail=470798.69140625Mb
2020-10-12 20:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046644
2020-10-12 20:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115249
2020-10-12 20:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.83203125Mb; avail=470798.65625Mb
2020-10-12 20:12:32 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.669 | nll_loss 5.305 | ppl 39.53 | wps 57216.3 | wpb 2252.3 | bsz 85.2 | num_updates 4200 | best_loss 6.657
2020-10-12 20:12:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:12:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_last.pt (epoch 35 @ 4200 updates, score 6.669) (writing took 2.221271696000258 seconds)
2020-10-12 20:12:34 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 20:12:34 | INFO | train | epoch 035 | loss 5.906 | nll_loss 4.559 | ppl 23.57 | wps 18075.4 | ups 3.14 | wpb 5750.2 | bsz 216.1 | num_updates 4200 | lr 0.00019518 | gnorm 1.315 | clip 0 | train_wall 32 | wall 1465
2020-10-12 20:12:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 20:12:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 20:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17893.2265625Mb; avail=470779.25Mb
2020-10-12 20:12:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000939
2020-10-12 20:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005803
2020-10-12 20:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.734375Mb; avail=470779.7421875Mb
2020-10-12 20:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000212
2020-10-12 20:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.734375Mb; avail=470779.7421875Mb
2020-10-12 20:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091183
2020-10-12 20:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098010
2020-10-12 20:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.38671875Mb; avail=470780.18359375Mb
2020-10-12 20:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17892.39453125Mb; avail=470780.0625Mb
2020-10-12 20:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003714
2020-10-12 20:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.4296875Mb; avail=470779.94140625Mb
2020-10-12 20:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000177
2020-10-12 20:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.4296875Mb; avail=470779.94140625Mb
2020-10-12 20:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090535
2020-10-12 20:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095178
2020-10-12 20:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17892.48828125Mb; avail=470779.94140625Mb
2020-10-12 20:12:34 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 20:13:02 | INFO | train_inner | epoch 036:    100 / 120 loss=5.794, nll_loss=4.429, ppl=21.54, wps=17637.6, ups=3.07, wpb=5748, bsz=214.4, num_updates=4300, lr=0.000192897, gnorm=1.314, clip=0, train_wall=27, wall=1493
2020-10-12 20:13:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17831.546875Mb; avail=470841.375Mb
2020-10-12 20:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001722
2020-10-12 20:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17831.546875Mb; avail=470841.375Mb
2020-10-12 20:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066757
2020-10-12 20:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17831.546875Mb; avail=470841.375Mb
2020-10-12 20:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047026
2020-10-12 20:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116347
2020-10-12 20:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17831.5078125Mb; avail=470841.6171875Mb
2020-10-12 20:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17831.5078125Mb; avail=470841.6171875Mb
2020-10-12 20:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001185
2020-10-12 20:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17831.5078125Mb; avail=470841.6171875Mb
2020-10-12 20:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066353
2020-10-12 20:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17831.51171875Mb; avail=470841.25390625Mb
2020-10-12 20:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047024
2020-10-12 20:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115322
2020-10-12 20:13:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17831.6875Mb; avail=470841.49609375Mb
2020-10-12 20:13:10 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.585 | nll_loss 5.203 | ppl 36.83 | wps 56856.7 | wpb 2252.3 | bsz 85.2 | num_updates 4320 | best_loss 6.585
2020-10-12 20:13:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:13:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 36 @ 4320 updates, score 6.585) (writing took 4.434364767999796 seconds)
2020-10-12 20:13:14 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 20:13:14 | INFO | train | epoch 036 | loss 5.797 | nll_loss 4.432 | ppl 21.59 | wps 17132.7 | ups 2.98 | wpb 5750.2 | bsz 216.1 | num_updates 4320 | lr 0.00019245 | gnorm 1.317 | clip 0 | train_wall 32 | wall 1505
2020-10-12 20:13:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 20:13:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 20:13:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17832.09375Mb; avail=470840.62890625Mb
2020-10-12 20:13:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000879
2020-10-12 20:13:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005695
2020-10-12 20:13:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17832.09375Mb; avail=470840.62890625Mb
2020-10-12 20:13:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000217
2020-10-12 20:13:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17832.07421875Mb; avail=470840.62890625Mb
2020-10-12 20:13:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090615
2020-10-12 20:13:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097284
2020-10-12 20:13:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17830.89453125Mb; avail=470841.85546875Mb
2020-10-12 20:13:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17830.89453125Mb; avail=470841.85546875Mb
2020-10-12 20:13:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004303
2020-10-12 20:13:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17830.89453125Mb; avail=470841.85546875Mb
2020-10-12 20:13:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000210
2020-10-12 20:13:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17830.89453125Mb; avail=470841.85546875Mb
2020-10-12 20:13:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091474
2020-10-12 20:13:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096770
2020-10-12 20:13:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17831.1015625Mb; avail=470841.61328125Mb
2020-10-12 20:13:14 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 20:13:36 | INFO | train_inner | epoch 037:     80 / 120 loss=5.732, nll_loss=4.355, ppl=20.46, wps=16673.9, ups=2.89, wpb=5771.5, bsz=212.5, num_updates=4400, lr=0.000190693, gnorm=1.338, clip=0, train_wall=26, wall=1528
2020-10-12 20:13:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:13:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17851.42578125Mb; avail=470821.375Mb
2020-10-12 20:13:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001650
2020-10-12 20:13:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.42578125Mb; avail=470821.375Mb
2020-10-12 20:13:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066811
2020-10-12 20:13:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.56640625Mb; avail=470821.73828125Mb
2020-10-12 20:13:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046600
2020-10-12 20:13:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115872
2020-10-12 20:13:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.64453125Mb; avail=470821.859375Mb
2020-10-12 20:13:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17850.64453125Mb; avail=470821.859375Mb
2020-10-12 20:13:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001202
2020-10-12 20:13:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.64453125Mb; avail=470821.859375Mb
2020-10-12 20:13:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068435
2020-10-12 20:13:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17850.82421875Mb; avail=470821.5Mb
2020-10-12 20:13:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046218
2020-10-12 20:13:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116645
2020-10-12 20:13:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.01953125Mb; avail=470821.5Mb
2020-10-12 20:13:50 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.58 | nll_loss 5.203 | ppl 36.83 | wps 57111.3 | wpb 2252.3 | bsz 85.2 | num_updates 4440 | best_loss 6.58
2020-10-12 20:13:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:13:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 37 @ 4440 updates, score 6.58) (writing took 4.440540231000341 seconds)
2020-10-12 20:13:54 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 20:13:54 | INFO | train | epoch 037 | loss 5.699 | nll_loss 4.316 | ppl 19.92 | wps 17178.6 | ups 2.99 | wpb 5750.2 | bsz 216.1 | num_updates 4440 | lr 0.000189832 | gnorm 1.335 | clip 0 | train_wall 32 | wall 1546
2020-10-12 20:13:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 20:13:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 20:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17849.58203125Mb; avail=470823.0703125Mb
2020-10-12 20:13:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000867
2020-10-12 20:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006019
2020-10-12 20:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.6328125Mb; avail=470822.828125Mb
2020-10-12 20:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000256
2020-10-12 20:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.6328125Mb; avail=470822.828125Mb
2020-10-12 20:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.099089
2020-10-12 20:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.106448
2020-10-12 20:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.6171875Mb; avail=470823.0703125Mb
2020-10-12 20:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17849.75390625Mb; avail=470822.828125Mb
2020-10-12 20:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003833
2020-10-12 20:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.75390625Mb; avail=470822.828125Mb
2020-10-12 20:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000199
2020-10-12 20:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.75390625Mb; avail=470822.828125Mb
2020-10-12 20:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091387
2020-10-12 20:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096183
2020-10-12 20:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17849.8671875Mb; avail=470822.70703125Mb
2020-10-12 20:13:55 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 20:14:11 | INFO | train_inner | epoch 038:     60 / 120 loss=5.63, nll_loss=4.237, ppl=18.86, wps=16318.2, ups=2.86, wpb=5703.7, bsz=226.6, num_updates=4500, lr=0.000188562, gnorm=1.375, clip=0, train_wall=27, wall=1563
2020-10-12 20:14:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17917.09765625Mb; avail=470755.57421875Mb
2020-10-12 20:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001720
2020-10-12 20:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17917.08203125Mb; avail=470755.4765625Mb
2020-10-12 20:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067938
2020-10-12 20:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17917.21484375Mb; avail=470755.28125Mb
2020-10-12 20:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048109
2020-10-12 20:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118592
2020-10-12 20:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17917.11328125Mb; avail=470755.40234375Mb
2020-10-12 20:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17917.26171875Mb; avail=470755.16015625Mb
2020-10-12 20:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001236
2020-10-12 20:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17917.26171875Mb; avail=470755.16015625Mb
2020-10-12 20:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069343
2020-10-12 20:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17917.83984375Mb; avail=470754.796875Mb
2020-10-12 20:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048186
2020-10-12 20:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119568
2020-10-12 20:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17918.0703125Mb; avail=470754.5546875Mb
2020-10-12 20:14:30 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.533 | nll_loss 5.136 | ppl 35.16 | wps 56028.3 | wpb 2252.3 | bsz 85.2 | num_updates 4560 | best_loss 6.533
2020-10-12 20:14:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:14:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 38 @ 4560 updates, score 6.533) (writing took 6.8919754070002455 seconds)
2020-10-12 20:14:37 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 20:14:37 | INFO | train | epoch 038 | loss 5.611 | nll_loss 4.214 | ppl 18.55 | wps 16052.8 | ups 2.79 | wpb 5750.2 | bsz 216.1 | num_updates 4560 | lr 0.000187317 | gnorm 1.366 | clip 0 | train_wall 32 | wall 1589
2020-10-12 20:14:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 20:14:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 20:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17927.1953125Mb; avail=470749.44140625Mb
2020-10-12 20:14:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000888
2020-10-12 20:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005876
2020-10-12 20:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17927.01171875Mb; avail=470749.16015625Mb
2020-10-12 20:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000222
2020-10-12 20:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17926.8828125Mb; avail=470749.046875Mb
2020-10-12 20:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091577
2020-10-12 20:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098569
2020-10-12 20:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17927.2421875Mb; avail=470747.75Mb
2020-10-12 20:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17926.87890625Mb; avail=470747.1875Mb
2020-10-12 20:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004052
2020-10-12 20:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17926.83984375Mb; avail=470747.0Mb
2020-10-12 20:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000200
2020-10-12 20:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17927.84765625Mb; avail=470746.9765625Mb
2020-10-12 20:14:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091517
2020-10-12 20:14:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096627
2020-10-12 20:14:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17927.5546875Mb; avail=470745.9609375Mb
2020-10-12 20:14:38 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 20:14:49 | INFO | train_inner | epoch 039:     40 / 120 loss=5.579, nll_loss=4.176, ppl=18.08, wps=15611.2, ups=2.69, wpb=5809.8, bsz=208.8, num_updates=4600, lr=0.000186501, gnorm=1.363, clip=0, train_wall=27, wall=1600
2020-10-12 20:15:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17924.9453125Mb; avail=470747.62890625Mb
2020-10-12 20:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001616
2020-10-12 20:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17924.9453125Mb; avail=470747.62890625Mb
2020-10-12 20:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067983
2020-10-12 20:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17924.984375Mb; avail=470747.80859375Mb
2020-10-12 20:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047026
2020-10-12 20:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117451
2020-10-12 20:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17925.01171875Mb; avail=470747.6875Mb
2020-10-12 20:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17924.9375Mb; avail=470747.9296875Mb
2020-10-12 20:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001156
2020-10-12 20:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17924.9375Mb; avail=470747.9296875Mb
2020-10-12 20:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066659
2020-10-12 20:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17924.86328125Mb; avail=470747.80859375Mb
2020-10-12 20:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047153
2020-10-12 20:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115739
2020-10-12 20:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17925.07421875Mb; avail=470747.80859375Mb
2020-10-12 20:15:13 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.592 | nll_loss 5.188 | ppl 36.45 | wps 57333.6 | wpb 2252.3 | bsz 85.2 | num_updates 4680 | best_loss 6.533
2020-10-12 20:15:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:15:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_last.pt (epoch 39 @ 4680 updates, score 6.592) (writing took 4.5020172170006845 seconds)
2020-10-12 20:15:18 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 20:15:18 | INFO | train | epoch 039 | loss 5.51 | nll_loss 4.095 | ppl 17.09 | wps 17157.9 | ups 2.98 | wpb 5750.2 | bsz 216.1 | num_updates 4680 | lr 0.0001849 | gnorm 1.364 | clip 0 | train_wall 32 | wall 1629
2020-10-12 20:15:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 20:15:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 20:15:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19160.28125Mb; avail=469516.3984375Mb
2020-10-12 20:15:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000645
2020-10-12 20:15:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005274
2020-10-12 20:15:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19159.984375Mb; avail=469515.87109375Mb
2020-10-12 20:15:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-12 20:15:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19159.85546875Mb; avail=469515.7578125Mb
2020-10-12 20:15:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.096801
2020-10-12 20:15:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.103028
2020-10-12 20:15:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19160.09765625Mb; avail=469514.93359375Mb
2020-10-12 20:15:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19159.078125Mb; avail=469515.19921875Mb
2020-10-12 20:15:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005664
2020-10-12 20:15:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19160.0078125Mb; avail=469515.1484375Mb
2020-10-12 20:15:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:15:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19159.2734375Mb; avail=469514.54296875Mb
2020-10-12 20:15:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.089914
2020-10-12 20:15:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096575
2020-10-12 20:15:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19159.828125Mb; avail=469513.83203125Mb
2020-10-12 20:15:18 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 20:15:23 | INFO | train_inner | epoch 040:     20 / 120 loss=5.481, nll_loss=4.061, ppl=16.7, wps=16368.1, ups=2.88, wpb=5683, bsz=216.3, num_updates=4700, lr=0.000184506, gnorm=1.346, clip=0, train_wall=27, wall=1635
2020-10-12 20:15:51 | INFO | train_inner | epoch 040:    120 / 120 loss=5.431, nll_loss=4.003, ppl=16.03, wps=20966.2, ups=3.62, wpb=5785.2, bsz=217.8, num_updates=4800, lr=0.000182574, gnorm=1.348, clip=0, train_wall=27, wall=1662
2020-10-12 20:15:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17950.0078125Mb; avail=470722.48828125Mb
2020-10-12 20:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001653
2020-10-12 20:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17950.0078125Mb; avail=470722.48828125Mb
2020-10-12 20:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067456
2020-10-12 20:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17949.875Mb; avail=470722.609375Mb
2020-10-12 20:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049353
2020-10-12 20:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119337
2020-10-12 20:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17949.77734375Mb; avail=470722.48828125Mb
2020-10-12 20:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17949.77734375Mb; avail=470722.48828125Mb
2020-10-12 20:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001278
2020-10-12 20:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17949.77734375Mb; avail=470722.48828125Mb
2020-10-12 20:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066791
2020-10-12 20:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17950.3828125Mb; avail=470721.8828125Mb
2020-10-12 20:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048628
2020-10-12 20:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117482
2020-10-12 20:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17950.3828125Mb; avail=470722.125Mb
2020-10-12 20:15:53 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.519 | nll_loss 5.101 | ppl 34.33 | wps 56490.5 | wpb 2252.3 | bsz 85.2 | num_updates 4800 | best_loss 6.519
2020-10-12 20:15:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:15:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/M2O/checkpoint_best.pt (epoch 40 @ 4800 updates, score 6.519) (writing took 4.466870986000686 seconds)
2020-10-12 20:15:58 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 20:15:58 | INFO | train | epoch 040 | loss 5.423 | nll_loss 3.994 | ppl 15.93 | wps 17114.9 | ups 2.98 | wpb 5750.2 | bsz 216.1 | num_updates 4800 | lr 0.000182574 | gnorm 1.355 | clip 0 | train_wall 32 | wall 1669
2020-10-12 20:15:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 20:15:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 20:15:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17951.79296875Mb; avail=470720.0390625Mb
2020-10-12 20:15:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000708
2020-10-12 20:15:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005687
2020-10-12 20:15:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17952.046875Mb; avail=470719.91796875Mb
2020-10-12 20:15:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000207
2020-10-12 20:15:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17952.046875Mb; avail=470719.91796875Mb
2020-10-12 20:15:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090040
2020-10-12 20:15:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096746
2020-10-12 20:15:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17952.296875Mb; avail=470720.16015625Mb
2020-10-12 20:15:58 | INFO | fairseq_cli.train | done training in 1669.1 seconds
