2020-10-12 20:17:13 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_azefas_sepspm8000/O2M/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok='tgt', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='eng-aze,eng-fas', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_azefas_sepspm8000/O2M/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 20:17:13 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 20:17:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['aze', 'eng', 'fas']
2020-10-12 20:17:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | [aze] dictionary: 21987 types
2020-10-12 20:17:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 21987 types
2020-10-12 20:17:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | [fas] dictionary: 21987 types
2020-10-12 20:17:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 20:17:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15750.7421875Mb; avail=472918.13671875Mb
2020-10-12 20:17:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 20:17:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:eng-aze': 1, 'main:eng-fas': 1}
2020-10-12 20:17:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-aze src_langtok: 21984; tgt_langtok: None
2020-10-12 20:17:13 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azefas_sepspm8000/O2M/valid.eng-aze.eng
2020-10-12 20:17:13 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azefas_sepspm8000/O2M/valid.eng-aze.aze
2020-10-12 20:17:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azefas_sepspm8000/O2M/ valid eng-aze 671 examples
2020-10-12 20:17:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-fas src_langtok: 21986; tgt_langtok: None
2020-10-12 20:17:13 | INFO | fairseq.data.data_utils | loaded 3930 examples from: fairseq/data-bin/ted_azefas_sepspm8000/O2M/valid.eng-fas.eng
2020-10-12 20:17:13 | INFO | fairseq.data.data_utils | loaded 3930 examples from: fairseq/data-bin/ted_azefas_sepspm8000/O2M/valid.eng-fas.fas
2020-10-12 20:17:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azefas_sepspm8000/O2M/ valid eng-fas 3930 examples
2020-10-12 20:17:14 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21987, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21987, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=21987, bias=False)
  )
)
2020-10-12 20:17:14 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 20:17:14 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 20:17:14 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 20:17:14 | INFO | fairseq_cli.train | num. model params: 42800640 (num. trained: 42800640)
2020-10-12 20:17:18 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 20:17:18 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 20:17:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 20:17:18 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 20:17:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 20:17:18 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 20:17:18 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 20:17:18 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_last.pt
2020-10-12 20:17:18 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16088.39453125Mb; avail=472574.99609375Mb
2020-10-12 20:17:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 20:17:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:eng-aze': 1, 'main:eng-fas': 1}
2020-10-12 20:17:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-aze src_langtok: 21984; tgt_langtok: None
2020-10-12 20:17:18 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azefas_sepspm8000/O2M/train.eng-aze.eng
2020-10-12 20:17:18 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azefas_sepspm8000/O2M/train.eng-aze.aze
2020-10-12 20:17:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azefas_sepspm8000/O2M/ train eng-aze 5946 examples
2020-10-12 20:17:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-fas src_langtok: 21986; tgt_langtok: None
2020-10-12 20:17:18 | INFO | fairseq.data.data_utils | loaded 19981 examples from: fairseq/data-bin/ted_azefas_sepspm8000/O2M/train.eng-fas.eng
2020-10-12 20:17:18 | INFO | fairseq.data.data_utils | loaded 19981 examples from: fairseq/data-bin/ted_azefas_sepspm8000/O2M/train.eng-fas.fas
2020-10-12 20:17:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azefas_sepspm8000/O2M/ train eng-fas 19981 examples
2020-10-12 20:17:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:eng-aze', 5946), ('main:eng-fas', 19981)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 20:17:18 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 20:17:18 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 25927
2020-10-12 20:17:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 25927; virtual dataset size 25927
2020-10-12 20:17:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 20:17:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:eng-aze': 5946, 'main:eng-fas': 19981}; raw total size: 25927
2020-10-12 20:17:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:eng-aze': 5946, 'main:eng-fas': 19981}; resampled total size: 25927
2020-10-12 20:17:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 20:17:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.004730
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16095.453125Mb; avail=472565.3671875Mb
2020-10-12 20:17:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000478
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004874
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16102.11328125Mb; avail=472558.70703125Mb
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16103.32421875Mb; avail=472558.1015625Mb
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092985
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098847
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16165.2265625Mb; avail=472495.8203125Mb
2020-10-12 20:17:18 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16208.91796875Mb; avail=472452.26171875Mb
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003761
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16210.4921875Mb; avail=472450.6875Mb
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16210.4921875Mb; avail=472450.6875Mb
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091149
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095852
2020-10-12 20:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16276.109375Mb; avail=472384.66015625Mb
2020-10-12 20:17:18 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 20:17:46 | INFO | train_inner | epoch 001:    100 / 129 loss=14.371, nll_loss=14.283, ppl=19938.4, wps=19493.5, ups=3.63, wpb=5370.7, bsz=199.7, num_updates=100, lr=5.0975e-06, gnorm=3.978, clip=0, train_wall=28, wall=29
2020-10-12 20:17:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18599.046875Mb; avail=470049.97265625Mb
2020-10-12 20:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001856
2020-10-12 20:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18598.66015625Mb; avail=470049.6328125Mb
2020-10-12 20:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069514
2020-10-12 20:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18598.7578125Mb; avail=470049.42578125Mb
2020-10-12 20:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049348
2020-10-12 20:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121608
2020-10-12 20:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18598.07421875Mb; avail=470049.05078125Mb
2020-10-12 20:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18598.73828125Mb; avail=470048.2890625Mb
2020-10-12 20:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001253
2020-10-12 20:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18598.48046875Mb; avail=470048.0625Mb
2020-10-12 20:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068915
2020-10-12 20:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18598.8984375Mb; avail=470046.4453125Mb
2020-10-12 20:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048787
2020-10-12 20:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119796
2020-10-12 20:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18597.9921875Mb; avail=470047.0390625Mb
/home/ubuntu/cmu_11737/assign2/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2020-10-12 20:17:57 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.677 | nll_loss 12.38 | ppl 5331.66 | wps 49042.2 | wpb 1838.8 | bsz 68.7 | num_updates 129
2020-10-12 20:17:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:17:59 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 1 @ 129 updates, score 12.677) (writing took 1.6133255120003014 seconds)
2020-10-12 20:17:59 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 20:17:59 | INFO | train | epoch 001 | loss 14.114 | nll_loss 13.998 | ppl 16356.8 | wps 17210.4 | ups 3.21 | wpb 5357.6 | bsz 201 | num_updates 129 | lr 6.54678e-06 | gnorm 3.573 | clip 0 | train_wall 35 | wall 41
2020-10-12 20:17:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 20:17:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 20:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17935.9296875Mb; avail=470689.16015625Mb
2020-10-12 20:17:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000733
2020-10-12 20:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005184
2020-10-12 20:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17935.9296875Mb; avail=470689.16015625Mb
2020-10-12 20:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-12 20:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17935.9296875Mb; avail=470689.16015625Mb
2020-10-12 20:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090535
2020-10-12 20:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096691
2020-10-12 20:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17930.07421875Mb; avail=470695.0390625Mb
2020-10-12 20:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17930.1796875Mb; avail=470694.91796875Mb
2020-10-12 20:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003813
2020-10-12 20:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17930.1796875Mb; avail=470694.91796875Mb
2020-10-12 20:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000198
2020-10-12 20:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17930.1796875Mb; avail=470694.91796875Mb
2020-10-12 20:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092174
2020-10-12 20:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096983
2020-10-12 20:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17884.80859375Mb; avail=470740.3203125Mb
2020-10-12 20:17:59 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 20:18:19 | INFO | train_inner | epoch 002:     71 / 129 loss=12.942, nll_loss=12.689, ppl=6604.52, wps=16552.6, ups=3.1, wpb=5340.3, bsz=201.8, num_updates=200, lr=1.0095e-05, gnorm=1.84, clip=0, train_wall=27, wall=61
2020-10-12 20:18:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18419.9453125Mb; avail=470172.6171875Mb
2020-10-12 20:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001723
2020-10-12 20:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18419.9453125Mb; avail=470172.6171875Mb
2020-10-12 20:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069665
2020-10-12 20:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18420.0234375Mb; avail=470172.73828125Mb
2020-10-12 20:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049965
2020-10-12 20:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122169
2020-10-12 20:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18419.53125Mb; avail=470173.23046875Mb
2020-10-12 20:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18420.38671875Mb; avail=470172.578125Mb
2020-10-12 20:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001466
2020-10-12 20:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18420.38671875Mb; avail=470172.578125Mb
2020-10-12 20:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069330
2020-10-12 20:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18420.42578125Mb; avail=470172.5703125Mb
2020-10-12 20:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050317
2020-10-12 20:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121905
2020-10-12 20:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18420.38671875Mb; avail=470172.8125Mb
2020-10-12 20:18:37 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.855 | nll_loss 11.463 | ppl 2823.06 | wps 48337.5 | wpb 1838.8 | bsz 68.7 | num_updates 258 | best_loss 11.855
2020-10-12 20:18:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:18:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 2 @ 258 updates, score 11.855) (writing took 6.53057537299992 seconds)
2020-10-12 20:18:44 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 20:18:44 | INFO | train | epoch 002 | loss 12.65 | nll_loss 12.365 | ppl 5274.56 | wps 15362.2 | ups 2.87 | wpb 5357.6 | bsz 201 | num_updates 258 | lr 1.29936e-05 | gnorm 1.594 | clip 0 | train_wall 34 | wall 86
2020-10-12 20:18:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 20:18:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 20:18:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17871.6796875Mb; avail=470719.87109375Mb
2020-10-12 20:18:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001060
2020-10-12 20:18:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006691
2020-10-12 20:18:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.6796875Mb; avail=470719.87109375Mb
2020-10-12 20:18:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000223
2020-10-12 20:18:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.6796875Mb; avail=470719.87109375Mb
2020-10-12 20:18:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095104
2020-10-12 20:18:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102922
2020-10-12 20:18:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.09765625Mb; avail=470726.75390625Mb
2020-10-12 20:18:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17865.34375Mb; avail=470726.51953125Mb
2020-10-12 20:18:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003713
2020-10-12 20:18:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.34375Mb; avail=470726.51953125Mb
2020-10-12 20:18:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 20:18:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.34375Mb; avail=470726.51953125Mb
2020-10-12 20:18:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092182
2020-10-12 20:18:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096803
2020-10-12 20:18:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.56640625Mb; avail=470726.27734375Mb
2020-10-12 20:18:44 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 20:18:56 | INFO | train_inner | epoch 003:     42 / 129 loss=12.295, nll_loss=11.969, ppl=4009.24, wps=14322.2, ups=2.72, wpb=5274.5, bsz=189.9, num_updates=300, lr=1.50925e-05, gnorm=1.438, clip=0, train_wall=26, wall=98
2020-10-12 20:19:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17835.53125Mb; avail=470761.4765625Mb
2020-10-12 20:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001760
2020-10-12 20:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.38671875Mb; avail=470761.3828125Mb
2020-10-12 20:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069167
2020-10-12 20:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.16796875Mb; avail=470760.6875Mb
2020-10-12 20:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049208
2020-10-12 20:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121218
2020-10-12 20:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.06640625Mb; avail=470760.0Mb
2020-10-12 20:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17836.01171875Mb; avail=470759.578125Mb
2020-10-12 20:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001388
2020-10-12 20:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.6640625Mb; avail=470759.44140625Mb
2020-10-12 20:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.085094
2020-10-12 20:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.6171875Mb; avail=470757.95703125Mb
2020-10-12 20:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047695
2020-10-12 20:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.135200
2020-10-12 20:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.80078125Mb; avail=470757.1796875Mb
2020-10-12 20:19:22 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.967 | nll_loss 10.439 | ppl 1387.84 | wps 47679.4 | wpb 1838.8 | bsz 68.7 | num_updates 387 | best_loss 10.967
2020-10-12 20:19:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:19:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 3 @ 387 updates, score 10.967) (writing took 4.46054660199934 seconds)
2020-10-12 20:19:27 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 20:19:27 | INFO | train | epoch 003 | loss 11.786 | nll_loss 11.396 | ppl 2695.15 | wps 16209.7 | ups 3.03 | wpb 5357.6 | bsz 201 | num_updates 387 | lr 1.94403e-05 | gnorm 1.515 | clip 0 | train_wall 34 | wall 129
2020-10-12 20:19:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 20:19:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 20:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17843.03125Mb; avail=470748.6328125Mb
2020-10-12 20:19:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000915
2020-10-12 20:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006072
2020-10-12 20:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17843.03125Mb; avail=470748.6328125Mb
2020-10-12 20:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 20:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17843.03125Mb; avail=470748.6328125Mb
2020-10-12 20:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094604
2020-10-12 20:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101846
2020-10-12 20:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17837.21875Mb; avail=470754.41796875Mb
2020-10-12 20:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17836.5703125Mb; avail=470754.5625Mb
2020-10-12 20:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003897
2020-10-12 20:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.5703125Mb; avail=470754.5625Mb
2020-10-12 20:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 20:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.5703125Mb; avail=470754.5625Mb
2020-10-12 20:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091939
2020-10-12 20:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096786
2020-10-12 20:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17836.7734375Mb; avail=470755.4921875Mb
2020-10-12 20:19:27 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 20:19:31 | INFO | train_inner | epoch 004:     13 / 129 loss=11.588, nll_loss=11.172, ppl=2307.7, wps=15700.1, ups=2.85, wpb=5509.1, bsz=213.2, num_updates=400, lr=2.009e-05, gnorm=1.599, clip=0, train_wall=27, wall=133
2020-10-12 20:19:58 | INFO | train_inner | epoch 004:    113 / 129 loss=10.962, nll_loss=10.443, ppl=1392.41, wps=19548, ups=3.67, wpb=5321.5, bsz=199.9, num_updates=500, lr=2.50875e-05, gnorm=1.223, clip=0, train_wall=26, wall=160
2020-10-12 20:20:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18555.50390625Mb; avail=470037.08984375Mb
2020-10-12 20:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002141
2020-10-12 20:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18555.50390625Mb; avail=470037.08984375Mb
2020-10-12 20:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078920
2020-10-12 20:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18555.40625Mb; avail=470036.96875Mb
2020-10-12 20:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056439
2020-10-12 20:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138485
2020-10-12 20:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18555.37890625Mb; avail=470037.1953125Mb
2020-10-12 20:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18555.41015625Mb; avail=470037.1640625Mb
2020-10-12 20:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001320
2020-10-12 20:20:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18555.41015625Mb; avail=470037.1640625Mb
2020-10-12 20:20:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079339
2020-10-12 20:20:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18555.8359375Mb; avail=470036.6328125Mb
2020-10-12 20:20:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055740
2020-10-12 20:20:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137230
2020-10-12 20:20:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18555.87890625Mb; avail=470036.48046875Mb
2020-10-12 20:20:05 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.334 | nll_loss 9.682 | ppl 821.19 | wps 47200.1 | wpb 1838.8 | bsz 68.7 | num_updates 516 | best_loss 10.334
2020-10-12 20:20:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:20:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 4 @ 516 updates, score 10.334) (writing took 8.553836515000512 seconds)
2020-10-12 20:20:14 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 20:20:14 | INFO | train | epoch 004 | loss 10.957 | nll_loss 10.437 | ppl 1386.24 | wps 14630.6 | ups 2.73 | wpb 5357.6 | bsz 201 | num_updates 516 | lr 2.58871e-05 | gnorm 1.291 | clip 0 | train_wall 34 | wall 176
2020-10-12 20:20:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 20:20:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 20:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17844.7421875Mb; avail=470747.15234375Mb
2020-10-12 20:20:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000771
2020-10-12 20:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006034
2020-10-12 20:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17844.7421875Mb; avail=470747.15234375Mb
2020-10-12 20:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000238
2020-10-12 20:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17844.7421875Mb; avail=470747.15234375Mb
2020-10-12 20:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093583
2020-10-12 20:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100671
2020-10-12 20:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.9375Mb; avail=470753.09375Mb
2020-10-12 20:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17838.8046875Mb; avail=470753.21484375Mb
2020-10-12 20:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003692
2020-10-12 20:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.8046875Mb; avail=470753.21484375Mb
2020-10-12 20:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 20:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.8046875Mb; avail=470753.21484375Mb
2020-10-12 20:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091988
2020-10-12 20:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096585
2020-10-12 20:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17838.93359375Mb; avail=470753.09375Mb
2020-10-12 20:20:14 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 20:20:37 | INFO | train_inner | epoch 005:     84 / 129 loss=10.604, nll_loss=10.008, ppl=1029.79, wps=13655.7, ups=2.56, wpb=5334.8, bsz=208.2, num_updates=600, lr=3.0085e-05, gnorm=1.233, clip=0, train_wall=26, wall=199
2020-10-12 20:20:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17819.40234375Mb; avail=470772.484375Mb
2020-10-12 20:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001961
2020-10-12 20:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.0078125Mb; avail=470771.87890625Mb
2020-10-12 20:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078175
2020-10-12 20:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17819.88671875Mb; avail=470772.0Mb
2020-10-12 20:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052545
2020-10-12 20:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.133508
2020-10-12 20:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17819.7109375Mb; avail=470772.0Mb
2020-10-12 20:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17819.78515625Mb; avail=470771.63671875Mb
2020-10-12 20:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001196
2020-10-12 20:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17819.78515625Mb; avail=470771.63671875Mb
2020-10-12 20:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077079
2020-10-12 20:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.078125Mb; avail=470771.2734375Mb
2020-10-12 20:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051571
2020-10-12 20:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130624
2020-10-12 20:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17820.125Mb; avail=470771.63671875Mb
2020-10-12 20:20:52 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.062 | nll_loss 9.342 | ppl 648.78 | wps 49317.4 | wpb 1838.8 | bsz 68.7 | num_updates 645 | best_loss 10.062
2020-10-12 20:20:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:20:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 5 @ 645 updates, score 10.062) (writing took 4.459000259000277 seconds)
2020-10-12 20:20:57 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 20:20:57 | INFO | train | epoch 005 | loss 10.531 | nll_loss 9.917 | ppl 966.93 | wps 16204.7 | ups 3.02 | wpb 5357.6 | bsz 201 | num_updates 645 | lr 3.23339e-05 | gnorm 1.187 | clip 0 | train_wall 34 | wall 219
2020-10-12 20:20:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 20:20:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 20:20:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17804.85546875Mb; avail=470786.80859375Mb
2020-10-12 20:20:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000874
2020-10-12 20:20:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006233
2020-10-12 20:20:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17804.85546875Mb; avail=470786.80859375Mb
2020-10-12 20:20:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000236
2020-10-12 20:20:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17804.85546875Mb; avail=470786.80859375Mb
2020-10-12 20:20:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094469
2020-10-12 20:20:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101828
2020-10-12 20:20:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17799.58203125Mb; avail=470792.1015625Mb
2020-10-12 20:20:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17799.6953125Mb; avail=470792.1015625Mb
2020-10-12 20:20:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003774
2020-10-12 20:20:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17799.6953125Mb; avail=470792.1015625Mb
2020-10-12 20:20:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:20:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17799.6953125Mb; avail=470792.1015625Mb
2020-10-12 20:20:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093737
2020-10-12 20:20:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098592
2020-10-12 20:20:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.0Mb; avail=470791.6484375Mb
2020-10-12 20:20:57 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 20:21:12 | INFO | train_inner | epoch 006:     55 / 129 loss=10.379, nll_loss=9.728, ppl=848.05, wps=15232.4, ups=2.86, wpb=5332.6, bsz=187.5, num_updates=700, lr=3.50825e-05, gnorm=1.146, clip=0, train_wall=27, wall=234
2020-10-12 20:21:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17804.9375Mb; avail=470787.27734375Mb
2020-10-12 20:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001839
2020-10-12 20:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17804.9375Mb; avail=470787.27734375Mb
2020-10-12 20:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068372
2020-10-12 20:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.75390625Mb; avail=470786.4296875Mb
2020-10-12 20:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052574
2020-10-12 20:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123610
2020-10-12 20:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.82421875Mb; avail=470786.30859375Mb
2020-10-12 20:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17805.94140625Mb; avail=470786.30859375Mb
2020-10-12 20:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001318
2020-10-12 20:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.94140625Mb; avail=470786.30859375Mb
2020-10-12 20:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072884
2020-10-12 20:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17806.0546875Mb; avail=470786.1953125Mb
2020-10-12 20:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064392
2020-10-12 20:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139637
2020-10-12 20:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.81640625Mb; avail=470786.4296875Mb
2020-10-12 20:21:35 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.891 | nll_loss 9.13 | ppl 560.33 | wps 48117.3 | wpb 1838.8 | bsz 68.7 | num_updates 774 | best_loss 9.891
2020-10-12 20:21:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:21:39 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 6 @ 774 updates, score 9.891) (writing took 4.456633877000058 seconds)
2020-10-12 20:21:39 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 20:21:39 | INFO | train | epoch 006 | loss 10.3 | nll_loss 9.631 | ppl 792.92 | wps 16105.6 | ups 3.01 | wpb 5357.6 | bsz 201 | num_updates 774 | lr 3.87807e-05 | gnorm 1.166 | clip 0 | train_wall 34 | wall 262
2020-10-12 20:21:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 20:21:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 20:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17811.6796875Mb; avail=470780.578125Mb
2020-10-12 20:21:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000881
2020-10-12 20:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005336
2020-10-12 20:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.6796875Mb; avail=470780.578125Mb
2020-10-12 20:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000217
2020-10-12 20:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.6796875Mb; avail=470780.578125Mb
2020-10-12 20:21:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092249
2020-10-12 20:21:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098637
2020-10-12 20:21:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.30078125Mb; avail=470787.17578125Mb
2020-10-12 20:21:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17805.27734375Mb; avail=470787.296875Mb
2020-10-12 20:21:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003800
2020-10-12 20:21:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.21875Mb; avail=470787.296875Mb
2020-10-12 20:21:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000204
2020-10-12 20:21:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.21875Mb; avail=470787.296875Mb
2020-10-12 20:21:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091863
2020-10-12 20:21:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096618
2020-10-12 20:21:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17805.30078125Mb; avail=470787.41796875Mb
2020-10-12 20:21:40 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 20:21:47 | INFO | train_inner | epoch 007:     26 / 129 loss=10.26, nll_loss=9.582, ppl=766.17, wps=15668.1, ups=2.86, wpb=5484.2, bsz=214.1, num_updates=800, lr=4.008e-05, gnorm=1.263, clip=0, train_wall=27, wall=269
2020-10-12 20:22:14 | INFO | train_inner | epoch 007:    126 / 129 loss=10.103, nll_loss=9.395, ppl=673.43, wps=19377.7, ups=3.67, wpb=5273.6, bsz=196.2, num_updates=900, lr=4.50775e-05, gnorm=1.262, clip=0, train_wall=26, wall=296
2020-10-12 20:22:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:22:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17824.0078125Mb; avail=470767.9765625Mb
2020-10-12 20:22:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001982
2020-10-12 20:22:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.61328125Mb; avail=470767.37109375Mb
2020-10-12 20:22:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069625
2020-10-12 20:22:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.71875Mb; avail=470767.25Mb
2020-10-12 20:22:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047915
2020-10-12 20:22:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120352
2020-10-12 20:22:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.7421875Mb; avail=470767.12890625Mb
2020-10-12 20:22:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17824.71484375Mb; avail=470767.25Mb
2020-10-12 20:22:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001302
2020-10-12 20:22:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.71484375Mb; avail=470767.25Mb
2020-10-12 20:22:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068479
2020-10-12 20:22:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.82421875Mb; avail=470767.12890625Mb
2020-10-12 20:22:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047822
2020-10-12 20:22:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118548
2020-10-12 20:22:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17824.83984375Mb; avail=470767.0078125Mb
2020-10-12 20:22:18 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.733 | nll_loss 8.942 | ppl 491.85 | wps 49151.1 | wpb 1838.8 | bsz 68.7 | num_updates 903 | best_loss 9.733
2020-10-12 20:22:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:22:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 7 @ 903 updates, score 9.733) (writing took 4.540343824000047 seconds)
2020-10-12 20:22:22 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 20:22:22 | INFO | train | epoch 007 | loss 10.134 | nll_loss 9.431 | ppl 690.44 | wps 16101.2 | ups 3.01 | wpb 5357.6 | bsz 201 | num_updates 903 | lr 4.52274e-05 | gnorm 1.314 | clip 0 | train_wall 34 | wall 305
2020-10-12 20:22:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 20:22:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 20:22:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17833.171875Mb; avail=470758.51953125Mb
2020-10-12 20:22:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000715
2020-10-12 20:22:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005952
2020-10-12 20:22:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17833.22265625Mb; avail=470758.3984375Mb
2020-10-12 20:22:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 20:22:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17833.22265625Mb; avail=470758.3984375Mb
2020-10-12 20:22:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092968
2020-10-12 20:22:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100096
2020-10-12 20:22:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.171875Mb; avail=470764.51953125Mb
2020-10-12 20:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17827.17578125Mb; avail=470764.51953125Mb
2020-10-12 20:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003835
2020-10-12 20:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.16015625Mb; avail=470764.640625Mb
2020-10-12 20:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.16015625Mb; avail=470764.640625Mb
2020-10-12 20:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091268
2020-10-12 20:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096038
2020-10-12 20:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17827.27734375Mb; avail=470764.15625Mb
2020-10-12 20:22:23 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 20:22:49 | INFO | train_inner | epoch 008:     97 / 129 loss=10.006, nll_loss=9.28, ppl=621.67, wps=15415.8, ups=2.87, wpb=5371.3, bsz=195.6, num_updates=1000, lr=5.0075e-05, gnorm=1.247, clip=0, train_wall=26, wall=331
2020-10-12 20:22:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17812.19921875Mb; avail=470779.4140625Mb
2020-10-12 20:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001960
2020-10-12 20:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17812.19921875Mb; avail=470779.4140625Mb
2020-10-12 20:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.117007
2020-10-12 20:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17812.99609375Mb; avail=470778.80859375Mb
2020-10-12 20:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054150
2020-10-12 20:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.174061
2020-10-12 20:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17812.90625Mb; avail=470778.80859375Mb
2020-10-12 20:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17812.9453125Mb; avail=470778.56640625Mb
2020-10-12 20:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001214
2020-10-12 20:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17812.9453125Mb; avail=470778.56640625Mb
2020-10-12 20:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070119
2020-10-12 20:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17812.8046875Mb; avail=470778.921875Mb
2020-10-12 20:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048331
2020-10-12 20:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120530
2020-10-12 20:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17812.66796875Mb; avail=470778.8515625Mb
2020-10-12 20:23:00 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.645 | nll_loss 8.848 | ppl 460.81 | wps 48731.8 | wpb 1838.8 | bsz 68.7 | num_updates 1032 | best_loss 9.645
2020-10-12 20:23:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:23:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 8 @ 1032 updates, score 9.645) (writing took 7.342799197000204 seconds)
2020-10-12 20:23:08 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 20:23:08 | INFO | train | epoch 008 | loss 9.969 | nll_loss 9.237 | ppl 603.58 | wps 15235.6 | ups 2.84 | wpb 5357.6 | bsz 201 | num_updates 1032 | lr 5.16742e-05 | gnorm 1.279 | clip 0 | train_wall 34 | wall 350
2020-10-12 20:23:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 20:23:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 20:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17816.125Mb; avail=470775.90625Mb
2020-10-12 20:23:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000664
2020-10-12 20:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005088
2020-10-12 20:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.125Mb; avail=470775.90625Mb
2020-10-12 20:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.125Mb; avail=470775.90625Mb
2020-10-12 20:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092848
2020-10-12 20:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098939
2020-10-12 20:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17810.7890625Mb; avail=470781.44921875Mb
2020-10-12 20:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17810.83984375Mb; avail=470782.05859375Mb
2020-10-12 20:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003822
2020-10-12 20:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17810.83984375Mb; avail=470782.05859375Mb
2020-10-12 20:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000187
2020-10-12 20:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17810.83984375Mb; avail=470782.05859375Mb
2020-10-12 20:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092554
2020-10-12 20:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097372
2020-10-12 20:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.35546875Mb; avail=470782.984375Mb
2020-10-12 20:23:08 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 20:23:27 | INFO | train_inner | epoch 009:     68 / 129 loss=9.832, nll_loss=9.079, ppl=540.72, wps=14228.8, ups=2.66, wpb=5346.9, bsz=212.8, num_updates=1100, lr=5.50725e-05, gnorm=1.269, clip=0, train_wall=26, wall=369
2020-10-12 20:23:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18460.06640625Mb; avail=470132.4921875Mb
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001904
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18460.06640625Mb; avail=470132.4921875Mb
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071191
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18420.2734375Mb; avail=470172.375Mb
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049386
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123424
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18419.9609375Mb; avail=470172.375Mb
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18419.953125Mb; avail=470172.625Mb
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001269
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18419.953125Mb; avail=470172.625Mb
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070890
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18420.03515625Mb; avail=470172.50390625Mb
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048230
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121187
2020-10-12 20:23:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18420.8125Mb; avail=470172.140625Mb
2020-10-12 20:23:46 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 9.438 | nll_loss 8.589 | ppl 384.97 | wps 48619.6 | wpb 1838.8 | bsz 68.7 | num_updates 1161 | best_loss 9.438
2020-10-12 20:23:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:24:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 9 @ 1161 updates, score 9.438) (writing took 19.367238056000133 seconds)
2020-10-12 20:24:05 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 20:24:05 | INFO | train | epoch 009 | loss 9.816 | nll_loss 9.058 | ppl 532.86 | wps 12004.8 | ups 2.24 | wpb 5357.6 | bsz 201 | num_updates 1161 | lr 5.8121e-05 | gnorm 1.191 | clip 0 | train_wall 34 | wall 407
2020-10-12 20:24:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 20:24:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 20:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17803.45703125Mb; avail=470788.87890625Mb
2020-10-12 20:24:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000649
2020-10-12 20:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005262
2020-10-12 20:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.45703125Mb; avail=470788.87890625Mb
2020-10-12 20:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 20:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17803.45703125Mb; avail=470788.87890625Mb
2020-10-12 20:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091400
2020-10-12 20:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097613
2020-10-12 20:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17796.7578125Mb; avail=470795.5703125Mb
2020-10-12 20:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17797.3046875Mb; avail=470795.0234375Mb
2020-10-12 20:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003719
2020-10-12 20:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17797.3046875Mb; avail=470795.0234375Mb
2020-10-12 20:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000205
2020-10-12 20:24:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17797.3046875Mb; avail=470795.0234375Mb
2020-10-12 20:24:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091009
2020-10-12 20:24:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095689
2020-10-12 20:24:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17797.39453125Mb; avail=470794.62890625Mb
2020-10-12 20:24:06 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 20:24:16 | INFO | train_inner | epoch 010:     39 / 129 loss=9.781, nll_loss=9.016, ppl=517.55, wps=10775.1, ups=2.01, wpb=5360.1, bsz=199.3, num_updates=1200, lr=6.007e-05, gnorm=1.205, clip=0, train_wall=26, wall=419
2020-10-12 20:24:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18442.41796875Mb; avail=470150.0234375Mb
2020-10-12 20:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001829
2020-10-12 20:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18442.41796875Mb; avail=470150.0234375Mb
2020-10-12 20:24:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069874
2020-10-12 20:24:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18442.33203125Mb; avail=470150.12890625Mb
2020-10-12 20:24:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047593
2020-10-12 20:24:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120146
2020-10-12 20:24:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18442.24609375Mb; avail=470150.0078125Mb
2020-10-12 20:24:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17964.40234375Mb; avail=470628.04296875Mb
2020-10-12 20:24:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001226
2020-10-12 20:24:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17926.50390625Mb; avail=470665.94140625Mb
2020-10-12 20:24:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071440
2020-10-12 20:24:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17868.44140625Mb; avail=470729.68359375Mb
2020-10-12 20:24:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048037
2020-10-12 20:24:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122135
2020-10-12 20:24:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17868.01171875Mb; avail=470731.03125Mb
2020-10-12 20:24:43 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 9.347 | nll_loss 8.493 | ppl 360.31 | wps 48917.6 | wpb 1838.8 | bsz 68.7 | num_updates 1290 | best_loss 9.347
2020-10-12 20:24:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:24:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 10 @ 1290 updates, score 9.347) (writing took 4.561406598999383 seconds)
2020-10-12 20:24:48 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 20:24:48 | INFO | train | epoch 010 | loss 9.688 | nll_loss 8.907 | ppl 480.2 | wps 16213.5 | ups 3.03 | wpb 5357.6 | bsz 201 | num_updates 1290 | lr 6.45678e-05 | gnorm 1.252 | clip 0 | train_wall 34 | wall 450
2020-10-12 20:24:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 20:24:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 20:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18457.7890625Mb; avail=470134.3125Mb
2020-10-12 20:24:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000777
2020-10-12 20:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005222
2020-10-12 20:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18457.7890625Mb; avail=470134.3125Mb
2020-10-12 20:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000186
2020-10-12 20:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18457.7890625Mb; avail=470134.3125Mb
2020-10-12 20:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090971
2020-10-12 20:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097155
2020-10-12 20:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18451.4609375Mb; avail=470140.515625Mb
2020-10-12 20:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18451.48828125Mb; avail=470140.515625Mb
2020-10-12 20:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003735
2020-10-12 20:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18451.48828125Mb; avail=470140.515625Mb
2020-10-12 20:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18451.48828125Mb; avail=470140.515625Mb
2020-10-12 20:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091136
2020-10-12 20:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095823
2020-10-12 20:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18451.30859375Mb; avail=470140.515625Mb
2020-10-12 20:24:48 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 20:24:51 | INFO | train_inner | epoch 011:     10 / 129 loss=9.671, nll_loss=8.886, ppl=473.25, wps=15340.3, ups=2.89, wpb=5315.9, bsz=189, num_updates=1300, lr=6.50675e-05, gnorm=1.225, clip=0, train_wall=26, wall=453
2020-10-12 20:25:18 | INFO | train_inner | epoch 011:    110 / 129 loss=9.576, nll_loss=8.778, ppl=438.83, wps=19916.6, ups=3.64, wpb=5471, bsz=211.5, num_updates=1400, lr=7.0065e-05, gnorm=1.249, clip=0, train_wall=27, wall=481
2020-10-12 20:25:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:25:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17841.74609375Mb; avail=470750.98046875Mb
2020-10-12 20:25:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001677
2020-10-12 20:25:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.74609375Mb; avail=470750.98046875Mb
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072261
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.875Mb; avail=470750.74609375Mb
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049974
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125657
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.22265625Mb; avail=470750.1484375Mb
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17842.18359375Mb; avail=470750.1875Mb
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001426
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.18359375Mb; avail=470750.1875Mb
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068189
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.18359375Mb; avail=470750.1875Mb
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046686
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117128
2020-10-12 20:25:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.18359375Mb; avail=470750.1875Mb
2020-10-12 20:25:26 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 9.238 | nll_loss 8.355 | ppl 327.5 | wps 49047 | wpb 1838.8 | bsz 68.7 | num_updates 1419 | best_loss 9.238
2020-10-12 20:25:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:25:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 11 @ 1419 updates, score 9.238) (writing took 7.824031384999216 seconds)
2020-10-12 20:25:34 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 20:25:34 | INFO | train | epoch 011 | loss 9.572 | nll_loss 8.773 | ppl 437.52 | wps 14951.1 | ups 2.79 | wpb 5357.6 | bsz 201 | num_updates 1419 | lr 7.10145e-05 | gnorm 1.279 | clip 0 | train_wall 34 | wall 496
2020-10-12 20:25:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 20:25:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 20:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18430.62890625Mb; avail=470161.5Mb
2020-10-12 20:25:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000911
2020-10-12 20:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005519
2020-10-12 20:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18430.62890625Mb; avail=470161.5Mb
2020-10-12 20:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-12 20:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18430.62890625Mb; avail=470161.5Mb
2020-10-12 20:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091627
2020-10-12 20:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098090
2020-10-12 20:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18424.12109375Mb; avail=470168.625Mb
2020-10-12 20:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18423.62890625Mb; avail=470169.1171875Mb
2020-10-12 20:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003768
2020-10-12 20:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18423.62890625Mb; avail=470169.1171875Mb
2020-10-12 20:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000180
2020-10-12 20:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18423.62890625Mb; avail=470169.1171875Mb
2020-10-12 20:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090291
2020-10-12 20:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094976
2020-10-12 20:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18423.62890625Mb; avail=470169.1171875Mb
2020-10-12 20:25:34 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 20:25:56 | INFO | train_inner | epoch 012:     81 / 129 loss=9.463, nll_loss=8.648, ppl=401.15, wps=13716.1, ups=2.68, wpb=5126.3, bsz=193.3, num_updates=1500, lr=7.50625e-05, gnorm=1.256, clip=0, train_wall=26, wall=518
2020-10-12 20:26:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17839.0390625Mb; avail=470752.5859375Mb
2020-10-12 20:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001921
2020-10-12 20:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.0390625Mb; avail=470752.5859375Mb
2020-10-12 20:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067963
2020-10-12 20:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.640625Mb; avail=470752.1015625Mb
2020-10-12 20:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047094
2020-10-12 20:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117812
2020-10-12 20:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.4921875Mb; avail=470752.34375Mb
2020-10-12 20:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17839.4921875Mb; avail=470752.34375Mb
2020-10-12 20:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001200
2020-10-12 20:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.4921875Mb; avail=470752.34375Mb
2020-10-12 20:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069229
2020-10-12 20:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17839.375Mb; avail=470752.22265625Mb
2020-10-12 20:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047076
2020-10-12 20:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118270
2020-10-12 20:26:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17848.3359375Mb; avail=470743.3828125Mb
2020-10-12 20:26:12 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 9.123 | nll_loss 8.234 | ppl 301.15 | wps 49360.4 | wpb 1838.8 | bsz 68.7 | num_updates 1548 | best_loss 9.123
2020-10-12 20:26:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:26:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 12 @ 1548 updates, score 9.123) (writing took 12.529858467999475 seconds)
2020-10-12 20:26:24 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 20:26:24 | INFO | train | epoch 012 | loss 9.455 | nll_loss 8.638 | ppl 398.27 | wps 13803.9 | ups 2.58 | wpb 5357.6 | bsz 201 | num_updates 1548 | lr 7.74613e-05 | gnorm 1.168 | clip 0 | train_wall 33 | wall 546
2020-10-12 20:26:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 20:26:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 20:26:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18501.19921875Mb; avail=470090.9921875Mb
2020-10-12 20:26:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000958
2020-10-12 20:26:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006322
2020-10-12 20:26:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18495.31640625Mb; avail=470096.77734375Mb
2020-10-12 20:26:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000386
2020-10-12 20:26:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18495.31640625Mb; avail=470096.77734375Mb
2020-10-12 20:26:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103153
2020-10-12 20:26:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110781
2020-10-12 20:26:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18494.2578125Mb; avail=470098.2265625Mb
2020-10-12 20:26:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18494.25Mb; avail=470098.2265625Mb
2020-10-12 20:26:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004367
2020-10-12 20:26:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18494.265625Mb; avail=470098.10546875Mb
2020-10-12 20:26:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-12 20:26:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18494.265625Mb; avail=470098.10546875Mb
2020-10-12 20:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103634
2020-10-12 20:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109210
2020-10-12 20:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18494.28515625Mb; avail=470098.33203125Mb
2020-10-12 20:26:25 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 20:26:39 | INFO | train_inner | epoch 013:     52 / 129 loss=9.408, nll_loss=8.584, ppl=383.69, wps=12971.9, ups=2.33, wpb=5559.4, bsz=197.7, num_updates=1600, lr=8.006e-05, gnorm=1.179, clip=0, train_wall=26, wall=561
2020-10-12 20:26:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17865.1171875Mb; avail=470726.19921875Mb
2020-10-12 20:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001933
2020-10-12 20:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.21484375Mb; avail=470726.19921875Mb
2020-10-12 20:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069823
2020-10-12 20:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.03515625Mb; avail=470726.078125Mb
2020-10-12 20:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047582
2020-10-12 20:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120203
2020-10-12 20:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.0546875Mb; avail=470726.3203125Mb
2020-10-12 20:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17865.09765625Mb; avail=470726.19921875Mb
2020-10-12 20:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001179
2020-10-12 20:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.09765625Mb; avail=470726.19921875Mb
2020-10-12 20:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068622
2020-10-12 20:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.3359375Mb; avail=470725.95703125Mb
2020-10-12 20:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047407
2020-10-12 20:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117990
2020-10-12 20:27:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17865.1328125Mb; avail=470726.078125Mb
2020-10-12 20:27:02 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 9.028 | nll_loss 8.12 | ppl 278.22 | wps 48792.1 | wpb 1838.8 | bsz 68.7 | num_updates 1677 | best_loss 9.028
2020-10-12 20:27:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:27:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 13 @ 1677 updates, score 9.028) (writing took 10.431621292999807 seconds)
2020-10-12 20:27:13 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 20:27:13 | INFO | train | epoch 013 | loss 9.345 | nll_loss 8.511 | ppl 364.91 | wps 14247.1 | ups 2.66 | wpb 5357.6 | bsz 201 | num_updates 1677 | lr 8.39081e-05 | gnorm 1.234 | clip 0 | train_wall 34 | wall 595
2020-10-12 20:27:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 20:27:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 20:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17851.73828125Mb; avail=470740.1328125Mb
2020-10-12 20:27:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000673
2020-10-12 20:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005400
2020-10-12 20:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.59375Mb; avail=470740.375Mb
2020-10-12 20:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000190
2020-10-12 20:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17851.59375Mb; avail=470740.375Mb
2020-10-12 20:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091603
2020-10-12 20:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098025
2020-10-12 20:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17845.5859375Mb; avail=470746.3984375Mb
2020-10-12 20:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17843.078125Mb; avail=470749.1015625Mb
2020-10-12 20:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003893
2020-10-12 20:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17843.078125Mb; avail=470749.1015625Mb
2020-10-12 20:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000185
2020-10-12 20:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17843.078125Mb; avail=470749.1015625Mb
2020-10-12 20:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090810
2020-10-12 20:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095668
2020-10-12 20:27:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17899.9921875Mb; avail=470691.71484375Mb
2020-10-12 20:27:13 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 20:27:19 | INFO | train_inner | epoch 014:     23 / 129 loss=9.294, nll_loss=8.453, ppl=350.51, wps=12839.2, ups=2.47, wpb=5191.2, bsz=202.2, num_updates=1700, lr=8.50575e-05, gnorm=1.276, clip=0, train_wall=26, wall=601
2020-10-12 20:27:46 | INFO | train_inner | epoch 014:    123 / 129 loss=9.263, nll_loss=8.416, ppl=341.55, wps=19948, ups=3.67, wpb=5431, bsz=204.6, num_updates=1800, lr=9.0055e-05, gnorm=1.228, clip=0, train_wall=26, wall=629
2020-10-12 20:27:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17841.359375Mb; avail=470749.40625Mb
2020-10-12 20:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002054
2020-10-12 20:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.359375Mb; avail=470749.40625Mb
2020-10-12 20:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070719
2020-10-12 20:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17841.43359375Mb; avail=470749.53515625Mb
2020-10-12 20:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047595
2020-10-12 20:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121291
2020-10-12 20:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.5703125Mb; avail=470748.6875Mb
2020-10-12 20:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17842.62890625Mb; avail=470748.4453125Mb
2020-10-12 20:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001287
2020-10-12 20:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.62890625Mb; avail=470748.4453125Mb
2020-10-12 20:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070597
2020-10-12 20:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17843.05078125Mb; avail=470748.32421875Mb
2020-10-12 20:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046789
2020-10-12 20:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119433
2020-10-12 20:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17842.45703125Mb; avail=470748.81640625Mb
2020-10-12 20:27:51 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.94 | nll_loss 8.02 | ppl 259.66 | wps 49299.9 | wpb 1838.8 | bsz 68.7 | num_updates 1806 | best_loss 8.94
2020-10-12 20:27:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:28:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 14 @ 1806 updates, score 8.94) (writing took 14.836952943000142 seconds)
2020-10-12 20:28:06 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 20:28:06 | INFO | train | epoch 014 | loss 9.242 | nll_loss 8.393 | ppl 336.06 | wps 13055.1 | ups 2.44 | wpb 5357.6 | bsz 201 | num_updates 1806 | lr 9.03549e-05 | gnorm 1.25 | clip 0 | train_wall 34 | wall 648
2020-10-12 20:28:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 20:28:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17815.37109375Mb; avail=470776.58203125Mb
2020-10-12 20:28:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000841
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005803
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17813.40234375Mb; avail=470780.51953125Mb
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000233
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.46484375Mb; avail=470782.48828125Mb
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.126032
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.133005
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17808.95703125Mb; avail=470783.3359375Mb
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17809.46875Mb; avail=470782.82421875Mb
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006566
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.46875Mb; avail=470782.82421875Mb
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000388
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.46875Mb; avail=470782.82421875Mb
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.148474
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.156775
2020-10-12 20:28:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17809.234375Mb; avail=470783.140625Mb
2020-10-12 20:28:06 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 20:28:31 | INFO | train_inner | epoch 015:     94 / 129 loss=9.151, nll_loss=8.288, ppl=312.55, wps=11926.8, ups=2.23, wpb=5360.1, bsz=192.9, num_updates=1900, lr=9.50525e-05, gnorm=1.275, clip=0, train_wall=26, wall=673
2020-10-12 20:28:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:28:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19560.078125Mb; avail=469032.5234375Mb
2020-10-12 20:28:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001969
2020-10-12 20:28:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19560.078125Mb; avail=469032.5234375Mb
2020-10-12 20:28:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069897
2020-10-12 20:28:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19560.9921875Mb; avail=469031.796875Mb
2020-10-12 20:28:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047444
2020-10-12 20:28:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120126
2020-10-12 20:28:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19561.046875Mb; avail=469031.67578125Mb
2020-10-12 20:28:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19561.078125Mb; avail=469031.5546875Mb
2020-10-12 20:28:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001221
2020-10-12 20:28:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19561.078125Mb; avail=469031.5546875Mb
2020-10-12 20:28:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069078
2020-10-12 20:28:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19561.171875Mb; avail=469031.5546875Mb
2020-10-12 20:28:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047720
2020-10-12 20:28:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118789
2020-10-12 20:28:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19561.22265625Mb; avail=469031.5546875Mb
2020-10-12 20:28:44 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.862 | nll_loss 7.927 | ppl 243.36 | wps 49397.5 | wpb 1838.8 | bsz 68.7 | num_updates 1935 | best_loss 8.862
2020-10-12 20:28:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:29:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 15 @ 1935 updates, score 8.862) (writing took 18.944618835000256 seconds)
2020-10-12 20:29:03 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 20:29:03 | INFO | train | epoch 015 | loss 9.136 | nll_loss 8.271 | ppl 308.92 | wps 12114.2 | ups 2.26 | wpb 5357.6 | bsz 201 | num_updates 1935 | lr 9.68016e-05 | gnorm 1.29 | clip 0 | train_wall 34 | wall 705
2020-10-12 20:29:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 20:29:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 20:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18322.7734375Mb; avail=470270.33984375Mb
2020-10-12 20:29:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000693
2020-10-12 20:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005266
2020-10-12 20:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18191.3515625Mb; avail=470400.27734375Mb
2020-10-12 20:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18187.4140625Mb; avail=470407.66015625Mb
2020-10-12 20:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091360
2020-10-12 20:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097582
2020-10-12 20:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17829.359375Mb; avail=470769.40625Mb
2020-10-12 20:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17822.13671875Mb; avail=470777.25Mb
2020-10-12 20:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003772
2020-10-12 20:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.09765625Mb; avail=470777.0625Mb
2020-10-12 20:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 20:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17821.36328125Mb; avail=470776.94921875Mb
2020-10-12 20:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092895
2020-10-12 20:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097600
2020-10-12 20:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17822.48046875Mb; avail=470775.7421875Mb
2020-10-12 20:29:03 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 20:29:21 | INFO | train_inner | epoch 016:     65 / 129 loss=9.057, nll_loss=8.181, ppl=290.22, wps=10793.3, ups=2.02, wpb=5353.9, bsz=215.3, num_updates=2000, lr=0.00010005, gnorm=1.318, clip=0, train_wall=27, wall=723
2020-10-12 20:29:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18432.71875Mb; avail=470162.18359375Mb
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001841
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18433.54296875Mb; avail=470162.22265625Mb
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068362
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18446.07421875Mb; avail=470147.859375Mb
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047874
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119013
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18448.37890625Mb; avail=470145.1875Mb
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18448.3359375Mb; avail=470144.42578125Mb
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001274
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18449.2890625Mb; avail=470143.97265625Mb
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068131
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18448.90234375Mb; avail=470143.234375Mb
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046698
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117046
2020-10-12 20:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18448.69921875Mb; avail=470143.23828125Mb
2020-10-12 20:29:41 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.783 | nll_loss 7.845 | ppl 229.88 | wps 47212.6 | wpb 1838.8 | bsz 68.7 | num_updates 2064 | best_loss 8.783
2020-10-12 20:29:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:29:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 16 @ 2064 updates, score 8.783) (writing took 7.703301239000211 seconds)
2020-10-12 20:29:49 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 20:29:49 | INFO | train | epoch 016 | loss 9.029 | nll_loss 8.148 | ppl 283.61 | wps 14994.7 | ups 2.8 | wpb 5357.6 | bsz 201 | num_updates 2064 | lr 0.000103248 | gnorm 1.293 | clip 0 | train_wall 34 | wall 751
2020-10-12 20:29:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 20:29:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 20:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18466.08984375Mb; avail=470132.796875Mb
2020-10-12 20:29:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000744
2020-10-12 20:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006157
2020-10-12 20:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18465.8828125Mb; avail=470132.80078125Mb
2020-10-12 20:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000209
2020-10-12 20:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18466.359375Mb; avail=470132.57421875Mb
2020-10-12 20:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094583
2020-10-12 20:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102484
2020-10-12 20:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18459.2265625Mb; avail=470137.74609375Mb
2020-10-12 20:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18459.9453125Mb; avail=470136.63671875Mb
2020-10-12 20:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004170
2020-10-12 20:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18460.24609375Mb; avail=470136.48046875Mb
2020-10-12 20:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000215
2020-10-12 20:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18460.125Mb; avail=470136.24609375Mb
2020-10-12 20:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090310
2020-10-12 20:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095442
2020-10-12 20:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18460.37890625Mb; avail=470135.1015625Mb
2020-10-12 20:29:49 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 20:29:59 | INFO | train_inner | epoch 017:     36 / 129 loss=8.993, nll_loss=8.107, ppl=275.63, wps=13805.7, ups=2.64, wpb=5237.9, bsz=186.8, num_updates=2100, lr=0.000105048, gnorm=1.288, clip=0, train_wall=26, wall=761
2020-10-12 20:30:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:30:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17871.671875Mb; avail=470726.29296875Mb
2020-10-12 20:30:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002101
2020-10-12 20:30:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.140625Mb; avail=470725.9296875Mb
2020-10-12 20:30:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069510
2020-10-12 20:30:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17872.1953125Mb; avail=470725.8984375Mb
2020-10-12 20:30:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048339
2020-10-12 20:30:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121038
2020-10-12 20:30:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.3125Mb; avail=470725.16015625Mb
2020-10-12 20:30:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17872.14453125Mb; avail=470724.73828125Mb
2020-10-12 20:30:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001281
2020-10-12 20:30:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.28125Mb; avail=470725.00390625Mb
2020-10-12 20:30:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068713
2020-10-12 20:30:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17872.38671875Mb; avail=470724.04296875Mb
2020-10-12 20:30:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046498
2020-10-12 20:30:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117259
2020-10-12 20:30:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17871.5625Mb; avail=470724.34765625Mb
2020-10-12 20:30:27 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 8.688 | nll_loss 7.729 | ppl 212.1 | wps 46809.4 | wpb 1838.8 | bsz 68.7 | num_updates 2193 | best_loss 8.688
2020-10-12 20:30:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:30:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 17 @ 2193 updates, score 8.688) (writing took 4.464553200999944 seconds)
2020-10-12 20:30:32 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 20:30:32 | INFO | train | epoch 017 | loss 8.921 | nll_loss 8.024 | ppl 260.24 | wps 16148.6 | ups 3.01 | wpb 5357.6 | bsz 201 | num_updates 2193 | lr 0.000109695 | gnorm 1.293 | clip 0 | train_wall 34 | wall 794
2020-10-12 20:30:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 20:30:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 20:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17863.80078125Mb; avail=470728.0625Mb
2020-10-12 20:30:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000862
2020-10-12 20:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006059
2020-10-12 20:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.30859375Mb; avail=470728.5546875Mb
2020-10-12 20:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 20:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17863.30859375Mb; avail=470728.5546875Mb
2020-10-12 20:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093595
2020-10-12 20:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101035
2020-10-12 20:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.1484375Mb; avail=470733.734375Mb
2020-10-12 20:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17858.1484375Mb; avail=470733.85546875Mb
2020-10-12 20:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003780
2020-10-12 20:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.23046875Mb; avail=470733.85546875Mb
2020-10-12 20:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000210
2020-10-12 20:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.23046875Mb; avail=470733.85546875Mb
2020-10-12 20:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090171
2020-10-12 20:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.094897
2020-10-12 20:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17858.2734375Mb; avail=470733.61328125Mb
2020-10-12 20:30:32 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 20:30:34 | INFO | train_inner | epoch 018:      7 / 129 loss=8.907, nll_loss=8.006, ppl=257.02, wps=15744, ups=2.85, wpb=5515.8, bsz=208.6, num_updates=2200, lr=0.000110045, gnorm=1.262, clip=0, train_wall=26, wall=796
2020-10-12 20:31:01 | INFO | train_inner | epoch 018:    107 / 129 loss=8.813, nll_loss=7.899, ppl=238.73, wps=19826.5, ups=3.68, wpb=5389.2, bsz=204.4, num_updates=2300, lr=0.000115043, gnorm=1.36, clip=0, train_wall=26, wall=823
2020-10-12 20:31:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18258.80078125Mb; avail=470332.4609375Mb
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001749
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18259.40625Mb; avail=470332.4609375Mb
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070009
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18259.890625Mb; avail=470331.9765625Mb
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048140
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120856
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18259.99609375Mb; avail=470331.87109375Mb
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18260.05078125Mb; avail=470331.62890625Mb
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001481
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18260.03125Mb; avail=470331.87109375Mb
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078337
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18261.05078125Mb; avail=470330.88671875Mb
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054966
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.135750
2020-10-12 20:31:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18261.3203125Mb; avail=470330.61328125Mb
2020-10-12 20:31:10 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 8.62 | nll_loss 7.647 | ppl 200.47 | wps 47491.4 | wpb 1838.8 | bsz 68.7 | num_updates 2322 | best_loss 8.62
2020-10-12 20:31:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:31:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 18 @ 2322 updates, score 8.62) (writing took 5.003423207999731 seconds)
2020-10-12 20:31:15 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 20:31:15 | INFO | train | epoch 018 | loss 8.816 | nll_loss 7.902 | ppl 239.11 | wps 15960.1 | ups 2.98 | wpb 5357.6 | bsz 201 | num_updates 2322 | lr 0.000116142 | gnorm 1.357 | clip 0 | train_wall 34 | wall 837
2020-10-12 20:31:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 20:31:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 20:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19425.77734375Mb; avail=469166.1640625Mb
2020-10-12 20:31:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001056
2020-10-12 20:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006694
2020-10-12 20:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19425.77734375Mb; avail=469166.1640625Mb
2020-10-12 20:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000214
2020-10-12 20:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19425.77734375Mb; avail=469166.1640625Mb
2020-10-12 20:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093351
2020-10-12 20:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101201
2020-10-12 20:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19419.7421875Mb; avail=469172.24609375Mb
2020-10-12 20:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19419.56640625Mb; avail=469172.3984375Mb
2020-10-12 20:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.017413
2020-10-12 20:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19060.76171875Mb; avail=469531.203125Mb
2020-10-12 20:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000195
2020-10-12 20:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19060.76171875Mb; avail=469531.203125Mb
2020-10-12 20:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091280
2020-10-12 20:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109657
2020-10-12 20:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19059.77734375Mb; avail=469532.1875Mb
2020-10-12 20:31:15 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 20:31:36 | INFO | train_inner | epoch 019:     78 / 129 loss=8.745, nll_loss=7.821, ppl=226.13, wps=14983.7, ups=2.84, wpb=5273.5, bsz=199.1, num_updates=2400, lr=0.00012004, gnorm=1.409, clip=0, train_wall=26, wall=858
2020-10-12 20:31:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17861.01171875Mb; avail=470730.94140625Mb
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001953
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17861.01171875Mb; avail=470730.94140625Mb
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071787
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.640625Mb; avail=470717.09765625Mb
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049231
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123910
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17879.18359375Mb; avail=470712.23828125Mb
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17879.19140625Mb; avail=470712.23828125Mb
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001142
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17879.19140625Mb; avail=470712.23828125Mb
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068759
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17879.6875Mb; avail=470711.84375Mb
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049495
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120164
2020-10-12 20:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17879.80859375Mb; avail=470711.72265625Mb
2020-10-12 20:31:53 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 8.555 | nll_loss 7.571 | ppl 190.1 | wps 49111.9 | wpb 1838.8 | bsz 68.7 | num_updates 2451 | best_loss 8.555
2020-10-12 20:31:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:32:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 19 @ 2451 updates, score 8.555) (writing took 10.247916790998715 seconds)
2020-10-12 20:32:03 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 20:32:03 | INFO | train | epoch 019 | loss 8.714 | nll_loss 7.785 | ppl 220.5 | wps 14312.7 | ups 2.67 | wpb 5357.6 | bsz 201 | num_updates 2451 | lr 0.000122589 | gnorm 1.348 | clip 0 | train_wall 34 | wall 885
2020-10-12 20:32:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 20:32:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 20:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17888.3046875Mb; avail=470703.421875Mb
2020-10-12 20:32:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000696
2020-10-12 20:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005194
2020-10-12 20:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17887.63671875Mb; avail=470703.9921875Mb
2020-10-12 20:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000201
2020-10-12 20:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17888.2421875Mb; avail=470703.38671875Mb
2020-10-12 20:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090739
2020-10-12 20:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096892
2020-10-12 20:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17887.21484375Mb; avail=470704.2734375Mb
2020-10-12 20:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17886.72265625Mb; avail=470704.765625Mb
2020-10-12 20:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003739
2020-10-12 20:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17886.84375Mb; avail=470704.64453125Mb
2020-10-12 20:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000175
2020-10-12 20:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17886.84375Mb; avail=470704.64453125Mb
2020-10-12 20:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090814
2020-10-12 20:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095488
2020-10-12 20:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17886.85546875Mb; avail=470704.64453125Mb
2020-10-12 20:32:03 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 20:32:17 | INFO | train_inner | epoch 020:     49 / 129 loss=8.667, nll_loss=7.731, ppl=212.47, wps=13065.4, ups=2.46, wpb=5310.5, bsz=197.4, num_updates=2500, lr=0.000125037, gnorm=1.278, clip=0, train_wall=26, wall=899
2020-10-12 20:32:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17914.05078125Mb; avail=470677.12890625Mb
2020-10-12 20:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002821
2020-10-12 20:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17914.05078125Mb; avail=470677.12890625Mb
2020-10-12 20:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069533
2020-10-12 20:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17914.421875Mb; avail=470676.64453125Mb
2020-10-12 20:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048093
2020-10-12 20:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121285
2020-10-12 20:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17914.53515625Mb; avail=470676.64453125Mb
2020-10-12 20:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17915.1640625Mb; avail=470675.91796875Mb
2020-10-12 20:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001193
2020-10-12 20:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17915.1640625Mb; avail=470675.91796875Mb
2020-10-12 20:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067675
2020-10-12 20:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17914.9765625Mb; avail=470675.796875Mb
2020-10-12 20:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048041
2020-10-12 20:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117668
2020-10-12 20:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17914.8203125Mb; avail=470676.546875Mb
2020-10-12 20:32:41 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 8.473 | nll_loss 7.483 | ppl 178.86 | wps 49407.6 | wpb 1838.8 | bsz 68.7 | num_updates 2580 | best_loss 8.473
2020-10-12 20:32:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:32:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 20 @ 2580 updates, score 8.473) (writing took 4.451889497999218 seconds)
2020-10-12 20:32:46 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 20:32:46 | INFO | train | epoch 020 | loss 8.597 | nll_loss 7.65 | ppl 200.8 | wps 16174.7 | ups 3.02 | wpb 5357.6 | bsz 201 | num_updates 2580 | lr 0.000129036 | gnorm 1.302 | clip 0 | train_wall 34 | wall 928
2020-10-12 20:32:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 20:32:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17868.08984375Mb; avail=470723.27734375Mb
2020-10-12 20:32:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000662
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005334
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17868.16796875Mb; avail=470723.3984375Mb
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000183
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17868.16796875Mb; avail=470723.3984375Mb
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091812
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098092
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.48046875Mb; avail=470728.72265625Mb
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17862.02734375Mb; avail=470729.09375Mb
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003692
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.02734375Mb; avail=470729.09375Mb
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000170
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.02734375Mb; avail=470729.09375Mb
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091243
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095848
2020-10-12 20:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17862.3671875Mb; avail=470729.0703125Mb
2020-10-12 20:32:46 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 20:32:52 | INFO | train_inner | epoch 021:     20 / 129 loss=8.566, nll_loss=7.614, ppl=195.84, wps=15781.1, ups=2.88, wpb=5485.8, bsz=209.3, num_updates=2600, lr=0.000130035, gnorm=1.294, clip=0, train_wall=26, wall=934
2020-10-12 20:33:19 | INFO | train_inner | epoch 021:    120 / 129 loss=8.482, nll_loss=7.517, ppl=183.16, wps=19408.7, ups=3.65, wpb=5320.7, bsz=194.9, num_updates=2700, lr=0.000135032, gnorm=1.301, clip=0, train_wall=27, wall=961
2020-10-12 20:33:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:33:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17896.26953125Mb; avail=470695.28515625Mb
2020-10-12 20:33:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001913
2020-10-12 20:33:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17896.26953125Mb; avail=470695.28515625Mb
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069615
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17896.27734375Mb; avail=470695.28515625Mb
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047906
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120247
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17896.30078125Mb; avail=470695.1640625Mb
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17897.1328125Mb; avail=470694.4375Mb
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001179
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17897.1328125Mb; avail=470694.4375Mb
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067605
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17897.1328125Mb; avail=470694.4375Mb
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048156
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117769
2020-10-12 20:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17897.1640625Mb; avail=470694.4453125Mb
2020-10-12 20:33:24 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 8.412 | nll_loss 7.397 | ppl 168.56 | wps 49473.7 | wpb 1838.8 | bsz 68.7 | num_updates 2709 | best_loss 8.412
2020-10-12 20:33:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:33:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 21 @ 2709 updates, score 8.412) (writing took 7.504653870999391 seconds)
2020-10-12 20:33:32 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 20:33:32 | INFO | train | epoch 021 | loss 8.487 | nll_loss 7.523 | ppl 183.97 | wps 15058.7 | ups 2.81 | wpb 5357.6 | bsz 201 | num_updates 2709 | lr 0.000135482 | gnorm 1.298 | clip 0 | train_wall 34 | wall 974
2020-10-12 20:33:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 20:33:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18371.69140625Mb; avail=470220.26171875Mb
2020-10-12 20:33:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000818
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005848
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18379.67578125Mb; avail=470212.27734375Mb
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000213
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18380.28125Mb; avail=470211.671875Mb
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101800
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108684
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18499.828125Mb; avail=470091.93359375Mb
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18572.9921875Mb; avail=470018.8515625Mb
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004416
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18582.078125Mb; avail=470009.640625Mb
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000244
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18583.89453125Mb; avail=470008.4296875Mb
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.101599
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.107948
2020-10-12 20:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18724.0625Mb; avail=469867.671875Mb
2020-10-12 20:33:32 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 20:33:57 | INFO | train_inner | epoch 022:     91 / 129 loss=8.393, nll_loss=7.414, ppl=170.54, wps=14378.7, ups=2.64, wpb=5453.9, bsz=206.2, num_updates=2800, lr=0.00014003, gnorm=1.31, clip=0, train_wall=26, wall=999
2020-10-12 20:34:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:34:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18453.5859375Mb; avail=470137.234375Mb
2020-10-12 20:34:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002852
2020-10-12 20:34:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18454.19140625Mb; avail=470137.234375Mb
2020-10-12 20:34:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074050
2020-10-12 20:34:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18454.2265625Mb; avail=470136.9921875Mb
2020-10-12 20:34:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050612
2020-10-12 20:34:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128677
2020-10-12 20:34:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18454.8203125Mb; avail=470136.38671875Mb
2020-10-12 20:34:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18454.84375Mb; avail=470136.38671875Mb
2020-10-12 20:34:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001171
2020-10-12 20:34:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18454.84375Mb; avail=470136.38671875Mb
2020-10-12 20:34:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072295
2020-10-12 20:34:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18454.85546875Mb; avail=470136.265625Mb
2020-10-12 20:34:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050167
2020-10-12 20:34:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124423
2020-10-12 20:34:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18454.7578125Mb; avail=470136.265625Mb
2020-10-12 20:34:10 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 8.364 | nll_loss 7.331 | ppl 161.05 | wps 48819.8 | wpb 1838.8 | bsz 68.7 | num_updates 2838 | best_loss 8.364
2020-10-12 20:34:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:34:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 22 @ 2838 updates, score 8.364) (writing took 4.962085608998677 seconds)
2020-10-12 20:34:15 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 20:34:15 | INFO | train | epoch 022 | loss 8.378 | nll_loss 7.397 | ppl 168.52 | wps 16011.3 | ups 2.99 | wpb 5357.6 | bsz 201 | num_updates 2838 | lr 0.000141929 | gnorm 1.315 | clip 0 | train_wall 34 | wall 1017
2020-10-12 20:34:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 20:34:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 20:34:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18486.3515625Mb; avail=470105.24609375Mb
2020-10-12 20:34:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000803
2020-10-12 20:34:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005977
2020-10-12 20:34:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18486.3515625Mb; avail=470105.24609375Mb
2020-10-12 20:34:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000242
2020-10-12 20:34:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18486.3515625Mb; avail=470105.24609375Mb
2020-10-12 20:34:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.105549
2020-10-12 20:34:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.112806
2020-10-12 20:34:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18479.79296875Mb; avail=470111.69140625Mb
2020-10-12 20:34:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18479.3203125Mb; avail=470112.27734375Mb
2020-10-12 20:34:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006200
2020-10-12 20:34:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18479.3203125Mb; avail=470112.27734375Mb
2020-10-12 20:34:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000245
2020-10-12 20:34:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18479.3203125Mb; avail=470112.27734375Mb
2020-10-12 20:34:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.104054
2020-10-12 20:34:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111380
2020-10-12 20:34:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18479.54296875Mb; avail=470112.24609375Mb
2020-10-12 20:34:15 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 20:34:32 | INFO | train_inner | epoch 023:     62 / 129 loss=8.282, nll_loss=7.286, ppl=156.1, wps=15014.8, ups=2.84, wpb=5289.8, bsz=201.3, num_updates=2900, lr=0.000145028, gnorm=1.339, clip=0, train_wall=26, wall=1034
2020-10-12 20:34:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:34:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17965.26953125Mb; avail=470625.3828125Mb
2020-10-12 20:34:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001825
2020-10-12 20:34:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17965.26953125Mb; avail=470625.3828125Mb
2020-10-12 20:34:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070718
2020-10-12 20:34:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17966.36328125Mb; avail=470624.77734375Mb
2020-10-12 20:34:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047594
2020-10-12 20:34:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120946
2020-10-12 20:34:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17966.39453125Mb; avail=470624.53515625Mb
2020-10-12 20:34:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17966.4140625Mb; avail=470624.4140625Mb
2020-10-12 20:34:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001389
2020-10-12 20:34:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17966.4140625Mb; avail=470624.4140625Mb
2020-10-12 20:34:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070123
2020-10-12 20:34:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17966.31640625Mb; avail=470624.51171875Mb
2020-10-12 20:34:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046971
2020-10-12 20:34:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119280
2020-10-12 20:34:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17966.30859375Mb; avail=470624.51171875Mb
2020-10-12 20:34:53 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 8.296 | nll_loss 7.265 | ppl 153.81 | wps 46845 | wpb 1838.8 | bsz 68.7 | num_updates 2967 | best_loss 8.296
2020-10-12 20:34:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:35:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 23 @ 2967 updates, score 8.296) (writing took 9.212201895001272 seconds)
2020-10-12 20:35:02 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 20:35:02 | INFO | train | epoch 023 | loss 8.27 | nll_loss 7.272 | ppl 154.53 | wps 14569.2 | ups 2.72 | wpb 5357.6 | bsz 201 | num_updates 2967 | lr 0.000148376 | gnorm 1.314 | clip 0 | train_wall 34 | wall 1065
2020-10-12 20:35:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 20:35:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 20:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17792.6953125Mb; avail=470781.48828125Mb
2020-10-12 20:35:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000789
2020-10-12 20:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005587
2020-10-12 20:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17792.6953125Mb; avail=470781.48828125Mb
2020-10-12 20:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000233
2020-10-12 20:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17792.6953125Mb; avail=470781.48828125Mb
2020-10-12 20:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091142
2020-10-12 20:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097745
2020-10-12 20:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17786.6640625Mb; avail=470787.5234375Mb
2020-10-12 20:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17787.76953125Mb; avail=470786.3046875Mb
2020-10-12 20:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003822
2020-10-12 20:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17788.375Mb; avail=470785.69921875Mb
2020-10-12 20:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000212
2020-10-12 20:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17788.375Mb; avail=470785.69921875Mb
2020-10-12 20:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092835
2020-10-12 20:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097646
2020-10-12 20:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.578125Mb; avail=470772.1484375Mb
2020-10-12 20:35:03 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 20:35:12 | INFO | train_inner | epoch 024:     33 / 129 loss=8.242, nll_loss=7.239, ppl=151.03, wps=13227.1, ups=2.54, wpb=5211.4, bsz=190.7, num_updates=3000, lr=0.000150025, gnorm=1.3, clip=0, train_wall=26, wall=1074
2020-10-12 20:35:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19038.83984375Mb; avail=469535.58203125Mb
2020-10-12 20:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001939
2020-10-12 20:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19038.83984375Mb; avail=469535.58203125Mb
2020-10-12 20:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077896
2020-10-12 20:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19038.7265625Mb; avail=469535.671875Mb
2020-10-12 20:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053930
2020-10-12 20:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.134788
2020-10-12 20:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19038.7578125Mb; avail=469535.640625Mb
2020-10-12 20:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19039.1328125Mb; avail=469535.26171875Mb
2020-10-12 20:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001308
2020-10-12 20:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19039.1328125Mb; avail=469535.26171875Mb
2020-10-12 20:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068742
2020-10-12 20:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19039.3046875Mb; avail=469534.8828125Mb
2020-10-12 20:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046751
2020-10-12 20:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117728
2020-10-12 20:35:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19039.33984375Mb; avail=469534.76171875Mb
2020-10-12 20:35:41 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.227 | nll_loss 7.167 | ppl 143.75 | wps 49028.3 | wpb 1838.8 | bsz 68.7 | num_updates 3096 | best_loss 8.227
2020-10-12 20:35:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:35:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 24 @ 3096 updates, score 8.227) (writing took 9.384393258998898 seconds)
2020-10-12 20:35:50 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 20:35:50 | INFO | train | epoch 024 | loss 8.15 | nll_loss 7.132 | ppl 140.3 | wps 14466 | ups 2.7 | wpb 5357.6 | bsz 201 | num_updates 3096 | lr 0.000154823 | gnorm 1.275 | clip 0 | train_wall 34 | wall 1112
2020-10-12 20:35:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 20:35:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 20:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19149.3828125Mb; avail=469424.9296875Mb
2020-10-12 20:35:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000922
2020-10-12 20:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005740
2020-10-12 20:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19149.35546875Mb; avail=469424.4453125Mb
2020-10-12 20:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000227
2020-10-12 20:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19149.37109375Mb; avail=469424.4453125Mb
2020-10-12 20:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092025
2020-10-12 20:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098875
2020-10-12 20:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19142.9609375Mb; avail=469431.23828125Mb
2020-10-12 20:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19142.9609375Mb; avail=469431.23828125Mb
2020-10-12 20:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003825
2020-10-12 20:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19142.9609375Mb; avail=469431.23828125Mb
2020-10-12 20:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000257
2020-10-12 20:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19142.9609375Mb; avail=469431.23828125Mb
2020-10-12 20:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092839
2020-10-12 20:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097760
2020-10-12 20:35:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19142.8515625Mb; avail=469431.26171875Mb
2020-10-12 20:35:50 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 20:35:52 | INFO | train_inner | epoch 025:      4 / 129 loss=8.149, nll_loss=7.13, ppl=140.11, wps=13552.7, ups=2.5, wpb=5426.3, bsz=207.3, num_updates=3100, lr=0.000155023, gnorm=1.276, clip=0, train_wall=27, wall=1114
2020-10-12 20:36:19 | INFO | train_inner | epoch 025:    104 / 129 loss=8.061, nll_loss=7.03, ppl=130.65, wps=19476, ups=3.68, wpb=5286.7, bsz=188.8, num_updates=3200, lr=0.00016002, gnorm=1.352, clip=0, train_wall=26, wall=1141
2020-10-12 20:36:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17955.55859375Mb; avail=470618.41015625Mb
2020-10-12 20:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001918
2020-10-12 20:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17955.06640625Mb; avail=470618.90234375Mb
2020-10-12 20:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077397
2020-10-12 20:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17955.0546875Mb; avail=470618.90234375Mb
2020-10-12 20:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051223
2020-10-12 20:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.131545
2020-10-12 20:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17956.015625Mb; avail=470618.046875Mb
2020-10-12 20:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17955.5390625Mb; avail=470618.5390625Mb
2020-10-12 20:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001447
2020-10-12 20:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17955.5390625Mb; avail=470618.5390625Mb
2020-10-12 20:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070356
2020-10-12 20:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17955.39453125Mb; avail=470618.65234375Mb
2020-10-12 20:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049091
2020-10-12 20:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121764
2020-10-12 20:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17955.39453125Mb; avail=470618.65234375Mb
2020-10-12 20:36:29 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.179 | nll_loss 7.113 | ppl 138.4 | wps 47809.1 | wpb 1838.8 | bsz 68.7 | num_updates 3225 | best_loss 8.179
2020-10-12 20:36:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:36:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 25 @ 3225 updates, score 8.179) (writing took 4.7660323590007465 seconds)
2020-10-12 20:36:34 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 20:36:34 | INFO | train | epoch 025 | loss 8.044 | nll_loss 7.01 | ppl 128.92 | wps 15920.8 | ups 2.97 | wpb 5357.6 | bsz 201 | num_updates 3225 | lr 0.000161269 | gnorm 1.357 | clip 0 | train_wall 34 | wall 1156
2020-10-12 20:36:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 20:36:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 20:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18038.37109375Mb; avail=470537.70703125Mb
2020-10-12 20:36:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000748
2020-10-12 20:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005274
2020-10-12 20:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18044.52734375Mb; avail=470532.015625Mb
2020-10-12 20:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-12 20:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18045.1328125Mb; avail=470531.41015625Mb
2020-10-12 20:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090845
2020-10-12 20:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097134
2020-10-12 20:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18047.4609375Mb; avail=470529.08203125Mb
2020-10-12 20:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18047.4921875Mb; avail=470529.05078125Mb
2020-10-12 20:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003906
2020-10-12 20:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18047.4921875Mb; avail=470529.05078125Mb
2020-10-12 20:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000201
2020-10-12 20:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18047.4921875Mb; avail=470529.05078125Mb
2020-10-12 20:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090624
2020-10-12 20:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095618
2020-10-12 20:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18066.80859375Mb; avail=470510.01171875Mb
2020-10-12 20:36:34 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 20:36:55 | INFO | train_inner | epoch 026:     75 / 129 loss=7.961, nll_loss=6.914, ppl=120.59, wps=15599.6, ups=2.79, wpb=5585.5, bsz=217, num_updates=3300, lr=0.000165018, gnorm=1.315, clip=0, train_wall=27, wall=1177
2020-10-12 20:37:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16112.796875Mb; avail=472476.515625Mb
2020-10-12 20:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001984
2020-10-12 20:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16112.30078125Mb; avail=472475.828125Mb
2020-10-12 20:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070847
2020-10-12 20:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16117.828125Mb; avail=472468.90625Mb
2020-10-12 20:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049708
2020-10-12 20:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123667
2020-10-12 20:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16123.3125Mb; avail=472462.5390625Mb
2020-10-12 20:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16130.28125Mb; avail=472455.37109375Mb
2020-10-12 20:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001397
2020-10-12 20:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16130.28125Mb; avail=472455.37109375Mb
2020-10-12 20:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069919
2020-10-12 20:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16137.0078125Mb; avail=472449.1328125Mb
2020-10-12 20:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048775
2020-10-12 20:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120947
2020-10-12 20:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16137.8359375Mb; avail=472448.3046875Mb
2020-10-12 20:37:12 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.118 | nll_loss 7.042 | ppl 131.76 | wps 48587.2 | wpb 1838.8 | bsz 68.7 | num_updates 3354 | best_loss 8.118
2020-10-12 20:37:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:37:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 26 @ 3354 updates, score 8.118) (writing took 10.53659781299939 seconds)
2020-10-12 20:37:23 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 20:37:23 | INFO | train | epoch 026 | loss 7.922 | nll_loss 6.868 | ppl 116.77 | wps 14101.4 | ups 2.63 | wpb 5357.6 | bsz 201 | num_updates 3354 | lr 0.000167716 | gnorm 1.347 | clip 0 | train_wall 34 | wall 1205
2020-10-12 20:37:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 20:37:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 20:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18461.66015625Mb; avail=470112.40625Mb
2020-10-12 20:37:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000854
2020-10-12 20:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005833
2020-10-12 20:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18461.66015625Mb; avail=470112.40625Mb
2020-10-12 20:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000242
2020-10-12 20:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18461.66015625Mb; avail=470112.40625Mb
2020-10-12 20:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.102498
2020-10-12 20:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.109450
2020-10-12 20:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18455.83984375Mb; avail=470118.11328125Mb
2020-10-12 20:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18455.88671875Mb; avail=470118.06640625Mb
2020-10-12 20:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004254
2020-10-12 20:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18455.88671875Mb; avail=470118.06640625Mb
2020-10-12 20:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000184
2020-10-12 20:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18455.96875Mb; avail=470117.9453125Mb
2020-10-12 20:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.103072
2020-10-12 20:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.108377
2020-10-12 20:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18455.1328125Mb; avail=470119.58203125Mb
2020-10-12 20:37:23 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 20:37:35 | INFO | train_inner | epoch 027:     46 / 129 loss=7.859, nll_loss=6.795, ppl=111.06, wps=12734.6, ups=2.45, wpb=5201.9, bsz=191.9, num_updates=3400, lr=0.000170015, gnorm=1.371, clip=0, train_wall=26, wall=1218
2020-10-12 20:37:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:37:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18468.046875Mb; avail=470105.95703125Mb
2020-10-12 20:37:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002044
2020-10-12 20:37:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18468.65234375Mb; avail=470105.3515625Mb
2020-10-12 20:37:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070740
2020-10-12 20:37:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18468.62890625Mb; avail=470105.2578125Mb
2020-10-12 20:37:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049608
2020-10-12 20:37:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123220
2020-10-12 20:37:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18468.671875Mb; avail=470105.2890625Mb
2020-10-12 20:37:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18468.671875Mb; avail=470105.2890625Mb
2020-10-12 20:37:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001159
2020-10-12 20:37:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18468.671875Mb; avail=470105.2890625Mb
2020-10-12 20:37:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068669
2020-10-12 20:37:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18468.66796875Mb; avail=470105.41015625Mb
2020-10-12 20:37:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047978
2020-10-12 20:37:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118586
2020-10-12 20:37:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18468.75Mb; avail=470105.4140625Mb
2020-10-12 20:38:01 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.083 | nll_loss 6.99 | ppl 127.11 | wps 49177 | wpb 1838.8 | bsz 68.7 | num_updates 3483 | best_loss 8.083
2020-10-12 20:38:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:38:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 27 @ 3483 updates, score 8.083) (writing took 9.805255376999412 seconds)
2020-10-12 20:38:11 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 20:38:11 | INFO | train | epoch 027 | loss 7.81 | nll_loss 6.738 | ppl 106.74 | wps 14355.9 | ups 2.68 | wpb 5357.6 | bsz 201 | num_updates 3483 | lr 0.000174163 | gnorm 1.359 | clip 0 | train_wall 34 | wall 1253
2020-10-12 20:38:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 20:38:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 20:38:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17938.79296875Mb; avail=470641.421875Mb
2020-10-12 20:38:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000682
2020-10-12 20:38:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006828
2020-10-12 20:38:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17938.30078125Mb; avail=470641.66796875Mb
2020-10-12 20:38:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000250
2020-10-12 20:38:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17938.79296875Mb; avail=470642.0390625Mb
2020-10-12 20:38:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090947
2020-10-12 20:38:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098929
2020-10-12 20:38:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17930.875Mb; avail=470648.98828125Mb
2020-10-12 20:38:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17931.01953125Mb; avail=470649.5390625Mb
2020-10-12 20:38:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003818
2020-10-12 20:38:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17930.35546875Mb; avail=470648.97265625Mb
2020-10-12 20:38:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000220
2020-10-12 20:38:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17930.9609375Mb; avail=470649.46484375Mb
2020-10-12 20:38:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092909
2020-10-12 20:38:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097692
2020-10-12 20:38:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17931.71484375Mb; avail=470647.625Mb
2020-10-12 20:38:11 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 20:38:16 | INFO | train_inner | epoch 028:     17 / 129 loss=7.776, nll_loss=6.698, ppl=103.83, wps=13300.7, ups=2.49, wpb=5346.7, bsz=204.6, num_updates=3500, lr=0.000175013, gnorm=1.363, clip=0, train_wall=26, wall=1258
2020-10-12 20:38:42 | INFO | train_inner | epoch 028:    117 / 129 loss=7.669, nll_loss=6.575, ppl=95.33, wps=19940.6, ups=3.73, wpb=5339.9, bsz=201, num_updates=3600, lr=0.00018001, gnorm=1.349, clip=0, train_wall=26, wall=1285
2020-10-12 20:38:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17874.2578125Mb; avail=470680.08984375Mb
2020-10-12 20:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001844
2020-10-12 20:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.2578125Mb; avail=470680.08984375Mb
2020-10-12 20:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070271
2020-10-12 20:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.15625Mb; avail=470680.08984375Mb
2020-10-12 20:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048450
2020-10-12 20:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121379
2020-10-12 20:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.83984375Mb; avail=470680.6953125Mb
2020-10-12 20:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17873.9765625Mb; avail=470680.453125Mb
2020-10-12 20:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001202
2020-10-12 20:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17873.9765625Mb; avail=470680.453125Mb
2020-10-12 20:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068578
2020-10-12 20:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.18359375Mb; avail=470680.33203125Mb
2020-10-12 20:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049533
2020-10-12 20:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120125
2020-10-12 20:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17874.19921875Mb; avail=470680.2109375Mb
2020-10-12 20:38:49 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.018 | nll_loss 6.917 | ppl 120.8 | wps 49505.3 | wpb 1838.8 | bsz 68.7 | num_updates 3612 | best_loss 8.018
2020-10-12 20:38:49 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:39:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 28 @ 3612 updates, score 8.018) (writing took 18.36012756799937 seconds)
2020-10-12 20:39:07 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 20:39:07 | INFO | train | epoch 028 | loss 7.682 | nll_loss 6.59 | ppl 96.31 | wps 12284.1 | ups 2.29 | wpb 5357.6 | bsz 201 | num_updates 3612 | lr 0.00018061 | gnorm 1.329 | clip 0 | train_wall 34 | wall 1309
2020-10-12 20:39:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 20:39:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 20:39:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17816.8359375Mb; avail=470737.57421875Mb
2020-10-12 20:39:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000928
2020-10-12 20:39:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005557
2020-10-12 20:39:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.8359375Mb; avail=470737.57421875Mb
2020-10-12 20:39:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000206
2020-10-12 20:39:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17816.8359375Mb; avail=470737.57421875Mb
2020-10-12 20:39:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091088
2020-10-12 20:39:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097628
2020-10-12 20:39:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17810.8046875Mb; avail=470743.4453125Mb
2020-10-12 20:39:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17810.73828125Mb; avail=470743.91015625Mb
2020-10-12 20:39:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003824
2020-10-12 20:39:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17810.75390625Mb; avail=470743.7890625Mb
2020-10-12 20:39:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000199
2020-10-12 20:39:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17810.75390625Mb; avail=470743.7890625Mb
2020-10-12 20:39:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092352
2020-10-12 20:39:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097122
2020-10-12 20:39:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.21484375Mb; avail=470743.2890625Mb
2020-10-12 20:39:07 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 20:39:31 | INFO | train_inner | epoch 029:     88 / 129 loss=7.569, nll_loss=6.459, ppl=87.97, wps=10887.8, ups=2.07, wpb=5257.7, bsz=204.2, num_updates=3700, lr=0.000185008, gnorm=1.39, clip=0, train_wall=26, wall=1333
2020-10-12 20:39:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18417.15234375Mb; avail=470138.078125Mb
2020-10-12 20:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001958
2020-10-12 20:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18418.234375Mb; avail=470137.375Mb
2020-10-12 20:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070388
2020-10-12 20:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18418.00390625Mb; avail=470137.6015625Mb
2020-10-12 20:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049105
2020-10-12 20:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122302
2020-10-12 20:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18418.30078125Mb; avail=470136.94921875Mb
2020-10-12 20:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18417.95703125Mb; avail=470136.52734375Mb
2020-10-12 20:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001242
2020-10-12 20:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18417.828125Mb; avail=470136.4140625Mb
2020-10-12 20:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070737
2020-10-12 20:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18417.80078125Mb; avail=470136.921875Mb
2020-10-12 20:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049101
2020-10-12 20:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121869
2020-10-12 20:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18417.73828125Mb; avail=470136.55859375Mb
2020-10-12 20:39:45 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.984 | nll_loss 6.85 | ppl 115.32 | wps 49482.8 | wpb 1838.8 | bsz 68.7 | num_updates 3741 | best_loss 7.984
2020-10-12 20:39:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:39:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 29 @ 3741 updates, score 7.984) (writing took 12.070714710000175 seconds)
2020-10-12 20:39:57 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 20:39:57 | INFO | train | epoch 029 | loss 7.573 | nll_loss 6.462 | ppl 88.15 | wps 13827.7 | ups 2.58 | wpb 5357.6 | bsz 201 | num_updates 3741 | lr 0.000187056 | gnorm 1.369 | clip 0 | train_wall 34 | wall 1359
2020-10-12 20:39:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 20:39:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17802.17578125Mb; avail=470752.796875Mb
2020-10-12 20:39:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000788
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005647
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.2890625Mb; avail=470752.19140625Mb
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000207
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17802.2890625Mb; avail=470752.19140625Mb
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091166
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097790
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17795.78125Mb; avail=470758.99609375Mb
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17796.046875Mb; avail=470758.6328125Mb
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003678
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17796.65234375Mb; avail=470758.02734375Mb
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17796.65234375Mb; avail=470758.02734375Mb
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.090439
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095065
2020-10-12 20:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17811.6484375Mb; avail=470743.07421875Mb
2020-10-12 20:39:57 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 20:40:13 | INFO | train_inner | epoch 030:     59 / 129 loss=7.526, nll_loss=6.406, ppl=84.81, wps=12897.4, ups=2.35, wpb=5482.8, bsz=207.1, num_updates=3800, lr=0.000190005, gnorm=1.33, clip=0, train_wall=27, wall=1375
2020-10-12 20:40:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19035.37890625Mb; avail=469519.0546875Mb
2020-10-12 20:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001789
2020-10-12 20:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19035.37890625Mb; avail=469519.0546875Mb
2020-10-12 20:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069285
2020-10-12 20:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19035.96875Mb; avail=469518.5703125Mb
2020-10-12 20:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048718
2020-10-12 20:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120588
2020-10-12 20:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19035.90234375Mb; avail=469518.44921875Mb
2020-10-12 20:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19035.8203125Mb; avail=469518.93359375Mb
2020-10-12 20:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001205
2020-10-12 20:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19035.8203125Mb; avail=469518.93359375Mb
2020-10-12 20:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068527
2020-10-12 20:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19035.8359375Mb; avail=469518.8125Mb
2020-10-12 20:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048708
2020-10-12 20:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119222
2020-10-12 20:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19035.83203125Mb; avail=469518.69140625Mb
2020-10-12 20:40:35 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.941 | nll_loss 6.792 | ppl 110.82 | wps 49449.9 | wpb 1838.8 | bsz 68.7 | num_updates 3870 | best_loss 7.941
2020-10-12 20:40:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:40:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 30 @ 3870 updates, score 7.941) (writing took 15.127686692001589 seconds)
2020-10-12 20:40:50 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 20:40:50 | INFO | train | epoch 030 | loss 7.457 | nll_loss 6.327 | ppl 80.26 | wps 13021.8 | ups 2.43 | wpb 5357.6 | bsz 201 | num_updates 3870 | lr 0.000193503 | gnorm 1.367 | clip 0 | train_wall 34 | wall 1412
2020-10-12 20:40:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 20:40:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 20:40:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17807.12109375Mb; avail=470747.6015625Mb
2020-10-12 20:40:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000657
2020-10-12 20:40:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005303
2020-10-12 20:40:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17807.12109375Mb; avail=470747.6015625Mb
2020-10-12 20:40:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000232
2020-10-12 20:40:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17807.12109375Mb; avail=470747.6015625Mb
2020-10-12 20:40:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095886
2020-10-12 20:40:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102362
2020-10-12 20:40:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.76953125Mb; avail=470753.65625Mb
2020-10-12 20:40:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17800.23046875Mb; avail=470754.30078125Mb
2020-10-12 20:40:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004091
2020-10-12 20:40:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.23046875Mb; avail=470754.30078125Mb
2020-10-12 20:40:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000224
2020-10-12 20:40:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.23046875Mb; avail=470754.30078125Mb
2020-10-12 20:40:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093769
2020-10-12 20:40:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098942
2020-10-12 20:40:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17800.2578125Mb; avail=470754.31640625Mb
2020-10-12 20:40:50 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 20:40:59 | INFO | train_inner | epoch 031:     30 / 129 loss=7.431, nll_loss=6.294, ppl=78.49, wps=12193.1, ups=2.21, wpb=5525.6, bsz=195.4, num_updates=3900, lr=0.000195003, gnorm=1.336, clip=0, train_wall=26, wall=1421
2020-10-12 20:41:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:41:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18440.17578125Mb; avail=470114.5Mb
2020-10-12 20:41:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001632
2020-10-12 20:41:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18440.140625Mb; avail=470114.5Mb
2020-10-12 20:41:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069104
2020-10-12 20:41:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18440.296875Mb; avail=470114.13671875Mb
2020-10-12 20:41:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048728
2020-10-12 20:41:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120256
2020-10-12 20:41:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18440.1953125Mb; avail=470114.13671875Mb
2020-10-12 20:41:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18440.16015625Mb; avail=470114.2578125Mb
2020-10-12 20:41:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001174
2020-10-12 20:41:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18440.16015625Mb; avail=470114.2578125Mb
2020-10-12 20:41:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068896
2020-10-12 20:41:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18440.0390625Mb; avail=470114.37890625Mb
2020-10-12 20:41:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048372
2020-10-12 20:41:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119242
2020-10-12 20:41:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18000.0234375Mb; avail=470554.39453125Mb
2020-10-12 20:41:28 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.888 | nll_loss 6.727 | ppl 105.96 | wps 49497.6 | wpb 1838.8 | bsz 68.7 | num_updates 3999 | best_loss 7.888
2020-10-12 20:41:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:41:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 31 @ 3999 updates, score 7.888) (writing took 4.501089223998861 seconds)
2020-10-12 20:41:33 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 20:41:33 | INFO | train | epoch 031 | loss 7.34 | nll_loss 6.19 | ppl 72.99 | wps 16297.6 | ups 3.04 | wpb 5357.6 | bsz 201 | num_updates 3999 | lr 0.00019995 | gnorm 1.38 | clip 0 | train_wall 34 | wall 1455
2020-10-12 20:41:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 20:41:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 20:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16617.26171875Mb; avail=471939.3359375Mb
2020-10-12 20:41:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000928
2020-10-12 20:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006115
2020-10-12 20:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16620.21484375Mb; avail=471936.1875Mb
2020-10-12 20:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000206
2020-10-12 20:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16620.21484375Mb; avail=471936.1875Mb
2020-10-12 20:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091825
2020-10-12 20:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099052
2020-10-12 20:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16684.76953125Mb; avail=471871.9140625Mb
2020-10-12 20:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16715.1484375Mb; avail=471841.76171875Mb
2020-10-12 20:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003680
2020-10-12 20:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16715.75390625Mb; avail=471841.15625Mb
2020-10-12 20:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000205
2020-10-12 20:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16715.75390625Mb; avail=471841.15625Mb
2020-10-12 20:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092051
2020-10-12 20:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096793
2020-10-12 20:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16716.6875Mb; avail=471840.45703125Mb
2020-10-12 20:41:33 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 20:41:33 | INFO | train_inner | epoch 032:      1 / 129 loss=7.346, nll_loss=6.196, ppl=73.31, wps=15170.2, ups=2.89, wpb=5246.6, bsz=201.4, num_updates=4000, lr=0.0002, gnorm=1.402, clip=0, train_wall=26, wall=1455
2020-10-12 20:42:00 | INFO | train_inner | epoch 032:    101 / 129 loss=7.228, nll_loss=6.059, ppl=66.65, wps=20000.2, ups=3.68, wpb=5427.7, bsz=201.9, num_updates=4100, lr=0.000197546, gnorm=1.375, clip=0, train_wall=26, wall=1482
2020-10-12 20:42:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17943.953125Mb; avail=470610.4765625Mb
2020-10-12 20:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001697
2020-10-12 20:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17943.953125Mb; avail=470610.4765625Mb
2020-10-12 20:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069166
2020-10-12 20:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17943.765625Mb; avail=470610.35546875Mb
2020-10-12 20:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049245
2020-10-12 20:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120934
2020-10-12 20:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17943.953125Mb; avail=470610.4765625Mb
2020-10-12 20:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17944.48828125Mb; avail=470610.234375Mb
2020-10-12 20:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001283
2020-10-12 20:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17944.48828125Mb; avail=470610.234375Mb
2020-10-12 20:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069653
2020-10-12 20:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17944.49609375Mb; avail=470610.11328125Mb
2020-10-12 20:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048095
2020-10-12 20:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119900
2020-10-12 20:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17944.8125Mb; avail=470609.65234375Mb
2020-10-12 20:42:11 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.888 | nll_loss 6.714 | ppl 105.02 | wps 49254.9 | wpb 1838.8 | bsz 68.7 | num_updates 4128 | best_loss 7.888
2020-10-12 20:42:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:42:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 32 @ 4128 updates, score 7.888) (writing took 4.4568339889992785 seconds)
2020-10-12 20:42:15 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 20:42:15 | INFO | train | epoch 032 | loss 7.218 | nll_loss 6.047 | ppl 66.12 | wps 16255 | ups 3.03 | wpb 5357.6 | bsz 201 | num_updates 4128 | lr 0.000196875 | gnorm 1.389 | clip 0 | train_wall 34 | wall 1497
2020-10-12 20:42:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 20:42:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 20:42:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17910.13671875Mb; avail=470644.1953125Mb
2020-10-12 20:42:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001084
2020-10-12 20:42:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006193
2020-10-12 20:42:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17910.13671875Mb; avail=470644.1953125Mb
2020-10-12 20:42:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000189
2020-10-12 20:42:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17910.13671875Mb; avail=470644.1953125Mb
2020-10-12 20:42:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091955
2020-10-12 20:42:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099207
2020-10-12 20:42:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17918.578125Mb; avail=470635.8515625Mb
2020-10-12 20:42:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17925.0078125Mb; avail=470629.3125Mb
2020-10-12 20:42:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003716
2020-10-12 20:42:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17925.0078125Mb; avail=470629.3125Mb
2020-10-12 20:42:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000171
2020-10-12 20:42:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17925.0078125Mb; avail=470629.3125Mb
2020-10-12 20:42:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091941
2020-10-12 20:42:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096555
2020-10-12 20:42:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17924.93359375Mb; avail=470629.3125Mb
2020-10-12 20:42:15 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 20:42:35 | INFO | train_inner | epoch 033:     72 / 129 loss=7.112, nll_loss=5.924, ppl=60.72, wps=15434.2, ups=2.9, wpb=5320.3, bsz=200.1, num_updates=4200, lr=0.00019518, gnorm=1.401, clip=0, train_wall=26, wall=1517
2020-10-12 20:42:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:42:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17952.85546875Mb; avail=470601.8359375Mb
2020-10-12 20:42:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001724
2020-10-12 20:42:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17952.85546875Mb; avail=470601.8359375Mb
2020-10-12 20:42:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070511
2020-10-12 20:42:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17952.76953125Mb; avail=470601.71484375Mb
2020-10-12 20:42:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049131
2020-10-12 20:42:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122172
2020-10-12 20:42:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17952.7265625Mb; avail=470601.95703125Mb
2020-10-12 20:42:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17952.7265625Mb; avail=470601.95703125Mb
2020-10-12 20:42:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001153
2020-10-12 20:42:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17952.7265625Mb; avail=470601.95703125Mb
2020-10-12 20:42:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070583
2020-10-12 20:42:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17952.6953125Mb; avail=470602.078125Mb
2020-10-12 20:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048726
2020-10-12 20:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121395
2020-10-12 20:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17952.70703125Mb; avail=470601.95703125Mb
2020-10-12 20:42:53 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.808 | nll_loss 6.617 | ppl 98.16 | wps 49342.3 | wpb 1838.8 | bsz 68.7 | num_updates 4257 | best_loss 7.808
2020-10-12 20:42:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:43:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 33 @ 4257 updates, score 7.808) (writing took 6.901176422999924 seconds)
2020-10-12 20:43:00 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 20:43:00 | INFO | train | epoch 033 | loss 7.092 | nll_loss 5.9 | ppl 59.7 | wps 15358.6 | ups 2.87 | wpb 5357.6 | bsz 201 | num_updates 4257 | lr 0.000193869 | gnorm 1.365 | clip 0 | train_wall 34 | wall 1542
2020-10-12 20:43:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 20:43:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 20:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17942.22265625Mb; avail=470614.6171875Mb
2020-10-12 20:43:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000840
2020-10-12 20:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005548
2020-10-12 20:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17942.53125Mb; avail=470614.203125Mb
2020-10-12 20:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000197
2020-10-12 20:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17942.53125Mb; avail=470614.6953125Mb
2020-10-12 20:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094214
2020-10-12 20:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100799
2020-10-12 20:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17935.50390625Mb; avail=470621.375Mb
2020-10-12 20:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17935.33984375Mb; avail=470620.35546875Mb
2020-10-12 20:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004107
2020-10-12 20:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17935.29296875Mb; avail=470620.37890625Mb
2020-10-12 20:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 20:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17935.8984375Mb; avail=470620.265625Mb
2020-10-12 20:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094453
2020-10-12 20:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099637
2020-10-12 20:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17935.21875Mb; avail=470619.74609375Mb
2020-10-12 20:43:00 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 20:43:12 | INFO | train_inner | epoch 034:     43 / 129 loss=7.008, nll_loss=5.803, ppl=55.83, wps=14297.7, ups=2.69, wpb=5309.1, bsz=202.3, num_updates=4300, lr=0.000192897, gnorm=1.329, clip=0, train_wall=26, wall=1554
2020-10-12 20:43:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15472.28515625Mb; avail=473094.625Mb
2020-10-12 20:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001927
2020-10-12 20:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15472.28515625Mb; avail=473094.625Mb
2020-10-12 20:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070156
2020-10-12 20:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15472.28515625Mb; avail=473094.625Mb
2020-10-12 20:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048823
2020-10-12 20:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121783
2020-10-12 20:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15472.265625Mb; avail=473094.74609375Mb
2020-10-12 20:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15472.2734375Mb; avail=473094.625Mb
2020-10-12 20:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001201
2020-10-12 20:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15472.2734375Mb; avail=473094.625Mb
2020-10-12 20:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071169
2020-10-12 20:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15472.30078125Mb; avail=473094.625Mb
2020-10-12 20:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048625
2020-10-12 20:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121840
2020-10-12 20:43:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15472.2890625Mb; avail=473094.74609375Mb
2020-10-12 20:43:38 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.799 | nll_loss 6.605 | ppl 97.37 | wps 48878.6 | wpb 1838.8 | bsz 68.7 | num_updates 4386 | best_loss 7.799
2020-10-12 20:43:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:43:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 34 @ 4386 updates, score 7.799) (writing took 4.473325721999572 seconds)
2020-10-12 20:43:42 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 20:43:42 | INFO | train | epoch 034 | loss 6.968 | nll_loss 5.755 | ppl 54.02 | wps 16319.5 | ups 3.05 | wpb 5357.6 | bsz 201 | num_updates 4386 | lr 0.000190997 | gnorm 1.376 | clip 0 | train_wall 34 | wall 1585
2020-10-12 20:43:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 20:43:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 20:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15457.71484375Mb; avail=473109.11328125Mb
2020-10-12 20:43:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001048
2020-10-12 20:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005615
2020-10-12 20:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15457.71484375Mb; avail=473109.11328125Mb
2020-10-12 20:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000194
2020-10-12 20:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15457.71484375Mb; avail=473109.11328125Mb
2020-10-12 20:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094140
2020-10-12 20:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100718
2020-10-12 20:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15451.1640625Mb; avail=473115.74609375Mb
2020-10-12 20:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15451.0546875Mb; avail=473115.74609375Mb
2020-10-12 20:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003726
2020-10-12 20:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15451.0546875Mb; avail=473115.74609375Mb
2020-10-12 20:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000178
2020-10-12 20:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15451.0546875Mb; avail=473115.74609375Mb
2020-10-12 20:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091069
2020-10-12 20:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.095705
2020-10-12 20:43:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15451.046875Mb; avail=473115.74609375Mb
2020-10-12 20:43:43 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 20:43:46 | INFO | train_inner | epoch 035:     14 / 129 loss=6.986, nll_loss=5.774, ppl=54.72, wps=15317.8, ups=2.89, wpb=5291.9, bsz=194.3, num_updates=4400, lr=0.000190693, gnorm=1.426, clip=0, train_wall=26, wall=1589
2020-10-12 20:44:14 | INFO | train_inner | epoch 035:    114 / 129 loss=6.869, nll_loss=5.638, ppl=49.81, wps=19902.7, ups=3.68, wpb=5405.8, bsz=198.7, num_updates=4500, lr=0.000188562, gnorm=1.367, clip=0, train_wall=26, wall=1616
2020-10-12 20:44:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16227.015625Mb; avail=472340.57421875Mb
2020-10-12 20:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001857
2020-10-12 20:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16227.015625Mb; avail=472340.57421875Mb
2020-10-12 20:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068484
2020-10-12 20:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16227.12109375Mb; avail=472340.6640625Mb
2020-10-12 20:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047004
2020-10-12 20:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118140
2020-10-12 20:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16226.7890625Mb; avail=472340.2734375Mb
2020-10-12 20:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16226.7890625Mb; avail=472340.2734375Mb
2020-10-12 20:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001206
2020-10-12 20:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16226.7890625Mb; avail=472340.2734375Mb
2020-10-12 20:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068438
2020-10-12 20:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16226.53515625Mb; avail=472340.515625Mb
2020-10-12 20:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048033
2020-10-12 20:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118428
2020-10-12 20:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16226.55078125Mb; avail=472340.5Mb
2020-10-12 20:44:21 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.736 | nll_loss 6.52 | ppl 91.79 | wps 48821.4 | wpb 1838.8 | bsz 68.7 | num_updates 4515 | best_loss 7.736
2020-10-12 20:44:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:44:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 35 @ 4515 updates, score 7.736) (writing took 4.713931114998559 seconds)
2020-10-12 20:44:25 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 20:44:25 | INFO | train | epoch 035 | loss 6.856 | nll_loss 5.624 | ppl 49.32 | wps 16067 | ups 3 | wpb 5357.6 | bsz 201 | num_updates 4515 | lr 0.000188248 | gnorm 1.38 | clip 0 | train_wall 34 | wall 1628
2020-10-12 20:44:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 20:44:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 20:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15522.921875Mb; avail=473043.88671875Mb
2020-10-12 20:44:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001003
2020-10-12 20:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.006155
2020-10-12 20:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15522.921875Mb; avail=473043.88671875Mb
2020-10-12 20:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000270
2020-10-12 20:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15522.921875Mb; avail=473043.88671875Mb
2020-10-12 20:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.094016
2020-10-12 20:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.101501
2020-10-12 20:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15516.7890625Mb; avail=473050.29296875Mb
2020-10-12 20:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15516.83203125Mb; avail=473050.05078125Mb
2020-10-12 20:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004189
2020-10-12 20:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15516.83203125Mb; avail=473050.05078125Mb
2020-10-12 20:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000251
2020-10-12 20:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15516.83203125Mb; avail=473050.05078125Mb
2020-10-12 20:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091486
2020-10-12 20:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096810
2020-10-12 20:44:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15516.453125Mb; avail=473049.9375Mb
2020-10-12 20:44:26 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 20:44:49 | INFO | train_inner | epoch 036:     85 / 129 loss=6.747, nll_loss=5.497, ppl=45.15, wps=15309.7, ups=2.83, wpb=5400.4, bsz=203.8, num_updates=4600, lr=0.000186501, gnorm=1.356, clip=0, train_wall=27, wall=1651
2020-10-12 20:45:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15604.4375Mb; avail=472962.3984375Mb
2020-10-12 20:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001872
2020-10-12 20:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15604.4375Mb; avail=472962.3984375Mb
2020-10-12 20:45:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069927
2020-10-12 20:45:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15604.796875Mb; avail=472962.52734375Mb
2020-10-12 20:45:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047271
2020-10-12 20:45:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119872
2020-10-12 20:45:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15604.73046875Mb; avail=472962.40625Mb
2020-10-12 20:45:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15604.73046875Mb; avail=472962.40625Mb
2020-10-12 20:45:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001135
2020-10-12 20:45:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15604.73046875Mb; avail=472962.40625Mb
2020-10-12 20:45:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069697
2020-10-12 20:45:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15604.73046875Mb; avail=472962.40625Mb
2020-10-12 20:45:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046896
2020-10-12 20:45:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118523
2020-10-12 20:45:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15604.68359375Mb; avail=472962.6484375Mb
2020-10-12 20:45:03 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.736 | nll_loss 6.5 | ppl 90.49 | wps 48819.4 | wpb 1838.8 | bsz 68.7 | num_updates 4644 | best_loss 7.736
2020-10-12 20:45:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:45:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 36 @ 4644 updates, score 7.736) (writing took 4.4514562229996955 seconds)
2020-10-12 20:45:08 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 20:45:08 | INFO | train | epoch 036 | loss 6.741 | nll_loss 5.489 | ppl 44.91 | wps 16282.8 | ups 3.04 | wpb 5357.6 | bsz 201 | num_updates 4644 | lr 0.000185615 | gnorm 1.385 | clip 0 | train_wall 34 | wall 1670
2020-10-12 20:45:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 20:45:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 20:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15611.14453125Mb; avail=472955.69921875Mb
2020-10-12 20:45:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000656
2020-10-12 20:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005292
2020-10-12 20:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15611.3359375Mb; avail=472955.70703125Mb
2020-10-12 20:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000193
2020-10-12 20:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15611.3359375Mb; avail=472955.70703125Mb
2020-10-12 20:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093066
2020-10-12 20:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099300
2020-10-12 20:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15604.76171875Mb; avail=472962.37109375Mb
2020-10-12 20:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15604.76953125Mb; avail=472962.37109375Mb
2020-10-12 20:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003820
2020-10-12 20:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15604.76953125Mb; avail=472962.37109375Mb
2020-10-12 20:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000192
2020-10-12 20:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15604.76953125Mb; avail=472962.37109375Mb
2020-10-12 20:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.092475
2020-10-12 20:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097243
2020-10-12 20:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15604.91015625Mb; avail=472962.25Mb
2020-10-12 20:45:08 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 20:45:23 | INFO | train_inner | epoch 037:     56 / 129 loss=6.672, nll_loss=5.408, ppl=42.46, wps=15515.8, ups=2.92, wpb=5321.5, bsz=201.7, num_updates=4700, lr=0.000184506, gnorm=1.408, clip=0, train_wall=26, wall=1685
2020-10-12 20:45:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15572.15234375Mb; avail=472994.77734375Mb
2020-10-12 20:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001940
2020-10-12 20:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15572.7578125Mb; avail=472994.171875Mb
2020-10-12 20:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071410
2020-10-12 20:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15572.77734375Mb; avail=472994.171875Mb
2020-10-12 20:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049315
2020-10-12 20:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123515
2020-10-12 20:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15572.734375Mb; avail=472994.53515625Mb
2020-10-12 20:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15572.7421875Mb; avail=472994.4140625Mb
2020-10-12 20:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001218
2020-10-12 20:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15572.7421875Mb; avail=472994.4140625Mb
2020-10-12 20:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070557
2020-10-12 20:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15572.95703125Mb; avail=472994.17578125Mb
2020-10-12 20:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050261
2020-10-12 20:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123088
2020-10-12 20:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15572.9375Mb; avail=472994.17578125Mb
2020-10-12 20:45:46 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.718 | nll_loss 6.496 | ppl 90.25 | wps 48321.3 | wpb 1838.8 | bsz 68.7 | num_updates 4773 | best_loss 7.718
2020-10-12 20:45:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:45:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 37 @ 4773 updates, score 7.718) (writing took 4.459457003998978 seconds)
2020-10-12 20:45:50 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 20:45:50 | INFO | train | epoch 037 | loss 6.635 | nll_loss 5.364 | ppl 41.19 | wps 16242.8 | ups 3.03 | wpb 5357.6 | bsz 201 | num_updates 4773 | lr 0.00018309 | gnorm 1.407 | clip 0 | train_wall 34 | wall 1713
2020-10-12 20:45:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 20:45:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 20:45:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15558.4453125Mb; avail=473008.6953125Mb
2020-10-12 20:45:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000688
2020-10-12 20:45:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005426
2020-10-12 20:45:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15558.4453125Mb; avail=473008.6953125Mb
2020-10-12 20:45:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000222
2020-10-12 20:45:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15558.4453125Mb; avail=473008.6953125Mb
2020-10-12 20:45:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091648
2020-10-12 20:45:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098111
2020-10-12 20:45:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15552.92578125Mb; avail=473014.28125Mb
2020-10-12 20:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15553.08984375Mb; avail=473014.3359375Mb
2020-10-12 20:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003886
2020-10-12 20:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15553.08984375Mb; avail=473014.3359375Mb
2020-10-12 20:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000196
2020-10-12 20:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15553.08984375Mb; avail=473014.3359375Mb
2020-10-12 20:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091422
2020-10-12 20:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.096282
2020-10-12 20:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15553.05078125Mb; avail=473014.3359375Mb
2020-10-12 20:45:51 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 20:45:58 | INFO | train_inner | epoch 038:     27 / 129 loss=6.603, nll_loss=5.326, ppl=40.12, wps=15377.6, ups=2.87, wpb=5351.8, bsz=200.3, num_updates=4800, lr=0.000182574, gnorm=1.396, clip=0, train_wall=26, wall=1720
2020-10-12 20:46:25 | INFO | train_inner | epoch 038:    127 / 129 loss=6.544, nll_loss=5.256, ppl=38.2, wps=19814.6, ups=3.66, wpb=5411.3, bsz=205.3, num_updates=4900, lr=0.000180702, gnorm=1.406, clip=0, train_wall=27, wall=1747
2020-10-12 20:46:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10635.625Mb; avail=477955.8984375Mb
2020-10-12 20:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001721
2020-10-12 20:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10635.625Mb; avail=477955.8984375Mb
2020-10-12 20:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069770
2020-10-12 20:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10635.73046875Mb; avail=477955.77734375Mb
2020-10-12 20:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048008
2020-10-12 20:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120305
2020-10-12 20:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10635.74609375Mb; avail=477955.77734375Mb
2020-10-12 20:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10635.74609375Mb; avail=477955.77734375Mb
2020-10-12 20:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002214
2020-10-12 20:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10635.74609375Mb; avail=477955.77734375Mb
2020-10-12 20:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069835
2020-10-12 20:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10635.6328125Mb; avail=477955.8984375Mb
2020-10-12 20:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047486
2020-10-12 20:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120309
2020-10-12 20:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10635.6328125Mb; avail=477955.8984375Mb
2020-10-12 20:46:29 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.715 | nll_loss 6.469 | ppl 88.57 | wps 49047.8 | wpb 1838.8 | bsz 68.7 | num_updates 4902 | best_loss 7.715
2020-10-12 20:46:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:46:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 38 @ 4902 updates, score 7.715) (writing took 4.455306899999414 seconds)
2020-10-12 20:46:33 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 20:46:33 | INFO | train | epoch 038 | loss 6.528 | nll_loss 5.238 | ppl 37.73 | wps 16154.8 | ups 3.02 | wpb 5357.6 | bsz 201 | num_updates 4902 | lr 0.000180665 | gnorm 1.392 | clip 0 | train_wall 34 | wall 1755
2020-10-12 20:46:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 20:46:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 20:46:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10639.62890625Mb; avail=477951.59765625Mb
2020-10-12 20:46:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000704
2020-10-12 20:46:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005473
2020-10-12 20:46:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10639.6328125Mb; avail=477951.59765625Mb
2020-10-12 20:46:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000206
2020-10-12 20:46:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10639.6328125Mb; avail=477951.59765625Mb
2020-10-12 20:46:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093842
2020-10-12 20:46:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.100299
2020-10-12 20:46:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10633.49609375Mb; avail=477957.74609375Mb
2020-10-12 20:46:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10633.49609375Mb; avail=477957.74609375Mb
2020-10-12 20:46:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003825
2020-10-12 20:46:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10633.49609375Mb; avail=477957.74609375Mb
2020-10-12 20:46:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000191
2020-10-12 20:46:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10633.49609375Mb; avail=477957.74609375Mb
2020-10-12 20:46:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093549
2020-10-12 20:46:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.098362
2020-10-12 20:46:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10633.49609375Mb; avail=477957.74609375Mb
2020-10-12 20:46:33 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 20:47:00 | INFO | train_inner | epoch 039:     98 / 129 loss=6.42, nll_loss=5.113, ppl=34.6, wps=15449.6, ups=2.88, wpb=5360.7, bsz=206.3, num_updates=5000, lr=0.000178885, gnorm=1.42, clip=0, train_wall=26, wall=1782
2020-10-12 20:47:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10753.74609375Mb; avail=477838.12109375Mb
2020-10-12 20:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001804
2020-10-12 20:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10753.74609375Mb; avail=477838.12109375Mb
2020-10-12 20:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071047
2020-10-12 20:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10754.109375Mb; avail=477837.7578125Mb
2020-10-12 20:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050280
2020-10-12 20:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124139
2020-10-12 20:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10754.109375Mb; avail=477837.7578125Mb
2020-10-12 20:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10754.109375Mb; avail=477837.7578125Mb
2020-10-12 20:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001440
2020-10-12 20:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10754.22265625Mb; avail=477837.64453125Mb
2020-10-12 20:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069391
2020-10-12 20:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10754.22265625Mb; avail=477837.64453125Mb
2020-10-12 20:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048745
2020-10-12 20:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120337
2020-10-12 20:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10754.2734375Mb; avail=477837.765625Mb
2020-10-12 20:47:11 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.695 | nll_loss 6.436 | ppl 86.61 | wps 49062 | wpb 1838.8 | bsz 68.7 | num_updates 5031 | best_loss 7.695
2020-10-12 20:47:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:47:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 39 @ 5031 updates, score 7.695) (writing took 4.458329735000007 seconds)
2020-10-12 20:47:16 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 20:47:16 | INFO | train | epoch 039 | loss 6.431 | nll_loss 5.124 | ppl 34.86 | wps 16302.2 | ups 3.04 | wpb 5357.6 | bsz 201 | num_updates 5031 | lr 0.000178333 | gnorm 1.414 | clip 0 | train_wall 34 | wall 1798
2020-10-12 20:47:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 20:47:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 20:47:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=11348.12890625Mb; avail=477246.41015625Mb
2020-10-12 20:47:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000740
2020-10-12 20:47:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005819
2020-10-12 20:47:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=11347.84765625Mb; avail=477246.01171875Mb
2020-10-12 20:47:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000217
2020-10-12 20:47:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=11348.234375Mb; avail=477246.390625Mb
2020-10-12 20:47:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.095839
2020-10-12 20:47:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.102859
2020-10-12 20:47:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=11348.80859375Mb; avail=477244.265625Mb
2020-10-12 20:47:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=11348.0859375Mb; avail=477243.8984375Mb
2020-10-12 20:47:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005258
2020-10-12 20:47:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=11342.1796875Mb; avail=477249.8046875Mb
2020-10-12 20:47:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000266
2020-10-12 20:47:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=11342.1796875Mb; avail=477249.8046875Mb
2020-10-12 20:47:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.093019
2020-10-12 20:47:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.099496
2020-10-12 20:47:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=11342.45703125Mb; avail=477249.76953125Mb
2020-10-12 20:47:16 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 20:47:35 | INFO | train_inner | epoch 040:     69 / 129 loss=6.378, nll_loss=5.061, ppl=33.37, wps=15780, ups=2.89, wpb=5453.1, bsz=201.4, num_updates=5100, lr=0.000177123, gnorm=1.388, clip=0, train_wall=26, wall=1817
2020-10-12 20:47:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 20:47:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10712.921875Mb; avail=477878.6953125Mb
2020-10-12 20:47:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001867
2020-10-12 20:47:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10712.921875Mb; avail=477878.6953125Mb
2020-10-12 20:47:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069079
2020-10-12 20:47:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10712.921875Mb; avail=477878.6953125Mb
2020-10-12 20:47:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048515
2020-10-12 20:47:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120270
2020-10-12 20:47:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10712.921875Mb; avail=477878.6953125Mb
2020-10-12 20:47:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10712.921875Mb; avail=477878.6953125Mb
2020-10-12 20:47:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001308
2020-10-12 20:47:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10712.921875Mb; avail=477878.6953125Mb
2020-10-12 20:47:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068789
2020-10-12 20:47:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10713.04296875Mb; avail=477878.57421875Mb
2020-10-12 20:47:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048008
2020-10-12 20:47:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118871
2020-10-12 20:47:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10713.1484375Mb; avail=477878.57421875Mb
2020-10-12 20:47:53 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.667 | nll_loss 6.402 | ppl 84.56 | wps 49127 | wpb 1838.8 | bsz 68.7 | num_updates 5160 | best_loss 7.667
2020-10-12 20:47:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 20:47:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azefas_sepspm8000/O2M/checkpoint_best.pt (epoch 40 @ 5160 updates, score 7.667) (writing took 4.494595115000266 seconds)
2020-10-12 20:47:58 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 20:47:58 | INFO | train | epoch 040 | loss 6.326 | nll_loss 5 | ppl 32 | wps 16346 | ups 3.05 | wpb 5357.6 | bsz 201 | num_updates 5160 | lr 0.00017609 | gnorm 1.383 | clip 0 | train_wall 34 | wall 1840
2020-10-12 20:47:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 20:47:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 20:47:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10677.59375Mb; avail=477913.9375Mb
2020-10-12 20:47:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000743
2020-10-12 20:47:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005253
2020-10-12 20:47:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10677.59375Mb; avail=477913.9375Mb
2020-10-12 20:47:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000181
2020-10-12 20:47:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10677.59375Mb; avail=477913.9375Mb
2020-10-12 20:47:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.091456
2020-10-12 20:47:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.097628
2020-10-12 20:47:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10671.65625Mb; avail=477919.875Mb
2020-10-12 20:47:58 | INFO | fairseq_cli.train | done training in 1840.0 seconds
