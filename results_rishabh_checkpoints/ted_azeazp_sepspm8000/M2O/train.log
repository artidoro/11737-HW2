Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.
	Try to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.
Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.
	Try to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.
Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.
	Try to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.
Traceback (most recent call last):
  File "/home/ubuntu/anaconda3/envs/torch16/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/home/ubuntu/courses/fairseq/fairseq_cli/train.py", line 352, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/ubuntu/courses/fairseq/fairseq/distributed_utils.py", line 241, in call_main
    torch.multiprocessing.spawn(
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 111, in join
    raise Exception(
Exception: process 0 terminated with exit code 1
2020-10-13 06:22:34 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_azeazp_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='aze-eng,azp-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-13 06:22:34 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-13 06:22:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['aze', 'azp', 'eng']
2020-10-13 06:22:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | [aze] dictionary: 20827 types
2020-10-13 06:22:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | [azp] dictionary: 20827 types
2020-10-13 06:22:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 20827 types
2020-10-13 06:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-13 06:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=4629.328125Mb; avail=239937.40625Mb
2020-10-13 06:22:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-13 06:22:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:aze-eng': 1, 'main:azp-eng': 1}
2020-10-13 06:22:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:22:34 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/valid.aze-eng.aze
2020-10-13 06:22:34 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/valid.aze-eng.eng
2020-10-13 06:22:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azeazp_sepspm8000/M2O/ valid aze-eng 671 examples
2020-10-13 06:22:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:azp-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:22:34 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/valid.azp-eng.azp
2020-10-13 06:22:34 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/valid.azp-eng.eng
2020-10-13 06:22:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azeazp_sepspm8000/M2O/ valid azp-eng 671 examples
2020-10-13 06:22:35 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(20827, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(20827, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=20827, bias=False)
  )
)
2020-10-13 06:22:35 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-13 06:22:35 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-13 06:22:35 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-13 06:22:35 | INFO | fairseq_cli.train | num. model params: 42206720 (num. trained: 42206720)
2020-10-13 06:22:39 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-13 06:22:39 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-13 06:22:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-13 06:22:39 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-13 06:22:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-13 06:22:39 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-13 06:22:39 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-13 06:22:39 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt
2020-10-13 06:22:39 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-13 06:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-13 06:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6914.08203125Mb; avail=237642.6484375Mb
2020-10-13 06:22:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-13 06:22:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:aze-eng': 1, 'main:azp-eng': 1}
2020-10-13 06:22:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:22:39 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/train.aze-eng.aze
2020-10-13 06:22:39 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/train.aze-eng.eng
2020-10-13 06:22:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azeazp_sepspm8000/M2O/ train aze-eng 5946 examples
2020-10-13 06:22:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:azp-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:22:39 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/train.azp-eng.azp
2020-10-13 06:22:39 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/train.azp-eng.eng
2020-10-13 06:22:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azeazp_sepspm8000/M2O/ train azp-eng 5946 examples
2020-10-13 06:22:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:aze-eng', 5946), ('main:azp-eng', 5946)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-13 06:22:39 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-13 06:22:39 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 11892
2020-10-13 06:22:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 11892; virtual dataset size 11892
2020-10-13 06:22:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-13 06:22:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:aze-eng': 5946, 'main:azp-eng': 5946}; raw total size: 11892
2020-10-13 06:22:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:aze-eng': 5946, 'main:azp-eng': 5946}; resampled total size: 11892
2020-10-13 06:22:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-13 06:22:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.001905
2020-10-13 06:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6914.08203125Mb; avail=237642.6484375Mb
2020-10-13 06:22:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000246
2020-10-13 06:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002420
2020-10-13 06:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6914.08203125Mb; avail=237642.6484375Mb
2020-10-13 06:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000093
2020-10-13 06:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6914.08203125Mb; avail=237642.6484375Mb
2020-10-13 06:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.042506
2020-10-13 06:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.045696
2020-10-13 06:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6914.08203125Mb; avail=237642.6484375Mb
2020-10-13 06:22:39 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-13 06:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6914.08203125Mb; avail=237642.6484375Mb
2020-10-13 06:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001916
2020-10-13 06:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6914.08203125Mb; avail=237642.6484375Mb
2020-10-13 06:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000082
2020-10-13 06:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6914.08203125Mb; avail=237642.6484375Mb
2020-10-13 06:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040639
2020-10-13 06:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043313
2020-10-13 06:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6914.08203125Mb; avail=237642.6484375Mb
2020-10-13 06:22:39 | INFO | fairseq.trainer | begin training epoch 1
2020-10-13 06:22:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:22:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7073.98828125Mb; avail=237480.6484375Mb
2020-10-13 06:22:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000820
2020-10-13 06:22:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7073.98828125Mb; avail=237480.6484375Mb
2020-10-13 06:22:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017353
2020-10-13 06:22:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7073.98828125Mb; avail=237480.6484375Mb
2020-10-13 06:22:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013433
2020-10-13 06:22:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032375
2020-10-13 06:22:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7073.98828125Mb; avail=237480.6484375Mb
2020-10-13 06:22:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7073.98828125Mb; avail=237480.6484375Mb
2020-10-13 06:22:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000741
2020-10-13 06:22:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7073.98828125Mb; avail=237480.6484375Mb
2020-10-13 06:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017748
2020-10-13 06:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7073.98828125Mb; avail=237480.6484375Mb
2020-10-13 06:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.014216
2020-10-13 06:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.033605
2020-10-13 06:22:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7073.98828125Mb; avail=237480.6484375Mb
/home/ubuntu/courses/fairseq/fairseq/utils.py:340: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2020-10-13 06:22:55 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.391 | nll_loss 13.204 | ppl 9439.07 | wps 51367.8 | wpb 2112 | bsz 83.9 | num_updates 56
2020-10-13 06:22:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:22:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 1 @ 56 updates, score 13.391) (writing took 1.6712640281766653 seconds)
2020-10-13 06:22:57 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-13 06:22:57 | INFO | train | epoch 001 | loss 14.659 | nll_loss 14.614 | ppl 25069 | wps 17007.9 | ups 3.19 | wpb 5324.5 | bsz 212.4 | num_updates 56 | lr 2.8986e-06 | gnorm 6.175 | clip 0 | train_wall 15 | wall 18
2020-10-13 06:22:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-13 06:22:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-13 06:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7075.3125Mb; avail=237478.94921875Mb
2020-10-13 06:22:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000310
2020-10-13 06:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002760
2020-10-13 06:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.3125Mb; avail=237478.94921875Mb
2020-10-13 06:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000093
2020-10-13 06:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.3125Mb; avail=237478.94921875Mb
2020-10-13 06:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040543
2020-10-13 06:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044182
2020-10-13 06:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.3125Mb; avail=237478.94921875Mb
2020-10-13 06:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7075.3125Mb; avail=237478.94921875Mb
2020-10-13 06:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002084
2020-10-13 06:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.3125Mb; avail=237478.94921875Mb
2020-10-13 06:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000084
2020-10-13 06:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.3125Mb; avail=237478.94921875Mb
2020-10-13 06:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.039978
2020-10-13 06:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.042905
2020-10-13 06:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.91796875Mb; avail=237478.34375Mb
2020-10-13 06:22:57 | INFO | fairseq.trainer | begin training epoch 2
2020-10-13 06:23:09 | INFO | train_inner | epoch 002:     44 / 56 loss=13.984, nll_loss=13.862, ppl=14893, wps=18113.8, ups=3.37, wpb=5362.1, bsz=211.4, num_updates=100, lr=5.0975e-06, gnorm=4.999, clip=0, train_wall=26, wall=30
2020-10-13 06:23:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7081.21484375Mb; avail=237473.82421875Mb
2020-10-13 06:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000871
2020-10-13 06:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7081.21484375Mb; avail=237473.82421875Mb
2020-10-13 06:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017353
2020-10-13 06:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7082.91015625Mb; avail=237472.12890625Mb
2020-10-13 06:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013114
2020-10-13 06:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032164
2020-10-13 06:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.84765625Mb; avail=237469.19140625Mb
2020-10-13 06:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7086.34375Mb; avail=237468.6953125Mb
2020-10-13 06:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000718
2020-10-13 06:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7086.34375Mb; avail=237468.6953125Mb
2020-10-13 06:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017320
2020-10-13 06:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.84375Mb; avail=237469.1953125Mb
2020-10-13 06:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013185
2020-10-13 06:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032050
2020-10-13 06:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.0Mb; avail=237479.0390625Mb
2020-10-13 06:23:13 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 12.107 | nll_loss 11.759 | ppl 3466.66 | wps 50531 | wpb 2112 | bsz 83.9 | num_updates 112 | best_loss 12.107
2020-10-13 06:23:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:23:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 2 @ 112 updates, score 12.107) (writing took 4.702039291150868 seconds)
2020-10-13 06:23:18 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-13 06:23:18 | INFO | train | epoch 002 | loss 13.018 | nll_loss 12.787 | ppl 7068.65 | wps 14361.1 | ups 2.7 | wpb 5324.5 | bsz 212.4 | num_updates 112 | lr 5.6972e-06 | gnorm 3.377 | clip 0 | train_wall 14 | wall 39
2020-10-13 06:23:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-13 06:23:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-13 06:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7073.8125Mb; avail=237480.80078125Mb
2020-10-13 06:23:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000276
2020-10-13 06:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002770
2020-10-13 06:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7073.8125Mb; avail=237480.80078125Mb
2020-10-13 06:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000094
2020-10-13 06:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7073.8125Mb; avail=237480.80078125Mb
2020-10-13 06:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040013
2020-10-13 06:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043614
2020-10-13 06:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.83984375Mb; avail=237477.7734375Mb
2020-10-13 06:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7078.65625Mb; avail=237475.95703125Mb
2020-10-13 06:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001952
2020-10-13 06:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.26171875Mb; avail=237475.3515625Mb
2020-10-13 06:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000090
2020-10-13 06:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.26171875Mb; avail=237475.3515625Mb
2020-10-13 06:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040562
2020-10-13 06:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043357
2020-10-13 06:23:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.01953125Mb; avail=237470.59375Mb
2020-10-13 06:23:18 | INFO | fairseq.trainer | begin training epoch 3
2020-10-13 06:23:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:23:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7076.62890625Mb; avail=237477.671875Mb
2020-10-13 06:23:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000884
2020-10-13 06:23:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.62890625Mb; avail=237477.671875Mb
2020-10-13 06:23:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016926
2020-10-13 06:23:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.62890625Mb; avail=237477.671875Mb
2020-10-13 06:23:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013163
2020-10-13 06:23:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031974
2020-10-13 06:23:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.62890625Mb; avail=237477.671875Mb
2020-10-13 06:23:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7076.62890625Mb; avail=237477.671875Mb
2020-10-13 06:23:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000765
2020-10-13 06:23:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.62890625Mb; avail=237477.671875Mb
2020-10-13 06:23:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017128
2020-10-13 06:23:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.62890625Mb; avail=237477.671875Mb
2020-10-13 06:23:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012991
2020-10-13 06:23:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031629
2020-10-13 06:23:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.62890625Mb; avail=237477.671875Mb
2020-10-13 06:23:34 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 11.641 | nll_loss 11.236 | ppl 2411.85 | wps 52209.5 | wpb 2112 | bsz 83.9 | num_updates 168 | best_loss 11.641
2020-10-13 06:23:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:23:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 3 @ 168 updates, score 11.641) (writing took 4.676293481141329 seconds)
2020-10-13 06:23:38 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-13 06:23:38 | INFO | train | epoch 003 | loss 12.206 | nll_loss 11.88 | ppl 3769.38 | wps 14420.6 | ups 2.71 | wpb 5324.5 | bsz 212.4 | num_updates 168 | lr 8.4958e-06 | gnorm 2.151 | clip 0 | train_wall 14 | wall 60
2020-10-13 06:23:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-13 06:23:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-13 06:23:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7073.4765625Mb; avail=237481.16015625Mb
2020-10-13 06:23:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000299
2020-10-13 06:23:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002632
2020-10-13 06:23:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7073.4765625Mb; avail=237481.16015625Mb
2020-10-13 06:23:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000091
2020-10-13 06:23:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7073.4765625Mb; avail=237481.16015625Mb
2020-10-13 06:23:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040086
2020-10-13 06:23:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043555
2020-10-13 06:23:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7073.49609375Mb; avail=237481.23046875Mb
2020-10-13 06:23:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7073.49609375Mb; avail=237481.23046875Mb
2020-10-13 06:23:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001982
2020-10-13 06:23:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7073.49609375Mb; avail=237481.23046875Mb
2020-10-13 06:23:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000082
2020-10-13 06:23:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7073.49609375Mb; avail=237481.23046875Mb
2020-10-13 06:23:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040268
2020-10-13 06:23:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043072
2020-10-13 06:23:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7073.49609375Mb; avail=237481.23046875Mb
2020-10-13 06:23:39 | INFO | fairseq.trainer | begin training epoch 4
2020-10-13 06:23:47 | INFO | train_inner | epoch 004:     32 / 56 loss=12.13, nll_loss=11.795, ppl=3552.81, wps=13989.4, ups=2.62, wpb=5348.1, bsz=218.4, num_updates=200, lr=1.0095e-05, gnorm=2.146, clip=0, train_wall=25, wall=69
2020-10-13 06:23:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:23:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7076.140625Mb; avail=237478.25390625Mb
2020-10-13 06:23:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000841
2020-10-13 06:23:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.140625Mb; avail=237478.25390625Mb
2020-10-13 06:23:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017248
2020-10-13 06:23:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.140625Mb; avail=237478.25390625Mb
2020-10-13 06:23:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013486
2020-10-13 06:23:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032350
2020-10-13 06:23:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.140625Mb; avail=237478.25390625Mb
2020-10-13 06:23:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7076.140625Mb; avail=237478.25390625Mb
2020-10-13 06:23:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000731
2020-10-13 06:23:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.140625Mb; avail=237478.25390625Mb
2020-10-13 06:23:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017252
2020-10-13 06:23:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.5625Mb; avail=237475.83203125Mb
2020-10-13 06:23:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013410
2020-10-13 06:23:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032155
2020-10-13 06:23:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.7734375Mb; avail=237474.62109375Mb
2020-10-13 06:23:54 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 11.243 | nll_loss 10.787 | ppl 1766.44 | wps 51235.1 | wpb 2112 | bsz 83.9 | num_updates 224 | best_loss 11.243
2020-10-13 06:23:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:23:59 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 4 @ 224 updates, score 11.243) (writing took 4.670617541996762 seconds)
2020-10-13 06:23:59 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-13 06:23:59 | INFO | train | epoch 004 | loss 11.721 | nll_loss 11.338 | ppl 2589.46 | wps 14381.2 | ups 2.7 | wpb 5324.5 | bsz 212.4 | num_updates 224 | lr 1.12944e-05 | gnorm 1.823 | clip 0 | train_wall 14 | wall 80
2020-10-13 06:23:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-13 06:23:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-13 06:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7075.37890625Mb; avail=237479.25390625Mb
2020-10-13 06:23:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000304
2020-10-13 06:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002674
2020-10-13 06:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.37890625Mb; avail=237479.25390625Mb
2020-10-13 06:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000092
2020-10-13 06:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.37890625Mb; avail=237479.25390625Mb
2020-10-13 06:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040270
2020-10-13 06:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043767
2020-10-13 06:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.38671875Mb; avail=237479.24609375Mb
2020-10-13 06:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7075.38671875Mb; avail=237479.24609375Mb
2020-10-13 06:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001919
2020-10-13 06:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.38671875Mb; avail=237479.24609375Mb
2020-10-13 06:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000089
2020-10-13 06:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.38671875Mb; avail=237479.24609375Mb
2020-10-13 06:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040473
2020-10-13 06:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043199
2020-10-13 06:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.38671875Mb; avail=237479.24609375Mb
2020-10-13 06:23:59 | INFO | fairseq.trainer | begin training epoch 5
Traceback (most recent call last):
  File "/home/ubuntu/anaconda3/envs/torch16/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/home/ubuntu/courses/fairseq/fairseq_cli/train.py", line 352, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/ubuntu/courses/fairseq/fairseq/distributed_utils.py", line 258, in call_main
    main(args, **kwargs)
  File "/home/ubuntu/courses/fairseq/fairseq_cli/train.py", line 125, in main
    valid_losses, should_stop = train(args, trainer, task, epoch_itr)
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/ubuntu/courses/fairseq/fairseq_cli/train.py", line 208, in train
    log_output = trainer.train_step(samples)
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/ubuntu/courses/fairseq/fairseq/trainer.py", line 567, in train_step
    self.optimizer.step()
  File "/home/ubuntu/courses/fairseq/fairseq/optim/fairseq_optimizer.py", line 114, in step
    self.optimizer.step(closure)
  File "/home/ubuntu/courses/fairseq/fairseq/optim/adam.py", line 200, in step
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
KeyboardInterrupt
2020-10-13 06:24:11 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_azeazp_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='aze-eng,azp-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-13 06:24:11 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-13 06:24:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['aze', 'azp', 'eng']
2020-10-13 06:24:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | [aze] dictionary: 20827 types
2020-10-13 06:24:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | [azp] dictionary: 20827 types
2020-10-13 06:24:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 20827 types
2020-10-13 06:24:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-13 06:24:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=4630.484375Mb; avail=239936.2421875Mb
2020-10-13 06:24:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-13 06:24:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:aze-eng': 1, 'main:azp-eng': 1}
2020-10-13 06:24:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:24:11 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/valid.aze-eng.aze
2020-10-13 06:24:11 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/valid.aze-eng.eng
2020-10-13 06:24:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azeazp_sepspm8000/M2O/ valid aze-eng 671 examples
2020-10-13 06:24:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:azp-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:24:11 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/valid.azp-eng.azp
2020-10-13 06:24:11 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/valid.azp-eng.eng
2020-10-13 06:24:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azeazp_sepspm8000/M2O/ valid azp-eng 671 examples
2020-10-13 06:24:12 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(20827, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(20827, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=20827, bias=False)
  )
)
2020-10-13 06:24:12 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-13 06:24:12 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-13 06:24:12 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-13 06:24:12 | INFO | fairseq_cli.train | num. model params: 42206720 (num. trained: 42206720)
2020-10-13 06:24:15 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-13 06:24:15 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-13 06:24:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-13 06:24:15 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-13 06:24:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-13 06:24:15 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-13 06:24:15 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-13 06:24:16 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-13 06:24:16 | INFO | fairseq.trainer | loaded checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 5 @ 224 updates)
2020-10-13 06:24:16 | INFO | fairseq.trainer | loading train data for epoch 5
2020-10-13 06:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=5/None
2020-10-13 06:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.04296875Mb; avail=237105.6796875Mb
2020-10-13 06:24:16 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-13 06:24:16 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:aze-eng': 1, 'main:azp-eng': 1}
2020-10-13 06:24:16 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:24:16 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/train.aze-eng.aze
2020-10-13 06:24:16 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/train.aze-eng.eng
2020-10-13 06:24:16 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azeazp_sepspm8000/M2O/ train aze-eng 5946 examples
2020-10-13 06:24:16 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:azp-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:24:16 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/train.azp-eng.azp
2020-10-13 06:24:16 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/train.azp-eng.eng
2020-10-13 06:24:16 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azeazp_sepspm8000/M2O/ train azp-eng 5946 examples
2020-10-13 06:24:16 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:aze-eng', 5946), ('main:azp-eng', 5946)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-13 06:24:16 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-13 06:24:16 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 11892
2020-10-13 06:24:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 11892; virtual dataset size 11892
2020-10-13 06:24:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=1
2020-10-13 06:24:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:aze-eng': 5946, 'main:azp-eng': 5946}; raw total size: 11892
2020-10-13 06:24:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:aze-eng': 5946, 'main:azp-eng': 5946}; resampled total size: 11892
2020-10-13 06:24:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-13 06:24:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.001946
2020-10-13 06:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7451.6484375Mb; avail=237105.07421875Mb
2020-10-13 06:24:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000222
2020-10-13 06:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002534
2020-10-13 06:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.6484375Mb; avail=237105.07421875Mb
2020-10-13 06:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000125
2020-10-13 06:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.6484375Mb; avail=237105.07421875Mb
2020-10-13 06:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.046054
2020-10-13 06:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.049532
2020-10-13 06:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.40625Mb; avail=237097.31640625Mb
2020-10-13 06:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7462.33203125Mb; avail=237094.390625Mb
2020-10-13 06:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002022
2020-10-13 06:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.9375Mb; avail=237093.78515625Mb
2020-10-13 06:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000120
2020-10-13 06:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.9375Mb; avail=237093.78515625Mb
2020-10-13 06:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.046165
2020-10-13 06:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.049148
2020-10-13 06:24:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.9375Mb; avail=237098.78515625Mb
2020-10-13 06:24:16 | INFO | fairseq.trainer | begin training epoch 5
/home/ubuntu/courses/fairseq/fairseq/utils.py:340: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
Traceback (most recent call last):
  File "/home/ubuntu/anaconda3/envs/torch16/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/home/ubuntu/courses/fairseq/fairseq_cli/train.py", line 352, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/ubuntu/courses/fairseq/fairseq/distributed_utils.py", line 258, in call_main
    main(args, **kwargs)
  File "/home/ubuntu/courses/fairseq/fairseq_cli/train.py", line 125, in main
    valid_losses, should_stop = train(args, trainer, task, epoch_itr)
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/ubuntu/courses/fairseq/fairseq_cli/train.py", line 208, in train
    log_output = trainer.train_step(samples)
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/ubuntu/courses/fairseq/fairseq/trainer.py", line 471, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/home/ubuntu/courses/fairseq/fairseq/tasks/fairseq_task.py", line 415, in train_step
    optimizer.backward(loss)
  File "/home/ubuntu/courses/fairseq/fairseq/optim/fairseq_optimizer.py", line 95, in backward
    loss.backward()
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/site-packages/torch/tensor.py", line 185, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/site-packages/torch/autograd/__init__.py", line 125, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
2020-10-13 06:24:34 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_azeazp_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='aze-eng,azp-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=80, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-13 06:24:34 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-13 06:24:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['aze', 'azp', 'eng']
2020-10-13 06:24:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | [aze] dictionary: 20827 types
2020-10-13 06:24:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | [azp] dictionary: 20827 types
2020-10-13 06:24:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 20827 types
2020-10-13 06:24:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-13 06:24:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=4633.29296875Mb; avail=239933.43359375Mb
2020-10-13 06:24:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-13 06:24:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:aze-eng': 1, 'main:azp-eng': 1}
2020-10-13 06:24:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:24:34 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/valid.aze-eng.aze
2020-10-13 06:24:34 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/valid.aze-eng.eng
2020-10-13 06:24:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azeazp_sepspm8000/M2O/ valid aze-eng 671 examples
2020-10-13 06:24:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:azp-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:24:34 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/valid.azp-eng.azp
2020-10-13 06:24:34 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/valid.azp-eng.eng
2020-10-13 06:24:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azeazp_sepspm8000/M2O/ valid azp-eng 671 examples
2020-10-13 06:24:35 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(20827, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(20827, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=20827, bias=False)
  )
)
2020-10-13 06:24:35 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-13 06:24:35 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-13 06:24:35 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-13 06:24:35 | INFO | fairseq_cli.train | num. model params: 42206720 (num. trained: 42206720)
2020-10-13 06:24:38 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-13 06:24:38 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-13 06:24:38 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-13 06:24:38 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-13 06:24:38 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-13 06:24:38 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-13 06:24:38 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-13 06:24:39 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-13 06:24:39 | INFO | fairseq.trainer | loaded checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 5 @ 224 updates)
2020-10-13 06:24:39 | INFO | fairseq.trainer | loading train data for epoch 5
2020-10-13 06:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=5/None
2020-10-13 06:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.14453125Mb; avail=237104.57421875Mb
2020-10-13 06:24:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-13 06:24:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:aze-eng': 1, 'main:azp-eng': 1}
2020-10-13 06:24:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:24:39 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/train.aze-eng.aze
2020-10-13 06:24:39 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/train.aze-eng.eng
2020-10-13 06:24:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azeazp_sepspm8000/M2O/ train aze-eng 5946 examples
2020-10-13 06:24:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:azp-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:24:39 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/train.azp-eng.azp
2020-10-13 06:24:39 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azeazp_sepspm8000/M2O/train.azp-eng.eng
2020-10-13 06:24:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azeazp_sepspm8000/M2O/ train azp-eng 5946 examples
2020-10-13 06:24:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:aze-eng', 5946), ('main:azp-eng', 5946)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-13 06:24:39 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-13 06:24:39 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 11892
2020-10-13 06:24:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 11892; virtual dataset size 11892
2020-10-13 06:24:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=1
2020-10-13 06:24:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:aze-eng': 5946, 'main:azp-eng': 5946}; raw total size: 11892
2020-10-13 06:24:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:aze-eng': 5946, 'main:azp-eng': 5946}; resampled total size: 11892
2020-10-13 06:24:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-13 06:24:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.001951
2020-10-13 06:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7452.14453125Mb; avail=237104.57421875Mb
2020-10-13 06:24:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000240
2020-10-13 06:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002540
2020-10-13 06:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.14453125Mb; avail=237104.57421875Mb
2020-10-13 06:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000121
2020-10-13 06:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.14453125Mb; avail=237104.57421875Mb
2020-10-13 06:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040680
2020-10-13 06:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044156
2020-10-13 06:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.14453125Mb; avail=237104.57421875Mb
2020-10-13 06:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7452.14453125Mb; avail=237104.57421875Mb
2020-10-13 06:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001997
2020-10-13 06:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.14453125Mb; avail=237104.57421875Mb
2020-10-13 06:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000111
2020-10-13 06:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.14453125Mb; avail=237104.57421875Mb
2020-10-13 06:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040166
2020-10-13 06:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043085
2020-10-13 06:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.14453125Mb; avail=237104.57421875Mb
2020-10-13 06:24:39 | INFO | fairseq.trainer | begin training epoch 5
2020-10-13 06:24:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7451.08984375Mb; avail=237103.30078125Mb
2020-10-13 06:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000844
2020-10-13 06:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.08984375Mb; avail=237103.30078125Mb
2020-10-13 06:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017771
2020-10-13 06:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.08984375Mb; avail=237103.30078125Mb
2020-10-13 06:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013497
2020-10-13 06:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032933
2020-10-13 06:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.08984375Mb; avail=237103.30078125Mb
2020-10-13 06:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7451.08984375Mb; avail=237103.30078125Mb
2020-10-13 06:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000707
2020-10-13 06:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.08984375Mb; avail=237103.30078125Mb
2020-10-13 06:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017392
2020-10-13 06:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.08984375Mb; avail=237103.30078125Mb
2020-10-13 06:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013179
2020-10-13 06:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032065
2020-10-13 06:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.08984375Mb; avail=237103.30078125Mb
/home/ubuntu/courses/fairseq/fairseq/utils.py:340: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2020-10-13 06:24:56 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.83 | nll_loss 10.319 | ppl 1277.15 | wps 51516.6 | wpb 2112 | bsz 83.9 | num_updates 280 | best_loss 10.83
2020-10-13 06:24:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:25:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 5 @ 280 updates, score 10.83) (writing took 4.568187581840903 seconds)
2020-10-13 06:25:00 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-13 06:25:00 | INFO | train | epoch 005 | loss 11.49 | nll_loss 11.08 | ppl 2164.75 | wps 16003.3 | ups 3.01 | wpb 5324.5 | bsz 212.4 | num_updates 280 | lr 1.4093e-05 | gnorm 1.692 | clip 0 | train_wall 29 | wall 0
2020-10-13 06:25:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=1
2020-10-13 06:25:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=2
2020-10-13 06:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7459.76171875Mb; avail=237094.875Mb
2020-10-13 06:25:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000272
2020-10-13 06:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002652
2020-10-13 06:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.76171875Mb; avail=237094.875Mb
2020-10-13 06:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000093
2020-10-13 06:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.76171875Mb; avail=237094.875Mb
2020-10-13 06:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041969
2020-10-13 06:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.045466
2020-10-13 06:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7449.87109375Mb; avail=237104.765625Mb
2020-10-13 06:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7450.36328125Mb; avail=237104.2734375Mb
2020-10-13 06:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001977
2020-10-13 06:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7450.36328125Mb; avail=237104.2734375Mb
2020-10-13 06:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000092
2020-10-13 06:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7450.36328125Mb; avail=237104.2734375Mb
2020-10-13 06:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040406
2020-10-13 06:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043205
2020-10-13 06:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.20703125Mb; avail=237100.4296875Mb
2020-10-13 06:25:00 | INFO | fairseq.trainer | begin training epoch 6
2020-10-13 06:25:06 | INFO | train_inner | epoch 006:     20 / 56 loss=11.243, nll_loss=10.803, ppl=1786.06, wps=15403, ups=2.95, wpb=5225.6, bsz=206.1, num_updates=300, lr=1.50925e-05, gnorm=1.601, clip=0, train_wall=26, wall=0
2020-10-13 06:25:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7449.69140625Mb; avail=237104.58984375Mb
2020-10-13 06:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000864
2020-10-13 06:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7449.69140625Mb; avail=237104.58984375Mb
2020-10-13 06:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017683
2020-10-13 06:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7449.69140625Mb; avail=237104.58984375Mb
2020-10-13 06:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013305
2020-10-13 06:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032664
2020-10-13 06:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7449.69140625Mb; avail=237104.58984375Mb
2020-10-13 06:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7449.69140625Mb; avail=237104.58984375Mb
2020-10-13 06:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000721
2020-10-13 06:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7449.69140625Mb; avail=237104.58984375Mb
2020-10-13 06:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016894
2020-10-13 06:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7449.69140625Mb; avail=237104.58984375Mb
2020-10-13 06:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013224
2020-10-13 06:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031613
2020-10-13 06:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7449.69140625Mb; avail=237104.58984375Mb
2020-10-13 06:25:16 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 10.218 | nll_loss 9.616 | ppl 784.43 | wps 50970.3 | wpb 2112 | bsz 83.9 | num_updates 336 | best_loss 10.218
2020-10-13 06:25:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:25:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 6 @ 336 updates, score 10.218) (writing took 4.5674640028737485 seconds)
2020-10-13 06:25:21 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-13 06:25:21 | INFO | train | epoch 006 | loss 10.737 | nll_loss 10.233 | ppl 1203.86 | wps 14430.9 | ups 2.71 | wpb 5324.5 | bsz 212.4 | num_updates 336 | lr 1.68916e-05 | gnorm 1.44 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:25:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=2
2020-10-13 06:25:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=3
2020-10-13 06:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7449.484375Mb; avail=237105.140625Mb
2020-10-13 06:25:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000301
2020-10-13 06:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002661
2020-10-13 06:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7449.125Mb; avail=237105.5Mb
2020-10-13 06:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000094
2020-10-13 06:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7449.125Mb; avail=237105.5Mb
2020-10-13 06:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041008
2020-10-13 06:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044528
2020-10-13 06:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.85546875Mb; avail=237100.76953125Mb
2020-10-13 06:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7455.671875Mb; avail=237098.953125Mb
2020-10-13 06:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001953
2020-10-13 06:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7455.671875Mb; avail=237098.953125Mb
2020-10-13 06:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000085
2020-10-13 06:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7455.671875Mb; avail=237098.34765625Mb
2020-10-13 06:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041139
2020-10-13 06:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043926
2020-10-13 06:25:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.69921875Mb; avail=237095.92578125Mb
2020-10-13 06:25:21 | INFO | fairseq.trainer | begin training epoch 7
2020-10-13 06:25:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7461.47265625Mb; avail=237092.8203125Mb
2020-10-13 06:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000829
2020-10-13 06:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.47265625Mb; avail=237092.8203125Mb
2020-10-13 06:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017312
2020-10-13 06:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.12109375Mb; avail=237102.171875Mb
2020-10-13 06:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013083
2020-10-13 06:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031985
2020-10-13 06:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.12109375Mb; avail=237102.171875Mb
2020-10-13 06:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7452.12109375Mb; avail=237102.171875Mb
2020-10-13 06:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000683
2020-10-13 06:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.12109375Mb; avail=237102.171875Mb
2020-10-13 06:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017394
2020-10-13 06:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.12109375Mb; avail=237102.171875Mb
2020-10-13 06:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013023
2020-10-13 06:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031833
2020-10-13 06:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.12109375Mb; avail=237102.171875Mb
2020-10-13 06:25:37 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.555 | nll_loss 8.818 | ppl 451.25 | wps 51349.4 | wpb 2112 | bsz 83.9 | num_updates 392 | best_loss 9.555
2020-10-13 06:25:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:25:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 7 @ 392 updates, score 9.555) (writing took 4.551503501832485 seconds)
2020-10-13 06:25:41 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-13 06:25:41 | INFO | train | epoch 007 | loss 10.071 | nll_loss 9.468 | ppl 708.37 | wps 14505.1 | ups 2.72 | wpb 5324.5 | bsz 212.4 | num_updates 392 | lr 1.96902e-05 | gnorm 1.888 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:25:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=3
2020-10-13 06:25:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=4
2020-10-13 06:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7449.59765625Mb; avail=237105.0234375Mb
2020-10-13 06:25:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000299
2020-10-13 06:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002632
2020-10-13 06:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7449.59765625Mb; avail=237105.0234375Mb
2020-10-13 06:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000090
2020-10-13 06:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7449.59765625Mb; avail=237105.0234375Mb
2020-10-13 06:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041322
2020-10-13 06:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044807
2020-10-13 06:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7449.59765625Mb; avail=237105.0234375Mb
2020-10-13 06:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7449.59765625Mb; avail=237105.0234375Mb
2020-10-13 06:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001944
2020-10-13 06:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7449.59765625Mb; avail=237105.0234375Mb
2020-10-13 06:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 06:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7449.59765625Mb; avail=237105.0234375Mb
2020-10-13 06:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040606
2020-10-13 06:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043350
2020-10-13 06:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.34375Mb; avail=237103.37109375Mb
2020-10-13 06:25:41 | INFO | fairseq.trainer | begin training epoch 8
2020-10-13 06:25:44 | INFO | train_inner | epoch 008:      8 / 56 loss=10.258, nll_loss=9.683, ppl=821.9, wps=14064.2, ups=2.63, wpb=5348.4, bsz=213, num_updates=400, lr=2.009e-05, gnorm=1.668, clip=0, train_wall=26, wall=0
2020-10-13 06:25:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7450.453125Mb; avail=237103.94140625Mb
2020-10-13 06:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000823
2020-10-13 06:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7450.453125Mb; avail=237103.94140625Mb
2020-10-13 06:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017669
2020-10-13 06:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7450.453125Mb; avail=237103.94140625Mb
2020-10-13 06:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013582
2020-10-13 06:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032862
2020-10-13 06:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7450.453125Mb; avail=237103.94140625Mb
2020-10-13 06:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7451.55859375Mb; avail=237102.8359375Mb
2020-10-13 06:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000698
2020-10-13 06:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.55859375Mb; avail=237102.8359375Mb
2020-10-13 06:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017158
2020-10-13 06:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.375Mb; avail=237101.01953125Mb
2020-10-13 06:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013206
2020-10-13 06:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031829
2020-10-13 06:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.5859375Mb; avail=237099.80859375Mb
2020-10-13 06:25:58 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.24 | nll_loss 8.426 | ppl 344.05 | wps 50973.7 | wpb 2112 | bsz 83.9 | num_updates 448 | best_loss 9.24
2020-10-13 06:25:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:26:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 8 @ 448 updates, score 9.24) (writing took 4.557219272013754 seconds)
2020-10-13 06:26:02 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-13 06:26:02 | INFO | train | epoch 008 | loss 9.518 | nll_loss 8.813 | ppl 449.76 | wps 14333.7 | ups 2.69 | wpb 5324.5 | bsz 212.4 | num_updates 448 | lr 2.24888e-05 | gnorm 1.314 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:26:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=4
2020-10-13 06:26:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=5
2020-10-13 06:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7458.48828125Mb; avail=237096.12890625Mb
2020-10-13 06:26:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000282
2020-10-13 06:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002718
2020-10-13 06:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.09375Mb; avail=237095.5234375Mb
2020-10-13 06:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000112
2020-10-13 06:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.09375Mb; avail=237095.5234375Mb
2020-10-13 06:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041109
2020-10-13 06:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044767
2020-10-13 06:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.8828125Mb; avail=237092.8359375Mb
2020-10-13 06:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7451.65234375Mb; avail=237103.06640625Mb
2020-10-13 06:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002020
2020-10-13 06:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.65234375Mb; avail=237103.06640625Mb
2020-10-13 06:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000111
2020-10-13 06:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.65234375Mb; avail=237103.06640625Mb
2020-10-13 06:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040577
2020-10-13 06:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043483
2020-10-13 06:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.65234375Mb; avail=237103.06640625Mb
2020-10-13 06:26:02 | INFO | fairseq.trainer | begin training epoch 9
2020-10-13 06:26:16 | INFO | train_inner | epoch 009:     52 / 56 loss=9.341, nll_loss=8.601, ppl=388.17, wps=16452.1, ups=3.06, wpb=5375.3, bsz=215.2, num_updates=500, lr=2.50875e-05, gnorm=1.273, clip=0, train_wall=26, wall=0
2020-10-13 06:26:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:26:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7454.05859375Mb; avail=237100.33203125Mb
2020-10-13 06:26:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000842
2020-10-13 06:26:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.05859375Mb; avail=237100.33203125Mb
2020-10-13 06:26:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017619
2020-10-13 06:26:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.05859375Mb; avail=237100.33203125Mb
2020-10-13 06:26:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013359
2020-10-13 06:26:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032597
2020-10-13 06:26:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.04296875Mb; avail=237100.34765625Mb
2020-10-13 06:26:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7454.04296875Mb; avail=237100.34765625Mb
2020-10-13 06:26:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000717
2020-10-13 06:26:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.4609375Mb; avail=237100.34765625Mb
2020-10-13 06:26:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017352
2020-10-13 06:26:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.4609375Mb; avail=237100.34765625Mb
2020-10-13 06:26:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013105
2020-10-13 06:26:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031912
2020-10-13 06:26:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.9375Mb; avail=237101.68359375Mb
2020-10-13 06:26:18 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 9.027 | nll_loss 8.164 | ppl 286.84 | wps 51275.4 | wpb 2112 | bsz 83.9 | num_updates 504 | best_loss 9.027
2020-10-13 06:26:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:26:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 9 @ 504 updates, score 9.027) (writing took 4.5579168980475515 seconds)
2020-10-13 06:26:23 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-13 06:26:23 | INFO | train | epoch 009 | loss 9.209 | nll_loss 8.44 | ppl 347.23 | wps 14451.5 | ups 2.71 | wpb 5324.5 | bsz 212.4 | num_updates 504 | lr 2.52874e-05 | gnorm 1.249 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:26:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=5
2020-10-13 06:26:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=6
2020-10-13 06:26:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7451.9296875Mb; avail=237102.6953125Mb
2020-10-13 06:26:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000275
2020-10-13 06:26:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002617
2020-10-13 06:26:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.9296875Mb; avail=237102.6953125Mb
2020-10-13 06:26:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000092
2020-10-13 06:26:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.9296875Mb; avail=237102.6953125Mb
2020-10-13 06:26:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040617
2020-10-13 06:26:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044056
2020-10-13 06:26:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.9296875Mb; avail=237102.6953125Mb
2020-10-13 06:26:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7453.74609375Mb; avail=237100.87890625Mb
2020-10-13 06:26:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001958
2020-10-13 06:26:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.3515625Mb; avail=237100.2734375Mb
2020-10-13 06:26:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000082
2020-10-13 06:26:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.3515625Mb; avail=237100.2734375Mb
2020-10-13 06:26:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040149
2020-10-13 06:26:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.042914
2020-10-13 06:26:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.984375Mb; avail=237096.640625Mb
2020-10-13 06:26:23 | INFO | fairseq.trainer | begin training epoch 10
2020-10-13 06:26:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:26:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7456.03125Mb; avail=237098.453125Mb
2020-10-13 06:26:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000812
2020-10-13 06:26:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7456.63671875Mb; avail=237097.84765625Mb
2020-10-13 06:26:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017279
2020-10-13 06:26:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.84765625Mb; avail=237096.63671875Mb
2020-10-13 06:26:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012958
2020-10-13 06:26:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031812
2020-10-13 06:26:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.05859375Mb; avail=237095.42578125Mb
2020-10-13 06:26:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.18359375Mb; avail=237091.30078125Mb
2020-10-13 06:26:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000690
2020-10-13 06:26:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.18359375Mb; avail=237091.30078125Mb
2020-10-13 06:26:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017651
2020-10-13 06:26:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.18359375Mb; avail=237091.30078125Mb
2020-10-13 06:26:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013042
2020-10-13 06:26:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032125
2020-10-13 06:26:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.18359375Mb; avail=237091.30078125Mb
2020-10-13 06:26:39 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.946 | nll_loss 8.058 | ppl 266.55 | wps 51076.9 | wpb 2112 | bsz 83.9 | num_updates 560 | best_loss 8.946
2020-10-13 06:26:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:26:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 10 @ 560 updates, score 8.946) (writing took 4.557754714973271 seconds)
2020-10-13 06:26:43 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-13 06:26:43 | INFO | train | epoch 010 | loss 9.009 | nll_loss 8.19 | ppl 292.09 | wps 14386.1 | ups 2.7 | wpb 5324.5 | bsz 212.4 | num_updates 560 | lr 2.8086e-05 | gnorm 1.289 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:26:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=6
2020-10-13 06:26:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=7
2020-10-13 06:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7451.82421875Mb; avail=237102.80078125Mb
2020-10-13 06:26:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000286
2020-10-13 06:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002738
2020-10-13 06:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.82421875Mb; avail=237102.80078125Mb
2020-10-13 06:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000110
2020-10-13 06:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.82421875Mb; avail=237102.80078125Mb
2020-10-13 06:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040409
2020-10-13 06:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044043
2020-10-13 06:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.82421875Mb; avail=237102.80078125Mb
2020-10-13 06:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7451.82421875Mb; avail=237102.80078125Mb
2020-10-13 06:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001988
2020-10-13 06:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.82421875Mb; avail=237102.80078125Mb
2020-10-13 06:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000099
2020-10-13 06:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.82421875Mb; avail=237102.80078125Mb
2020-10-13 06:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040085
2020-10-13 06:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.042955
2020-10-13 06:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7451.82421875Mb; avail=237102.80078125Mb
2020-10-13 06:26:44 | INFO | fairseq.trainer | begin training epoch 11
2020-10-13 06:26:55 | INFO | train_inner | epoch 011:     40 / 56 loss=8.979, nll_loss=8.149, ppl=283.76, wps=13998.5, ups=2.61, wpb=5364.1, bsz=212, num_updates=600, lr=3.0085e-05, gnorm=1.352, clip=0, train_wall=26, wall=0
2020-10-13 06:26:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7453.9765625Mb; avail=237100.3046875Mb
2020-10-13 06:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000820
2020-10-13 06:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.9765625Mb; avail=237100.3046875Mb
2020-10-13 06:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017424
2020-10-13 06:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.9765625Mb; avail=237100.3046875Mb
2020-10-13 06:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013124
2020-10-13 06:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032158
2020-10-13 06:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.9765625Mb; avail=237100.3046875Mb
2020-10-13 06:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7453.9765625Mb; avail=237100.3046875Mb
2020-10-13 06:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000690
2020-10-13 06:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.9765625Mb; avail=237100.3046875Mb
2020-10-13 06:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017240
2020-10-13 06:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.9765625Mb; avail=237100.3046875Mb
2020-10-13 06:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013000
2020-10-13 06:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031688
2020-10-13 06:26:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.9765625Mb; avail=237100.3046875Mb
2020-10-13 06:27:00 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.808 | nll_loss 7.871 | ppl 234.08 | wps 51314.1 | wpb 2112 | bsz 83.9 | num_updates 616 | best_loss 8.808
2020-10-13 06:27:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:27:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 11 @ 616 updates, score 8.808) (writing took 4.555104062892497 seconds)
2020-10-13 06:27:04 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-13 06:27:04 | INFO | train | epoch 011 | loss 8.888 | nll_loss 8.032 | ppl 261.7 | wps 14298.6 | ups 2.69 | wpb 5324.5 | bsz 212.4 | num_updates 616 | lr 3.08846e-05 | gnorm 1.374 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:27:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=7
2020-10-13 06:27:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=8
2020-10-13 06:27:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7453.49609375Mb; avail=237101.13671875Mb
2020-10-13 06:27:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000273
2020-10-13 06:27:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002626
2020-10-13 06:27:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.49609375Mb; avail=237101.13671875Mb
2020-10-13 06:27:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000091
2020-10-13 06:27:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.49609375Mb; avail=237101.13671875Mb
2020-10-13 06:27:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041687
2020-10-13 06:27:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.045127
2020-10-13 06:27:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.5703125Mb; avail=237101.15234375Mb
2020-10-13 06:27:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7454.296875Mb; avail=237100.42578125Mb
2020-10-13 06:27:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001939
2020-10-13 06:27:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.90234375Mb; avail=237099.8203125Mb
2020-10-13 06:27:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000084
2020-10-13 06:27:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.90234375Mb; avail=237099.8203125Mb
2020-10-13 06:27:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040534
2020-10-13 06:27:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043285
2020-10-13 06:27:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.38671875Mb; avail=237096.33203125Mb
2020-10-13 06:27:04 | INFO | fairseq.trainer | begin training epoch 12
2020-10-13 06:27:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7457.03515625Mb; avail=237097.3515625Mb
2020-10-13 06:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000827
2020-10-13 06:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.03515625Mb; avail=237097.3515625Mb
2020-10-13 06:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017782
2020-10-13 06:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.8515625Mb; avail=237095.53515625Mb
2020-10-13 06:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013278
2020-10-13 06:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032656
2020-10-13 06:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.0625Mb; avail=237094.32421875Mb
2020-10-13 06:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.33203125Mb; avail=237091.0546875Mb
2020-10-13 06:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000715
2020-10-13 06:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.33203125Mb; avail=237091.0546875Mb
2020-10-13 06:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017436
2020-10-13 06:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.828125Mb; avail=237090.55859375Mb
2020-10-13 06:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013338
2020-10-13 06:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032231
2020-10-13 06:27:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.828125Mb; avail=237090.55859375Mb
2020-10-13 06:27:21 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.72 | nll_loss 7.769 | ppl 218.18 | wps 50660 | wpb 2112 | bsz 83.9 | num_updates 672 | best_loss 8.72
2020-10-13 06:27:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:27:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 12 @ 672 updates, score 8.72) (writing took 4.5745569879654795 seconds)
2020-10-13 06:27:25 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-13 06:27:25 | INFO | train | epoch 012 | loss 8.78 | nll_loss 7.896 | ppl 238.13 | wps 14353.5 | ups 2.7 | wpb 5324.5 | bsz 212.4 | num_updates 672 | lr 3.36832e-05 | gnorm 1.397 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:27:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=8
2020-10-13 06:27:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=9
2020-10-13 06:27:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7453.8671875Mb; avail=237100.75Mb
2020-10-13 06:27:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000299
2020-10-13 06:27:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002698
2020-10-13 06:27:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.8671875Mb; avail=237100.75Mb
2020-10-13 06:27:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000094
2020-10-13 06:27:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.8671875Mb; avail=237100.75Mb
2020-10-13 06:27:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040619
2020-10-13 06:27:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044161
2020-10-13 06:27:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.8671875Mb; avail=237100.75Mb
2020-10-13 06:27:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7453.3671875Mb; avail=237101.25Mb
2020-10-13 06:27:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001994
2020-10-13 06:27:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.17578125Mb; avail=237101.44140625Mb
2020-10-13 06:27:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000083
2020-10-13 06:27:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.17578125Mb; avail=237101.44140625Mb
2020-10-13 06:27:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040044
2020-10-13 06:27:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.042871
2020-10-13 06:27:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.17578125Mb; avail=237101.44140625Mb
2020-10-13 06:27:25 | INFO | fairseq.trainer | begin training epoch 13
2020-10-13 06:27:33 | INFO | train_inner | epoch 013:     28 / 56 loss=8.78, nll_loss=7.895, ppl=238.01, wps=13682.9, ups=2.62, wpb=5229.6, bsz=205.4, num_updates=700, lr=3.50825e-05, gnorm=1.43, clip=0, train_wall=26, wall=0
2020-10-13 06:27:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7454.2109375Mb; avail=237100.6875Mb
2020-10-13 06:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000830
2020-10-13 06:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.2109375Mb; avail=237100.6875Mb
2020-10-13 06:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017430
2020-10-13 06:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.2109375Mb; avail=237100.6875Mb
2020-10-13 06:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013328
2020-10-13 06:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032410
2020-10-13 06:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.2109375Mb; avail=237100.6875Mb
2020-10-13 06:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7454.2109375Mb; avail=237100.6875Mb
2020-10-13 06:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000711
2020-10-13 06:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.2109375Mb; avail=237100.6875Mb
2020-10-13 06:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017395
2020-10-13 06:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.2109375Mb; avail=237100.6875Mb
2020-10-13 06:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013101
2020-10-13 06:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031995
2020-10-13 06:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.2109375Mb; avail=237100.6875Mb
2020-10-13 06:27:41 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.619 | nll_loss 7.638 | ppl 199.23 | wps 49607.4 | wpb 2112 | bsz 83.9 | num_updates 728 | best_loss 8.619
2020-10-13 06:27:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:27:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 13 @ 728 updates, score 8.619) (writing took 4.581492487108335 seconds)
2020-10-13 06:27:46 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-13 06:27:46 | INFO | train | epoch 013 | loss 8.688 | nll_loss 7.784 | ppl 220.43 | wps 14373.3 | ups 2.7 | wpb 5324.5 | bsz 212.4 | num_updates 728 | lr 3.64818e-05 | gnorm 1.557 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:27:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=9
2020-10-13 06:27:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=10
2020-10-13 06:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7468.17578125Mb; avail=237086.44140625Mb
2020-10-13 06:27:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000483
2020-10-13 06:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.004102
2020-10-13 06:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.17578125Mb; avail=237085.921875Mb
2020-10-13 06:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000155
2020-10-13 06:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.6953125Mb; avail=237085.921875Mb
2020-10-13 06:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.064556
2020-10-13 06:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.069970
2020-10-13 06:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.484375Mb; avail=237091.1328125Mb
2020-10-13 06:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7454.73046875Mb; avail=237099.88671875Mb
2020-10-13 06:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001989
2020-10-13 06:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.73046875Mb; avail=237099.88671875Mb
2020-10-13 06:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000089
2020-10-13 06:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.73046875Mb; avail=237099.88671875Mb
2020-10-13 06:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.039832
2020-10-13 06:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.042641
2020-10-13 06:27:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.73046875Mb; avail=237099.88671875Mb
2020-10-13 06:27:46 | INFO | fairseq.trainer | begin training epoch 14
2020-10-13 06:28:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7456.08984375Mb; avail=237098.7890625Mb
2020-10-13 06:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000861
2020-10-13 06:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7456.08984375Mb; avail=237098.7890625Mb
2020-10-13 06:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017706
2020-10-13 06:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7456.08984375Mb; avail=237098.7890625Mb
2020-10-13 06:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013147
2020-10-13 06:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032484
2020-10-13 06:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7456.67578125Mb; avail=237098.203125Mb
2020-10-13 06:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7459.09765625Mb; avail=237095.78125Mb
2020-10-13 06:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000715
2020-10-13 06:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.09765625Mb; avail=237095.78125Mb
2020-10-13 06:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017332
2020-10-13 06:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.9140625Mb; avail=237093.96484375Mb
2020-10-13 06:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013427
2020-10-13 06:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032219
2020-10-13 06:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.125Mb; avail=237092.75390625Mb
2020-10-13 06:28:02 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.534 | nll_loss 7.542 | ppl 186.37 | wps 50756 | wpb 2112 | bsz 83.9 | num_updates 784 | best_loss 8.534
2020-10-13 06:28:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:28:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 14 @ 784 updates, score 8.534) (writing took 4.565795247908682 seconds)
2020-10-13 06:28:07 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-13 06:28:07 | INFO | train | epoch 014 | loss 8.579 | nll_loss 7.656 | ppl 201.75 | wps 14375.5 | ups 2.7 | wpb 5324.5 | bsz 212.4 | num_updates 784 | lr 3.92804e-05 | gnorm 1.48 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:28:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=10
2020-10-13 06:28:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=11
2020-10-13 06:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7452.30078125Mb; avail=237102.3046875Mb
2020-10-13 06:28:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000290
2020-10-13 06:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002667
2020-10-13 06:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.30078125Mb; avail=237102.3046875Mb
2020-10-13 06:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000094
2020-10-13 06:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.30078125Mb; avail=237102.3046875Mb
2020-10-13 06:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040564
2020-10-13 06:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044091
2020-10-13 06:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.30078125Mb; avail=237102.3046875Mb
2020-10-13 06:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7452.30078125Mb; avail=237102.3046875Mb
2020-10-13 06:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001948
2020-10-13 06:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.30078125Mb; avail=237102.3046875Mb
2020-10-13 06:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000118
2020-10-13 06:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.30078125Mb; avail=237102.3046875Mb
2020-10-13 06:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040969
2020-10-13 06:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043783
2020-10-13 06:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.30078125Mb; avail=237102.3046875Mb
2020-10-13 06:28:07 | INFO | fairseq.trainer | begin training epoch 15
2020-10-13 06:28:11 | INFO | train_inner | epoch 015:     16 / 56 loss=8.594, nll_loss=7.674, ppl=204.28, wps=14052.8, ups=2.61, wpb=5384.6, bsz=218.8, num_updates=800, lr=4.008e-05, gnorm=1.505, clip=0, train_wall=26, wall=0
2020-10-13 06:28:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7470.15625Mb; avail=237084.15234375Mb
2020-10-13 06:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000820
2020-10-13 06:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.15625Mb; avail=237084.15234375Mb
2020-10-13 06:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017519
2020-10-13 06:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.54296875Mb; avail=237080.765625Mb
2020-10-13 06:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012896
2020-10-13 06:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031992
2020-10-13 06:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.19140625Mb; avail=237090.1171875Mb
2020-10-13 06:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7464.6953125Mb; avail=237089.61328125Mb
2020-10-13 06:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000657
2020-10-13 06:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.6953125Mb; avail=237089.61328125Mb
2020-10-13 06:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017296
2020-10-13 06:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.359375Mb; avail=237099.94921875Mb
2020-10-13 06:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013048
2020-10-13 06:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031737
2020-10-13 06:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.359375Mb; avail=237099.94921875Mb
2020-10-13 06:28:23 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.498 | nll_loss 7.479 | ppl 178.39 | wps 50738.3 | wpb 2112 | bsz 83.9 | num_updates 840 | best_loss 8.498
2020-10-13 06:28:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:28:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 15 @ 840 updates, score 8.498) (writing took 4.569912197999656 seconds)
2020-10-13 06:28:27 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-13 06:28:27 | INFO | train | epoch 015 | loss 8.477 | nll_loss 7.538 | ppl 185.84 | wps 14396.1 | ups 2.7 | wpb 5324.5 | bsz 212.4 | num_updates 840 | lr 4.2079e-05 | gnorm 1.518 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:28:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=11
2020-10-13 06:28:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=12
2020-10-13 06:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7453.58203125Mb; avail=237101.0546875Mb
2020-10-13 06:28:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000280
2020-10-13 06:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002661
2020-10-13 06:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.58203125Mb; avail=237101.0546875Mb
2020-10-13 06:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000106
2020-10-13 06:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.58203125Mb; avail=237101.0546875Mb
2020-10-13 06:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040754
2020-10-13 06:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044272
2020-10-13 06:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.58203125Mb; avail=237101.0546875Mb
2020-10-13 06:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7453.58203125Mb; avail=237101.0546875Mb
2020-10-13 06:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001994
2020-10-13 06:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.58203125Mb; avail=237101.0546875Mb
2020-10-13 06:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000089
2020-10-13 06:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.58203125Mb; avail=237101.0546875Mb
2020-10-13 06:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040805
2020-10-13 06:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043640
2020-10-13 06:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.65625Mb; avail=237096.98046875Mb
2020-10-13 06:28:27 | INFO | fairseq.trainer | begin training epoch 16
2020-10-13 06:28:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7454.46484375Mb; avail=237100.57421875Mb
2020-10-13 06:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000844
2020-10-13 06:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.46484375Mb; avail=237100.57421875Mb
2020-10-13 06:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017806
2020-10-13 06:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.46484375Mb; avail=237100.57421875Mb
2020-10-13 06:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013638
2020-10-13 06:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.033097
2020-10-13 06:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.46484375Mb; avail=237100.57421875Mb
2020-10-13 06:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7454.46484375Mb; avail=237100.57421875Mb
2020-10-13 06:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000700
2020-10-13 06:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.46484375Mb; avail=237100.57421875Mb
2020-10-13 06:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017370
2020-10-13 06:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7454.46484375Mb; avail=237100.57421875Mb
2020-10-13 06:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013481
2020-10-13 06:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032368
2020-10-13 06:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7456.2734375Mb; avail=237098.765625Mb
2020-10-13 06:28:44 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.391 | nll_loss 7.366 | ppl 164.92 | wps 50600.9 | wpb 2112 | bsz 83.9 | num_updates 896 | best_loss 8.391
2020-10-13 06:28:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:28:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 16 @ 896 updates, score 8.391) (writing took 4.56843984592706 seconds)
2020-10-13 06:28:48 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-13 06:28:48 | INFO | train | epoch 016 | loss 8.382 | nll_loss 7.429 | ppl 172.3 | wps 14289.2 | ups 2.68 | wpb 5324.5 | bsz 212.4 | num_updates 896 | lr 4.48776e-05 | gnorm 1.575 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:28:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=12
2020-10-13 06:28:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=13
2020-10-13 06:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7452.765625Mb; avail=237101.85546875Mb
2020-10-13 06:28:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000298
2020-10-13 06:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002708
2020-10-13 06:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.765625Mb; avail=237101.85546875Mb
2020-10-13 06:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000095
2020-10-13 06:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.765625Mb; avail=237101.85546875Mb
2020-10-13 06:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041217
2020-10-13 06:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044766
2020-10-13 06:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.765625Mb; avail=237101.85546875Mb
2020-10-13 06:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7452.765625Mb; avail=237101.85546875Mb
2020-10-13 06:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001959
2020-10-13 06:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.765625Mb; avail=237101.85546875Mb
2020-10-13 06:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000085
2020-10-13 06:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.765625Mb; avail=237101.85546875Mb
2020-10-13 06:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040775
2020-10-13 06:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043605
2020-10-13 06:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.765625Mb; avail=237101.85546875Mb
2020-10-13 06:28:48 | INFO | fairseq.trainer | begin training epoch 17
2020-10-13 06:28:49 | INFO | train_inner | epoch 017:      4 / 56 loss=8.394, nll_loss=7.443, ppl=174, wps=13772.9, ups=2.61, wpb=5268.5, bsz=209, num_updates=900, lr=4.50775e-05, gnorm=1.544, clip=0, train_wall=26, wall=0
2020-10-13 06:29:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7453.4453125Mb; avail=237101.17578125Mb
2020-10-13 06:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000830
2020-10-13 06:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7453.4453125Mb; avail=237101.17578125Mb
2020-10-13 06:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017563
2020-10-13 06:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7455.8671875Mb; avail=237098.75390625Mb
2020-10-13 06:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013257
2020-10-13 06:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032441
2020-10-13 06:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.078125Mb; avail=237097.54296875Mb
2020-10-13 06:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7458.2890625Mb; avail=237096.33203125Mb
2020-10-13 06:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000734
2020-10-13 06:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.2890625Mb; avail=237096.33203125Mb
2020-10-13 06:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017244
2020-10-13 06:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.41015625Mb; avail=237092.2109375Mb
2020-10-13 06:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013102
2020-10-13 06:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031823
2020-10-13 06:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.41015625Mb; avail=237092.2109375Mb
2020-10-13 06:29:04 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 8.318 | nll_loss 7.283 | ppl 155.79 | wps 50186.1 | wpb 2112 | bsz 83.9 | num_updates 952 | best_loss 8.318
2020-10-13 06:29:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:29:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 17 @ 952 updates, score 8.318) (writing took 4.57738930801861 seconds)
2020-10-13 06:29:09 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-13 06:29:09 | INFO | train | epoch 017 | loss 8.293 | nll_loss 7.327 | ppl 160.55 | wps 14398.1 | ups 2.7 | wpb 5324.5 | bsz 212.4 | num_updates 952 | lr 4.76762e-05 | gnorm 1.481 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:29:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=13
2020-10-13 06:29:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=14
2020-10-13 06:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7462.2578125Mb; avail=237092.36328125Mb
2020-10-13 06:29:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000272
2020-10-13 06:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002684
2020-10-13 06:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.2578125Mb; avail=237092.36328125Mb
2020-10-13 06:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000093
2020-10-13 06:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.2578125Mb; avail=237092.36328125Mb
2020-10-13 06:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040728
2020-10-13 06:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044243
2020-10-13 06:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.90625Mb; avail=237101.71484375Mb
2020-10-13 06:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7452.90625Mb; avail=237101.71484375Mb
2020-10-13 06:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001990
2020-10-13 06:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.90625Mb; avail=237101.71484375Mb
2020-10-13 06:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000087
2020-10-13 06:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.90625Mb; avail=237101.71484375Mb
2020-10-13 06:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040909
2020-10-13 06:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043721
2020-10-13 06:29:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7452.90625Mb; avail=237101.71484375Mb
2020-10-13 06:29:09 | INFO | fairseq.trainer | begin training epoch 18
2020-10-13 06:29:22 | INFO | train_inner | epoch 018:     48 / 56 loss=8.262, nll_loss=7.292, ppl=156.67, wps=16402.1, ups=3.06, wpb=5357.8, bsz=215.3, num_updates=1000, lr=5.0075e-05, gnorm=1.594, clip=0, train_wall=26, wall=0
2020-10-13 06:29:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7458.2265625Mb; avail=237096.25Mb
2020-10-13 06:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000874
2020-10-13 06:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.2265625Mb; avail=237096.25Mb
2020-10-13 06:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017702
2020-10-13 06:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.21875Mb; avail=237096.2578125Mb
2020-10-13 06:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013112
2020-10-13 06:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032471
2020-10-13 06:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.87109375Mb; avail=237096.60546875Mb
2020-10-13 06:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7457.87109375Mb; avail=237096.60546875Mb
2020-10-13 06:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000729
2020-10-13 06:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.87109375Mb; avail=237096.60546875Mb
2020-10-13 06:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016895
2020-10-13 06:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.703125Mb; avail=237096.78125Mb
2020-10-13 06:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013112
2020-10-13 06:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031474
2020-10-13 06:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.703125Mb; avail=237096.78125Mb
2020-10-13 06:29:25 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 8.261 | nll_loss 7.224 | ppl 149.53 | wps 51318.5 | wpb 2112 | bsz 83.9 | num_updates 1008 | best_loss 8.261
2020-10-13 06:29:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:29:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 18 @ 1008 updates, score 8.261) (writing took 4.554049706086516 seconds)
2020-10-13 06:29:30 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-13 06:29:30 | INFO | train | epoch 018 | loss 8.211 | nll_loss 7.234 | ppl 150.56 | wps 14374.6 | ups 2.7 | wpb 5324.5 | bsz 212.4 | num_updates 1008 | lr 5.04748e-05 | gnorm 1.688 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:29:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=14
2020-10-13 06:29:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=15
2020-10-13 06:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7455.484375Mb; avail=237099.14453125Mb
2020-10-13 06:29:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000315
2020-10-13 06:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002710
2020-10-13 06:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7455.484375Mb; avail=237099.14453125Mb
2020-10-13 06:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000091
2020-10-13 06:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7455.484375Mb; avail=237099.14453125Mb
2020-10-13 06:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041329
2020-10-13 06:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044857
2020-10-13 06:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7455.484375Mb; avail=237099.14453125Mb
2020-10-13 06:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7455.484375Mb; avail=237099.14453125Mb
2020-10-13 06:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001968
2020-10-13 06:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7455.484375Mb; avail=237099.14453125Mb
2020-10-13 06:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 06:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7455.484375Mb; avail=237099.14453125Mb
2020-10-13 06:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041816
2020-10-13 06:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044596
2020-10-13 06:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7455.484375Mb; avail=237099.14453125Mb
2020-10-13 06:29:30 | INFO | fairseq.trainer | begin training epoch 19
2020-10-13 06:29:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7457.81640625Mb; avail=237096.4765625Mb
2020-10-13 06:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000846
2020-10-13 06:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.81640625Mb; avail=237096.4765625Mb
2020-10-13 06:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017327
2020-10-13 06:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.81640625Mb; avail=237096.4765625Mb
2020-10-13 06:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013569
2020-10-13 06:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032551
2020-10-13 06:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.81640625Mb; avail=237096.4765625Mb
2020-10-13 06:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7457.81640625Mb; avail=237096.4765625Mb
2020-10-13 06:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000739
2020-10-13 06:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.81640625Mb; avail=237096.4765625Mb
2020-10-13 06:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017630
2020-10-13 06:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.79296875Mb; avail=237096.5078125Mb
2020-10-13 06:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013391
2020-10-13 06:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032522
2020-10-13 06:29:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.79296875Mb; avail=237096.5078125Mb
2020-10-13 06:29:46 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 8.187 | nll_loss 7.146 | ppl 141.58 | wps 50789.3 | wpb 2112 | bsz 83.9 | num_updates 1064 | best_loss 8.187
2020-10-13 06:29:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:29:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 19 @ 1064 updates, score 8.187) (writing took 4.563392660114914 seconds)
2020-10-13 06:29:50 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-13 06:29:50 | INFO | train | epoch 019 | loss 8.121 | nll_loss 7.133 | ppl 140.36 | wps 14346.2 | ups 2.69 | wpb 5324.5 | bsz 212.4 | num_updates 1064 | lr 5.32734e-05 | gnorm 1.484 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:29:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=15
2020-10-13 06:29:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=16
2020-10-13 06:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7467.5234375Mb; avail=237087.10546875Mb
2020-10-13 06:29:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000311
2020-10-13 06:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003036
2020-10-13 06:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.5234375Mb; avail=237087.10546875Mb
2020-10-13 06:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000093
2020-10-13 06:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.5234375Mb; avail=237087.10546875Mb
2020-10-13 06:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.045966
2020-10-13 06:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.049942
2020-10-13 06:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.6796875Mb; avail=237096.94921875Mb
2020-10-13 06:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7457.6796875Mb; avail=237096.94921875Mb
2020-10-13 06:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001930
2020-10-13 06:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.6796875Mb; avail=237096.94921875Mb
2020-10-13 06:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000083
2020-10-13 06:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.6796875Mb; avail=237096.94921875Mb
2020-10-13 06:29:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040669
2020-10-13 06:29:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043435
2020-10-13 06:29:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.6796875Mb; avail=237096.94921875Mb
2020-10-13 06:29:51 | INFO | fairseq.trainer | begin training epoch 20
2020-10-13 06:30:00 | INFO | train_inner | epoch 020:     36 / 56 loss=8.108, nll_loss=7.119, ppl=138.99, wps=13915.1, ups=2.62, wpb=5314.2, bsz=212.1, num_updates=1100, lr=5.50725e-05, gnorm=1.521, clip=0, train_wall=26, wall=0
2020-10-13 06:30:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7458.37109375Mb; avail=237096.01953125Mb
2020-10-13 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000838
2020-10-13 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.37109375Mb; avail=237096.01953125Mb
2020-10-13 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017617
2020-10-13 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.37109375Mb; avail=237096.01953125Mb
2020-10-13 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013155
2020-10-13 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032414
2020-10-13 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.37109375Mb; avail=237096.01953125Mb
2020-10-13 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7458.37109375Mb; avail=237096.01953125Mb
2020-10-13 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000709
2020-10-13 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.37109375Mb; avail=237096.01953125Mb
2020-10-13 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017248
2020-10-13 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.37109375Mb; avail=237096.01953125Mb
2020-10-13 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013078
2020-10-13 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031839
2020-10-13 06:30:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.37109375Mb; avail=237096.01953125Mb
2020-10-13 06:30:07 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 8.119 | nll_loss 7.079 | ppl 135.23 | wps 50644.5 | wpb 2112 | bsz 83.9 | num_updates 1120 | best_loss 8.119
2020-10-13 06:30:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:30:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 20 @ 1120 updates, score 8.119) (writing took 4.563211887143552 seconds)
2020-10-13 06:30:11 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-13 06:30:11 | INFO | train | epoch 020 | loss 8.039 | nll_loss 7.04 | ppl 131.63 | wps 14418 | ups 2.71 | wpb 5324.5 | bsz 212.4 | num_updates 1120 | lr 5.6072e-05 | gnorm 1.539 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:30:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=16
2020-10-13 06:30:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=17
2020-10-13 06:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7467.16796875Mb; avail=237087.47265625Mb
2020-10-13 06:30:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000319
2020-10-13 06:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003013
2020-10-13 06:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.16796875Mb; avail=237087.47265625Mb
2020-10-13 06:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000094
2020-10-13 06:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.16796875Mb; avail=237087.47265625Mb
2020-10-13 06:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.045200
2020-10-13 06:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.049098
2020-10-13 06:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.30859375Mb; avail=237097.33203125Mb
2020-10-13 06:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7457.30859375Mb; avail=237097.33203125Mb
2020-10-13 06:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001953
2020-10-13 06:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.30859375Mb; avail=237097.33203125Mb
2020-10-13 06:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000109
2020-10-13 06:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.30859375Mb; avail=237097.33203125Mb
2020-10-13 06:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040512
2020-10-13 06:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043302
2020-10-13 06:30:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.29296875Mb; avail=237097.34765625Mb
2020-10-13 06:30:11 | INFO | fairseq.trainer | begin training epoch 21
2020-10-13 06:30:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7469.60546875Mb; avail=237085.08203125Mb
2020-10-13 06:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000862
2020-10-13 06:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7469.60546875Mb; avail=237085.08203125Mb
2020-10-13 06:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017527
2020-10-13 06:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.1015625Mb; avail=237084.5859375Mb
2020-10-13 06:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013367
2020-10-13 06:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032587
2020-10-13 06:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.1015625Mb; avail=237084.5859375Mb
2020-10-13 06:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7460.2578125Mb; avail=237094.4296875Mb
2020-10-13 06:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000744
2020-10-13 06:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.2578125Mb; avail=237094.4296875Mb
2020-10-13 06:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017333
2020-10-13 06:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.2578125Mb; avail=237094.4296875Mb
2020-10-13 06:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013113
2020-10-13 06:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031993
2020-10-13 06:30:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.2578125Mb; avail=237094.4296875Mb
2020-10-13 06:30:27 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 8.102 | nll_loss 7.034 | ppl 131.09 | wps 50534.9 | wpb 2112 | bsz 83.9 | num_updates 1176 | best_loss 8.102
2020-10-13 06:30:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:30:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 21 @ 1176 updates, score 8.102) (writing took 4.577479482162744 seconds)
2020-10-13 06:30:32 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-13 06:30:32 | INFO | train | epoch 021 | loss 7.959 | nll_loss 6.95 | ppl 123.63 | wps 14397.3 | ups 2.7 | wpb 5324.5 | bsz 212.4 | num_updates 1176 | lr 5.88706e-05 | gnorm 1.569 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:30:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=17
2020-10-13 06:30:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=18
2020-10-13 06:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7458.6875Mb; avail=237095.9453125Mb
2020-10-13 06:30:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000389
2020-10-13 06:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002990
2020-10-13 06:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.6875Mb; avail=237095.9453125Mb
2020-10-13 06:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000093
2020-10-13 06:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.6875Mb; avail=237095.9453125Mb
2020-10-13 06:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.045438
2020-10-13 06:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.049424
2020-10-13 06:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.6875Mb; avail=237095.9453125Mb
2020-10-13 06:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7458.6875Mb; avail=237095.9453125Mb
2020-10-13 06:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001946
2020-10-13 06:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.6875Mb; avail=237095.9453125Mb
2020-10-13 06:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000082
2020-10-13 06:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.6875Mb; avail=237095.9453125Mb
2020-10-13 06:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040690
2020-10-13 06:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043442
2020-10-13 06:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.23828125Mb; avail=237093.39453125Mb
2020-10-13 06:30:32 | INFO | fairseq.trainer | begin training epoch 22
2020-10-13 06:30:39 | INFO | train_inner | epoch 022:     24 / 56 loss=7.961, nll_loss=6.951, ppl=123.7, wps=14038.2, ups=2.62, wpb=5367.5, bsz=208.6, num_updates=1200, lr=6.007e-05, gnorm=1.559, clip=0, train_wall=26, wall=0
2020-10-13 06:30:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7459.40625Mb; avail=237094.8828125Mb
2020-10-13 06:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000826
2020-10-13 06:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.40625Mb; avail=237094.8828125Mb
2020-10-13 06:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017664
2020-10-13 06:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.40625Mb; avail=237094.8828125Mb
2020-10-13 06:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013043
2020-10-13 06:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032336
2020-10-13 06:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.40625Mb; avail=237094.8828125Mb
2020-10-13 06:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7459.40625Mb; avail=237094.8828125Mb
2020-10-13 06:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000697
2020-10-13 06:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.40625Mb; avail=237094.8828125Mb
2020-10-13 06:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017519
2020-10-13 06:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.40625Mb; avail=237094.8828125Mb
2020-10-13 06:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013214
2020-10-13 06:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032173
2020-10-13 06:30:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.26953125Mb; avail=237095.01953125Mb
2020-10-13 06:30:48 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 8.068 | nll_loss 6.994 | ppl 127.45 | wps 51051.1 | wpb 2112 | bsz 83.9 | num_updates 1232 | best_loss 8.068
2020-10-13 06:30:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:30:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 22 @ 1232 updates, score 8.068) (writing took 4.566783428890631 seconds)
2020-10-13 06:30:53 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-13 06:30:53 | INFO | train | epoch 022 | loss 7.888 | nll_loss 6.869 | ppl 116.92 | wps 14398.8 | ups 2.7 | wpb 5324.5 | bsz 212.4 | num_updates 1232 | lr 6.16692e-05 | gnorm 1.605 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:30:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=18
2020-10-13 06:30:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=19
2020-10-13 06:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7457.44140625Mb; avail=237097.19140625Mb
2020-10-13 06:30:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000290
2020-10-13 06:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002978
2020-10-13 06:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.44140625Mb; avail=237097.19140625Mb
2020-10-13 06:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000131
2020-10-13 06:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.44140625Mb; avail=237097.19140625Mb
2020-10-13 06:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.044699
2020-10-13 06:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.048594
2020-10-13 06:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.640625Mb; avail=237094.9921875Mb
2020-10-13 06:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7462.0625Mb; avail=237092.5703125Mb
2020-10-13 06:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001966
2020-10-13 06:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.0625Mb; avail=237092.5703125Mb
2020-10-13 06:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000084
2020-10-13 06:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.0625Mb; avail=237092.5703125Mb
2020-10-13 06:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.039966
2020-10-13 06:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.042733
2020-10-13 06:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.2734375Mb; avail=237087.359375Mb
2020-10-13 06:30:53 | INFO | fairseq.trainer | begin training epoch 23
2020-10-13 06:31:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.7578125Mb; avail=237090.6484375Mb
2020-10-13 06:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000824
2020-10-13 06:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.7578125Mb; avail=237090.6484375Mb
2020-10-13 06:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017609
2020-10-13 06:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.57421875Mb; avail=237088.83203125Mb
2020-10-13 06:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013182
2020-10-13 06:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032395
2020-10-13 06:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.78515625Mb; avail=237087.62109375Mb
2020-10-13 06:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7470.33203125Mb; avail=237084.07421875Mb
2020-10-13 06:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000711
2020-10-13 06:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.33203125Mb; avail=237084.07421875Mb
2020-10-13 06:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017302
2020-10-13 06:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.33203125Mb; avail=237084.07421875Mb
2020-10-13 06:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013166
2020-10-13 06:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031956
2020-10-13 06:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7469.83984375Mb; avail=237084.56640625Mb
2020-10-13 06:31:09 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.983 | nll_loss 6.912 | ppl 120.45 | wps 51105 | wpb 2112 | bsz 83.9 | num_updates 1288 | best_loss 7.983
2020-10-13 06:31:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:31:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 23 @ 1288 updates, score 7.983) (writing took 4.5551579690072685 seconds)
2020-10-13 06:31:13 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-13 06:31:13 | INFO | train | epoch 023 | loss 7.802 | nll_loss 6.77 | ppl 109.16 | wps 14374.5 | ups 2.7 | wpb 5324.5 | bsz 212.4 | num_updates 1288 | lr 6.44678e-05 | gnorm 1.606 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:31:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=19
2020-10-13 06:31:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=20
2020-10-13 06:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7458.96875Mb; avail=237095.671875Mb
2020-10-13 06:31:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000283
2020-10-13 06:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002986
2020-10-13 06:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.96875Mb; avail=237095.671875Mb
2020-10-13 06:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000094
2020-10-13 06:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.96875Mb; avail=237095.671875Mb
2020-10-13 06:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.044853
2020-10-13 06:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.048720
2020-10-13 06:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.96875Mb; avail=237095.671875Mb
2020-10-13 06:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7458.96875Mb; avail=237095.671875Mb
2020-10-13 06:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001990
2020-10-13 06:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.96875Mb; avail=237095.671875Mb
2020-10-13 06:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 06:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.96875Mb; avail=237095.671875Mb
2020-10-13 06:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.039798
2020-10-13 06:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.042606
2020-10-13 06:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7458.96875Mb; avail=237095.671875Mb
2020-10-13 06:31:13 | INFO | fairseq.trainer | begin training epoch 24
2020-10-13 06:31:17 | INFO | train_inner | epoch 024:     12 / 56 loss=7.792, nll_loss=6.759, ppl=108.33, wps=13878, ups=2.62, wpb=5298.6, bsz=219.3, num_updates=1300, lr=6.50675e-05, gnorm=1.592, clip=0, train_wall=26, wall=0
2020-10-13 06:31:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7468.66015625Mb; avail=237085.65234375Mb
2020-10-13 06:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000836
2020-10-13 06:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.66015625Mb; avail=237085.65234375Mb
2020-10-13 06:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.018196
2020-10-13 06:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.66015625Mb; avail=237085.65234375Mb
2020-10-13 06:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013448
2020-10-13 06:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.033269
2020-10-13 06:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.66015625Mb; avail=237085.65234375Mb
2020-10-13 06:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7459.30859375Mb; avail=237095.00390625Mb
2020-10-13 06:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000718
2020-10-13 06:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.30859375Mb; avail=237095.00390625Mb
2020-10-13 06:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017916
2020-10-13 06:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.30859375Mb; avail=237095.00390625Mb
2020-10-13 06:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013448
2020-10-13 06:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032855
2020-10-13 06:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.30859375Mb; avail=237095.00390625Mb
2020-10-13 06:31:29 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.967 | nll_loss 6.892 | ppl 118.77 | wps 51110.7 | wpb 2112 | bsz 83.9 | num_updates 1344 | best_loss 7.967
2020-10-13 06:31:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:31:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 24 @ 1344 updates, score 7.967) (writing took 4.54782064515166 seconds)
2020-10-13 06:31:34 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-13 06:31:34 | INFO | train | epoch 024 | loss 7.727 | nll_loss 6.686 | ppl 102.93 | wps 14368.4 | ups 2.7 | wpb 5324.5 | bsz 212.4 | num_updates 1344 | lr 6.72664e-05 | gnorm 1.558 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:31:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=20
2020-10-13 06:31:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=21
2020-10-13 06:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7457.64453125Mb; avail=237096.98046875Mb
2020-10-13 06:31:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000325
2020-10-13 06:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003013
2020-10-13 06:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.64453125Mb; avail=237096.98046875Mb
2020-10-13 06:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000091
2020-10-13 06:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.64453125Mb; avail=237096.98046875Mb
2020-10-13 06:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.044743
2020-10-13 06:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.048638
2020-10-13 06:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.64453125Mb; avail=237096.98046875Mb
2020-10-13 06:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7457.64453125Mb; avail=237096.98046875Mb
2020-10-13 06:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001942
2020-10-13 06:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.64453125Mb; avail=237096.98046875Mb
2020-10-13 06:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 06:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7457.64453125Mb; avail=237096.98046875Mb
2020-10-13 06:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040692
2020-10-13 06:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043451
2020-10-13 06:31:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.4921875Mb; avail=237095.1328125Mb
2020-10-13 06:31:34 | INFO | fairseq.trainer | begin training epoch 25
2020-10-13 06:31:49 | INFO | train_inner | epoch 025:     56 / 56 loss=7.694, nll_loss=6.648, ppl=100.26, wps=16266.2, ups=3.07, wpb=5298.6, bsz=208.4, num_updates=1400, lr=7.0065e-05, gnorm=1.577, clip=0, train_wall=26, wall=0
2020-10-13 06:31:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7460.6484375Mb; avail=237094.4375Mb
2020-10-13 06:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000840
2020-10-13 06:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.6484375Mb; avail=237094.4375Mb
2020-10-13 06:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017741
2020-10-13 06:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.6484375Mb; avail=237094.4375Mb
2020-10-13 06:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013660
2020-10-13 06:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.033026
2020-10-13 06:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.4609375Mb; avail=237094.625Mb
2020-10-13 06:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7461.046875Mb; avail=237094.0390625Mb
2020-10-13 06:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000732
2020-10-13 06:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.65234375Mb; avail=237093.43359375Mb
2020-10-13 06:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017710
2020-10-13 06:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.4609375Mb; avail=237091.625Mb
2020-10-13 06:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013292
2020-10-13 06:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032521
2020-10-13 06:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.671875Mb; avail=237090.4140625Mb
2020-10-13 06:31:50 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.935 | nll_loss 6.85 | ppl 115.4 | wps 50773.9 | wpb 2112 | bsz 83.9 | num_updates 1400 | best_loss 7.935
2020-10-13 06:31:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:31:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 25 @ 1400 updates, score 7.935) (writing took 4.557756345020607 seconds)
2020-10-13 06:31:55 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-13 06:31:55 | INFO | train | epoch 025 | loss 7.654 | nll_loss 6.602 | ppl 97.13 | wps 14395.2 | ups 2.7 | wpb 5324.5 | bsz 212.4 | num_updates 1400 | lr 7.0065e-05 | gnorm 1.591 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:31:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=21
2020-10-13 06:31:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=22
2020-10-13 06:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7460.515625Mb; avail=237094.125Mb
2020-10-13 06:31:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000373
2020-10-13 06:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003012
2020-10-13 06:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.515625Mb; avail=237094.125Mb
2020-10-13 06:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000094
2020-10-13 06:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.515625Mb; avail=237094.125Mb
2020-10-13 06:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.045640
2020-10-13 06:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.049536
2020-10-13 06:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.515625Mb; avail=237094.125Mb
2020-10-13 06:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7460.515625Mb; avail=237094.125Mb
2020-10-13 06:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001980
2020-10-13 06:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.515625Mb; avail=237094.125Mb
2020-10-13 06:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000082
2020-10-13 06:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.515625Mb; avail=237094.125Mb
2020-10-13 06:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041158
2020-10-13 06:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043956
2020-10-13 06:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.515625Mb; avail=237094.125Mb
2020-10-13 06:31:55 | INFO | fairseq.trainer | begin training epoch 26
2020-10-13 06:32:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:32:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7472.71484375Mb; avail=237081.69140625Mb
2020-10-13 06:32:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000856
2020-10-13 06:32:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.3203125Mb; avail=237081.0859375Mb
2020-10-13 06:32:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017673
2020-10-13 06:32:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.01953125Mb; avail=237079.38671875Mb
2020-10-13 06:32:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013278
2020-10-13 06:32:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032579
2020-10-13 06:32:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.38671875Mb; avail=237088.01953125Mb
2020-10-13 06:32:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7470.51171875Mb; avail=237083.89453125Mb
2020-10-13 06:32:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000727
2020-10-13 06:32:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.51171875Mb; avail=237083.89453125Mb
2020-10-13 06:32:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017357
2020-10-13 06:32:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.51171875Mb; avail=237083.89453125Mb
2020-10-13 06:32:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013171
2020-10-13 06:32:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031993
2020-10-13 06:32:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.51171875Mb; avail=237083.89453125Mb
2020-10-13 06:32:11 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.947 | nll_loss 6.849 | ppl 115.28 | wps 50949.7 | wpb 2112 | bsz 83.9 | num_updates 1456 | best_loss 7.935
2020-10-13 06:32:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:32:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 26 @ 1456 updates, score 7.947) (writing took 2.371992042986676 seconds)
2020-10-13 06:32:13 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-13 06:32:13 | INFO | train | epoch 026 | loss 7.578 | nll_loss 6.516 | ppl 91.49 | wps 16024.6 | ups 3.01 | wpb 5324.5 | bsz 212.4 | num_updates 1456 | lr 7.28636e-05 | gnorm 1.533 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:32:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=22
2020-10-13 06:32:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=23
2020-10-13 06:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7460.19140625Mb; avail=237094.44921875Mb
2020-10-13 06:32:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000285
2020-10-13 06:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002975
2020-10-13 06:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.19140625Mb; avail=237094.44921875Mb
2020-10-13 06:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000090
2020-10-13 06:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.19140625Mb; avail=237094.44921875Mb
2020-10-13 06:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.045096
2020-10-13 06:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.049057
2020-10-13 06:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.19140625Mb; avail=237094.44921875Mb
2020-10-13 06:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7462.01953125Mb; avail=237092.015625Mb
2020-10-13 06:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001958
2020-10-13 06:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.625Mb; avail=237092.015625Mb
2020-10-13 06:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000085
2020-10-13 06:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.625Mb; avail=237092.015625Mb
2020-10-13 06:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040189
2020-10-13 06:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.042986
2020-10-13 06:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.8125Mb; avail=237086.9140625Mb
2020-10-13 06:32:13 | INFO | fairseq.trainer | begin training epoch 27
2020-10-13 06:32:26 | INFO | train_inner | epoch 027:     44 / 56 loss=7.554, nll_loss=6.488, ppl=89.74, wps=14641.8, ups=2.75, wpb=5330.6, bsz=216, num_updates=1500, lr=7.50625e-05, gnorm=1.612, clip=0, train_wall=26, wall=0
2020-10-13 06:32:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7471.30859375Mb; avail=237083.078125Mb
2020-10-13 06:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000797
2020-10-13 06:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.30859375Mb; avail=237083.078125Mb
2020-10-13 06:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017289
2020-10-13 06:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.83203125Mb; avail=237082.5546875Mb
2020-10-13 06:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013372
2020-10-13 06:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032236
2020-10-13 06:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.83203125Mb; avail=237082.5546875Mb
2020-10-13 06:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7462.0625Mb; avail=237092.32421875Mb
2020-10-13 06:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000677
2020-10-13 06:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.0625Mb; avail=237092.32421875Mb
2020-10-13 06:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017220
2020-10-13 06:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.0625Mb; avail=237092.32421875Mb
2020-10-13 06:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013094
2020-10-13 06:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031753
2020-10-13 06:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.0625Mb; avail=237092.32421875Mb
2020-10-13 06:32:30 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.864 | nll_loss 6.76 | ppl 108.4 | wps 50784.8 | wpb 2112 | bsz 83.9 | num_updates 1512 | best_loss 7.864
2020-10-13 06:32:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:32:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 27 @ 1512 updates, score 7.864) (writing took 4.560462736990303 seconds)
2020-10-13 06:32:34 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-13 06:32:34 | INFO | train | epoch 027 | loss 7.515 | nll_loss 6.443 | ppl 87.01 | wps 14251.8 | ups 2.68 | wpb 5324.5 | bsz 212.4 | num_updates 1512 | lr 7.56622e-05 | gnorm 1.662 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:32:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=23
2020-10-13 06:32:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=24
2020-10-13 06:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7460.8359375Mb; avail=237093.79296875Mb
2020-10-13 06:32:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000307
2020-10-13 06:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002985
2020-10-13 06:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.8359375Mb; avail=237093.79296875Mb
2020-10-13 06:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000156
2020-10-13 06:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.8359375Mb; avail=237093.79296875Mb
2020-10-13 06:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.045189
2020-10-13 06:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.049156
2020-10-13 06:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.828125Mb; avail=237093.80078125Mb
2020-10-13 06:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7460.8203125Mb; avail=237093.80859375Mb
2020-10-13 06:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001957
2020-10-13 06:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.8203125Mb; avail=237093.80859375Mb
2020-10-13 06:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000083
2020-10-13 06:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.8203125Mb; avail=237093.80859375Mb
2020-10-13 06:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040599
2020-10-13 06:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043382
2020-10-13 06:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.55078125Mb; avail=237094.078125Mb
2020-10-13 06:32:34 | INFO | fairseq.trainer | begin training epoch 28
2020-10-13 06:32:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7473.78515625Mb; avail=237081.09765625Mb
2020-10-13 06:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000875
2020-10-13 06:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.78515625Mb; avail=237081.09765625Mb
2020-10-13 06:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017696
2020-10-13 06:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.53515625Mb; avail=237080.10546875Mb
2020-10-13 06:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013352
2020-10-13 06:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032689
2020-10-13 06:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.67578125Mb; avail=237088.96484375Mb
2020-10-13 06:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7470.01953125Mb; avail=237084.62109375Mb
2020-10-13 06:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000705
2020-10-13 06:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.01953125Mb; avail=237084.62109375Mb
2020-10-13 06:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017691
2020-10-13 06:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.01953125Mb; avail=237084.62109375Mb
2020-10-13 06:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013128
2020-10-13 06:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032258
2020-10-13 06:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.5703125Mb; avail=237084.0703125Mb
2020-10-13 06:32:50 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.875 | nll_loss 6.75 | ppl 107.66 | wps 50860.5 | wpb 2112 | bsz 83.9 | num_updates 1568 | best_loss 7.864
2020-10-13 06:32:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:32:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 28 @ 1568 updates, score 7.875) (writing took 2.4295908308122307 seconds)
2020-10-13 06:32:53 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-13 06:32:53 | INFO | train | epoch 028 | loss 7.461 | nll_loss 6.38 | ppl 83.31 | wps 16043.4 | ups 3.01 | wpb 5324.5 | bsz 212.4 | num_updates 1568 | lr 7.84608e-05 | gnorm 1.857 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:32:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=24
2020-10-13 06:32:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=25
2020-10-13 06:32:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.71484375Mb; avail=237090.890625Mb
2020-10-13 06:32:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000311
2020-10-13 06:32:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003098
2020-10-13 06:32:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.71484375Mb; avail=237090.890625Mb
2020-10-13 06:32:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000095
2020-10-13 06:32:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.71484375Mb; avail=237090.890625Mb
2020-10-13 06:32:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.044091
2020-10-13 06:32:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.048109
2020-10-13 06:32:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.375Mb; avail=237084.23046875Mb
2020-10-13 06:32:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7472.6953125Mb; avail=237081.3046875Mb
2020-10-13 06:32:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001994
2020-10-13 06:32:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.30078125Mb; avail=237081.3046875Mb
2020-10-13 06:32:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000105
2020-10-13 06:32:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.30078125Mb; avail=237081.3046875Mb
2020-10-13 06:32:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040505
2020-10-13 06:32:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043335
2020-10-13 06:32:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.08984375Mb; avail=237087.515625Mb
2020-10-13 06:32:53 | INFO | fairseq.trainer | begin training epoch 29
2020-10-13 06:33:02 | INFO | train_inner | epoch 029:     32 / 56 loss=7.431, nll_loss=6.346, ppl=81.32, wps=14814.4, ups=2.78, wpb=5329.2, bsz=213.9, num_updates=1600, lr=8.006e-05, gnorm=1.77, clip=0, train_wall=26, wall=0
2020-10-13 06:33:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7467.56640625Mb; avail=237087.44140625Mb
2020-10-13 06:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000864
2020-10-13 06:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.56640625Mb; avail=237087.44140625Mb
2020-10-13 06:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017683
2020-10-13 06:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.8125Mb; avail=237083.1953125Mb
2020-10-13 06:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013349
2020-10-13 06:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032736
2020-10-13 06:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7472.31640625Mb; avail=237082.69140625Mb
2020-10-13 06:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7472.1953125Mb; avail=237082.8125Mb
2020-10-13 06:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000736
2020-10-13 06:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7472.1953125Mb; avail=237082.8125Mb
2020-10-13 06:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017431
2020-10-13 06:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.3515625Mb; avail=237092.65625Mb
2020-10-13 06:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013388
2020-10-13 06:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032352
2020-10-13 06:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.48046875Mb; avail=237092.76171875Mb
2020-10-13 06:33:09 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.769 | nll_loss 6.675 | ppl 102.21 | wps 51153.7 | wpb 2112 | bsz 83.9 | num_updates 1624 | best_loss 7.769
2020-10-13 06:33:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:33:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 29 @ 1624 updates, score 7.769) (writing took 4.551912970142439 seconds)
2020-10-13 06:33:14 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-13 06:33:14 | INFO | train | epoch 029 | loss 7.368 | nll_loss 6.273 | ppl 77.33 | wps 14389.2 | ups 2.7 | wpb 5324.5 | bsz 212.4 | num_updates 1624 | lr 8.12594e-05 | gnorm 1.667 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:33:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=25
2020-10-13 06:33:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=26
2020-10-13 06:33:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7462.42578125Mb; avail=237092.21484375Mb
2020-10-13 06:33:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000358
2020-10-13 06:33:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002963
2020-10-13 06:33:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.42578125Mb; avail=237092.21484375Mb
2020-10-13 06:33:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000095
2020-10-13 06:33:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.42578125Mb; avail=237092.21484375Mb
2020-10-13 06:33:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.044829
2020-10-13 06:33:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.048830
2020-10-13 06:33:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.42578125Mb; avail=237092.21484375Mb
2020-10-13 06:33:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7462.42578125Mb; avail=237092.21484375Mb
2020-10-13 06:33:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001959
2020-10-13 06:33:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.42578125Mb; avail=237092.21484375Mb
2020-10-13 06:33:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000084
2020-10-13 06:33:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.42578125Mb; avail=237092.21484375Mb
2020-10-13 06:33:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040511
2020-10-13 06:33:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043301
2020-10-13 06:33:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.42578125Mb; avail=237092.21484375Mb
2020-10-13 06:33:14 | INFO | fairseq.trainer | begin training epoch 30
2020-10-13 06:33:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7466.69921875Mb; avail=237087.65625Mb
2020-10-13 06:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000821
2020-10-13 06:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.69921875Mb; avail=237087.65625Mb
2020-10-13 06:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017828
2020-10-13 06:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.515625Mb; avail=237085.83984375Mb
2020-10-13 06:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013138
2020-10-13 06:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032598
2020-10-13 06:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.54296875Mb; avail=237082.8125Mb
2020-10-13 06:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7473.359375Mb; avail=237080.99609375Mb
2020-10-13 06:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000707
2020-10-13 06:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.359375Mb; avail=237080.99609375Mb
2020-10-13 06:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017361
2020-10-13 06:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.6953125Mb; avail=237079.66015625Mb
2020-10-13 06:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012993
2020-10-13 06:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031799
2020-10-13 06:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.5546875Mb; avail=237087.80078125Mb
2020-10-13 06:33:30 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.802 | nll_loss 6.673 | ppl 102.07 | wps 51229.6 | wpb 2112 | bsz 83.9 | num_updates 1680 | best_loss 7.769
2020-10-13 06:33:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:33:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 30 @ 1680 updates, score 7.802) (writing took 2.36876249499619 seconds)
2020-10-13 06:33:32 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-13 06:33:32 | INFO | train | epoch 030 | loss 7.301 | nll_loss 6.198 | ppl 73.42 | wps 16173.7 | ups 3.04 | wpb 5324.5 | bsz 212.4 | num_updates 1680 | lr 8.4058e-05 | gnorm 1.809 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:33:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=26
2020-10-13 06:33:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=27
2020-10-13 06:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7462.4765625Mb; avail=237092.1171875Mb
2020-10-13 06:33:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000314
2020-10-13 06:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002992
2020-10-13 06:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.4765625Mb; avail=237092.1171875Mb
2020-10-13 06:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000097
2020-10-13 06:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.4765625Mb; avail=237092.1171875Mb
2020-10-13 06:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.043927
2020-10-13 06:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.047938
2020-10-13 06:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.4765625Mb; avail=237092.1171875Mb
2020-10-13 06:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7462.37109375Mb; avail=237092.22265625Mb
2020-10-13 06:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002009
2020-10-13 06:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.37109375Mb; avail=237092.22265625Mb
2020-10-13 06:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000083
2020-10-13 06:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.37109375Mb; avail=237092.22265625Mb
2020-10-13 06:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040161
2020-10-13 06:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.042987
2020-10-13 06:33:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.04296875Mb; avail=237089.55078125Mb
2020-10-13 06:33:32 | INFO | fairseq.trainer | begin training epoch 31
2020-10-13 06:33:38 | INFO | train_inner | epoch 031:     20 / 56 loss=7.322, nll_loss=6.221, ppl=74.6, wps=14855.7, ups=2.79, wpb=5330.1, bsz=203.8, num_updates=1700, lr=8.50575e-05, gnorm=1.766, clip=0, train_wall=26, wall=0
2020-10-13 06:33:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7460.265625Mb; avail=237094.06640625Mb
2020-10-13 06:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000868
2020-10-13 06:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.265625Mb; avail=237094.06640625Mb
2020-10-13 06:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017760
2020-10-13 06:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.265625Mb; avail=237094.06640625Mb
2020-10-13 06:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013012
2020-10-13 06:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032416
2020-10-13 06:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.265625Mb; avail=237094.06640625Mb
2020-10-13 06:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7460.265625Mb; avail=237094.06640625Mb
2020-10-13 06:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000715
2020-10-13 06:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.265625Mb; avail=237094.06640625Mb
2020-10-13 06:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017058
2020-10-13 06:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.265625Mb; avail=237094.06640625Mb
2020-10-13 06:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013073
2020-10-13 06:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031590
2020-10-13 06:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.265625Mb; avail=237094.06640625Mb
2020-10-13 06:33:48 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.797 | nll_loss 6.66 | ppl 101.1 | wps 50973.4 | wpb 2112 | bsz 83.9 | num_updates 1736 | best_loss 7.769
2020-10-13 06:33:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:33:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 31 @ 1736 updates, score 7.797) (writing took 2.3575291961897165 seconds)
2020-10-13 06:33:50 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-13 06:33:50 | INFO | train | epoch 031 | loss 7.229 | nll_loss 6.114 | ppl 69.26 | wps 16163.3 | ups 3.04 | wpb 5324.5 | bsz 212.4 | num_updates 1736 | lr 8.68566e-05 | gnorm 1.712 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:33:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=27
2020-10-13 06:33:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=28
2020-10-13 06:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7461.6875Mb; avail=237092.8828125Mb
2020-10-13 06:33:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000272
2020-10-13 06:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002914
2020-10-13 06:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.6875Mb; avail=237092.8828125Mb
2020-10-13 06:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000096
2020-10-13 06:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.6875Mb; avail=237092.8828125Mb
2020-10-13 06:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.044512
2020-10-13 06:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.048379
2020-10-13 06:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.6875Mb; avail=237092.8828125Mb
2020-10-13 06:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7461.6875Mb; avail=237092.8828125Mb
2020-10-13 06:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001971
2020-10-13 06:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.6875Mb; avail=237092.8828125Mb
2020-10-13 06:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000084
2020-10-13 06:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.6875Mb; avail=237092.8828125Mb
2020-10-13 06:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040233
2020-10-13 06:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043004
2020-10-13 06:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.6875Mb; avail=237092.8828125Mb
2020-10-13 06:33:51 | INFO | fairseq.trainer | begin training epoch 32
2020-10-13 06:34:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7475.34375Mb; avail=237078.953125Mb
2020-10-13 06:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000858
2020-10-13 06:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.34375Mb; avail=237078.953125Mb
2020-10-13 06:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017486
2020-10-13 06:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7480.1015625Mb; avail=237074.1953125Mb
2020-10-13 06:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013184
2020-10-13 06:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032295
2020-10-13 06:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7480.6015625Mb; avail=237073.6953125Mb
2020-10-13 06:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7480.69921875Mb; avail=237073.59765625Mb
2020-10-13 06:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000717
2020-10-13 06:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7477.74609375Mb; avail=237078.51953125Mb
2020-10-13 06:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016928
2020-10-13 06:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.359375Mb; avail=237082.9375Mb
2020-10-13 06:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013166
2020-10-13 06:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031558
2020-10-13 06:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.515625Mb; avail=237092.78125Mb
2020-10-13 06:34:07 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.75 | nll_loss 6.65 | ppl 100.43 | wps 51178.6 | wpb 2112 | bsz 83.9 | num_updates 1792 | best_loss 7.75
2020-10-13 06:34:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:34:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 32 @ 1792 updates, score 7.75) (writing took 4.618601157097146 seconds)
2020-10-13 06:34:11 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-13 06:34:11 | INFO | train | epoch 032 | loss 7.145 | nll_loss 6.019 | ppl 64.83 | wps 14313 | ups 2.69 | wpb 5324.5 | bsz 212.4 | num_updates 1792 | lr 8.96552e-05 | gnorm 1.637 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:34:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=28
2020-10-13 06:34:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=29
2020-10-13 06:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7460.7421875Mb; avail=237093.8671875Mb
2020-10-13 06:34:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000316
2020-10-13 06:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003071
2020-10-13 06:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.7421875Mb; avail=237093.8671875Mb
2020-10-13 06:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000095
2020-10-13 06:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.7421875Mb; avail=237093.8671875Mb
2020-10-13 06:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.044898
2020-10-13 06:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.048851
2020-10-13 06:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.07421875Mb; avail=237092.53515625Mb
2020-10-13 06:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7465.0859375Mb; avail=237089.5234375Mb
2020-10-13 06:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001979
2020-10-13 06:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.0859375Mb; avail=237089.5234375Mb
2020-10-13 06:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000084
2020-10-13 06:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.0859375Mb; avail=237089.5234375Mb
2020-10-13 06:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040501
2020-10-13 06:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043315
2020-10-13 06:34:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.7578125Mb; avail=237079.8515625Mb
2020-10-13 06:34:11 | INFO | fairseq.trainer | begin training epoch 33
2020-10-13 06:34:14 | INFO | train_inner | epoch 033:      8 / 56 loss=7.151, nll_loss=6.026, ppl=65.15, wps=14704.7, ups=2.77, wpb=5308.4, bsz=213.6, num_updates=1800, lr=9.0055e-05, gnorm=1.681, clip=0, train_wall=26, wall=0
2020-10-13 06:34:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7459.81640625Mb; avail=237094.46484375Mb
2020-10-13 06:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000875
2020-10-13 06:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.81640625Mb; avail=237094.46484375Mb
2020-10-13 06:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017303
2020-10-13 06:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.81640625Mb; avail=237094.46484375Mb
2020-10-13 06:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013314
2020-10-13 06:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032319
2020-10-13 06:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.81640625Mb; avail=237094.46484375Mb
2020-10-13 06:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7459.81640625Mb; avail=237094.46484375Mb
2020-10-13 06:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000720
2020-10-13 06:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.81640625Mb; avail=237094.46484375Mb
2020-10-13 06:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017113
2020-10-13 06:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.81640625Mb; avail=237094.46484375Mb
2020-10-13 06:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013155
2020-10-13 06:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031812
2020-10-13 06:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.81640625Mb; avail=237094.46484375Mb
2020-10-13 06:34:28 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.702 | nll_loss 6.564 | ppl 94.61 | wps 50466 | wpb 2112 | bsz 83.9 | num_updates 1848 | best_loss 7.702
2020-10-13 06:34:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:34:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 33 @ 1848 updates, score 7.702) (writing took 4.550488736014813 seconds)
2020-10-13 06:34:32 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-13 06:34:32 | INFO | train | epoch 033 | loss 7.073 | nll_loss 5.938 | ppl 61.29 | wps 14308.8 | ups 2.69 | wpb 5324.5 | bsz 212.4 | num_updates 1848 | lr 9.24538e-05 | gnorm 1.7 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:34:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=29
2020-10-13 06:34:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=30
2020-10-13 06:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7461.1640625Mb; avail=237093.4609375Mb
2020-10-13 06:34:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000317
2020-10-13 06:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003180
2020-10-13 06:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.1640625Mb; avail=237093.4609375Mb
2020-10-13 06:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000141
2020-10-13 06:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.1640625Mb; avail=237093.4609375Mb
2020-10-13 06:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.045582
2020-10-13 06:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.049642
2020-10-13 06:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.1640625Mb; avail=237093.4609375Mb
2020-10-13 06:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7461.1640625Mb; avail=237093.4609375Mb
2020-10-13 06:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001965
2020-10-13 06:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.1640625Mb; avail=237093.4609375Mb
2020-10-13 06:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000084
2020-10-13 06:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.1640625Mb; avail=237093.4609375Mb
2020-10-13 06:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040032
2020-10-13 06:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.042843
2020-10-13 06:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.1640625Mb; avail=237093.4609375Mb
2020-10-13 06:34:32 | INFO | fairseq.trainer | begin training epoch 34
2020-10-13 06:34:46 | INFO | train_inner | epoch 034:     52 / 56 loss=7.035, nll_loss=5.893, ppl=59.41, wps=16320.5, ups=3.05, wpb=5356.4, bsz=214.5, num_updates=1900, lr=9.50525e-05, gnorm=1.631, clip=0, train_wall=26, wall=0
2020-10-13 06:34:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7471.0859375Mb; avail=237083.3203125Mb
2020-10-13 06:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000839
2020-10-13 06:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.0859375Mb; avail=237083.3203125Mb
2020-10-13 06:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017522
2020-10-13 06:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.58984375Mb; avail=237082.81640625Mb
2020-10-13 06:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013427
2020-10-13 06:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032561
2020-10-13 06:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.58984375Mb; avail=237082.81640625Mb
2020-10-13 06:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7462.23828125Mb; avail=237092.16796875Mb
2020-10-13 06:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000709
2020-10-13 06:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.23828125Mb; avail=237092.16796875Mb
2020-10-13 06:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017448
2020-10-13 06:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.23828125Mb; avail=237092.16796875Mb
2020-10-13 06:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013170
2020-10-13 06:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032085
2020-10-13 06:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.23828125Mb; avail=237092.16796875Mb
2020-10-13 06:34:48 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.679 | nll_loss 6.549 | ppl 93.64 | wps 50654 | wpb 2112 | bsz 83.9 | num_updates 1904 | best_loss 7.679
2020-10-13 06:34:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:34:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 34 @ 1904 updates, score 7.679) (writing took 4.559198435163125 seconds)
2020-10-13 06:34:53 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-13 06:34:53 | INFO | train | epoch 034 | loss 6.991 | nll_loss 5.843 | ppl 57.41 | wps 14330.1 | ups 2.69 | wpb 5324.5 | bsz 212.4 | num_updates 1904 | lr 9.52524e-05 | gnorm 1.646 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:34:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=30
2020-10-13 06:34:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=31
2020-10-13 06:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7464.76953125Mb; avail=237090.34375Mb
2020-10-13 06:34:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000280
2020-10-13 06:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002722
2020-10-13 06:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.76953125Mb; avail=237090.34375Mb
2020-10-13 06:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000098
2020-10-13 06:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.76953125Mb; avail=237090.34375Mb
2020-10-13 06:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041699
2020-10-13 06:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.045266
2020-10-13 06:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7469.5Mb; avail=237085.61328125Mb
2020-10-13 06:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7460.6953125Mb; avail=237094.41796875Mb
2020-10-13 06:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002085
2020-10-13 06:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.6953125Mb; avail=237094.41796875Mb
2020-10-13 06:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000092
2020-10-13 06:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.6953125Mb; avail=237094.41796875Mb
2020-10-13 06:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041325
2020-10-13 06:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044262
2020-10-13 06:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.93359375Mb; avail=237089.6640625Mb
2020-10-13 06:34:53 | INFO | fairseq.trainer | begin training epoch 35
2020-10-13 06:35:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7459.4453125Mb; avail=237095.0625Mb
2020-10-13 06:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000834
2020-10-13 06:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.4453125Mb; avail=237095.0625Mb
2020-10-13 06:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017641
2020-10-13 06:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.453125Mb; avail=237095.0546875Mb
2020-10-13 06:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013064
2020-10-13 06:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032312
2020-10-13 06:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.51171875Mb; avail=237095.0703125Mb
2020-10-13 06:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7459.51171875Mb; avail=237095.0703125Mb
2020-10-13 06:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000696
2020-10-13 06:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.51171875Mb; avail=237095.0703125Mb
2020-10-13 06:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017038
2020-10-13 06:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.76171875Mb; avail=237094.8203125Mb
2020-10-13 06:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013206
2020-10-13 06:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031699
2020-10-13 06:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.76171875Mb; avail=237094.8203125Mb
2020-10-13 06:35:09 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.723 | nll_loss 6.586 | ppl 96.09 | wps 48674.3 | wpb 2112 | bsz 83.9 | num_updates 1960 | best_loss 7.679
2020-10-13 06:35:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:35:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 35 @ 1960 updates, score 7.723) (writing took 2.35703421686776 seconds)
2020-10-13 06:35:11 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-13 06:35:11 | INFO | train | epoch 035 | loss 6.921 | nll_loss 5.762 | ppl 54.25 | wps 16082.4 | ups 3.02 | wpb 5324.5 | bsz 212.4 | num_updates 1960 | lr 9.8051e-05 | gnorm 1.751 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:35:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=31
2020-10-13 06:35:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=32
2020-10-13 06:35:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7462.81640625Mb; avail=237091.82421875Mb
2020-10-13 06:35:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000308
2020-10-13 06:35:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002713
2020-10-13 06:35:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.81640625Mb; avail=237091.82421875Mb
2020-10-13 06:35:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000096
2020-10-13 06:35:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.81640625Mb; avail=237091.82421875Mb
2020-10-13 06:35:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041383
2020-10-13 06:35:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044925
2020-10-13 06:35:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.81640625Mb; avail=237091.82421875Mb
2020-10-13 06:35:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7462.81640625Mb; avail=237091.82421875Mb
2020-10-13 06:35:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002020
2020-10-13 06:35:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.81640625Mb; avail=237091.82421875Mb
2020-10-13 06:35:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000083
2020-10-13 06:35:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.81640625Mb; avail=237091.82421875Mb
2020-10-13 06:35:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040300
2020-10-13 06:35:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043124
2020-10-13 06:35:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.81640625Mb; avail=237091.82421875Mb
2020-10-13 06:35:12 | INFO | fairseq.trainer | begin training epoch 36
2020-10-13 06:35:23 | INFO | train_inner | epoch 036:     40 / 56 loss=6.904, nll_loss=5.742, ppl=53.51, wps=14754.9, ups=2.77, wpb=5319.1, bsz=214, num_updates=2000, lr=0.00010005, gnorm=1.808, clip=0, train_wall=26, wall=0
2020-10-13 06:35:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7462.10546875Mb; avail=237092.21484375Mb
2020-10-13 06:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000857
2020-10-13 06:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.10546875Mb; avail=237092.21484375Mb
2020-10-13 06:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017452
2020-10-13 06:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.921875Mb; avail=237090.3984375Mb
2020-10-13 06:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013339
2020-10-13 06:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032466
2020-10-13 06:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.73828125Mb; avail=237088.58203125Mb
2020-10-13 06:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7468.66015625Mb; avail=237085.66015625Mb
2020-10-13 06:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000741
2020-10-13 06:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.66015625Mb; avail=237085.66015625Mb
2020-10-13 06:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017388
2020-10-13 06:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.609375Mb; avail=237079.7109375Mb
2020-10-13 06:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013545
2020-10-13 06:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032467
2020-10-13 06:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.8203125Mb; avail=237078.5Mb
2020-10-13 06:35:28 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.659 | nll_loss 6.513 | ppl 91.3 | wps 51146 | wpb 2112 | bsz 83.9 | num_updates 2016 | best_loss 7.659
2020-10-13 06:35:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:35:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 36 @ 2016 updates, score 7.659) (writing took 4.55710387788713 seconds)
2020-10-13 06:35:32 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-13 06:35:32 | INFO | train | epoch 036 | loss 6.882 | nll_loss 5.716 | ppl 52.55 | wps 14392.6 | ups 2.7 | wpb 5324.5 | bsz 212.4 | num_updates 2016 | lr 0.00010085 | gnorm 1.812 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:35:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=32
2020-10-13 06:35:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=33
2020-10-13 06:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.9609375Mb; avail=237090.6484375Mb
2020-10-13 06:35:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000274
2020-10-13 06:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002648
2020-10-13 06:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.56640625Mb; avail=237090.04296875Mb
2020-10-13 06:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000094
2020-10-13 06:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.56640625Mb; avail=237090.04296875Mb
2020-10-13 06:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041150
2020-10-13 06:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044614
2020-10-13 06:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.015625Mb; avail=237084.59375Mb
2020-10-13 06:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7470.54296875Mb; avail=237084.06640625Mb
2020-10-13 06:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002011
2020-10-13 06:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.54296875Mb; avail=237084.06640625Mb
2020-10-13 06:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000086
2020-10-13 06:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.54296875Mb; avail=237084.06640625Mb
2020-10-13 06:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040775
2020-10-13 06:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043581
2020-10-13 06:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.18359375Mb; avail=237093.42578125Mb
2020-10-13 06:35:32 | INFO | fairseq.trainer | begin training epoch 37
2020-10-13 06:35:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7464.64453125Mb; avail=237089.71875Mb
2020-10-13 06:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000843
2020-10-13 06:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.64453125Mb; avail=237089.71875Mb
2020-10-13 06:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017824
2020-10-13 06:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.64453125Mb; avail=237089.71875Mb
2020-10-13 06:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013440
2020-10-13 06:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032882
2020-10-13 06:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.64453125Mb; avail=237089.71875Mb
2020-10-13 06:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7464.64453125Mb; avail=237089.71875Mb
2020-10-13 06:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000722
2020-10-13 06:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.64453125Mb; avail=237089.71875Mb
2020-10-13 06:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017650
2020-10-13 06:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.64453125Mb; avail=237089.71875Mb
2020-10-13 06:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013148
2020-10-13 06:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032264
2020-10-13 06:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.64453125Mb; avail=237089.71875Mb
2020-10-13 06:35:48 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.633 | nll_loss 6.47 | ppl 88.62 | wps 50535.2 | wpb 2112 | bsz 83.9 | num_updates 2072 | best_loss 7.633
2020-10-13 06:35:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:35:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 37 @ 2072 updates, score 7.633) (writing took 4.567125309957191 seconds)
2020-10-13 06:35:53 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-13 06:35:53 | INFO | train | epoch 037 | loss 6.774 | nll_loss 5.594 | ppl 48.29 | wps 14314.5 | ups 2.69 | wpb 5324.5 | bsz 212.4 | num_updates 2072 | lr 0.000103648 | gnorm 1.724 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:35:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=33
2020-10-13 06:35:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=34
2020-10-13 06:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7465.58984375Mb; avail=237089.01953125Mb
2020-10-13 06:35:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000274
2020-10-13 06:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002705
2020-10-13 06:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.58984375Mb; avail=237089.01953125Mb
2020-10-13 06:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000092
2020-10-13 06:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.58984375Mb; avail=237089.01953125Mb
2020-10-13 06:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040995
2020-10-13 06:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044509
2020-10-13 06:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.58984375Mb; avail=237089.01953125Mb
2020-10-13 06:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7461.16015625Mb; avail=237093.44921875Mb
2020-10-13 06:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002045
2020-10-13 06:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.16015625Mb; avail=237093.44921875Mb
2020-10-13 06:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000084
2020-10-13 06:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.16015625Mb; avail=237093.44921875Mb
2020-10-13 06:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040649
2020-10-13 06:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043494
2020-10-13 06:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.16015625Mb; avail=237093.44921875Mb
2020-10-13 06:35:53 | INFO | fairseq.trainer | begin training epoch 38
2020-10-13 06:36:01 | INFO | train_inner | epoch 038:     28 / 56 loss=6.755, nll_loss=5.572, ppl=47.56, wps=13839.6, ups=2.62, wpb=5290.7, bsz=214.6, num_updates=2100, lr=0.000105048, gnorm=1.686, clip=0, train_wall=26, wall=0
2020-10-13 06:36:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:36:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7465.29296875Mb; avail=237089.7265625Mb
2020-10-13 06:36:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000839
2020-10-13 06:36:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.29296875Mb; avail=237089.7265625Mb
2020-10-13 06:36:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017809
2020-10-13 06:36:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.4453125Mb; avail=237093.57421875Mb
2020-10-13 06:36:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013299
2020-10-13 06:36:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032707
2020-10-13 06:36:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.40234375Mb; avail=237092.4921875Mb
2020-10-13 06:36:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7465.4296875Mb; avail=237089.46484375Mb
2020-10-13 06:36:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000705
2020-10-13 06:36:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.4140625Mb; avail=237089.48046875Mb
2020-10-13 06:36:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017341
2020-10-13 06:36:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.2265625Mb; avail=237089.66796875Mb
2020-10-13 06:36:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013231
2020-10-13 06:36:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032008
2020-10-13 06:36:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.9296875Mb; avail=237089.96484375Mb
2020-10-13 06:36:09 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.627 | nll_loss 6.461 | ppl 88.11 | wps 50896 | wpb 2112 | bsz 83.9 | num_updates 2128 | best_loss 7.627
2020-10-13 06:36:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:36:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 38 @ 2128 updates, score 7.627) (writing took 4.570079119177535 seconds)
2020-10-13 06:36:14 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-13 06:36:14 | INFO | train | epoch 038 | loss 6.702 | nll_loss 5.51 | ppl 45.58 | wps 14386.3 | ups 2.7 | wpb 5324.5 | bsz 212.4 | num_updates 2128 | lr 0.000106447 | gnorm 1.748 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:36:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=34
2020-10-13 06:36:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=35
2020-10-13 06:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7521.109375Mb; avail=237033.49609375Mb
2020-10-13 06:36:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000276
2020-10-13 06:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002691
2020-10-13 06:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7521.71484375Mb; avail=237032.890625Mb
2020-10-13 06:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000094
2020-10-13 06:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7521.71484375Mb; avail=237032.890625Mb
2020-10-13 06:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040743
2020-10-13 06:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044246
2020-10-13 06:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7523.53125Mb; avail=237031.07421875Mb
2020-10-13 06:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7525.34765625Mb; avail=237029.2578125Mb
2020-10-13 06:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002030
2020-10-13 06:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7525.34765625Mb; avail=237029.2578125Mb
2020-10-13 06:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000086
2020-10-13 06:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7525.34765625Mb; avail=237029.2578125Mb
2020-10-13 06:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040324
2020-10-13 06:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043154
2020-10-13 06:36:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7527.1640625Mb; avail=237027.44140625Mb
2020-10-13 06:36:14 | INFO | fairseq.trainer | begin training epoch 39
2020-10-13 06:36:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7471.89453125Mb; avail=237082.37890625Mb
2020-10-13 06:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000840
2020-10-13 06:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.89453125Mb; avail=237082.37890625Mb
2020-10-13 06:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017773
2020-10-13 06:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.10546875Mb; avail=237081.16796875Mb
2020-10-13 06:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013562
2020-10-13 06:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032954
2020-10-13 06:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.7109375Mb; avail=237080.5625Mb
2020-10-13 06:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7473.7109375Mb; avail=237080.5625Mb
2020-10-13 06:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000709
2020-10-13 06:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.7109375Mb; avail=237080.5625Mb
2020-10-13 06:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.018009
2020-10-13 06:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7477.94921875Mb; avail=237076.32421875Mb
2020-10-13 06:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013679
2020-10-13 06:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.033155
2020-10-13 06:36:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7480.37109375Mb; avail=237073.90234375Mb
2020-10-13 06:36:30 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.618 | nll_loss 6.455 | ppl 87.71 | wps 49734.6 | wpb 2112 | bsz 83.9 | num_updates 2184 | best_loss 7.618
2020-10-13 06:36:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:36:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 39 @ 2184 updates, score 7.618) (writing took 4.572610224829987 seconds)
2020-10-13 06:36:35 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-13 06:36:35 | INFO | train | epoch 039 | loss 6.615 | nll_loss 5.412 | ppl 42.56 | wps 14319 | ups 2.69 | wpb 5324.5 | bsz 212.4 | num_updates 2184 | lr 0.000109245 | gnorm 1.655 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:36:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=35
2020-10-13 06:36:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=36
2020-10-13 06:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7652.50390625Mb; avail=236902.06640625Mb
2020-10-13 06:36:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000311
2020-10-13 06:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002690
2020-10-13 06:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7653.1015625Mb; avail=236901.46875Mb
2020-10-13 06:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000092
2020-10-13 06:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7653.1015625Mb; avail=236901.46875Mb
2020-10-13 06:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040931
2020-10-13 06:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044437
2020-10-13 06:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7660.640625Mb; avail=236893.9296875Mb
2020-10-13 06:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7670.34375Mb; avail=236884.2265625Mb
2020-10-13 06:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002081
2020-10-13 06:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7670.34375Mb; avail=236884.2265625Mb
2020-10-13 06:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000115
2020-10-13 06:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7670.34375Mb; avail=236884.2265625Mb
2020-10-13 06:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040506
2020-10-13 06:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043480
2020-10-13 06:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7670.8984375Mb; avail=236883.671875Mb
2020-10-13 06:36:35 | INFO | fairseq.trainer | begin training epoch 40
2020-10-13 06:36:39 | INFO | train_inner | epoch 040:     16 / 56 loss=6.64, nll_loss=5.439, ppl=43.38, wps=13870.1, ups=2.61, wpb=5312.6, bsz=209.2, num_updates=2200, lr=0.000110045, gnorm=1.719, clip=0, train_wall=26, wall=0
2020-10-13 06:36:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:36:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7467.3828125Mb; avail=237086.99609375Mb
2020-10-13 06:36:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000828
2020-10-13 06:36:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.3828125Mb; avail=237086.99609375Mb
2020-10-13 06:36:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017386
2020-10-13 06:36:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.3828125Mb; avail=237086.99609375Mb
2020-10-13 06:36:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013297
2020-10-13 06:36:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032261
2020-10-13 06:36:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.3828125Mb; avail=237086.99609375Mb
2020-10-13 06:36:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7467.3828125Mb; avail=237086.99609375Mb
2020-10-13 06:36:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000690
2020-10-13 06:36:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.3828125Mb; avail=237086.99609375Mb
2020-10-13 06:36:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017417
2020-10-13 06:36:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.3828125Mb; avail=237086.99609375Mb
2020-10-13 06:36:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013078
2020-10-13 06:36:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031913
2020-10-13 06:36:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.3828125Mb; avail=237086.99609375Mb
2020-10-13 06:36:51 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.649 | nll_loss 6.497 | ppl 90.31 | wps 50740.5 | wpb 2112 | bsz 83.9 | num_updates 2240 | best_loss 7.618
2020-10-13 06:36:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:36:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 40 @ 2240 updates, score 7.649) (writing took 2.36641833698377 seconds)
2020-10-13 06:36:53 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-13 06:36:53 | INFO | train | epoch 040 | loss 6.547 | nll_loss 5.332 | ppl 40.29 | wps 16091.7 | ups 3.02 | wpb 5324.5 | bsz 212.4 | num_updates 2240 | lr 0.000112044 | gnorm 1.861 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:36:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=36
2020-10-13 06:36:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=37
2020-10-13 06:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7465.8671875Mb; avail=237088.75390625Mb
2020-10-13 06:36:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000275
2020-10-13 06:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002664
2020-10-13 06:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.8671875Mb; avail=237088.75390625Mb
2020-10-13 06:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000089
2020-10-13 06:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.8671875Mb; avail=237088.75390625Mb
2020-10-13 06:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040990
2020-10-13 06:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044499
2020-10-13 06:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.8671875Mb; avail=237088.75390625Mb
2020-10-13 06:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7465.8671875Mb; avail=237088.75390625Mb
2020-10-13 06:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001978
2020-10-13 06:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.8671875Mb; avail=237088.75390625Mb
2020-10-13 06:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 06:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.8671875Mb; avail=237088.75390625Mb
2020-10-13 06:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040688
2020-10-13 06:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043457
2020-10-13 06:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.8671875Mb; avail=237088.75390625Mb
2020-10-13 06:36:53 | INFO | fairseq.trainer | begin training epoch 41
2020-10-13 06:37:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:37:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7459.53515625Mb; avail=237095.203125Mb
2020-10-13 06:37:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000839
2020-10-13 06:37:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.53515625Mb; avail=237095.203125Mb
2020-10-13 06:37:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017942
2020-10-13 06:37:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.53515625Mb; avail=237095.203125Mb
2020-10-13 06:37:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013277
2020-10-13 06:37:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032823
2020-10-13 06:37:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.53515625Mb; avail=237095.203125Mb
2020-10-13 06:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7459.53515625Mb; avail=237095.203125Mb
2020-10-13 06:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000691
2020-10-13 06:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.53515625Mb; avail=237095.203125Mb
2020-10-13 06:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017072
2020-10-13 06:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.53515625Mb; avail=237095.203125Mb
2020-10-13 06:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013255
2020-10-13 06:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031741
2020-10-13 06:37:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.33203125Mb; avail=237094.40625Mb
2020-10-13 06:37:09 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.637 | nll_loss 6.467 | ppl 88.45 | wps 48476.4 | wpb 2112 | bsz 83.9 | num_updates 2296 | best_loss 7.618
2020-10-13 06:37:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:37:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 41 @ 2296 updates, score 7.637) (writing took 2.376992546953261 seconds)
2020-10-13 06:37:12 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2020-10-13 06:37:12 | INFO | train | epoch 041 | loss 6.479 | nll_loss 5.253 | ppl 38.13 | wps 16000.4 | ups 3.01 | wpb 5324.5 | bsz 212.4 | num_updates 2296 | lr 0.000114843 | gnorm 1.829 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:37:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=42/shard_epoch=37
2020-10-13 06:37:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=42/shard_epoch=38
2020-10-13 06:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7459.875Mb; avail=237094.75Mb
2020-10-13 06:37:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000271
2020-10-13 06:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002681
2020-10-13 06:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.875Mb; avail=237094.75Mb
2020-10-13 06:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000092
2020-10-13 06:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.875Mb; avail=237094.75Mb
2020-10-13 06:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.042105
2020-10-13 06:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.045646
2020-10-13 06:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.875Mb; avail=237094.75Mb
2020-10-13 06:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7459.875Mb; avail=237094.75Mb
2020-10-13 06:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002027
2020-10-13 06:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.875Mb; avail=237094.75Mb
2020-10-13 06:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000083
2020-10-13 06:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.875Mb; avail=237094.75Mb
2020-10-13 06:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040235
2020-10-13 06:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043066
2020-10-13 06:37:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.875Mb; avail=237094.75Mb
2020-10-13 06:37:12 | INFO | fairseq.trainer | begin training epoch 42
2020-10-13 06:37:13 | INFO | train_inner | epoch 042:      4 / 56 loss=6.512, nll_loss=5.291, ppl=39.16, wps=15755, ups=2.94, wpb=5352, bsz=213.7, num_updates=2300, lr=0.000115043, gnorm=1.85, clip=0, train_wall=26, wall=0
2020-10-13 06:37:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7461.796875Mb; avail=237092.58984375Mb
2020-10-13 06:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000819
2020-10-13 06:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.796875Mb; avail=237092.58984375Mb
2020-10-13 06:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017446
2020-10-13 06:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.796875Mb; avail=237092.58984375Mb
2020-10-13 06:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013224
2020-10-13 06:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032290
2020-10-13 06:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.796875Mb; avail=237092.58984375Mb
2020-10-13 06:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7461.796875Mb; avail=237092.58984375Mb
2020-10-13 06:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000697
2020-10-13 06:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.796875Mb; avail=237092.58984375Mb
2020-10-13 06:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017295
2020-10-13 06:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.5078125Mb; avail=237092.87890625Mb
2020-10-13 06:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013053
2020-10-13 06:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031789
2020-10-13 06:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.5078125Mb; avail=237092.87890625Mb
2020-10-13 06:37:28 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.633 | nll_loss 6.455 | ppl 87.74 | wps 51071 | wpb 2112 | bsz 83.9 | num_updates 2352 | best_loss 7.618
2020-10-13 06:37:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:37:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 42 @ 2352 updates, score 7.633) (writing took 2.365646791877225 seconds)
2020-10-13 06:37:30 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2020-10-13 06:37:30 | INFO | train | epoch 042 | loss 6.385 | nll_loss 5.146 | ppl 35.41 | wps 16118.4 | ups 3.03 | wpb 5324.5 | bsz 212.4 | num_updates 2352 | lr 0.000117641 | gnorm 1.805 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:37:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=43/shard_epoch=38
2020-10-13 06:37:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=43/shard_epoch=39
2020-10-13 06:37:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7483.58203125Mb; avail=237071.0390625Mb
2020-10-13 06:37:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000305
2020-10-13 06:37:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002706
2020-10-13 06:37:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7483.58203125Mb; avail=237071.0390625Mb
2020-10-13 06:37:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000094
2020-10-13 06:37:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7483.58203125Mb; avail=237071.0390625Mb
2020-10-13 06:37:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041019
2020-10-13 06:37:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044551
2020-10-13 06:37:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.40234375Mb; avail=237091.21875Mb
2020-10-13 06:37:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.40234375Mb; avail=237091.21875Mb
2020-10-13 06:37:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001976
2020-10-13 06:37:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.40234375Mb; avail=237091.21875Mb
2020-10-13 06:37:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000086
2020-10-13 06:37:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.40234375Mb; avail=237091.21875Mb
2020-10-13 06:37:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040014
2020-10-13 06:37:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.042792
2020-10-13 06:37:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.40234375Mb; avail=237091.21875Mb
2020-10-13 06:37:30 | INFO | fairseq.trainer | begin training epoch 43
2020-10-13 06:37:43 | INFO | train_inner | epoch 043:     48 / 56 loss=6.339, nll_loss=5.094, ppl=34.15, wps=17553.6, ups=3.29, wpb=5332, bsz=209.8, num_updates=2400, lr=0.00012004, gnorm=1.833, clip=0, train_wall=26, wall=0
2020-10-13 06:37:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7466.53125Mb; avail=237088.0859375Mb
2020-10-13 06:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000878
2020-10-13 06:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.53125Mb; avail=237088.0859375Mb
2020-10-13 06:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017912
2020-10-13 06:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.53125Mb; avail=237088.0859375Mb
2020-10-13 06:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013274
2020-10-13 06:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032834
2020-10-13 06:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.53125Mb; avail=237088.0859375Mb
2020-10-13 06:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7466.53125Mb; avail=237088.0859375Mb
2020-10-13 06:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000699
2020-10-13 06:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.53125Mb; avail=237088.0859375Mb
2020-10-13 06:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017667
2020-10-13 06:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.10546875Mb; avail=237088.51171875Mb
2020-10-13 06:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013246
2020-10-13 06:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032362
2020-10-13 06:37:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.10546875Mb; avail=237088.51171875Mb
2020-10-13 06:37:46 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.611 | nll_loss 6.425 | ppl 85.9 | wps 50961.6 | wpb 2112 | bsz 83.9 | num_updates 2408 | best_loss 7.611
2020-10-13 06:37:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:37:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_best.pt (epoch 43 @ 2408 updates, score 7.611) (writing took 4.558733506826684 seconds)
2020-10-13 06:37:51 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2020-10-13 06:37:51 | INFO | train | epoch 043 | loss 6.306 | nll_loss 5.056 | ppl 33.27 | wps 14380.1 | ups 2.7 | wpb 5324.5 | bsz 212.4 | num_updates 2408 | lr 0.00012044 | gnorm 1.827 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:37:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=44/shard_epoch=39
2020-10-13 06:37:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=44/shard_epoch=40
2020-10-13 06:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7465.84765625Mb; avail=237088.76953125Mb
2020-10-13 06:37:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000285
2020-10-13 06:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002799
2020-10-13 06:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.84765625Mb; avail=237088.76953125Mb
2020-10-13 06:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000093
2020-10-13 06:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.84765625Mb; avail=237088.76953125Mb
2020-10-13 06:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040651
2020-10-13 06:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044261
2020-10-13 06:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.84765625Mb; avail=237088.76953125Mb
2020-10-13 06:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7465.84765625Mb; avail=237088.76953125Mb
2020-10-13 06:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001965
2020-10-13 06:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.84765625Mb; avail=237088.76953125Mb
2020-10-13 06:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000082
2020-10-13 06:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.84765625Mb; avail=237088.76953125Mb
2020-10-13 06:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040435
2020-10-13 06:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043199
2020-10-13 06:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.84765625Mb; avail=237088.76953125Mb
2020-10-13 06:37:51 | INFO | fairseq.trainer | begin training epoch 44
2020-10-13 06:38:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:38:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7479.70703125Mb; avail=237075.34765625Mb
2020-10-13 06:38:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000831
2020-10-13 06:38:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7480.3125Mb; avail=237074.7421875Mb
2020-10-13 06:38:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017640
2020-10-13 06:38:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7484.55078125Mb; avail=237070.50390625Mb
2020-10-13 06:38:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013322
2020-10-13 06:38:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032576
2020-10-13 06:38:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7484.078125Mb; avail=237070.9765625Mb
2020-10-13 06:38:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7484.578125Mb; avail=237070.4765625Mb
2020-10-13 06:38:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000700
2020-10-13 06:38:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7484.578125Mb; avail=237070.4765625Mb
2020-10-13 06:38:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017357
2020-10-13 06:38:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.25Mb; avail=237080.8046875Mb
2020-10-13 06:38:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013160
2020-10-13 06:38:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031959
2020-10-13 06:38:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.03125Mb; avail=237081.0234375Mb
2020-10-13 06:38:07 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.675 | nll_loss 6.501 | ppl 90.57 | wps 51197.8 | wpb 2112 | bsz 83.9 | num_updates 2464 | best_loss 7.611
2020-10-13 06:38:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:38:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 44 @ 2464 updates, score 7.675) (writing took 2.3646305170841515 seconds)
2020-10-13 06:38:09 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2020-10-13 06:38:09 | INFO | train | epoch 044 | loss 6.22 | nll_loss 4.956 | ppl 31.05 | wps 16157.4 | ups 3.03 | wpb 5324.5 | bsz 212.4 | num_updates 2464 | lr 0.000123238 | gnorm 1.752 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:38:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=45/shard_epoch=40
2020-10-13 06:38:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=45/shard_epoch=41
2020-10-13 06:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7475.37109375Mb; avail=237079.2578125Mb
2020-10-13 06:38:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000274
2020-10-13 06:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002654
2020-10-13 06:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.5859375Mb; avail=237079.04296875Mb
2020-10-13 06:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000091
2020-10-13 06:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.5859375Mb; avail=237079.04296875Mb
2020-10-13 06:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041028
2020-10-13 06:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044510
2020-10-13 06:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7478.546875Mb; avail=237076.17578125Mb
2020-10-13 06:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7483.390625Mb; avail=237071.33203125Mb
2020-10-13 06:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001949
2020-10-13 06:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7483.515625Mb; avail=237071.20703125Mb
2020-10-13 06:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000106
2020-10-13 06:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7483.515625Mb; avail=237071.20703125Mb
2020-10-13 06:38:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040267
2020-10-13 06:38:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043048
2020-10-13 06:38:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7484.109375Mb; avail=237070.61328125Mb
2020-10-13 06:38:10 | INFO | fairseq.trainer | begin training epoch 45
2020-10-13 06:38:19 | INFO | train_inner | epoch 045:     36 / 56 loss=6.193, nll_loss=4.926, ppl=30.4, wps=14813.4, ups=2.78, wpb=5332, bsz=215.6, num_updates=2500, lr=0.000125037, gnorm=1.75, clip=0, train_wall=26, wall=0
2020-10-13 06:38:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:38:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7473.40625Mb; avail=237080.98046875Mb
2020-10-13 06:38:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000833
2020-10-13 06:38:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.40625Mb; avail=237080.98046875Mb
2020-10-13 06:38:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017529
2020-10-13 06:38:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.40625Mb; avail=237080.98046875Mb
2020-10-13 06:38:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013383
2020-10-13 06:38:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032490
2020-10-13 06:38:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.40625Mb; avail=237080.98046875Mb
2020-10-13 06:38:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7473.40625Mb; avail=237080.98046875Mb
2020-10-13 06:38:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000690
2020-10-13 06:38:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.40625Mb; avail=237080.98046875Mb
2020-10-13 06:38:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017707
2020-10-13 06:38:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.40625Mb; avail=237080.98046875Mb
2020-10-13 06:38:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013341
2020-10-13 06:38:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032467
2020-10-13 06:38:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.40625Mb; avail=237080.98046875Mb
2020-10-13 06:38:26 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.655 | nll_loss 6.483 | ppl 89.44 | wps 51233.4 | wpb 2112 | bsz 83.9 | num_updates 2520 | best_loss 7.611
2020-10-13 06:38:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:38:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 45 @ 2520 updates, score 7.655) (writing took 2.359993290854618 seconds)
2020-10-13 06:38:28 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2020-10-13 06:38:28 | INFO | train | epoch 045 | loss 6.14 | nll_loss 4.865 | ppl 29.14 | wps 16000.7 | ups 3.01 | wpb 5324.5 | bsz 212.4 | num_updates 2520 | lr 0.000126037 | gnorm 1.748 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:38:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=46/shard_epoch=41
2020-10-13 06:38:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=46/shard_epoch=42
2020-10-13 06:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7473.53515625Mb; avail=237081.078125Mb
2020-10-13 06:38:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000289
2020-10-13 06:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002629
2020-10-13 06:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.53515625Mb; avail=237081.078125Mb
2020-10-13 06:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000091
2020-10-13 06:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.53515625Mb; avail=237081.078125Mb
2020-10-13 06:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.042716
2020-10-13 06:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.046169
2020-10-13 06:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.53515625Mb; avail=237081.078125Mb
2020-10-13 06:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7473.53515625Mb; avail=237081.078125Mb
2020-10-13 06:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001983
2020-10-13 06:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.53515625Mb; avail=237081.078125Mb
2020-10-13 06:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000082
2020-10-13 06:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.53515625Mb; avail=237081.078125Mb
2020-10-13 06:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040310
2020-10-13 06:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043083
2020-10-13 06:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.53515625Mb; avail=237081.078125Mb
2020-10-13 06:38:28 | INFO | fairseq.trainer | begin training epoch 46
2020-10-13 06:38:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7477.96484375Mb; avail=237076.36328125Mb
2020-10-13 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000829
2020-10-13 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7478.5703125Mb; avail=237075.7578125Mb
2020-10-13 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017466
2020-10-13 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7479.17578125Mb; avail=237075.15234375Mb
2020-10-13 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013350
2020-10-13 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032398
2020-10-13 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7479.37109375Mb; avail=237075.26171875Mb
2020-10-13 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7483.43359375Mb; avail=237071.19921875Mb
2020-10-13 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000703
2020-10-13 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7483.43359375Mb; avail=237071.19921875Mb
2020-10-13 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017167
2020-10-13 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7483.9296875Mb; avail=237070.703125Mb
2020-10-13 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013123
2020-10-13 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031736
2020-10-13 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7483.99609375Mb; avail=237070.63671875Mb
2020-10-13 06:38:44 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.659 | nll_loss 6.483 | ppl 89.42 | wps 51189.8 | wpb 2112 | bsz 83.9 | num_updates 2576 | best_loss 7.611
2020-10-13 06:38:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:38:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 46 @ 2576 updates, score 7.659) (writing took 2.359802003018558 seconds)
2020-10-13 06:38:47 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2020-10-13 06:38:47 | INFO | train | epoch 046 | loss 6.061 | nll_loss 4.773 | ppl 27.34 | wps 16055.2 | ups 3.02 | wpb 5324.5 | bsz 212.4 | num_updates 2576 | lr 0.000128836 | gnorm 1.859 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:38:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=47/shard_epoch=42
2020-10-13 06:38:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=47/shard_epoch=43
2020-10-13 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7483.93359375Mb; avail=237070.6953125Mb
2020-10-13 06:38:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000324
2020-10-13 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002708
2020-10-13 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.08984375Mb; avail=237080.5390625Mb
2020-10-13 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000093
2020-10-13 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.08984375Mb; avail=237080.5390625Mb
2020-10-13 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041706
2020-10-13 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.045248
2020-10-13 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.08984375Mb; avail=237080.5390625Mb
2020-10-13 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7474.08984375Mb; avail=237080.5390625Mb
2020-10-13 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001941
2020-10-13 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.08984375Mb; avail=237080.5390625Mb
2020-10-13 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.08984375Mb; avail=237080.5390625Mb
2020-10-13 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041598
2020-10-13 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044360
2020-10-13 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.08984375Mb; avail=237080.5390625Mb
2020-10-13 06:38:47 | INFO | fairseq.trainer | begin training epoch 47
2020-10-13 06:38:53 | INFO | train_inner | epoch 047:     24 / 56 loss=6.078, nll_loss=4.793, ppl=27.72, wps=15576.1, ups=2.95, wpb=5288.2, bsz=207.3, num_updates=2600, lr=0.000130035, gnorm=1.82, clip=0, train_wall=26, wall=0
2020-10-13 06:39:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7471.9609375Mb; avail=237082.33203125Mb
2020-10-13 06:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000864
2020-10-13 06:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.9609375Mb; avail=237082.33203125Mb
2020-10-13 06:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017134
2020-10-13 06:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.26953125Mb; avail=237080.0234375Mb
2020-10-13 06:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013253
2020-10-13 06:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032061
2020-10-13 06:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7476.0859375Mb; avail=237078.20703125Mb
2020-10-13 06:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7477.296875Mb; avail=237076.99609375Mb
2020-10-13 06:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000778
2020-10-13 06:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7477.296875Mb; avail=237076.99609375Mb
2020-10-13 06:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016874
2020-10-13 06:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7481.53515625Mb; avail=237072.7578125Mb
2020-10-13 06:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013420
2020-10-13 06:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031871
2020-10-13 06:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7481.1953125Mb; avail=237073.09765625Mb
2020-10-13 06:39:03 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.675 | nll_loss 6.463 | ppl 88.22 | wps 50847.2 | wpb 2112 | bsz 83.9 | num_updates 2632 | best_loss 7.611
2020-10-13 06:39:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:39:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 47 @ 2632 updates, score 7.675) (writing took 2.3671347571071237 seconds)
2020-10-13 06:39:05 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2020-10-13 06:39:05 | INFO | train | epoch 047 | loss 5.977 | nll_loss 4.676 | ppl 25.57 | wps 16044.9 | ups 3.01 | wpb 5324.5 | bsz 212.4 | num_updates 2632 | lr 0.000131634 | gnorm 1.772 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:39:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=48/shard_epoch=43
2020-10-13 06:39:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=48/shard_epoch=44
2020-10-13 06:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7472.421875Mb; avail=237082.20703125Mb
2020-10-13 06:39:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000330
2020-10-13 06:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002752
2020-10-13 06:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7472.421875Mb; avail=237082.20703125Mb
2020-10-13 06:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000091
2020-10-13 06:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7472.421875Mb; avail=237082.20703125Mb
2020-10-13 06:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040722
2020-10-13 06:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044330
2020-10-13 06:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7472.421875Mb; avail=237082.20703125Mb
2020-10-13 06:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7472.421875Mb; avail=237082.20703125Mb
2020-10-13 06:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001951
2020-10-13 06:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7472.421875Mb; avail=237082.20703125Mb
2020-10-13 06:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000082
2020-10-13 06:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7472.421875Mb; avail=237082.20703125Mb
2020-10-13 06:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040338
2020-10-13 06:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043088
2020-10-13 06:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7472.40625Mb; avail=237082.22265625Mb
2020-10-13 06:39:05 | INFO | fairseq.trainer | begin training epoch 48
2020-10-13 06:39:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7474.0390625Mb; avail=237080.58984375Mb
2020-10-13 06:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000860
2020-10-13 06:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.0390625Mb; avail=237080.58984375Mb
2020-10-13 06:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.020318
2020-10-13 06:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.0390625Mb; avail=237080.58984375Mb
2020-10-13 06:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013517
2020-10-13 06:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.035492
2020-10-13 06:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.0390625Mb; avail=237080.58984375Mb
2020-10-13 06:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7474.0390625Mb; avail=237080.58984375Mb
2020-10-13 06:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000692
2020-10-13 06:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.0390625Mb; avail=237080.58984375Mb
2020-10-13 06:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017229
2020-10-13 06:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.03125Mb; avail=237080.59765625Mb
2020-10-13 06:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013463
2020-10-13 06:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032132
2020-10-13 06:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.03125Mb; avail=237080.59765625Mb
2020-10-13 06:39:21 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.677 | nll_loss 6.493 | ppl 90.1 | wps 48996.4 | wpb 2112 | bsz 83.9 | num_updates 2688 | best_loss 7.611
2020-10-13 06:39:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:39:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 48 @ 2688 updates, score 7.677) (writing took 2.3814066408667713 seconds)
2020-10-13 06:39:24 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2020-10-13 06:39:24 | INFO | train | epoch 048 | loss 5.918 | nll_loss 4.607 | ppl 24.36 | wps 16011.9 | ups 3.01 | wpb 5324.5 | bsz 212.4 | num_updates 2688 | lr 0.000134433 | gnorm 2.026 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:39:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=49/shard_epoch=44
2020-10-13 06:39:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=49/shard_epoch=45
2020-10-13 06:39:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7465.15625Mb; avail=237089.46484375Mb
2020-10-13 06:39:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000314
2020-10-13 06:39:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002680
2020-10-13 06:39:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.15625Mb; avail=237089.46484375Mb
2020-10-13 06:39:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000093
2020-10-13 06:39:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.15625Mb; avail=237089.46484375Mb
2020-10-13 06:39:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040469
2020-10-13 06:39:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043980
2020-10-13 06:39:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.60546875Mb; avail=237083.015625Mb
2020-10-13 06:39:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7472.1015625Mb; avail=237082.51953125Mb
2020-10-13 06:39:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002007
2020-10-13 06:39:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7472.1015625Mb; avail=237082.51953125Mb
2020-10-13 06:39:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000086
2020-10-13 06:39:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7472.1015625Mb; avail=237082.51953125Mb
2020-10-13 06:39:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040389
2020-10-13 06:39:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043228
2020-10-13 06:39:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.2578125Mb; avail=237092.36328125Mb
2020-10-13 06:39:24 | INFO | fairseq.trainer | begin training epoch 49
2020-10-13 06:39:27 | INFO | train_inner | epoch 049:     12 / 56 loss=5.915, nll_loss=4.604, ppl=24.32, wps=15708.3, ups=2.95, wpb=5329.1, bsz=213.6, num_updates=2700, lr=0.000135032, gnorm=1.928, clip=0, train_wall=26, wall=0
2020-10-13 06:39:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7460.9765625Mb; avail=237093.34375Mb
2020-10-13 06:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000818
2020-10-13 06:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.9765625Mb; avail=237093.34375Mb
2020-10-13 06:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017345
2020-10-13 06:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.9765625Mb; avail=237093.34375Mb
2020-10-13 06:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013063
2020-10-13 06:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031980
2020-10-13 06:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.9765625Mb; avail=237093.34375Mb
2020-10-13 06:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7460.9765625Mb; avail=237093.34375Mb
2020-10-13 06:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000696
2020-10-13 06:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.9765625Mb; avail=237093.34375Mb
2020-10-13 06:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017359
2020-10-13 06:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.9765625Mb; avail=237093.34375Mb
2020-10-13 06:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012997
2020-10-13 06:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031795
2020-10-13 06:39:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.9765625Mb; avail=237093.34375Mb
2020-10-13 06:39:40 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.685 | nll_loss 6.487 | ppl 89.71 | wps 49144.5 | wpb 2112 | bsz 83.9 | num_updates 2744 | best_loss 7.611
2020-10-13 06:39:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:39:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 49 @ 2744 updates, score 7.685) (writing took 2.3563162109348923 seconds)
2020-10-13 06:39:42 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2020-10-13 06:39:42 | INFO | train | epoch 049 | loss 5.821 | nll_loss 4.494 | ppl 22.54 | wps 16100.8 | ups 3.02 | wpb 5324.5 | bsz 212.4 | num_updates 2744 | lr 0.000137231 | gnorm 1.956 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:39:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=50/shard_epoch=45
2020-10-13 06:39:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=50/shard_epoch=46
2020-10-13 06:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.69921875Mb; avail=237090.91796875Mb
2020-10-13 06:39:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000323
2020-10-13 06:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002785
2020-10-13 06:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.69921875Mb; avail=237090.91796875Mb
2020-10-13 06:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000092
2020-10-13 06:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.69921875Mb; avail=237090.91796875Mb
2020-10-13 06:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040577
2020-10-13 06:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044174
2020-10-13 06:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.81640625Mb; avail=237086.80078125Mb
2020-10-13 06:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7471.359375Mb; avail=237083.2578125Mb
2020-10-13 06:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002038
2020-10-13 06:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.359375Mb; avail=237083.2578125Mb
2020-10-13 06:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000084
2020-10-13 06:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.359375Mb; avail=237083.2578125Mb
2020-10-13 06:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040291
2020-10-13 06:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043125
2020-10-13 06:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.90234375Mb; avail=237094.8203125Mb
2020-10-13 06:39:42 | INFO | fairseq.trainer | begin training epoch 50
2020-10-13 06:39:58 | INFO | train_inner | epoch 050:     56 / 56 loss=5.761, nll_loss=4.426, ppl=21.5, wps=17521.9, ups=3.29, wpb=5332.4, bsz=213.3, num_updates=2800, lr=0.00014003, gnorm=1.901, clip=0, train_wall=26, wall=0
2020-10-13 06:39:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7459.859375Mb; avail=237094.5078125Mb
2020-10-13 06:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000803
2020-10-13 06:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.859375Mb; avail=237094.5078125Mb
2020-10-13 06:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017472
2020-10-13 06:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.859375Mb; avail=237094.5078125Mb
2020-10-13 06:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013165
2020-10-13 06:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032193
2020-10-13 06:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.859375Mb; avail=237094.5078125Mb
2020-10-13 06:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7459.859375Mb; avail=237094.5078125Mb
2020-10-13 06:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000693
2020-10-13 06:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.859375Mb; avail=237094.5078125Mb
2020-10-13 06:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017155
2020-10-13 06:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.859375Mb; avail=237094.5078125Mb
2020-10-13 06:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012942
2020-10-13 06:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031513
2020-10-13 06:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.859375Mb; avail=237094.5078125Mb
2020-10-13 06:39:59 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.725 | nll_loss 6.529 | ppl 92.34 | wps 50457 | wpb 2112 | bsz 83.9 | num_updates 2800 | best_loss 7.611
2020-10-13 06:39:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:40:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 50 @ 2800 updates, score 7.725) (writing took 2.370565630029887 seconds)
2020-10-13 06:40:01 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2020-10-13 06:40:01 | INFO | train | epoch 050 | loss 5.72 | nll_loss 4.379 | ppl 20.8 | wps 16031.7 | ups 3.01 | wpb 5324.5 | bsz 212.4 | num_updates 2800 | lr 0.00014003 | gnorm 1.854 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:40:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=51/shard_epoch=46
2020-10-13 06:40:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=51/shard_epoch=47
2020-10-13 06:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7459.6328125Mb; avail=237094.97265625Mb
2020-10-13 06:40:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000277
2020-10-13 06:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002652
2020-10-13 06:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.6328125Mb; avail=237094.97265625Mb
2020-10-13 06:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000091
2020-10-13 06:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.6328125Mb; avail=237094.97265625Mb
2020-10-13 06:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040665
2020-10-13 06:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044156
2020-10-13 06:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.87109375Mb; avail=237090.734375Mb
2020-10-13 06:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7465.6875Mb; avail=237088.91796875Mb
2020-10-13 06:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002011
2020-10-13 06:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.6875Mb; avail=237088.91796875Mb
2020-10-13 06:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000118
2020-10-13 06:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.6875Mb; avail=237088.91796875Mb
2020-10-13 06:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040816
2020-10-13 06:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043682
2020-10-13 06:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7469.81640625Mb; avail=237084.7890625Mb
2020-10-13 06:40:01 | INFO | fairseq.trainer | begin training epoch 51
2020-10-13 06:40:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7460.796875Mb; avail=237093.4921875Mb
2020-10-13 06:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000818
2020-10-13 06:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.796875Mb; avail=237093.4921875Mb
2020-10-13 06:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017192
2020-10-13 06:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.796875Mb; avail=237093.4921875Mb
2020-10-13 06:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013426
2020-10-13 06:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032194
2020-10-13 06:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.796875Mb; avail=237093.4921875Mb
2020-10-13 06:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7460.796875Mb; avail=237093.4921875Mb
2020-10-13 06:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000708
2020-10-13 06:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.796875Mb; avail=237093.4921875Mb
2020-10-13 06:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017462
2020-10-13 06:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.796875Mb; avail=237093.4921875Mb
2020-10-13 06:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013354
2020-10-13 06:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032280
2020-10-13 06:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.796875Mb; avail=237093.4921875Mb
2020-10-13 06:40:17 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.731 | nll_loss 6.543 | ppl 93.25 | wps 51237.1 | wpb 2112 | bsz 83.9 | num_updates 2856 | best_loss 7.611
2020-10-13 06:40:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:40:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 51 @ 2856 updates, score 7.731) (writing took 2.354091804008931 seconds)
2020-10-13 06:40:19 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2020-10-13 06:40:19 | INFO | train | epoch 051 | loss 5.662 | nll_loss 4.31 | ppl 19.84 | wps 16141.3 | ups 3.03 | wpb 5324.5 | bsz 212.4 | num_updates 2856 | lr 0.000142829 | gnorm 2.01 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:40:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=52/shard_epoch=47
2020-10-13 06:40:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=52/shard_epoch=48
2020-10-13 06:40:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7468.96484375Mb; avail=237085.65625Mb
2020-10-13 06:40:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000269
2020-10-13 06:40:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002635
2020-10-13 06:40:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.96484375Mb; avail=237085.65625Mb
2020-10-13 06:40:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000111
2020-10-13 06:40:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.96484375Mb; avail=237085.65625Mb
2020-10-13 06:40:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041583
2020-10-13 06:40:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.045074
2020-10-13 06:40:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.68359375Mb; avail=237095.03125Mb
2020-10-13 06:40:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7459.68359375Mb; avail=237095.03125Mb
2020-10-13 06:40:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002008
2020-10-13 06:40:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.68359375Mb; avail=237095.03125Mb
2020-10-13 06:40:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000086
2020-10-13 06:40:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.68359375Mb; avail=237095.03125Mb
2020-10-13 06:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040311
2020-10-13 06:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043148
2020-10-13 06:40:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7459.68359375Mb; avail=237095.03125Mb
2020-10-13 06:40:20 | INFO | fairseq.trainer | begin training epoch 52
2020-10-13 06:40:31 | INFO | train_inner | epoch 052:     44 / 56 loss=5.63, nll_loss=4.273, ppl=19.34, wps=16030.5, ups=2.96, wpb=5409.1, bsz=205.8, num_updates=2900, lr=0.000145028, gnorm=1.933, clip=0, train_wall=26, wall=0
2020-10-13 06:40:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7460.7421875Mb; avail=237093.8828125Mb
2020-10-13 06:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000825
2020-10-13 06:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.7421875Mb; avail=237093.8828125Mb
2020-10-13 06:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.018094
2020-10-13 06:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.7421875Mb; avail=237093.8828125Mb
2020-10-13 06:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013227
2020-10-13 06:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032919
2020-10-13 06:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7460.7421875Mb; avail=237093.8828125Mb
2020-10-13 06:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7461.359375Mb; avail=237093.265625Mb
2020-10-13 06:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000719
2020-10-13 06:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.359375Mb; avail=237093.265625Mb
2020-10-13 06:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017823
2020-10-13 06:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.17578125Mb; avail=237091.44921875Mb
2020-10-13 06:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013005
2020-10-13 06:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032267
2020-10-13 06:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.38671875Mb; avail=237090.23828125Mb
2020-10-13 06:40:35 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.75 | nll_loss 6.522 | ppl 91.89 | wps 50671 | wpb 2112 | bsz 83.9 | num_updates 2912 | best_loss 7.611
2020-10-13 06:40:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:40:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 52 @ 2912 updates, score 7.75) (writing took 2.371558122104034 seconds)
2020-10-13 06:40:38 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2020-10-13 06:40:38 | INFO | train | epoch 052 | loss 5.564 | nll_loss 4.197 | ppl 18.35 | wps 16179.9 | ups 3.04 | wpb 5324.5 | bsz 212.4 | num_updates 2912 | lr 0.000145627 | gnorm 1.936 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:40:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=53/shard_epoch=48
2020-10-13 06:40:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=53/shard_epoch=49
2020-10-13 06:40:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7460.8515625Mb; avail=237093.17578125Mb
2020-10-13 06:40:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000284
2020-10-13 06:40:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002699
2020-10-13 06:40:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.44140625Mb; avail=237093.17578125Mb
2020-10-13 06:40:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000095
2020-10-13 06:40:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.44140625Mb; avail=237093.17578125Mb
2020-10-13 06:40:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040489
2020-10-13 06:40:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044029
2020-10-13 06:40:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.6796875Mb; avail=237088.9375Mb
2020-10-13 06:40:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7470.40234375Mb; avail=237084.21484375Mb
2020-10-13 06:40:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002015
2020-10-13 06:40:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.40234375Mb; avail=237084.21484375Mb
2020-10-13 06:40:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000091
2020-10-13 06:40:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.40234375Mb; avail=237084.21484375Mb
2020-10-13 06:40:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040474
2020-10-13 06:40:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043318
2020-10-13 06:40:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.89453125Mb; avail=237083.72265625Mb
2020-10-13 06:40:38 | INFO | fairseq.trainer | begin training epoch 53
2020-10-13 06:40:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.3359375Mb; avail=237091.68359375Mb
2020-10-13 06:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000819
2020-10-13 06:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.3359375Mb; avail=237091.68359375Mb
2020-10-13 06:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017626
2020-10-13 06:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.3359375Mb; avail=237091.68359375Mb
2020-10-13 06:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013077
2020-10-13 06:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032295
2020-10-13 06:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.3359375Mb; avail=237091.68359375Mb
2020-10-13 06:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.3359375Mb; avail=237091.68359375Mb
2020-10-13 06:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000697
2020-10-13 06:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.3359375Mb; avail=237091.68359375Mb
2020-10-13 06:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017212
2020-10-13 06:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.328125Mb; avail=237091.69140625Mb
2020-10-13 06:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012887
2020-10-13 06:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031561
2020-10-13 06:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.328125Mb; avail=237091.69140625Mb
2020-10-13 06:40:54 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.732 | nll_loss 6.522 | ppl 91.89 | wps 50923.7 | wpb 2112 | bsz 83.9 | num_updates 2968 | best_loss 7.611
2020-10-13 06:40:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:40:56 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 53 @ 2968 updates, score 7.732) (writing took 2.36134004406631 seconds)
2020-10-13 06:40:56 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2020-10-13 06:40:56 | INFO | train | epoch 053 | loss 5.488 | nll_loss 4.109 | ppl 17.26 | wps 16220.9 | ups 3.05 | wpb 5324.5 | bsz 212.4 | num_updates 2968 | lr 0.000148426 | gnorm 1.902 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:40:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=54/shard_epoch=49
2020-10-13 06:40:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=54/shard_epoch=50
2020-10-13 06:40:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7462.71875Mb; avail=237091.9140625Mb
2020-10-13 06:40:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000301
2020-10-13 06:40:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002669
2020-10-13 06:40:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.71875Mb; avail=237091.9140625Mb
2020-10-13 06:40:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000092
2020-10-13 06:40:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.71875Mb; avail=237091.9140625Mb
2020-10-13 06:40:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040720
2020-10-13 06:40:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044207
2020-10-13 06:40:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.140625Mb; avail=237089.4921875Mb
2020-10-13 06:40:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7467.68359375Mb; avail=237086.94921875Mb
2020-10-13 06:40:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001964
2020-10-13 06:40:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.68359375Mb; avail=237086.94921875Mb
2020-10-13 06:40:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000083
2020-10-13 06:40:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.68359375Mb; avail=237086.94921875Mb
2020-10-13 06:40:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040393
2020-10-13 06:40:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043178
2020-10-13 06:40:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7472.31640625Mb; avail=237082.31640625Mb
2020-10-13 06:40:56 | INFO | fairseq.trainer | begin training epoch 54
2020-10-13 06:41:05 | INFO | train_inner | epoch 054:     32 / 56 loss=5.445, nll_loss=4.06, ppl=16.67, wps=15559.6, ups=2.98, wpb=5222.9, bsz=217.2, num_updates=3000, lr=0.000150025, gnorm=1.929, clip=0, train_wall=25, wall=0
2020-10-13 06:41:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:41:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.25Mb; avail=237091.15234375Mb
2020-10-13 06:41:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000862
2020-10-13 06:41:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.25Mb; avail=237091.15234375Mb
2020-10-13 06:41:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017583
2020-10-13 06:41:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.25Mb; avail=237091.15234375Mb
2020-10-13 06:41:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013151
2020-10-13 06:41:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032359
2020-10-13 06:41:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.06640625Mb; avail=237089.3359375Mb
2020-10-13 06:41:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7466.8828125Mb; avail=237087.51953125Mb
2020-10-13 06:41:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000726
2020-10-13 06:41:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.8828125Mb; avail=237087.51953125Mb
2020-10-13 06:41:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017052
2020-10-13 06:41:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.69921875Mb; avail=237085.703125Mb
2020-10-13 06:41:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013134
2020-10-13 06:41:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031638
2020-10-13 06:41:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7469.3046875Mb; avail=237085.09765625Mb
2020-10-13 06:41:12 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 7.773 | nll_loss 6.562 | ppl 94.48 | wps 50740 | wpb 2112 | bsz 83.9 | num_updates 3024 | best_loss 7.611
2020-10-13 06:41:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:41:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 54 @ 3024 updates, score 7.773) (writing took 2.3767620951402932 seconds)
2020-10-13 06:41:15 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2020-10-13 06:41:15 | INFO | train | epoch 054 | loss 5.387 | nll_loss 3.992 | ppl 15.91 | wps 16104.8 | ups 3.02 | wpb 5324.5 | bsz 212.4 | num_updates 3024 | lr 0.000151224 | gnorm 1.916 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:41:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=55/shard_epoch=50
2020-10-13 06:41:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=55/shard_epoch=51
2020-10-13 06:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7461.515625Mb; avail=237093.1015625Mb
2020-10-13 06:41:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000341
2020-10-13 06:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002948
2020-10-13 06:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.515625Mb; avail=237093.1015625Mb
2020-10-13 06:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000093
2020-10-13 06:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.515625Mb; avail=237093.1015625Mb
2020-10-13 06:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.044636
2020-10-13 06:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.048479
2020-10-13 06:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.515625Mb; avail=237093.1015625Mb
2020-10-13 06:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7461.515625Mb; avail=237093.1015625Mb
2020-10-13 06:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001936
2020-10-13 06:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.515625Mb; avail=237093.1015625Mb
2020-10-13 06:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000082
2020-10-13 06:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.515625Mb; avail=237093.1015625Mb
2020-10-13 06:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040515
2020-10-13 06:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043242
2020-10-13 06:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.515625Mb; avail=237093.1015625Mb
2020-10-13 06:41:15 | INFO | fairseq.trainer | begin training epoch 55
2020-10-13 06:41:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:41:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7461.5390625Mb; avail=237092.875Mb
2020-10-13 06:41:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000871
2020-10-13 06:41:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.5390625Mb; avail=237092.875Mb
2020-10-13 06:41:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017977
2020-10-13 06:41:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.5390625Mb; avail=237092.875Mb
2020-10-13 06:41:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013319
2020-10-13 06:41:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.033028
2020-10-13 06:41:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.5390625Mb; avail=237092.875Mb
2020-10-13 06:41:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7461.5390625Mb; avail=237092.875Mb
2020-10-13 06:41:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000756
2020-10-13 06:41:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.5390625Mb; avail=237092.875Mb
2020-10-13 06:41:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017421
2020-10-13 06:41:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.5390625Mb; avail=237092.875Mb
2020-10-13 06:41:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013300
2020-10-13 06:41:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032265
2020-10-13 06:41:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.5390625Mb; avail=237092.875Mb
2020-10-13 06:41:31 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 7.795 | nll_loss 6.579 | ppl 95.59 | wps 51143 | wpb 2112 | bsz 83.9 | num_updates 3080 | best_loss 7.611
2020-10-13 06:41:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:41:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 55 @ 3080 updates, score 7.795) (writing took 2.3596150958910584 seconds)
2020-10-13 06:41:33 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2020-10-13 06:41:33 | INFO | train | epoch 055 | loss 5.306 | nll_loss 3.898 | ppl 14.91 | wps 15923.9 | ups 2.99 | wpb 5324.5 | bsz 212.4 | num_updates 3080 | lr 0.000154023 | gnorm 1.882 | clip 0 | train_wall 15 | wall 0
2020-10-13 06:41:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=56/shard_epoch=51
2020-10-13 06:41:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=56/shard_epoch=52
2020-10-13 06:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7461.47265625Mb; avail=237093.1640625Mb
2020-10-13 06:41:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000280
2020-10-13 06:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002965
2020-10-13 06:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.47265625Mb; avail=237093.1640625Mb
2020-10-13 06:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000091
2020-10-13 06:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.47265625Mb; avail=237093.1640625Mb
2020-10-13 06:41:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.044632
2020-10-13 06:41:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.048510
2020-10-13 06:41:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.47265625Mb; avail=237093.1640625Mb
2020-10-13 06:41:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7461.47265625Mb; avail=237093.1640625Mb
2020-10-13 06:41:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001973
2020-10-13 06:41:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.47265625Mb; avail=237093.1640625Mb
2020-10-13 06:41:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000083
2020-10-13 06:41:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.47265625Mb; avail=237093.1640625Mb
2020-10-13 06:41:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040446
2020-10-13 06:41:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043221
2020-10-13 06:41:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.47265625Mb; avail=237093.1640625Mb
2020-10-13 06:41:34 | INFO | fairseq.trainer | begin training epoch 56
2020-10-13 06:41:39 | INFO | train_inner | epoch 056:     20 / 56 loss=5.316, nll_loss=3.909, ppl=15.02, wps=15492.3, ups=2.94, wpb=5269.8, bsz=207.6, num_updates=3100, lr=0.000155023, gnorm=1.913, clip=0, train_wall=26, wall=0
2020-10-13 06:41:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7468.140625Mb; avail=237086.24609375Mb
2020-10-13 06:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000833
2020-10-13 06:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.140625Mb; avail=237086.24609375Mb
2020-10-13 06:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017778
2020-10-13 06:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.7734375Mb; avail=237082.61328125Mb
2020-10-13 06:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013491
2020-10-13 06:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032887
2020-10-13 06:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.68359375Mb; avail=237079.703125Mb
2020-10-13 06:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7476.5Mb; avail=237077.88671875Mb
2020-10-13 06:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000729
2020-10-13 06:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7476.5Mb; avail=237077.88671875Mb
2020-10-13 06:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017756
2020-10-13 06:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.25390625Mb; avail=237086.1328125Mb
2020-10-13 06:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.014413
2020-10-13 06:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.033644
2020-10-13 06:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7472.38671875Mb; avail=237082.0Mb
2020-10-13 06:41:50 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 7.822 | nll_loss 6.605 | ppl 97.32 | wps 50808.4 | wpb 2112 | bsz 83.9 | num_updates 3136 | best_loss 7.611
2020-10-13 06:41:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:41:52 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 56 @ 3136 updates, score 7.822) (writing took 2.3723374819383025 seconds)
2020-10-13 06:41:52 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2020-10-13 06:41:52 | INFO | train | epoch 056 | loss 5.233 | nll_loss 3.812 | ppl 14.05 | wps 16032.4 | ups 3.01 | wpb 5324.5 | bsz 212.4 | num_updates 3136 | lr 0.000156822 | gnorm 2.106 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:41:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=57/shard_epoch=52
2020-10-13 06:41:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=57/shard_epoch=53
2020-10-13 06:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7466.953125Mb; avail=237087.67578125Mb
2020-10-13 06:41:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000317
2020-10-13 06:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002836
2020-10-13 06:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.55859375Mb; avail=237087.0703125Mb
2020-10-13 06:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000137
2020-10-13 06:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.55859375Mb; avail=237087.0703125Mb
2020-10-13 06:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.044963
2020-10-13 06:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.048835
2020-10-13 06:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7472.30078125Mb; avail=237082.328125Mb
2020-10-13 06:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7466.51171875Mb; avail=237090.0859375Mb
2020-10-13 06:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001972
2020-10-13 06:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.57421875Mb; avail=237092.0546875Mb
2020-10-13 06:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000086
2020-10-13 06:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.57421875Mb; avail=237092.0546875Mb
2020-10-13 06:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040260
2020-10-13 06:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043059
2020-10-13 06:41:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.546875Mb; avail=237087.08203125Mb
2020-10-13 06:41:52 | INFO | fairseq.trainer | begin training epoch 57
2020-10-13 06:42:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7461.46875Mb; avail=237092.92578125Mb
2020-10-13 06:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000805
2020-10-13 06:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.46875Mb; avail=237092.92578125Mb
2020-10-13 06:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.018553
2020-10-13 06:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.453125Mb; avail=237092.94140625Mb
2020-10-13 06:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013818
2020-10-13 06:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.033999
2020-10-13 06:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.453125Mb; avail=237092.94140625Mb
2020-10-13 06:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7461.453125Mb; avail=237092.94140625Mb
2020-10-13 06:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000667
2020-10-13 06:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.453125Mb; avail=237092.94140625Mb
2020-10-13 06:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017342
2020-10-13 06:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.453125Mb; avail=237092.94140625Mb
2020-10-13 06:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013390
2020-10-13 06:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032149
2020-10-13 06:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7461.453125Mb; avail=237092.94140625Mb
2020-10-13 06:42:09 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 7.846 | nll_loss 6.626 | ppl 98.8 | wps 49417.7 | wpb 2112 | bsz 83.9 | num_updates 3192 | best_loss 7.611
2020-10-13 06:42:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:42:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 57 @ 3192 updates, score 7.846) (writing took 2.372087821830064 seconds)
2020-10-13 06:42:11 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2020-10-13 06:42:11 | INFO | train | epoch 057 | loss 5.139 | nll_loss 3.702 | ppl 13.01 | wps 15836 | ups 2.97 | wpb 5324.5 | bsz 212.4 | num_updates 3192 | lr 0.00015962 | gnorm 1.918 | clip 0 | train_wall 15 | wall 0
2020-10-13 06:42:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=58/shard_epoch=53
2020-10-13 06:42:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=58/shard_epoch=54
2020-10-13 06:42:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.734375Mb; avail=237090.890625Mb
2020-10-13 06:42:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000363
2020-10-13 06:42:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002898
2020-10-13 06:42:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.734375Mb; avail=237090.890625Mb
2020-10-13 06:42:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000094
2020-10-13 06:42:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.734375Mb; avail=237090.890625Mb
2020-10-13 06:42:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.044883
2020-10-13 06:42:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.048626
2020-10-13 06:42:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.734375Mb; avail=237090.890625Mb
2020-10-13 06:42:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.734375Mb; avail=237090.890625Mb
2020-10-13 06:42:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002002
2020-10-13 06:42:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.734375Mb; avail=237090.890625Mb
2020-10-13 06:42:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000091
2020-10-13 06:42:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.734375Mb; avail=237090.890625Mb
2020-10-13 06:42:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040302
2020-10-13 06:42:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043140
2020-10-13 06:42:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.734375Mb; avail=237090.890625Mb
2020-10-13 06:42:11 | INFO | fairseq.trainer | begin training epoch 58
2020-10-13 06:42:13 | INFO | train_inner | epoch 058:      8 / 56 loss=5.171, nll_loss=3.739, ppl=13.35, wps=15776.7, ups=2.92, wpb=5403.8, bsz=221.5, num_updates=3200, lr=0.00016002, gnorm=2.021, clip=0, train_wall=26, wall=0
2020-10-13 06:42:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.03515625Mb; avail=237091.96875Mb
2020-10-13 06:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000844
2020-10-13 06:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.03515625Mb; avail=237091.96875Mb
2020-10-13 06:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017635
2020-10-13 06:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.6328125Mb; avail=237092.37109375Mb
2020-10-13 06:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013632
2020-10-13 06:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032901
2020-10-13 06:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.62109375Mb; avail=237092.0234375Mb
2020-10-13 06:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7462.34765625Mb; avail=237092.29296875Mb
2020-10-13 06:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000724
2020-10-13 06:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.34765625Mb; avail=237092.29296875Mb
2020-10-13 06:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017633
2020-10-13 06:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.34765625Mb; avail=237092.29296875Mb
2020-10-13 06:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.014952
2020-10-13 06:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.034074
2020-10-13 06:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.34765625Mb; avail=237092.29296875Mb
2020-10-13 06:42:27 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 7.867 | nll_loss 6.641 | ppl 99.8 | wps 51119.5 | wpb 2112 | bsz 83.9 | num_updates 3248 | best_loss 7.611
2020-10-13 06:42:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:42:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 58 @ 3248 updates, score 7.867) (writing took 2.3611851017922163 seconds)
2020-10-13 06:42:30 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2020-10-13 06:42:30 | INFO | train | epoch 058 | loss 5.062 | nll_loss 3.612 | ppl 12.23 | wps 15943.4 | ups 2.99 | wpb 5324.5 | bsz 212.4 | num_updates 3248 | lr 0.000162419 | gnorm 2.023 | clip 0 | train_wall 15 | wall 0
2020-10-13 06:42:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=59/shard_epoch=54
2020-10-13 06:42:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=59/shard_epoch=55
2020-10-13 06:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.64453125Mb; avail=237090.99609375Mb
2020-10-13 06:42:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000283
2020-10-13 06:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003066
2020-10-13 06:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.64453125Mb; avail=237090.99609375Mb
2020-10-13 06:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000094
2020-10-13 06:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.64453125Mb; avail=237090.99609375Mb
2020-10-13 06:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.044850
2020-10-13 06:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.048828
2020-10-13 06:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.64453125Mb; avail=237090.99609375Mb
2020-10-13 06:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.64453125Mb; avail=237090.99609375Mb
2020-10-13 06:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001998
2020-10-13 06:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.64453125Mb; avail=237090.99609375Mb
2020-10-13 06:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000082
2020-10-13 06:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.64453125Mb; avail=237090.99609375Mb
2020-10-13 06:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040151
2020-10-13 06:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.042974
2020-10-13 06:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.64453125Mb; avail=237090.99609375Mb
2020-10-13 06:42:30 | INFO | fairseq.trainer | begin training epoch 59
2020-10-13 06:42:44 | INFO | train_inner | epoch 059:     52 / 56 loss=5.008, nll_loss=3.549, ppl=11.7, wps=17367.3, ups=3.25, wpb=5335.7, bsz=211.6, num_updates=3300, lr=0.000165018, gnorm=1.968, clip=0, train_wall=26, wall=0
2020-10-13 06:42:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:42:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7467.72265625Mb; avail=237086.59375Mb
2020-10-13 06:42:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000837
2020-10-13 06:42:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.72265625Mb; avail=237086.59375Mb
2020-10-13 06:42:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017772
2020-10-13 06:42:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.93359375Mb; avail=237085.3828125Mb
2020-10-13 06:42:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013482
2020-10-13 06:42:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032867
2020-10-13 06:42:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7469.66015625Mb; avail=237084.65625Mb
2020-10-13 06:42:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7473.19140625Mb; avail=237081.125Mb
2020-10-13 06:42:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000709
2020-10-13 06:42:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.19140625Mb; avail=237081.125Mb
2020-10-13 06:42:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017551
2020-10-13 06:42:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.796875Mb; avail=237080.51953125Mb
2020-10-13 06:42:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013245
2020-10-13 06:42:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032248
2020-10-13 06:42:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.80859375Mb; avail=237078.5078125Mb
2020-10-13 06:42:46 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 7.913 | nll_loss 6.694 | ppl 103.5 | wps 49219.5 | wpb 2112 | bsz 83.9 | num_updates 3304 | best_loss 7.611
2020-10-13 06:42:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:42:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 59 @ 3304 updates, score 7.913) (writing took 2.412835668073967 seconds)
2020-10-13 06:42:48 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2020-10-13 06:42:48 | INFO | train | epoch 059 | loss 4.961 | nll_loss 3.494 | ppl 11.27 | wps 15904.1 | ups 2.99 | wpb 5324.5 | bsz 212.4 | num_updates 3304 | lr 0.000165217 | gnorm 1.889 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:42:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=60/shard_epoch=55
2020-10-13 06:42:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=60/shard_epoch=56
2020-10-13 06:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7474.84765625Mb; avail=237079.16796875Mb
2020-10-13 06:42:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000340
2020-10-13 06:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003017
2020-10-13 06:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.3671875Mb; avail=237078.6484375Mb
2020-10-13 06:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000095
2020-10-13 06:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.3671875Mb; avail=237078.6484375Mb
2020-10-13 06:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.044645
2020-10-13 06:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.048601
2020-10-13 06:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.15234375Mb; avail=237083.86328125Mb
2020-10-13 06:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7471.96875Mb; avail=237082.046875Mb
2020-10-13 06:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001933
2020-10-13 06:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.96875Mb; avail=237082.046875Mb
2020-10-13 06:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000120
2020-10-13 06:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.96875Mb; avail=237082.046875Mb
2020-10-13 06:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041068
2020-10-13 06:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043865
2020-10-13 06:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.14453125Mb; avail=237081.5625Mb
2020-10-13 06:42:48 | INFO | fairseq.trainer | begin training epoch 60
2020-10-13 06:43:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.1640625Mb; avail=237091.265625Mb
2020-10-13 06:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000833
2020-10-13 06:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.1640625Mb; avail=237091.265625Mb
2020-10-13 06:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017671
2020-10-13 06:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.1640625Mb; avail=237091.265625Mb
2020-10-13 06:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013284
2020-10-13 06:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032554
2020-10-13 06:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.1640625Mb; avail=237091.265625Mb
2020-10-13 06:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.1640625Mb; avail=237091.265625Mb
2020-10-13 06:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000680
2020-10-13 06:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.1640625Mb; avail=237091.265625Mb
2020-10-13 06:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017300
2020-10-13 06:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.1640625Mb; avail=237091.265625Mb
2020-10-13 06:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013266
2020-10-13 06:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032001
2020-10-13 06:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.1640625Mb; avail=237091.265625Mb
2020-10-13 06:43:05 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 7.95 | nll_loss 6.726 | ppl 105.88 | wps 51048.9 | wpb 2112 | bsz 83.9 | num_updates 3360 | best_loss 7.611
2020-10-13 06:43:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:43:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 60 @ 3360 updates, score 7.95) (writing took 2.3578545849304646 seconds)
2020-10-13 06:43:07 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2020-10-13 06:43:07 | INFO | train | epoch 060 | loss 4.879 | nll_loss 3.399 | ppl 10.55 | wps 16075.3 | ups 3.02 | wpb 5324.5 | bsz 212.4 | num_updates 3360 | lr 0.000168016 | gnorm 1.904 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:43:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=61/shard_epoch=56
2020-10-13 06:43:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=61/shard_epoch=57
2020-10-13 06:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.51171875Mb; avail=237091.1328125Mb
2020-10-13 06:43:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000286
2020-10-13 06:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002995
2020-10-13 06:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.51171875Mb; avail=237091.1328125Mb
2020-10-13 06:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000092
2020-10-13 06:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.51171875Mb; avail=237091.1328125Mb
2020-10-13 06:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.045225
2020-10-13 06:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.049158
2020-10-13 06:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.51171875Mb; avail=237091.1328125Mb
2020-10-13 06:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.51171875Mb; avail=237091.1328125Mb
2020-10-13 06:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001969
2020-10-13 06:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.51171875Mb; avail=237091.1328125Mb
2020-10-13 06:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000082
2020-10-13 06:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.51171875Mb; avail=237091.1328125Mb
2020-10-13 06:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040529
2020-10-13 06:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043332
2020-10-13 06:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.51171875Mb; avail=237091.1328125Mb
2020-10-13 06:43:07 | INFO | fairseq.trainer | begin training epoch 61
2020-10-13 06:43:18 | INFO | train_inner | epoch 061:     40 / 56 loss=4.845, nll_loss=3.359, ppl=10.26, wps=15748, ups=2.95, wpb=5344.4, bsz=206.8, num_updates=3400, lr=0.000170015, gnorm=1.907, clip=0, train_wall=26, wall=0
2020-10-13 06:43:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7474.67578125Mb; avail=237079.703125Mb
2020-10-13 06:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001284
2020-10-13 06:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.67578125Mb; avail=237079.703125Mb
2020-10-13 06:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.033323
2020-10-13 06:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7480.5390625Mb; avail=237073.83984375Mb
2020-10-13 06:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.019724
2020-10-13 06:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.055402
2020-10-13 06:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7482.9609375Mb; avail=237071.41796875Mb
2020-10-13 06:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7478.953125Mb; avail=237074.91015625Mb
2020-10-13 06:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000734
2020-10-13 06:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7479.46875Mb; avail=237074.91015625Mb
2020-10-13 06:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017528
2020-10-13 06:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7484.1015625Mb; avail=237070.27734375Mb
2020-10-13 06:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013528
2020-10-13 06:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032534
2020-10-13 06:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7484.65625Mb; avail=237069.72265625Mb
2020-10-13 06:43:23 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.999 | nll_loss 6.774 | ppl 109.41 | wps 51051.9 | wpb 2112 | bsz 83.9 | num_updates 3416 | best_loss 7.611
2020-10-13 06:43:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:43:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 61 @ 3416 updates, score 7.999) (writing took 2.353853598004207 seconds)
2020-10-13 06:43:25 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2020-10-13 06:43:25 | INFO | train | epoch 061 | loss 4.784 | nll_loss 3.288 | ppl 9.76 | wps 16056 | ups 3.02 | wpb 5324.5 | bsz 212.4 | num_updates 3416 | lr 0.000170815 | gnorm 1.898 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:43:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=62/shard_epoch=57
2020-10-13 06:43:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=62/shard_epoch=58
2020-10-13 06:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.16015625Mb; avail=237091.4609375Mb
2020-10-13 06:43:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000381
2020-10-13 06:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002984
2020-10-13 06:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.16015625Mb; avail=237091.4609375Mb
2020-10-13 06:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000094
2020-10-13 06:43:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.16015625Mb; avail=237091.4609375Mb
2020-10-13 06:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.045874
2020-10-13 06:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.049797
2020-10-13 06:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.16015625Mb; avail=237091.4609375Mb
2020-10-13 06:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7465.46875Mb; avail=237089.15234375Mb
2020-10-13 06:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001931
2020-10-13 06:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.46875Mb; avail=237089.15234375Mb
2020-10-13 06:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000084
2020-10-13 06:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.46875Mb; avail=237089.15234375Mb
2020-10-13 06:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040609
2020-10-13 06:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043339
2020-10-13 06:43:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7469.5859375Mb; avail=237085.03515625Mb
2020-10-13 06:43:26 | INFO | fairseq.trainer | begin training epoch 62
2020-10-13 06:43:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:43:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7465.546875Mb; avail=237088.72265625Mb
2020-10-13 06:43:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000839
2020-10-13 06:43:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.546875Mb; avail=237088.72265625Mb
2020-10-13 06:43:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017474
2020-10-13 06:43:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.546875Mb; avail=237088.72265625Mb
2020-10-13 06:43:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013390
2020-10-13 06:43:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032473
2020-10-13 06:43:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.546875Mb; avail=237088.72265625Mb
2020-10-13 06:43:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7465.546875Mb; avail=237088.72265625Mb
2020-10-13 06:43:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000701
2020-10-13 06:43:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.546875Mb; avail=237088.72265625Mb
2020-10-13 06:43:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017588
2020-10-13 06:43:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.546875Mb; avail=237088.72265625Mb
2020-10-13 06:43:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013271
2020-10-13 06:43:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032344
2020-10-13 06:43:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.546875Mb; avail=237088.72265625Mb
2020-10-13 06:43:42 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 8 | nll_loss 6.781 | ppl 110 | wps 50708.7 | wpb 2112 | bsz 83.9 | num_updates 3472 | best_loss 7.611
2020-10-13 06:43:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:43:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 62 @ 3472 updates, score 8.0) (writing took 2.3710615390446037 seconds)
2020-10-13 06:43:44 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2020-10-13 06:43:44 | INFO | train | epoch 062 | loss 4.755 | nll_loss 3.251 | ppl 9.52 | wps 16036.3 | ups 3.01 | wpb 5324.5 | bsz 212.4 | num_updates 3472 | lr 0.000173613 | gnorm 2.166 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:43:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=63/shard_epoch=58
2020-10-13 06:43:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=63/shard_epoch=59
2020-10-13 06:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.88671875Mb; avail=237090.74609375Mb
2020-10-13 06:43:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000364
2020-10-13 06:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003052
2020-10-13 06:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.88671875Mb; avail=237090.74609375Mb
2020-10-13 06:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000099
2020-10-13 06:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.88671875Mb; avail=237090.74609375Mb
2020-10-13 06:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.045131
2020-10-13 06:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.049146
2020-10-13 06:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.88671875Mb; avail=237090.74609375Mb
2020-10-13 06:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.88671875Mb; avail=237090.74609375Mb
2020-10-13 06:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002017
2020-10-13 06:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.88671875Mb; avail=237090.74609375Mb
2020-10-13 06:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000082
2020-10-13 06:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.88671875Mb; avail=237090.74609375Mb
2020-10-13 06:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.039930
2020-10-13 06:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.042758
2020-10-13 06:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.88671875Mb; avail=237090.74609375Mb
2020-10-13 06:43:44 | INFO | fairseq.trainer | begin training epoch 63
2020-10-13 06:43:52 | INFO | train_inner | epoch 063:     28 / 56 loss=4.723, nll_loss=3.214, ppl=9.28, wps=15748.5, ups=2.94, wpb=5356.3, bsz=219.1, num_updates=3500, lr=0.000175013, gnorm=1.996, clip=0, train_wall=26, wall=0
2020-10-13 06:43:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7468.796875Mb; avail=237085.81640625Mb
2020-10-13 06:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000849
2020-10-13 06:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.0078125Mb; avail=237084.60546875Mb
2020-10-13 06:44:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017554
2020-10-13 06:44:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.03515625Mb; avail=237081.578125Mb
2020-10-13 06:44:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013179
2020-10-13 06:44:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032352
2020-10-13 06:44:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7477.87890625Mb; avail=237076.734375Mb
2020-10-13 06:44:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7480.19140625Mb; avail=237074.421875Mb
2020-10-13 06:44:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000710
2020-10-13 06:44:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7480.19140625Mb; avail=237074.421875Mb
2020-10-13 06:44:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017167
2020-10-13 06:44:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7484.4296875Mb; avail=237070.18359375Mb
2020-10-13 06:44:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013104
2020-10-13 06:44:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031715
2020-10-13 06:44:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.55859375Mb; avail=237081.05859375Mb
2020-10-13 06:44:00 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 8.059 | nll_loss 6.834 | ppl 114.07 | wps 50076.7 | wpb 2112 | bsz 83.9 | num_updates 3528 | best_loss 7.611
2020-10-13 06:44:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:44:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 63 @ 3528 updates, score 8.059) (writing took 2.364019440021366 seconds)
2020-10-13 06:44:03 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2020-10-13 06:44:03 | INFO | train | epoch 063 | loss 4.62 | nll_loss 3.094 | ppl 8.54 | wps 15992 | ups 3 | wpb 5324.5 | bsz 212.4 | num_updates 3528 | lr 0.000176412 | gnorm 1.825 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:44:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=64/shard_epoch=59
2020-10-13 06:44:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=64/shard_epoch=60
2020-10-13 06:44:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7464.6875Mb; avail=237089.93359375Mb
2020-10-13 06:44:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000273
2020-10-13 06:44:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002672
2020-10-13 06:44:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.6875Mb; avail=237089.93359375Mb
2020-10-13 06:44:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000095
2020-10-13 06:44:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.6875Mb; avail=237089.93359375Mb
2020-10-13 06:44:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040461
2020-10-13 06:44:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043951
2020-10-13 06:44:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.6875Mb; avail=237089.93359375Mb
2020-10-13 06:44:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7464.6875Mb; avail=237089.93359375Mb
2020-10-13 06:44:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001924
2020-10-13 06:44:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.6875Mb; avail=237089.93359375Mb
2020-10-13 06:44:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000083
2020-10-13 06:44:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.6875Mb; avail=237089.93359375Mb
2020-10-13 06:44:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040115
2020-10-13 06:44:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.042846
2020-10-13 06:44:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.71484375Mb; avail=237086.90625Mb
2020-10-13 06:44:03 | INFO | fairseq.trainer | begin training epoch 64
2020-10-13 06:44:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7464.0234375Mb; avail=237090.27734375Mb
2020-10-13 06:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000858
2020-10-13 06:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.0234375Mb; avail=237090.27734375Mb
2020-10-13 06:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017375
2020-10-13 06:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.0234375Mb; avail=237090.27734375Mb
2020-10-13 06:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013255
2020-10-13 06:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032248
2020-10-13 06:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.0234375Mb; avail=237090.27734375Mb
2020-10-13 06:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7464.0234375Mb; avail=237090.27734375Mb
2020-10-13 06:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000717
2020-10-13 06:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.0234375Mb; avail=237090.27734375Mb
2020-10-13 06:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017202
2020-10-13 06:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.0234375Mb; avail=237090.27734375Mb
2020-10-13 06:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013174
2020-10-13 06:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031822
2020-10-13 06:44:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.0234375Mb; avail=237090.27734375Mb
2020-10-13 06:44:19 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 8.081 | nll_loss 6.868 | ppl 116.84 | wps 51111.4 | wpb 2112 | bsz 83.9 | num_updates 3584 | best_loss 7.611
2020-10-13 06:44:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:44:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 64 @ 3584 updates, score 8.081) (writing took 2.353434421820566 seconds)
2020-10-13 06:44:21 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2020-10-13 06:44:21 | INFO | train | epoch 064 | loss 4.548 | nll_loss 3.009 | ppl 8.05 | wps 16104.4 | ups 3.02 | wpb 5324.5 | bsz 212.4 | num_updates 3584 | lr 0.00017921 | gnorm 1.924 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:44:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=65/shard_epoch=60
2020-10-13 06:44:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=65/shard_epoch=61
2020-10-13 06:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.76171875Mb; avail=237090.859375Mb
2020-10-13 06:44:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000305
2020-10-13 06:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002651
2020-10-13 06:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.76171875Mb; avail=237090.859375Mb
2020-10-13 06:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000094
2020-10-13 06:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.76171875Mb; avail=237090.859375Mb
2020-10-13 06:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040999
2020-10-13 06:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044486
2020-10-13 06:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.76171875Mb; avail=237090.859375Mb
2020-10-13 06:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.76171875Mb; avail=237090.859375Mb
2020-10-13 06:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001974
2020-10-13 06:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.76171875Mb; avail=237090.859375Mb
2020-10-13 06:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000082
2020-10-13 06:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.76171875Mb; avail=237090.859375Mb
2020-10-13 06:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040717
2020-10-13 06:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043509
2020-10-13 06:44:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.76171875Mb; avail=237090.859375Mb
2020-10-13 06:44:21 | INFO | fairseq.trainer | begin training epoch 65
2020-10-13 06:44:26 | INFO | train_inner | epoch 065:     16 / 56 loss=4.541, nll_loss=3.001, ppl=8.01, wps=15469.5, ups=2.95, wpb=5237.9, bsz=214, num_updates=3600, lr=0.00018001, gnorm=1.971, clip=0, train_wall=26, wall=0
2020-10-13 06:44:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7468.73046875Mb; avail=237085.66015625Mb
2020-10-13 06:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000866
2020-10-13 06:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.73046875Mb; avail=237085.66015625Mb
2020-10-13 06:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017079
2020-10-13 06:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.546875Mb; avail=237083.84375Mb
2020-10-13 06:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013452
2020-10-13 06:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032222
2020-10-13 06:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.15234375Mb; avail=237083.23828125Mb
2020-10-13 06:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7475.99609375Mb; avail=237078.39453125Mb
2020-10-13 06:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000745
2020-10-13 06:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.99609375Mb; avail=237078.39453125Mb
2020-10-13 06:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017447
2020-10-13 06:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.99609375Mb; avail=237078.39453125Mb
2020-10-13 06:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013287
2020-10-13 06:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032273
2020-10-13 06:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7476.49609375Mb; avail=237077.89453125Mb
2020-10-13 06:44:37 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 8.116 | nll_loss 6.884 | ppl 118.08 | wps 50847.1 | wpb 2112 | bsz 83.9 | num_updates 3640 | best_loss 7.611
2020-10-13 06:44:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:44:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 65 @ 3640 updates, score 8.116) (writing took 2.3491741460748017 seconds)
2020-10-13 06:44:40 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2020-10-13 06:44:40 | INFO | train | epoch 065 | loss 4.484 | nll_loss 2.931 | ppl 7.63 | wps 16010.3 | ups 3.01 | wpb 5324.5 | bsz 212.4 | num_updates 3640 | lr 0.000182009 | gnorm 2.009 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:44:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=66/shard_epoch=61
2020-10-13 06:44:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=66/shard_epoch=62
2020-10-13 06:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7467.234375Mb; avail=237087.40234375Mb
2020-10-13 06:44:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000274
2020-10-13 06:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002639
2020-10-13 06:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.234375Mb; avail=237087.40234375Mb
2020-10-13 06:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000095
2020-10-13 06:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.234375Mb; avail=237087.40234375Mb
2020-10-13 06:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040449
2020-10-13 06:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043960
2020-10-13 06:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.8671875Mb; avail=237083.76953125Mb
2020-10-13 06:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7474.74609375Mb; avail=237079.890625Mb
2020-10-13 06:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001935
2020-10-13 06:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.74609375Mb; avail=237079.890625Mb
2020-10-13 06:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000085
2020-10-13 06:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.74609375Mb; avail=237079.890625Mb
2020-10-13 06:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040498
2020-10-13 06:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043252
2020-10-13 06:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.65625Mb; avail=237088.98046875Mb
2020-10-13 06:44:40 | INFO | fairseq.trainer | begin training epoch 66
2020-10-13 06:44:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7464.80859375Mb; avail=237089.453125Mb
2020-10-13 06:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000821
2020-10-13 06:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.4140625Mb; avail=237088.84765625Mb
2020-10-13 06:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017366
2020-10-13 06:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.74609375Mb; avail=237087.515625Mb
2020-10-13 06:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013080
2020-10-13 06:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032022
2020-10-13 06:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.19921875Mb; avail=237086.0625Mb
2020-10-13 06:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7469.3046875Mb; avail=237084.95703125Mb
2020-10-13 06:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000696
2020-10-13 06:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7469.3046875Mb; avail=237084.95703125Mb
2020-10-13 06:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017354
2020-10-13 06:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.53125Mb; avail=237080.73046875Mb
2020-10-13 06:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013059
2020-10-13 06:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031838
2020-10-13 06:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7473.53125Mb; avail=237080.73046875Mb
2020-10-13 06:44:56 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 8.153 | nll_loss 6.919 | ppl 121.01 | wps 50058.6 | wpb 2112 | bsz 83.9 | num_updates 3696 | best_loss 7.611
2020-10-13 06:44:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:44:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 66 @ 3696 updates, score 8.153) (writing took 2.4148088849615306 seconds)
2020-10-13 06:44:58 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2020-10-13 06:44:58 | INFO | train | epoch 066 | loss 4.413 | nll_loss 2.848 | ppl 7.2 | wps 16045.7 | ups 3.01 | wpb 5324.5 | bsz 212.4 | num_updates 3696 | lr 0.000184808 | gnorm 2.087 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:44:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=67/shard_epoch=62
2020-10-13 06:44:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=67/shard_epoch=63
2020-10-13 06:44:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7463.55859375Mb; avail=237091.0390625Mb
2020-10-13 06:44:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000271
2020-10-13 06:44:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002684
2020-10-13 06:44:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.55859375Mb; avail=237091.0390625Mb
2020-10-13 06:44:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000093
2020-10-13 06:44:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7463.55859375Mb; avail=237091.0390625Mb
2020-10-13 06:44:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040705
2020-10-13 06:44:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044203
2020-10-13 06:44:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7462.8203125Mb; avail=237091.77734375Mb
2020-10-13 06:44:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7464.03125Mb; avail=237090.56640625Mb
2020-10-13 06:44:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001998
2020-10-13 06:44:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.63671875Mb; avail=237089.9609375Mb
2020-10-13 06:44:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000083
2020-10-13 06:44:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.63671875Mb; avail=237089.9609375Mb
2020-10-13 06:44:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040219
2020-10-13 06:44:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043020
2020-10-13 06:44:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.26953125Mb; avail=237086.328125Mb
2020-10-13 06:44:59 | INFO | fairseq.trainer | begin training epoch 67
2020-10-13 06:45:00 | INFO | train_inner | epoch 067:      4 / 56 loss=4.446, nll_loss=2.886, ppl=7.39, wps=15692.8, ups=2.95, wpb=5320.6, bsz=208.8, num_updates=3700, lr=0.000185008, gnorm=2.011, clip=0, train_wall=26, wall=0
2020-10-13 06:45:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7474.296875Mb; avail=237080.09375Mb
2020-10-13 06:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000845
2020-10-13 06:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.296875Mb; avail=237080.09375Mb
2020-10-13 06:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017839
2020-10-13 06:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.79296875Mb; avail=237079.59765625Mb
2020-10-13 06:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013231
2020-10-13 06:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032723
2020-10-13 06:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.79296875Mb; avail=237079.59765625Mb
2020-10-13 06:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7465.44140625Mb; avail=237088.94921875Mb
2020-10-13 06:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000714
2020-10-13 06:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.44140625Mb; avail=237088.94921875Mb
2020-10-13 06:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017658
2020-10-13 06:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.44140625Mb; avail=237088.94921875Mb
2020-10-13 06:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013142
2020-10-13 06:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032315
2020-10-13 06:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.44140625Mb; avail=237088.94921875Mb
2020-10-13 06:45:15 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 8.194 | nll_loss 6.952 | ppl 123.82 | wps 50965.5 | wpb 2112 | bsz 83.9 | num_updates 3752 | best_loss 7.611
2020-10-13 06:45:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:45:17 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 67 @ 3752 updates, score 8.194) (writing took 2.3757936260662973 seconds)
2020-10-13 06:45:17 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2020-10-13 06:45:17 | INFO | train | epoch 067 | loss 4.303 | nll_loss 2.719 | ppl 6.59 | wps 16067.9 | ups 3.02 | wpb 5324.5 | bsz 212.4 | num_updates 3752 | lr 0.000187606 | gnorm 1.935 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:45:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=68/shard_epoch=63
2020-10-13 06:45:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=68/shard_epoch=64
2020-10-13 06:45:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7474.734375Mb; avail=237079.2890625Mb
2020-10-13 06:45:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000300
2020-10-13 06:45:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002738
2020-10-13 06:45:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.33984375Mb; avail=237079.2890625Mb
2020-10-13 06:45:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000118
2020-10-13 06:45:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.33984375Mb; avail=237079.2890625Mb
2020-10-13 06:45:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040541
2020-10-13 06:45:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044174
2020-10-13 06:45:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7480.0703125Mb; avail=237074.55859375Mb
2020-10-13 06:45:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7469.734375Mb; avail=237084.89453125Mb
2020-10-13 06:45:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002052
2020-10-13 06:45:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7469.734375Mb; avail=237084.89453125Mb
2020-10-13 06:45:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000111
2020-10-13 06:45:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7469.734375Mb; avail=237084.89453125Mb
2020-10-13 06:45:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040140
2020-10-13 06:45:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043082
2020-10-13 06:45:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7472.76171875Mb; avail=237081.8671875Mb
2020-10-13 06:45:17 | INFO | fairseq.trainer | begin training epoch 68
2020-10-13 06:45:30 | INFO | train_inner | epoch 068:     48 / 56 loss=4.278, nll_loss=2.69, ppl=6.45, wps=17643.6, ups=3.27, wpb=5391.4, bsz=215.8, num_updates=3800, lr=0.000190005, gnorm=2.027, clip=0, train_wall=26, wall=0
2020-10-13 06:45:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7467.79296875Mb; avail=237086.828125Mb
2020-10-13 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000859
2020-10-13 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.79296875Mb; avail=237086.828125Mb
2020-10-13 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017902
2020-10-13 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.79296875Mb; avail=237086.828125Mb
2020-10-13 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013056
2020-10-13 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032589
2020-10-13 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.79296875Mb; avail=237086.828125Mb
2020-10-13 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7467.79296875Mb; avail=237086.828125Mb
2020-10-13 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000712
2020-10-13 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.79296875Mb; avail=237086.828125Mb
2020-10-13 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017673
2020-10-13 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.79296875Mb; avail=237086.828125Mb
2020-10-13 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012989
2020-10-13 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032132
2020-10-13 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7467.79296875Mb; avail=237086.828125Mb
2020-10-13 06:45:33 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 8.225 | nll_loss 6.996 | ppl 127.62 | wps 50866.2 | wpb 2112 | bsz 83.9 | num_updates 3808 | best_loss 7.611
2020-10-13 06:45:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:45:36 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 68 @ 3808 updates, score 8.225) (writing took 2.3760499749332666 seconds)
2020-10-13 06:45:36 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2020-10-13 06:45:36 | INFO | train | epoch 068 | loss 4.245 | nll_loss 2.649 | ppl 6.27 | wps 16000.7 | ups 3.01 | wpb 5324.5 | bsz 212.4 | num_updates 3808 | lr 0.000190405 | gnorm 2.111 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:45:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=69/shard_epoch=64
2020-10-13 06:45:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=69/shard_epoch=65
2020-10-13 06:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7477.81640625Mb; avail=237076.8125Mb
2020-10-13 06:45:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000283
2020-10-13 06:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002666
2020-10-13 06:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7477.81640625Mb; avail=237076.8125Mb
2020-10-13 06:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000095
2020-10-13 06:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7477.81640625Mb; avail=237076.8125Mb
2020-10-13 06:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041289
2020-10-13 06:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044804
2020-10-13 06:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.95703125Mb; avail=237085.671875Mb
2020-10-13 06:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7468.95703125Mb; avail=237085.671875Mb
2020-10-13 06:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002012
2020-10-13 06:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.95703125Mb; avail=237085.671875Mb
2020-10-13 06:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000085
2020-10-13 06:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.95703125Mb; avail=237085.671875Mb
2020-10-13 06:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040804
2020-10-13 06:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043637
2020-10-13 06:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7468.95703125Mb; avail=237085.671875Mb
2020-10-13 06:45:36 | INFO | fairseq.trainer | begin training epoch 69
2020-10-13 06:45:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7464.99609375Mb; avail=237089.5Mb
2020-10-13 06:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000823
2020-10-13 06:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.99609375Mb; avail=237089.5Mb
2020-10-13 06:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.018015
2020-10-13 06:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.99609375Mb; avail=237089.5Mb
2020-10-13 06:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013283
2020-10-13 06:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032875
2020-10-13 06:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.99609375Mb; avail=237089.5Mb
2020-10-13 06:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7464.99609375Mb; avail=237089.5Mb
2020-10-13 06:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000687
2020-10-13 06:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.99609375Mb; avail=237089.5Mb
2020-10-13 06:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017664
2020-10-13 06:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.99609375Mb; avail=237089.5Mb
2020-10-13 06:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013157
2020-10-13 06:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032270
2020-10-13 06:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7464.99609375Mb; avail=237089.5Mb
2020-10-13 06:45:52 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 8.269 | nll_loss 7.03 | ppl 130.71 | wps 50712.1 | wpb 2112 | bsz 83.9 | num_updates 3864 | best_loss 7.611
2020-10-13 06:45:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:45:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 69 @ 3864 updates, score 8.269) (writing took 2.3629739910829812 seconds)
2020-10-13 06:45:54 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2020-10-13 06:45:54 | INFO | train | epoch 069 | loss 4.165 | nll_loss 2.554 | ppl 5.87 | wps 16086.3 | ups 3.02 | wpb 5324.5 | bsz 212.4 | num_updates 3864 | lr 0.000193203 | gnorm 1.957 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:45:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=70/shard_epoch=65
2020-10-13 06:45:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=70/shard_epoch=66
2020-10-13 06:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7483.125Mb; avail=237071.5Mb
2020-10-13 06:45:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000273
2020-10-13 06:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002721
2020-10-13 06:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7483.125Mb; avail=237071.5Mb
2020-10-13 06:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000112
2020-10-13 06:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7483.125Mb; avail=237071.5Mb
2020-10-13 06:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041447
2020-10-13 06:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.045073
2020-10-13 06:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7476.046875Mb; avail=237078.578125Mb
2020-10-13 06:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7479.08984375Mb; avail=237075.53515625Mb
2020-10-13 06:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002056
2020-10-13 06:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7479.58203125Mb; avail=237075.04296875Mb
2020-10-13 06:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000112
2020-10-13 06:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7479.58203125Mb; avail=237075.04296875Mb
2020-10-13 06:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041026
2020-10-13 06:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043969
2020-10-13 06:45:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7469.70703125Mb; avail=237084.91796875Mb
2020-10-13 06:45:54 | INFO | fairseq.trainer | begin training epoch 70
2020-10-13 06:46:04 | INFO | train_inner | epoch 070:     36 / 56 loss=4.138, nll_loss=2.524, ppl=5.75, wps=15451.9, ups=2.97, wpb=5201.8, bsz=203.5, num_updates=3900, lr=0.000195003, gnorm=1.952, clip=0, train_wall=26, wall=0
2020-10-13 06:46:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7466.76171875Mb; avail=237088.15625Mb
2020-10-13 06:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000817
2020-10-13 06:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.76171875Mb; avail=237088.15625Mb
2020-10-13 06:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017319
2020-10-13 06:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.76171875Mb; avail=237088.15625Mb
2020-10-13 06:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013487
2020-10-13 06:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032380
2020-10-13 06:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.76171875Mb; avail=237088.15625Mb
2020-10-13 06:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7466.76171875Mb; avail=237088.15625Mb
2020-10-13 06:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000681
2020-10-13 06:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.76171875Mb; avail=237088.15625Mb
2020-10-13 06:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017019
2020-10-13 06:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.4296875Mb; avail=237089.1875Mb
2020-10-13 06:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013455
2020-10-13 06:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031884
2020-10-13 06:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.4375Mb; avail=237089.1875Mb
2020-10-13 06:46:10 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 8.292 | nll_loss 7.068 | ppl 134.2 | wps 50395.2 | wpb 2112 | bsz 83.9 | num_updates 3920 | best_loss 7.611
2020-10-13 06:46:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:46:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 70 @ 3920 updates, score 8.292) (writing took 2.387213940033689 seconds)
2020-10-13 06:46:13 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2020-10-13 06:46:13 | INFO | train | epoch 070 | loss 4.087 | nll_loss 2.462 | ppl 5.51 | wps 16155.2 | ups 3.03 | wpb 5324.5 | bsz 212.4 | num_updates 3920 | lr 0.000196002 | gnorm 1.885 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:46:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=71/shard_epoch=66
2020-10-13 06:46:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=71/shard_epoch=67
2020-10-13 06:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7470.3984375Mb; avail=237084.23046875Mb
2020-10-13 06:46:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000276
2020-10-13 06:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002685
2020-10-13 06:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.00390625Mb; avail=237083.625Mb
2020-10-13 06:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000095
2020-10-13 06:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.00390625Mb; avail=237083.625Mb
2020-10-13 06:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041441
2020-10-13 06:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044963
2020-10-13 06:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7476.34765625Mb; avail=237078.28125Mb
2020-10-13 06:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7476.34765625Mb; avail=237078.28125Mb
2020-10-13 06:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002048
2020-10-13 06:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7476.34765625Mb; avail=237078.28125Mb
2020-10-13 06:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000086
2020-10-13 06:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7476.34765625Mb; avail=237078.28125Mb
2020-10-13 06:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040594
2020-10-13 06:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043468
2020-10-13 06:46:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.50390625Mb; avail=237088.125Mb
2020-10-13 06:46:13 | INFO | fairseq.trainer | begin training epoch 71
2020-10-13 06:46:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:46:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7466.22265625Mb; avail=237088.09375Mb
2020-10-13 06:46:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000850
2020-10-13 06:46:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.22265625Mb; avail=237088.09375Mb
2020-10-13 06:46:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017247
2020-10-13 06:46:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.22265625Mb; avail=237088.09375Mb
2020-10-13 06:46:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013040
2020-10-13 06:46:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031886
2020-10-13 06:46:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.22265625Mb; avail=237088.09375Mb
2020-10-13 06:46:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7466.22265625Mb; avail=237088.09375Mb
2020-10-13 06:46:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000683
2020-10-13 06:46:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.22265625Mb; avail=237088.09375Mb
2020-10-13 06:46:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017305
2020-10-13 06:46:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.22265625Mb; avail=237088.09375Mb
2020-10-13 06:46:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013049
2020-10-13 06:46:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031792
2020-10-13 06:46:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.22265625Mb; avail=237088.09375Mb
2020-10-13 06:46:29 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 8.38 | nll_loss 7.146 | ppl 141.59 | wps 50152.4 | wpb 2112 | bsz 83.9 | num_updates 3976 | best_loss 7.611
2020-10-13 06:46:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:46:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 71 @ 3976 updates, score 8.38) (writing took 2.3598358689341694 seconds)
2020-10-13 06:46:31 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2020-10-13 06:46:31 | INFO | train | epoch 071 | loss 3.989 | nll_loss 2.348 | ppl 5.09 | wps 15959.9 | ups 3 | wpb 5324.5 | bsz 212.4 | num_updates 3976 | lr 0.000198801 | gnorm 1.846 | clip 0 | train_wall 15 | wall 0
2020-10-13 06:46:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=72/shard_epoch=67
2020-10-13 06:46:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=72/shard_epoch=68
2020-10-13 06:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7477.32421875Mb; avail=237077.30859375Mb
2020-10-13 06:46:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000271
2020-10-13 06:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002656
2020-10-13 06:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7477.9296875Mb; avail=237076.703125Mb
2020-10-13 06:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000093
2020-10-13 06:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7470.0546875Mb; avail=237086.0546875Mb
2020-10-13 06:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040872
2020-10-13 06:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044360
2020-10-13 06:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7474.6328125Mb; avail=237080.0Mb
2020-10-13 06:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7475.12890625Mb; avail=237079.50390625Mb
2020-10-13 06:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002062
2020-10-13 06:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.12890625Mb; avail=237079.50390625Mb
2020-10-13 06:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000090
2020-10-13 06:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.12890625Mb; avail=237079.50390625Mb
2020-10-13 06:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040343
2020-10-13 06:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043222
2020-10-13 06:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.77734375Mb; avail=237088.85546875Mb
2020-10-13 06:46:31 | INFO | fairseq.trainer | begin training epoch 72
2020-10-13 06:46:38 | INFO | train_inner | epoch 072:     24 / 56 loss=3.999, nll_loss=2.358, ppl=5.13, wps=15930.6, ups=2.94, wpb=5421, bsz=216.9, num_updates=4000, lr=0.0002, gnorm=1.856, clip=0, train_wall=26, wall=0
2020-10-13 06:46:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7466.19140625Mb; avail=237088.44140625Mb
2020-10-13 06:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000833
2020-10-13 06:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.19140625Mb; avail=237088.44140625Mb
2020-10-13 06:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017770
2020-10-13 06:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.19140625Mb; avail=237088.44140625Mb
2020-10-13 06:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013129
2020-10-13 06:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032484
2020-10-13 06:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.19140625Mb; avail=237088.44140625Mb
2020-10-13 06:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7466.04296875Mb; avail=237088.58984375Mb
2020-10-13 06:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000699
2020-10-13 06:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.04296875Mb; avail=237088.58984375Mb
2020-10-13 06:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017590
2020-10-13 06:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.04296875Mb; avail=237088.58984375Mb
2020-10-13 06:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013022
2020-10-13 06:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032030
2020-10-13 06:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.04296875Mb; avail=237088.58984375Mb
2020-10-13 06:46:47 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 8.427 | nll_loss 7.174 | ppl 144.44 | wps 51118.2 | wpb 2112 | bsz 83.9 | num_updates 4032 | best_loss 7.611
2020-10-13 06:46:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:46:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 72 @ 4032 updates, score 8.427) (writing took 2.3680372450035065 seconds)
2020-10-13 06:46:50 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2020-10-13 06:46:50 | INFO | train | epoch 072 | loss 3.927 | nll_loss 2.272 | ppl 4.83 | wps 16163.3 | ups 3.04 | wpb 5324.5 | bsz 212.4 | num_updates 4032 | lr 0.000199205 | gnorm 1.86 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:46:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=73/shard_epoch=68
2020-10-13 06:46:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=73/shard_epoch=69
2020-10-13 06:46:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7466.55078125Mb; avail=237088.08203125Mb
2020-10-13 06:46:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000271
2020-10-13 06:46:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002647
2020-10-13 06:46:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.55078125Mb; avail=237088.08203125Mb
2020-10-13 06:46:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000091
2020-10-13 06:46:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.55078125Mb; avail=237088.08203125Mb
2020-10-13 06:46:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040892
2020-10-13 06:46:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044348
2020-10-13 06:46:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.55078125Mb; avail=237088.08203125Mb
2020-10-13 06:46:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7466.55078125Mb; avail=237088.08203125Mb
2020-10-13 06:46:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001988
2020-10-13 06:46:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.55078125Mb; avail=237088.08203125Mb
2020-10-13 06:46:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 06:46:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.55078125Mb; avail=237088.08203125Mb
2020-10-13 06:46:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041146
2020-10-13 06:46:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043946
2020-10-13 06:46:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7466.55078125Mb; avail=237088.08203125Mb
2020-10-13 06:46:50 | INFO | fairseq.trainer | begin training epoch 73
2020-10-13 06:47:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7469.37890625Mb; avail=237084.91796875Mb
2020-10-13 06:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000825
2020-10-13 06:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7469.37890625Mb; avail=237084.91796875Mb
2020-10-13 06:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017285
2020-10-13 06:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7471.80078125Mb; avail=237082.49609375Mb
2020-10-13 06:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013284
2020-10-13 06:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032149
2020-10-13 06:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.43359375Mb; avail=237078.86328125Mb
2020-10-13 06:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7478.33984375Mb; avail=237075.95703125Mb
2020-10-13 06:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000697
2020-10-13 06:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7478.33984375Mb; avail=237075.95703125Mb
2020-10-13 06:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017242
2020-10-13 06:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7483.59765625Mb; avail=237070.69921875Mb
2020-10-13 06:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013195
2020-10-13 06:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031887
2020-10-13 06:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7484.79296875Mb; avail=237069.50390625Mb
2020-10-13 06:47:06 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 8.488 | nll_loss 7.259 | ppl 153.22 | wps 51193.9 | wpb 2112 | bsz 83.9 | num_updates 4088 | best_loss 7.611
2020-10-13 06:47:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:47:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 73 @ 4088 updates, score 8.488) (writing took 2.3684781109914184 seconds)
2020-10-13 06:47:08 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2020-10-13 06:47:08 | INFO | train | epoch 073 | loss 3.855 | nll_loss 2.187 | ppl 4.55 | wps 16172.4 | ups 3.04 | wpb 5324.5 | bsz 212.4 | num_updates 4088 | lr 0.000197836 | gnorm 1.904 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:47:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=74/shard_epoch=69
2020-10-13 06:47:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=74/shard_epoch=70
2020-10-13 06:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7465.0546875Mb; avail=237089.578125Mb
2020-10-13 06:47:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000270
2020-10-13 06:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002656
2020-10-13 06:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.0546875Mb; avail=237089.578125Mb
2020-10-13 06:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000090
2020-10-13 06:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.0546875Mb; avail=237089.578125Mb
2020-10-13 06:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041626
2020-10-13 06:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.045096
2020-10-13 06:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.046875Mb; avail=237089.5859375Mb
2020-10-13 06:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7465.046875Mb; avail=237089.5859375Mb
2020-10-13 06:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001929
2020-10-13 06:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.046875Mb; avail=237089.5859375Mb
2020-10-13 06:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000082
2020-10-13 06:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.046875Mb; avail=237089.5859375Mb
2020-10-13 06:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040800
2020-10-13 06:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043515
2020-10-13 06:47:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7465.0390625Mb; avail=237089.59375Mb
2020-10-13 06:47:08 | INFO | fairseq.trainer | begin training epoch 74
2020-10-13 06:47:12 | INFO | train_inner | epoch 074:     12 / 56 loss=3.869, nll_loss=2.202, ppl=4.6, wps=15643.3, ups=2.97, wpb=5260.7, bsz=209, num_updates=4100, lr=0.000197546, gnorm=1.884, clip=0, train_wall=25, wall=0
2020-10-13 06:47:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:47:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10214.6875Mb; avail=234328.03515625Mb
2020-10-13 06:47:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000861
2020-10-13 06:47:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10214.6875Mb; avail=234328.03515625Mb
2020-10-13 06:47:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017323
2020-10-13 06:47:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10214.6875Mb; avail=234328.03515625Mb
2020-10-13 06:47:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013027
2020-10-13 06:47:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031961
2020-10-13 06:47:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10214.6875Mb; avail=234328.03515625Mb
2020-10-13 06:47:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10213.953125Mb; avail=234328.03515625Mb
2020-10-13 06:47:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000685
2020-10-13 06:47:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10213.953125Mb; avail=234328.03515625Mb
2020-10-13 06:47:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017112
2020-10-13 06:47:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10213.953125Mb; avail=234328.03515625Mb
2020-10-13 06:47:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.012852
2020-10-13 06:47:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031379
2020-10-13 06:47:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10213.953125Mb; avail=234328.03515625Mb
2020-10-13 06:47:24 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 8.523 | nll_loss 7.317 | ppl 159.48 | wps 50055.1 | wpb 2112 | bsz 83.9 | num_updates 4144 | best_loss 7.611
2020-10-13 06:47:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:47:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 74 @ 4144 updates, score 8.523) (writing took 2.4245636691339314 seconds)
2020-10-13 06:47:27 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2020-10-13 06:47:27 | INFO | train | epoch 074 | loss 3.783 | nll_loss 2.099 | ppl 4.28 | wps 15950.9 | ups 3 | wpb 5324.5 | bsz 212.4 | num_updates 4144 | lr 0.000196494 | gnorm 1.87 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:47:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=75/shard_epoch=70
2020-10-13 06:47:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=75/shard_epoch=71
2020-10-13 06:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10212.26953125Mb; avail=234329.90234375Mb
2020-10-13 06:47:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000268
2020-10-13 06:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002643
2020-10-13 06:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10212.26953125Mb; avail=234329.90234375Mb
2020-10-13 06:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000094
2020-10-13 06:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10212.26953125Mb; avail=234329.90234375Mb
2020-10-13 06:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041227
2020-10-13 06:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044736
2020-10-13 06:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10212.26953125Mb; avail=234329.90234375Mb
2020-10-13 06:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10212.26953125Mb; avail=234329.90234375Mb
2020-10-13 06:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002024
2020-10-13 06:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10212.26953125Mb; avail=234329.90234375Mb
2020-10-13 06:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000084
2020-10-13 06:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10212.26953125Mb; avail=234329.90234375Mb
2020-10-13 06:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040388
2020-10-13 06:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043223
2020-10-13 06:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10212.26953125Mb; avail=234329.90234375Mb
2020-10-13 06:47:27 | INFO | fairseq.trainer | begin training epoch 75
2020-10-13 06:47:42 | INFO | train_inner | epoch 075:     56 / 56 loss=3.738, nll_loss=2.047, ppl=4.13, wps=17607.3, ups=3.28, wpb=5367.7, bsz=215.6, num_updates=4200, lr=0.00019518, gnorm=1.85, clip=0, train_wall=26, wall=0
2020-10-13 06:47:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7475.58203125Mb; avail=237078.7109375Mb
2020-10-13 06:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000873
2020-10-13 06:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.58203125Mb; avail=237078.7109375Mb
2020-10-13 06:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017310
2020-10-13 06:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.57421875Mb; avail=237078.71875Mb
2020-10-13 06:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013236
2020-10-13 06:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032167
2020-10-13 06:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.56640625Mb; avail=237078.7265625Mb
2020-10-13 06:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7475.55859375Mb; avail=237078.734375Mb
2020-10-13 06:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000742
2020-10-13 06:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.55859375Mb; avail=237078.734375Mb
2020-10-13 06:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017064
2020-10-13 06:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.55859375Mb; avail=237078.734375Mb
2020-10-13 06:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013025
2020-10-13 06:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031556
2020-10-13 06:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7475.55859375Mb; avail=237078.734375Mb
2020-10-13 06:47:43 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 8.55 | nll_loss 7.335 | ppl 161.49 | wps 50838.3 | wpb 2112 | bsz 83.9 | num_updates 4200 | best_loss 7.611
2020-10-13 06:47:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:47:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 75 @ 4200 updates, score 8.55) (writing took 2.3786820601671934 seconds)
2020-10-13 06:47:45 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2020-10-13 06:47:45 | INFO | train | epoch 075 | loss 3.698 | nll_loss 2 | ppl 4 | wps 16142 | ups 3.03 | wpb 5324.5 | bsz 212.4 | num_updates 4200 | lr 0.00019518 | gnorm 1.84 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:47:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=76/shard_epoch=71
2020-10-13 06:47:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=76/shard_epoch=72
2020-10-13 06:47:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7476.1484375Mb; avail=237078.484375Mb
2020-10-13 06:47:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000275
2020-10-13 06:47:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002623
2020-10-13 06:47:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7476.1484375Mb; avail=237078.484375Mb
2020-10-13 06:47:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000093
2020-10-13 06:47:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7476.1484375Mb; avail=237078.484375Mb
2020-10-13 06:47:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040504
2020-10-13 06:47:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043975
2020-10-13 06:47:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7476.1484375Mb; avail=237078.484375Mb
2020-10-13 06:47:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7476.1484375Mb; avail=237078.484375Mb
2020-10-13 06:47:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001978
2020-10-13 06:47:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7476.1484375Mb; avail=237078.484375Mb
2020-10-13 06:47:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 06:47:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7476.1484375Mb; avail=237078.484375Mb
2020-10-13 06:47:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040828
2020-10-13 06:47:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043602
2020-10-13 06:47:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7476.1484375Mb; avail=237078.484375Mb
2020-10-13 06:47:45 | INFO | fairseq.trainer | begin training epoch 76
2020-10-13 06:48:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:48:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9219.7890625Mb; avail=235324.0078125Mb
2020-10-13 06:48:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000878
2020-10-13 06:48:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9221.0Mb; avail=235323.40234375Mb
2020-10-13 06:48:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017185
2020-10-13 06:48:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9222.81640625Mb; avail=235321.5859375Mb
2020-10-13 06:48:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013428
2020-10-13 06:48:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032291
2020-10-13 06:48:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9231.53515625Mb; avail=235312.8671875Mb
2020-10-13 06:48:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9257.125Mb; avail=235287.27734375Mb
2020-10-13 06:48:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000779
2020-10-13 06:48:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9258.3359375Mb; avail=235286.06640625Mb
2020-10-13 06:48:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017276
2020-10-13 06:48:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9264.6328125Mb; avail=235279.76953125Mb
2020-10-13 06:48:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013226
2020-10-13 06:48:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032094
2020-10-13 06:48:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9274.3203125Mb; avail=235270.08203125Mb
2020-10-13 06:48:02 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 8.588 | nll_loss 7.376 | ppl 166.14 | wps 50140.1 | wpb 2112 | bsz 83.9 | num_updates 4256 | best_loss 7.611
2020-10-13 06:48:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:48:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 76 @ 4256 updates, score 8.588) (writing took 2.386967479949817 seconds)
2020-10-13 06:48:04 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2020-10-13 06:48:04 | INFO | train | epoch 076 | loss 3.633 | nll_loss 1.923 | ppl 3.79 | wps 15866.7 | ups 2.98 | wpb 5324.5 | bsz 212.4 | num_updates 4256 | lr 0.000193892 | gnorm 1.755 | clip 0 | train_wall 15 | wall 0
2020-10-13 06:48:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=77/shard_epoch=72
2020-10-13 06:48:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=77/shard_epoch=73
2020-10-13 06:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10199.02734375Mb; avail=234343.18359375Mb
2020-10-13 06:48:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000273
2020-10-13 06:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002687
2020-10-13 06:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10199.02734375Mb; avail=234343.18359375Mb
2020-10-13 06:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000091
2020-10-13 06:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10199.02734375Mb; avail=234343.18359375Mb
2020-10-13 06:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040670
2020-10-13 06:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044172
2020-10-13 06:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10199.23046875Mb; avail=234342.98046875Mb
2020-10-13 06:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10199.01953125Mb; avail=234343.5625Mb
2020-10-13 06:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001930
2020-10-13 06:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10199.01953125Mb; avail=234343.5625Mb
2020-10-13 06:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000084
2020-10-13 06:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10199.01953125Mb; avail=234343.5625Mb
2020-10-13 06:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040281
2020-10-13 06:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043050
2020-10-13 06:48:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10199.328125Mb; avail=234343.15234375Mb
2020-10-13 06:48:04 | INFO | fairseq.trainer | begin training epoch 77
2020-10-13 06:48:17 | INFO | train_inner | epoch 077:     44 / 56 loss=3.618, nll_loss=1.903, ppl=3.74, wps=15449.5, ups=2.88, wpb=5358.4, bsz=215.4, num_updates=4300, lr=0.000192897, gnorm=1.843, clip=0, train_wall=26, wall=0
2020-10-13 06:48:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10126.88671875Mb; avail=234415.25Mb
2020-10-13 06:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000838
2020-10-13 06:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10126.88671875Mb; avail=234415.25Mb
2020-10-13 06:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017773
2020-10-13 06:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10126.88671875Mb; avail=234415.25Mb
2020-10-13 06:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013078
2020-10-13 06:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032464
2020-10-13 06:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10126.88671875Mb; avail=234415.25Mb
2020-10-13 06:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10126.88671875Mb; avail=234415.25Mb
2020-10-13 06:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000704
2020-10-13 06:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10126.88671875Mb; avail=234415.25Mb
2020-10-13 06:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017328
2020-10-13 06:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10126.88671875Mb; avail=234415.25Mb
2020-10-13 06:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013136
2020-10-13 06:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.031914
2020-10-13 06:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10126.88671875Mb; avail=234415.25Mb
2020-10-13 06:48:21 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 8.613 | nll_loss 7.414 | ppl 170.51 | wps 50538.2 | wpb 2112 | bsz 83.9 | num_updates 4312 | best_loss 7.611
2020-10-13 06:48:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:48:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 77 @ 4312 updates, score 8.613) (writing took 2.355644282884896 seconds)
2020-10-13 06:48:23 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2020-10-13 06:48:23 | INFO | train | epoch 077 | loss 3.602 | nll_loss 1.882 | ppl 3.69 | wps 15621.6 | ups 2.93 | wpb 5324.5 | bsz 212.4 | num_updates 4312 | lr 0.000192629 | gnorm 1.937 | clip 0 | train_wall 15 | wall 0
2020-10-13 06:48:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=78/shard_epoch=73
2020-10-13 06:48:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=78/shard_epoch=74
2020-10-13 06:48:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10132.20703125Mb; avail=234410.6796875Mb
2020-10-13 06:48:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000276
2020-10-13 06:48:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002692
2020-10-13 06:48:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10132.20703125Mb; avail=234410.6796875Mb
2020-10-13 06:48:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000091
2020-10-13 06:48:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10132.70703125Mb; avail=234410.1796875Mb
2020-10-13 06:48:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041409
2020-10-13 06:48:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044946
2020-10-13 06:48:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10137.42578125Mb; avail=234405.4609375Mb
2020-10-13 06:48:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10137.42578125Mb; avail=234405.4609375Mb
2020-10-13 06:48:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002039
2020-10-13 06:48:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10127.58203125Mb; avail=234415.3046875Mb
2020-10-13 06:48:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000092
2020-10-13 06:48:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10127.58203125Mb; avail=234415.3046875Mb
2020-10-13 06:48:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040317
2020-10-13 06:48:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043181
2020-10-13 06:48:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10127.6953125Mb; avail=234415.19140625Mb
2020-10-13 06:48:23 | INFO | fairseq.trainer | begin training epoch 78
2020-10-13 06:48:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10126.38671875Mb; avail=234415.5703125Mb
2020-10-13 06:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000831
2020-10-13 06:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10126.38671875Mb; avail=234415.5703125Mb
2020-10-13 06:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017328
2020-10-13 06:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10126.38671875Mb; avail=234415.5703125Mb
2020-10-13 06:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013161
2020-10-13 06:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032072
2020-10-13 06:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10126.38671875Mb; avail=234415.5703125Mb
2020-10-13 06:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10127.83984375Mb; avail=234414.1171875Mb
2020-10-13 06:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000703
2020-10-13 06:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10127.83984375Mb; avail=234414.1171875Mb
2020-10-13 06:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017474
2020-10-13 06:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10130.26171875Mb; avail=234411.6953125Mb
2020-10-13 06:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013099
2020-10-13 06:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032005
2020-10-13 06:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10131.47265625Mb; avail=234410.484375Mb
2020-10-13 06:48:40 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 8.632 | nll_loss 7.433 | ppl 172.78 | wps 50986.1 | wpb 2112 | bsz 83.9 | num_updates 4368 | best_loss 7.611
2020-10-13 06:48:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:48:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 78 @ 4368 updates, score 8.632) (writing took 2.413037099177018 seconds)
2020-10-13 06:48:42 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2020-10-13 06:48:42 | INFO | train | epoch 078 | loss 3.525 | nll_loss 1.792 | ppl 3.46 | wps 15843.4 | ups 2.98 | wpb 5324.5 | bsz 212.4 | num_updates 4368 | lr 0.00019139 | gnorm 1.818 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:48:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=79/shard_epoch=74
2020-10-13 06:48:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=79/shard_epoch=75
2020-10-13 06:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10127.7734375Mb; avail=234414.42578125Mb
2020-10-13 06:48:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000276
2020-10-13 06:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002739
2020-10-13 06:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10127.7734375Mb; avail=234414.42578125Mb
2020-10-13 06:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000113
2020-10-13 06:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10127.7734375Mb; avail=234414.42578125Mb
2020-10-13 06:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040968
2020-10-13 06:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044602
2020-10-13 06:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10132.00390625Mb; avail=234410.1953125Mb
2020-10-13 06:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10133.8125Mb; avail=234408.38671875Mb
2020-10-13 06:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001974
2020-10-13 06:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10133.8125Mb; avail=234408.38671875Mb
2020-10-13 06:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000105
2020-10-13 06:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10133.8125Mb; avail=234408.38671875Mb
2020-10-13 06:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.043862
2020-10-13 06:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.046740
2020-10-13 06:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10137.83984375Mb; avail=234404.359375Mb
2020-10-13 06:48:42 | INFO | fairseq.trainer | begin training epoch 79
2020-10-13 06:48:51 | INFO | train_inner | epoch 079:     32 / 56 loss=3.508, nll_loss=1.771, ppl=3.41, wps=15598.6, ups=2.92, wpb=5335.7, bsz=210.3, num_updates=4400, lr=0.000190693, gnorm=1.806, clip=0, train_wall=26, wall=0
2020-10-13 06:48:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10126.51171875Mb; avail=234415.71875Mb
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000829
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10126.51171875Mb; avail=234415.71875Mb
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017224
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10126.51171875Mb; avail=234415.71875Mb
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013441
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032259
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10126.51171875Mb; avail=234415.71875Mb
2020-10-13 06:48:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10126.51171875Mb; avail=234415.71875Mb
2020-10-13 06:48:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000714
2020-10-13 06:48:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10126.51171875Mb; avail=234415.71875Mb
2020-10-13 06:48:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017666
2020-10-13 06:48:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10126.51171875Mb; avail=234415.71875Mb
2020-10-13 06:48:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013297
2020-10-13 06:48:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032432
2020-10-13 06:48:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10126.51171875Mb; avail=234415.71875Mb
2020-10-13 06:48:58 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 8.709 | nll_loss 7.487 | ppl 179.35 | wps 50774.2 | wpb 2112 | bsz 83.9 | num_updates 4424 | best_loss 7.611
2020-10-13 06:48:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:49:01 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 79 @ 4424 updates, score 8.709) (writing took 2.37142754602246 seconds)
2020-10-13 06:49:01 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2020-10-13 06:49:01 | INFO | train | epoch 079 | loss 3.448 | nll_loss 1.701 | ppl 3.25 | wps 15995.9 | ups 3 | wpb 5324.5 | bsz 212.4 | num_updates 4424 | lr 0.000190175 | gnorm 1.786 | clip 0 | train_wall 14 | wall 0
2020-10-13 06:49:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=80/shard_epoch=75
2020-10-13 06:49:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=80/shard_epoch=76
2020-10-13 06:49:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10130.18359375Mb; avail=234412.01953125Mb
2020-10-13 06:49:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000298
2020-10-13 06:49:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002687
2020-10-13 06:49:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10130.18359375Mb; avail=234412.01953125Mb
2020-10-13 06:49:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000099
2020-10-13 06:49:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10130.18359375Mb; avail=234412.01953125Mb
2020-10-13 06:49:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040627
2020-10-13 06:49:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.044150
2020-10-13 06:49:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10130.18359375Mb; avail=234412.01953125Mb
2020-10-13 06:49:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10130.18359375Mb; avail=234412.01953125Mb
2020-10-13 06:49:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001972
2020-10-13 06:49:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10130.18359375Mb; avail=234412.01953125Mb
2020-10-13 06:49:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000088
2020-10-13 06:49:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10130.18359375Mb; avail=234412.01953125Mb
2020-10-13 06:49:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.040265
2020-10-13 06:49:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.043104
2020-10-13 06:49:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10130.18359375Mb; avail=234412.01953125Mb
2020-10-13 06:49:01 | INFO | fairseq.trainer | begin training epoch 80
2020-10-13 06:49:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10126.44140625Mb; avail=234415.8359375Mb
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000839
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10126.44140625Mb; avail=234415.8359375Mb
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017456
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10128.0Mb; avail=234414.27734375Mb
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013234
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032321
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10129.2109375Mb; avail=234413.06640625Mb
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10131.02734375Mb; avail=234411.25Mb
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000715
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10131.02734375Mb; avail=234411.25Mb
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.017243
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10133.328125Mb; avail=234408.94921875Mb
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.013359
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032068
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10136.87109375Mb; avail=234405.40625Mb
2020-10-13 06:49:17 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 8.758 | nll_loss 7.535 | ppl 185.52 | wps 50501.5 | wpb 2112 | bsz 83.9 | num_updates 4480 | best_loss 7.611
2020-10-13 06:49:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:49:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azeazp_sepspm8000/M2O/checkpoint_last.pt (epoch 80 @ 4480 updates, score 8.758) (writing took 2.3682566250208765 seconds)
2020-10-13 06:49:19 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2020-10-13 06:49:19 | INFO | train | epoch 080 | loss 3.376 | nll_loss 1.615 | ppl 3.06 | wps 15894.9 | ups 2.99 | wpb 5324.5 | bsz 212.4 | num_updates 4480 | lr 0.000188982 | gnorm 1.676 | clip 0 | train_wall 15 | wall 0
2020-10-13 06:49:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=81/shard_epoch=76
2020-10-13 06:49:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=81/shard_epoch=77
2020-10-13 06:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10127.8125Mb; avail=234414.390625Mb
2020-10-13 06:49:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000275
2020-10-13 06:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002688
2020-10-13 06:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10127.8125Mb; avail=234414.390625Mb
2020-10-13 06:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000099
2020-10-13 06:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10127.8125Mb; avail=234414.390625Mb
2020-10-13 06:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.041710
2020-10-13 06:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.045237
2020-10-13 06:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10127.8125Mb; avail=234414.390625Mb
2020-10-13 06:49:20 | INFO | fairseq_cli.train | done training in 1480.5 seconds
