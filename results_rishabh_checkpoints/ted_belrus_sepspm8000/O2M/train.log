2020-10-12 06:34:53 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_belrus_sepspm8000/O2M/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok='tgt', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='eng-bel,eng-rus', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_belrus_sepspm8000/O2M/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 06:34:53 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 06:34:53 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['bel', 'eng', 'rus']
2020-10-12 06:34:53 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bel] dictionary: 21851 types
2020-10-12 06:34:53 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 21851 types
2020-10-12 06:34:53 | INFO | fairseq.data.multilingual.multilingual_data_manager | [rus] dictionary: 21851 types
2020-10-12 06:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 06:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=4077.6875Mb; avail=240488.8515625Mb
2020-10-12 06:34:53 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 06:34:53 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:eng-bel': 1, 'main:eng-rus': 1}
2020-10-12 06:34:53 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-bel src_langtok: 21848; tgt_langtok: None
2020-10-12 06:34:53 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belrus_sepspm8000/O2M/valid.eng-bel.eng
2020-10-12 06:34:53 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belrus_sepspm8000/O2M/valid.eng-bel.bel
2020-10-12 06:34:53 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrus_sepspm8000/O2M/ valid eng-bel 248 examples
2020-10-12 06:34:53 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-rus src_langtok: 21850; tgt_langtok: None
2020-10-12 06:34:53 | INFO | fairseq.data.data_utils | loaded 4814 examples from: fairseq/data-bin/ted_belrus_sepspm8000/O2M/valid.eng-rus.eng
2020-10-12 06:34:53 | INFO | fairseq.data.data_utils | loaded 4814 examples from: fairseq/data-bin/ted_belrus_sepspm8000/O2M/valid.eng-rus.rus
2020-10-12 06:34:53 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrus_sepspm8000/O2M/ valid eng-rus 4814 examples
2020-10-12 06:34:54 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21851, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21851, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=21851, bias=False)
  )
)
2020-10-12 06:34:54 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 06:34:54 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 06:34:54 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 06:34:54 | INFO | fairseq_cli.train | num. model params: 42731008 (num. trained: 42731008)
2020-10-12 06:34:58 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 06:34:58 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 06:34:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 06:34:58 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 06:34:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 06:34:58 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 06:34:58 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 06:34:58 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_last.pt
2020-10-12 06:34:58 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 06:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 06:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6363.70703125Mb; avail=238192.71484375Mb
2020-10-12 06:34:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-12 06:34:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:eng-bel': 1, 'main:eng-rus': 1}
2020-10-12 06:34:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-bel src_langtok: 21848; tgt_langtok: None
2020-10-12 06:34:58 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belrus_sepspm8000/O2M/train.eng-bel.eng
2020-10-12 06:34:58 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belrus_sepspm8000/O2M/train.eng-bel.bel
2020-10-12 06:34:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrus_sepspm8000/O2M/ train eng-bel 4509 examples
2020-10-12 06:34:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-rus src_langtok: 21850; tgt_langtok: None
2020-10-12 06:34:58 | INFO | fairseq.data.data_utils | loaded 208397 examples from: fairseq/data-bin/ted_belrus_sepspm8000/O2M/train.eng-rus.eng
2020-10-12 06:34:58 | INFO | fairseq.data.data_utils | loaded 208397 examples from: fairseq/data-bin/ted_belrus_sepspm8000/O2M/train.eng-rus.rus
2020-10-12 06:34:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrus_sepspm8000/O2M/ train eng-rus 208397 examples
2020-10-12 06:34:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:eng-bel', 4509), ('main:eng-rus', 208397)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 06:34:58 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 06:34:58 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 212906
2020-10-12 06:34:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 212906; virtual dataset size 212906
2020-10-12 06:34:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 06:34:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:eng-bel': 4509, 'main:eng-rus': 208397}; raw total size: 212906
2020-10-12 06:34:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:eng-bel': 4509, 'main:eng-rus': 208397}; resampled total size: 212906
2020-10-12 06:34:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 06:34:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.024968
2020-10-12 06:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6367.13671875Mb; avail=238189.28515625Mb
2020-10-12 06:34:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004746
2020-10-12 06:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.045618
2020-10-12 06:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6372.16015625Mb; avail=238184.3515625Mb
2020-10-12 06:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002480
2020-10-12 06:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6372.16015625Mb; avail=238184.3515625Mb
2020-10-12 06:34:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.805213
2020-10-12 06:34:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.854239
2020-10-12 06:34:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6392.875Mb; avail=238164.37109375Mb
2020-10-12 06:34:59 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 06:34:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6389.5625Mb; avail=238167.68359375Mb
2020-10-12 06:34:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035007
2020-10-12 06:34:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6379.71875Mb; avail=238177.52734375Mb
2020-10-12 06:34:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001676
2020-10-12 06:34:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6379.71875Mb; avail=238177.52734375Mb
2020-10-12 06:35:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.811856
2020-10-12 06:35:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.849365
2020-10-12 06:35:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6384.921875Mb; avail=238171.59765625Mb
2020-10-12 06:35:00 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 06:35:29 | INFO | train_inner | epoch 001:    100 / 764 loss=14.301, nll_loss=14.206, ppl=18892.5, wps=24952, ups=3.44, wpb=7258.5, bsz=269.4, num_updates=100, lr=5.0975e-06, gnorm=3.828, clip=0, train_wall=29, wall=32
2020-10-12 06:35:58 | INFO | train_inner | epoch 001:    200 / 764 loss=13.025, nll_loss=12.78, ppl=7034.1, wps=25231.2, ups=3.45, wpb=7318, bsz=296, num_updates=200, lr=1.0095e-05, gnorm=1.476, clip=0, train_wall=28, wall=61
2020-10-12 06:36:27 | INFO | train_inner | epoch 001:    300 / 764 loss=12.378, nll_loss=12.062, ppl=4275.22, wps=24670.8, ups=3.47, wpb=7117.2, bsz=277.2, num_updates=300, lr=1.50925e-05, gnorm=1.267, clip=0, train_wall=28, wall=89
2020-10-12 06:36:56 | INFO | train_inner | epoch 001:    400 / 764 loss=11.627, nll_loss=11.206, ppl=2363.09, wps=24249.2, ups=3.45, wpb=7021.9, bsz=285.5, num_updates=400, lr=2.009e-05, gnorm=1.614, clip=0, train_wall=28, wall=118
2020-10-12 06:37:25 | INFO | train_inner | epoch 001:    500 / 764 loss=11.184, nll_loss=10.675, ppl=1635.39, wps=25147.1, ups=3.41, wpb=7365.6, bsz=241.3, num_updates=500, lr=2.50875e-05, gnorm=1.178, clip=0, train_wall=28, wall=148
2020-10-12 06:37:55 | INFO | train_inner | epoch 001:    600 / 764 loss=10.834, nll_loss=10.241, ppl=1209.91, wps=25207.3, ups=3.42, wpb=7364, bsz=305, num_updates=600, lr=3.0085e-05, gnorm=1.317, clip=0, train_wall=28, wall=177
2020-10-12 06:38:24 | INFO | train_inner | epoch 001:    700 / 764 loss=10.685, nll_loss=10.047, ppl=1057.84, wps=25145.3, ups=3.43, wpb=7329.2, bsz=275, num_updates=700, lr=3.50825e-05, gnorm=1.252, clip=0, train_wall=28, wall=206
2020-10-12 06:38:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 06:38:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6623.65234375Mb; avail=237930.4609375Mb
2020-10-12 06:38:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001318
2020-10-12 06:38:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6623.65234375Mb; avail=237930.4609375Mb
2020-10-12 06:38:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068755
2020-10-12 06:38:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6623.625Mb; avail=237930.703125Mb
2020-10-12 06:38:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050207
2020-10-12 06:38:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121117
2020-10-12 06:38:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6623.50390625Mb; avail=237930.9296875Mb
2020-10-12 06:38:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6623.67578125Mb; avail=237930.4453125Mb
2020-10-12 06:38:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001254
2020-10-12 06:38:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6623.67578125Mb; avail=237930.4453125Mb
2020-10-12 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067533
2020-10-12 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6623.67578125Mb; avail=237930.4453125Mb
2020-10-12 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049852
2020-10-12 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119444
2020-10-12 06:38:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6623.67578125Mb; avail=237930.4453125Mb
/home/ubuntu/courses/fairseq/fairseq/utils.py:340: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2020-10-12 06:38:45 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.399 | nll_loss 9.684 | ppl 822.6 | wps 55212.2 | wpb 2283.9 | bsz 87.3 | num_updates 764
2020-10-12 06:38:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 06:38:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 1 @ 764 updates, score 10.399) (writing took 1.6787356911227107 seconds)
2020-10-12 06:38:47 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 06:38:47 | INFO | train | epoch 001 | loss 11.885 | nll_loss 11.461 | ppl 2819.14 | wps 24425.3 | ups 3.37 | wpb 7242.1 | bsz 278.7 | num_updates 764 | lr 3.82809e-05 | gnorm 1.687 | clip 0 | train_wall 216 | wall 229
2020-10-12 06:38:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 06:38:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6478.82421875Mb; avail=238075.6015625Mb
2020-10-12 06:38:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004853
2020-10-12 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044030
2020-10-12 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6484.484375Mb; avail=238069.94140625Mb
2020-10-12 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001580
2020-10-12 06:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6484.484375Mb; avail=238069.94140625Mb
2020-10-12 06:38:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.739800
2020-10-12 06:38:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.786231
2020-10-12 06:38:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6491.58203125Mb; avail=238062.8984375Mb
2020-10-12 06:38:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6486.66015625Mb; avail=238067.8203125Mb
2020-10-12 06:38:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033638
2020-10-12 06:38:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6487.22265625Mb; avail=238067.2578125Mb
2020-10-12 06:38:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001548
2020-10-12 06:38:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6487.22265625Mb; avail=238067.2578125Mb
2020-10-12 06:38:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.741563
2020-10-12 06:38:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.777561
2020-10-12 06:38:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6498.4765625Mb; avail=238056.00390625Mb
2020-10-12 06:38:48 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 06:38:59 | INFO | train_inner | epoch 002:     36 / 764 loss=10.598, nll_loss=9.938, ppl=980.63, wps=20325.5, ups=2.85, wpb=7138.5, bsz=266.4, num_updates=800, lr=4.008e-05, gnorm=1.294, clip=0, train_wall=28, wall=241
2020-10-12 06:39:28 | INFO | train_inner | epoch 002:    136 / 764 loss=10.472, nll_loss=9.792, ppl=886.55, wps=25053.7, ups=3.41, wpb=7352.3, bsz=295.8, num_updates=900, lr=4.50775e-05, gnorm=1.253, clip=0, train_wall=29, wall=270
2020-10-12 06:39:57 | INFO | train_inner | epoch 002:    236 / 764 loss=10.412, nll_loss=9.723, ppl=845.19, wps=25101.2, ups=3.47, wpb=7239.9, bsz=273.5, num_updates=1000, lr=5.0075e-05, gnorm=1.126, clip=0, train_wall=28, wall=299
2020-10-12 06:40:26 | INFO | train_inner | epoch 002:    336 / 764 loss=10.31, nll_loss=9.606, ppl=779.33, wps=25263.4, ups=3.41, wpb=7406.3, bsz=301.7, num_updates=1100, lr=5.50725e-05, gnorm=1.271, clip=0, train_wall=29, wall=329
2020-10-12 06:40:55 | INFO | train_inner | epoch 002:    436 / 764 loss=10.25, nll_loss=9.537, ppl=743.06, wps=24806.3, ups=3.45, wpb=7191.1, bsz=252.2, num_updates=1200, lr=6.007e-05, gnorm=1.121, clip=0, train_wall=28, wall=358
2020-10-12 06:41:24 | INFO | train_inner | epoch 002:    536 / 764 loss=10.101, nll_loss=9.367, ppl=660.42, wps=24519.4, ups=3.45, wpb=7097.3, bsz=288.7, num_updates=1300, lr=6.50675e-05, gnorm=1.209, clip=0, train_wall=28, wall=387
2020-10-12 06:41:53 | INFO | train_inner | epoch 002:    636 / 764 loss=10.007, nll_loss=9.257, ppl=611.8, wps=24708.9, ups=3.46, wpb=7140.7, bsz=291.5, num_updates=1400, lr=7.0065e-05, gnorm=1.399, clip=0, train_wall=28, wall=415
2020-10-12 06:42:22 | INFO | train_inner | epoch 002:    736 / 764 loss=9.982, nll_loss=9.227, ppl=599.27, wps=25069.5, ups=3.44, wpb=7285.8, bsz=258.9, num_updates=1500, lr=7.50625e-05, gnorm=1.236, clip=0, train_wall=28, wall=445
2020-10-12 06:42:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 06:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6580.55078125Mb; avail=237973.765625Mb
2020-10-12 06:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001369
2020-10-12 06:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6581.15625Mb; avail=237973.16015625Mb
2020-10-12 06:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068817
2020-10-12 06:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6591.99609375Mb; avail=237962.3203125Mb
2020-10-12 06:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050211
2020-10-12 06:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121251
2020-10-12 06:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6586.23046875Mb; avail=237968.0859375Mb
2020-10-12 06:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6577.4765625Mb; avail=237976.83984375Mb
2020-10-12 06:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001211
2020-10-12 06:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6577.4765625Mb; avail=237976.83984375Mb
2020-10-12 06:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067864
2020-10-12 06:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6577.4765625Mb; avail=237976.83984375Mb
2020-10-12 06:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050504
2020-10-12 06:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120354
2020-10-12 06:42:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6577.4765625Mb; avail=237976.83984375Mb
2020-10-12 06:42:33 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.58 | nll_loss 8.753 | ppl 431.55 | wps 55075.9 | wpb 2283.9 | bsz 87.3 | num_updates 1528 | best_loss 9.58
2020-10-12 06:42:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 06:42:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 2 @ 1528 updates, score 9.58) (writing took 4.7789219280239195 seconds)
2020-10-12 06:42:38 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 06:42:38 | INFO | train | epoch 002 | loss 10.224 | nll_loss 9.507 | ppl 727.78 | wps 23912.3 | ups 3.3 | wpb 7242.1 | bsz 278.7 | num_updates 1528 | lr 7.64618e-05 | gnorm 1.218 | clip 0 | train_wall 216 | wall 460
2020-10-12 06:42:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 06:42:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 06:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6499.75390625Mb; avail=238054.2890625Mb
2020-10-12 06:42:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004144
2020-10-12 06:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043128
2020-10-12 06:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6502.75390625Mb; avail=238051.2890625Mb
2020-10-12 06:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001547
2020-10-12 06:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6502.75390625Mb; avail=238051.2890625Mb
2020-10-12 06:42:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.746036
2020-10-12 06:42:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.791547
2020-10-12 06:42:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6509.734375Mb; avail=238044.765625Mb
2020-10-12 06:42:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6504.8125Mb; avail=238049.6875Mb
2020-10-12 06:42:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032828
2020-10-12 06:42:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6504.8125Mb; avail=238049.6875Mb
2020-10-12 06:42:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001553
2020-10-12 06:42:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6504.8125Mb; avail=238049.6875Mb
2020-10-12 06:42:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.742472
2020-10-12 06:42:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.777680
2020-10-12 06:42:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6510.9375Mb; avail=238043.5546875Mb
2020-10-12 06:42:40 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 06:43:01 | INFO | train_inner | epoch 003:     72 / 764 loss=9.845, nll_loss=9.071, ppl=537.92, wps=18623.2, ups=2.61, wpb=7139.8, bsz=253.3, num_updates=1600, lr=8.006e-05, gnorm=1.252, clip=0, train_wall=28, wall=483
2020-10-12 06:43:30 | INFO | train_inner | epoch 003:    172 / 764 loss=9.628, nll_loss=8.824, ppl=453.1, wps=24708.2, ups=3.47, wpb=7123.3, bsz=284.9, num_updates=1700, lr=8.50575e-05, gnorm=1.364, clip=0, train_wall=28, wall=512
2020-10-12 06:43:59 | INFO | train_inner | epoch 003:    272 / 764 loss=9.584, nll_loss=8.772, ppl=437.22, wps=25305.1, ups=3.43, wpb=7368.2, bsz=295.4, num_updates=1800, lr=9.0055e-05, gnorm=1.284, clip=0, train_wall=28, wall=541
2020-10-12 06:44:28 | INFO | train_inner | epoch 003:    372 / 764 loss=9.512, nll_loss=8.688, ppl=412.29, wps=25310.5, ups=3.44, wpb=7362.7, bsz=261.6, num_updates=1900, lr=9.50525e-05, gnorm=1.217, clip=0, train_wall=28, wall=570
2020-10-12 06:44:57 | INFO | train_inner | epoch 003:    472 / 764 loss=9.409, nll_loss=8.569, ppl=379.65, wps=24978.8, ups=3.47, wpb=7203.2, bsz=266.2, num_updates=2000, lr=0.00010005, gnorm=1.252, clip=0, train_wall=28, wall=599
2020-10-12 06:45:26 | INFO | train_inner | epoch 003:    572 / 764 loss=9.296, nll_loss=8.437, ppl=346.61, wps=25121, ups=3.42, wpb=7343.2, bsz=280.4, num_updates=2100, lr=0.000105048, gnorm=1.278, clip=0, train_wall=28, wall=628
2020-10-12 06:45:55 | INFO | train_inner | epoch 003:    672 / 764 loss=9.157, nll_loss=8.278, ppl=310.41, wps=24641, ups=3.45, wpb=7141.1, bsz=290.6, num_updates=2200, lr=0.000110045, gnorm=1.206, clip=0, train_wall=28, wall=657
2020-10-12 06:46:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 06:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6594.453125Mb; avail=237959.7890625Mb
2020-10-12 06:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001353
2020-10-12 06:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6594.453125Mb; avail=237959.7890625Mb
2020-10-12 06:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068848
2020-10-12 06:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6594.4296875Mb; avail=237959.8125Mb
2020-10-12 06:46:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049774
2020-10-12 06:46:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120771
2020-10-12 06:46:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6594.4296875Mb; avail=237959.8125Mb
2020-10-12 06:46:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6594.4296875Mb; avail=237959.8125Mb
2020-10-12 06:46:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001183
2020-10-12 06:46:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6594.4296875Mb; avail=237959.8125Mb
2020-10-12 06:46:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068063
2020-10-12 06:46:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6594.4296875Mb; avail=237959.8125Mb
2020-10-12 06:46:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050299
2020-10-12 06:46:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120300
2020-10-12 06:46:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6594.4296875Mb; avail=237959.8125Mb
2020-10-12 06:46:24 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.832 | nll_loss 7.889 | ppl 236.97 | wps 55535.5 | wpb 2283.9 | bsz 87.3 | num_updates 2292 | best_loss 8.832
2020-10-12 06:46:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 06:46:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 3 @ 2292 updates, score 8.832) (writing took 4.794713529990986 seconds)
2020-10-12 06:46:29 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 06:46:29 | INFO | train | epoch 003 | loss 9.43 | nll_loss 8.593 | ppl 386.19 | wps 23969.9 | ups 3.31 | wpb 7242.1 | bsz 278.7 | num_updates 2292 | lr 0.000114643 | gnorm 1.266 | clip 0 | train_wall 216 | wall 691
2020-10-12 06:46:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 06:46:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 06:46:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6572.671875Mb; avail=237981.515625Mb
2020-10-12 06:46:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004117
2020-10-12 06:46:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.042907
2020-10-12 06:46:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6573.8671875Mb; avail=237980.3203125Mb
2020-10-12 06:46:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001550
2020-10-12 06:46:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6573.8671875Mb; avail=237980.3203125Mb
2020-10-12 06:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.743646
2020-10-12 06:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.788948
2020-10-12 06:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6584.08203125Mb; avail=237970.57421875Mb
2020-10-12 06:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6584.98046875Mb; avail=237969.67578125Mb
2020-10-12 06:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033365
2020-10-12 06:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6584.984375Mb; avail=237969.671875Mb
2020-10-12 06:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001562
2020-10-12 06:46:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6584.984375Mb; avail=237969.671875Mb
2020-10-12 06:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.742191
2020-10-12 06:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.777964
2020-10-12 06:46:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6581.3671875Mb; avail=237973.2890625Mb
2020-10-12 06:46:31 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 06:46:33 | INFO | train_inner | epoch 004:      8 / 764 loss=9.119, nll_loss=8.232, ppl=300.74, wps=18927.8, ups=2.61, wpb=7243.7, bsz=290.7, num_updates=2300, lr=0.000115043, gnorm=1.273, clip=0, train_wall=28, wall=695
2020-10-12 06:47:02 | INFO | train_inner | epoch 004:    108 / 764 loss=8.992, nll_loss=8.087, ppl=271.89, wps=24801.1, ups=3.46, wpb=7178, bsz=296.5, num_updates=2400, lr=0.00012004, gnorm=1.297, clip=0, train_wall=28, wall=724
2020-10-12 06:47:31 | INFO | train_inner | epoch 004:    208 / 764 loss=8.974, nll_loss=8.064, ppl=267.65, wps=24527.9, ups=3.44, wpb=7124.4, bsz=264.4, num_updates=2500, lr=0.000125037, gnorm=1.143, clip=0, train_wall=28, wall=753
2020-10-12 06:48:00 | INFO | train_inner | epoch 004:    308 / 764 loss=8.917, nll_loss=7.999, ppl=255.81, wps=25138.2, ups=3.44, wpb=7314.2, bsz=267.2, num_updates=2600, lr=0.000130035, gnorm=1.158, clip=0, train_wall=28, wall=782
2020-10-12 06:48:29 | INFO | train_inner | epoch 004:    408 / 764 loss=8.765, nll_loss=7.825, ppl=226.82, wps=24966.7, ups=3.43, wpb=7288.4, bsz=289.4, num_updates=2700, lr=0.000135032, gnorm=1.211, clip=0, train_wall=28, wall=812
2020-10-12 06:48:58 | INFO | train_inner | epoch 004:    508 / 764 loss=8.736, nll_loss=7.792, ppl=221.56, wps=25283.6, ups=3.46, wpb=7303.6, bsz=262, num_updates=2800, lr=0.00014003, gnorm=1.156, clip=0, train_wall=28, wall=840
2020-10-12 06:49:27 | INFO | train_inner | epoch 004:    608 / 764 loss=8.595, nll_loss=7.632, ppl=198.3, wps=24743.7, ups=3.42, wpb=7237.5, bsz=300.1, num_updates=2900, lr=0.000145028, gnorm=1.212, clip=0, train_wall=28, wall=870
2020-10-12 06:49:57 | INFO | train_inner | epoch 004:    708 / 764 loss=8.558, nll_loss=7.587, ppl=192.25, wps=25288.9, ups=3.43, wpb=7365.6, bsz=269, num_updates=3000, lr=0.000150025, gnorm=1.11, clip=0, train_wall=28, wall=899
2020-10-12 06:50:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6672.328125Mb; avail=237881.8828125Mb
2020-10-12 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001317
2020-10-12 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6672.328125Mb; avail=237881.8828125Mb
2020-10-12 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068963
2020-10-12 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6672.328125Mb; avail=237881.8828125Mb
2020-10-12 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050344
2020-10-12 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121396
2020-10-12 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6672.328125Mb; avail=237881.8828125Mb
2020-10-12 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6672.328125Mb; avail=237881.8828125Mb
2020-10-12 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001184
2020-10-12 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6672.328125Mb; avail=237881.8828125Mb
2020-10-12 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069432
2020-10-12 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6672.328125Mb; avail=237881.8828125Mb
2020-10-12 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049946
2020-10-12 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121321
2020-10-12 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6672.328125Mb; avail=237881.8828125Mb
2020-10-12 06:50:15 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.131 | nll_loss 7.061 | ppl 133.48 | wps 55836.9 | wpb 2283.9 | bsz 87.3 | num_updates 3056 | best_loss 8.131
2020-10-12 06:50:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 06:50:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 4 @ 3056 updates, score 8.131) (writing took 4.786332843126729 seconds)
2020-10-12 06:50:20 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 06:50:20 | INFO | train | epoch 004 | loss 8.768 | nll_loss 7.828 | ppl 227.3 | wps 23933.3 | ups 3.3 | wpb 7242.1 | bsz 278.7 | num_updates 3056 | lr 0.000152824 | gnorm 1.188 | clip 0 | train_wall 216 | wall 922
2020-10-12 06:50:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 06:50:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 06:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6592.34375Mb; avail=237961.89453125Mb
2020-10-12 06:50:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005037
2020-10-12 06:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044586
2020-10-12 06:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6598.23828125Mb; avail=237956.0Mb
2020-10-12 06:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001624
2020-10-12 06:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6598.23828125Mb; avail=237956.0Mb
2020-10-12 06:50:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.744648
2020-10-12 06:50:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.791662
2020-10-12 06:50:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6604.81640625Mb; avail=237949.69140625Mb
2020-10-12 06:50:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6603.53515625Mb; avail=237950.97265625Mb
2020-10-12 06:50:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033552
2020-10-12 06:50:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6608.8203125Mb; avail=237945.6875Mb
2020-10-12 06:50:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001567
2020-10-12 06:50:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6608.8203125Mb; avail=237945.6875Mb
2020-10-12 06:50:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.749951
2020-10-12 06:50:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.785885
2020-10-12 06:50:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6604.49609375Mb; avail=237950.046875Mb
2020-10-12 06:50:22 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 06:50:35 | INFO | train_inner | epoch 005:     44 / 764 loss=8.404, nll_loss=7.412, ppl=170.33, wps=18827.4, ups=2.62, wpb=7182.4, bsz=290.5, num_updates=3100, lr=0.000155023, gnorm=1.214, clip=0, train_wall=28, wall=937
2020-10-12 06:51:04 | INFO | train_inner | epoch 005:    144 / 764 loss=8.286, nll_loss=7.278, ppl=155.18, wps=24525.9, ups=3.47, wpb=7066, bsz=293, num_updates=3200, lr=0.00016002, gnorm=1.213, clip=0, train_wall=28, wall=966
2020-10-12 06:51:33 | INFO | train_inner | epoch 005:    244 / 764 loss=8.24, nll_loss=7.224, ppl=149.48, wps=25200.4, ups=3.42, wpb=7367.4, bsz=288.8, num_updates=3300, lr=0.000165018, gnorm=1.226, clip=0, train_wall=28, wall=995
2020-10-12 06:52:02 | INFO | train_inner | epoch 005:    344 / 764 loss=8.142, nll_loss=7.111, ppl=138.24, wps=25049, ups=3.45, wpb=7266.8, bsz=286.8, num_updates=3400, lr=0.000170015, gnorm=1.225, clip=0, train_wall=28, wall=1024
2020-10-12 06:52:31 | INFO | train_inner | epoch 005:    444 / 764 loss=8.111, nll_loss=7.074, ppl=134.7, wps=24924.1, ups=3.44, wpb=7255.1, bsz=256.6, num_updates=3500, lr=0.000175013, gnorm=1.214, clip=0, train_wall=28, wall=1053
2020-10-12 06:53:00 | INFO | train_inner | epoch 005:    544 / 764 loss=7.988, nll_loss=6.933, ppl=122.15, wps=24881.4, ups=3.44, wpb=7223.6, bsz=262.1, num_updates=3600, lr=0.00018001, gnorm=1.273, clip=0, train_wall=28, wall=1082
2020-10-12 06:53:29 | INFO | train_inner | epoch 005:    644 / 764 loss=7.85, nll_loss=6.775, ppl=109.53, wps=25194.5, ups=3.47, wpb=7268.4, bsz=299, num_updates=3700, lr=0.000185008, gnorm=1.254, clip=0, train_wall=28, wall=1111
2020-10-12 06:53:58 | INFO | train_inner | epoch 005:    744 / 764 loss=7.829, nll_loss=6.749, ppl=107.54, wps=24854, ups=3.46, wpb=7179.3, bsz=269.6, num_updates=3800, lr=0.000190005, gnorm=1.258, clip=0, train_wall=28, wall=1140
2020-10-12 06:54:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 06:54:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6606.6953125Mb; avail=237947.6640625Mb
2020-10-12 06:54:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001402
2020-10-12 06:54:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6606.6953125Mb; avail=237947.6640625Mb
2020-10-12 06:54:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068551
2020-10-12 06:54:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6606.6953125Mb; avail=237947.6640625Mb
2020-10-12 06:54:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051278
2020-10-12 06:54:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122041
2020-10-12 06:54:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6606.6953125Mb; avail=237947.6640625Mb
2020-10-12 06:54:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6606.6953125Mb; avail=237947.6640625Mb
2020-10-12 06:54:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001221
2020-10-12 06:54:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6606.6953125Mb; avail=237947.6640625Mb
2020-10-12 06:54:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069088
2020-10-12 06:54:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6606.6953125Mb; avail=237947.6640625Mb
2020-10-12 06:54:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051558
2020-10-12 06:54:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122740
2020-10-12 06:54:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6606.6953125Mb; avail=237947.6640625Mb
2020-10-12 06:54:06 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.484 | nll_loss 6.3 | ppl 78.78 | wps 55758.5 | wpb 2283.9 | bsz 87.3 | num_updates 3820 | best_loss 7.484
2020-10-12 06:54:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 06:54:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 5 @ 3820 updates, score 7.484) (writing took 4.785874665947631 seconds)
2020-10-12 06:54:11 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 06:54:11 | INFO | train | epoch 005 | loss 8.077 | nll_loss 7.036 | ppl 131.25 | wps 23964.6 | ups 3.31 | wpb 7242.1 | bsz 278.7 | num_updates 3820 | lr 0.000191005 | gnorm 1.236 | clip 0 | train_wall 216 | wall 1153
2020-10-12 06:54:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 06:54:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 06:54:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6522.5Mb; avail=238031.9609375Mb
2020-10-12 06:54:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004152
2020-10-12 06:54:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043307
2020-10-12 06:54:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6525.49609375Mb; avail=238028.96484375Mb
2020-10-12 06:54:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001553
2020-10-12 06:54:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6525.49609375Mb; avail=238028.96484375Mb
2020-10-12 06:54:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.739512
2020-10-12 06:54:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.785190
2020-10-12 06:54:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6531.796875Mb; avail=238022.69921875Mb
2020-10-12 06:54:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6526.875Mb; avail=238027.62109375Mb
2020-10-12 06:54:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032778
2020-10-12 06:54:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6529.90234375Mb; avail=238024.59375Mb
2020-10-12 06:54:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001558
2020-10-12 06:54:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6530.5078125Mb; avail=238023.98828125Mb
2020-10-12 06:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.739948
2020-10-12 06:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.775126
2020-10-12 06:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6533.69921875Mb; avail=238020.796875Mb
2020-10-12 06:54:13 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 06:54:36 | INFO | train_inner | epoch 006:     80 / 764 loss=7.726, nll_loss=6.632, ppl=99.21, wps=18808.7, ups=2.63, wpb=7148, bsz=272.3, num_updates=3900, lr=0.000195003, gnorm=1.285, clip=0, train_wall=28, wall=1178
2020-10-12 06:55:05 | INFO | train_inner | epoch 006:    180 / 764 loss=7.604, nll_loss=6.492, ppl=90, wps=25410, ups=3.43, wpb=7412.3, bsz=281.3, num_updates=4000, lr=0.0002, gnorm=1.153, clip=0, train_wall=28, wall=1207
2020-10-12 06:55:34 | INFO | train_inner | epoch 006:    280 / 764 loss=7.494, nll_loss=6.366, ppl=82.46, wps=24338.9, ups=3.47, wpb=7006.4, bsz=280.1, num_updates=4100, lr=0.000197546, gnorm=1.252, clip=0, train_wall=28, wall=1236
2020-10-12 06:56:02 | INFO | train_inner | epoch 006:    380 / 764 loss=7.431, nll_loss=6.292, ppl=78.34, wps=24771.8, ups=3.47, wpb=7131.6, bsz=277.7, num_updates=4200, lr=0.00019518, gnorm=1.194, clip=0, train_wall=28, wall=1265
2020-10-12 06:56:31 | INFO | train_inner | epoch 006:    480 / 764 loss=7.384, nll_loss=6.237, ppl=75.41, wps=25101, ups=3.44, wpb=7298.5, bsz=275, num_updates=4300, lr=0.000192897, gnorm=1.16, clip=0, train_wall=28, wall=1294
2020-10-12 06:57:01 | INFO | train_inner | epoch 006:    580 / 764 loss=7.307, nll_loss=6.149, ppl=70.94, wps=24809.8, ups=3.44, wpb=7210.1, bsz=277.5, num_updates=4400, lr=0.000190693, gnorm=1.24, clip=0, train_wall=28, wall=1323
2020-10-12 06:57:30 | INFO | train_inner | epoch 006:    680 / 764 loss=7.263, nll_loss=6.097, ppl=68.45, wps=25493.8, ups=3.42, wpb=7457.4, bsz=275.7, num_updates=4500, lr=0.000188562, gnorm=1.157, clip=0, train_wall=28, wall=1352
2020-10-12 06:57:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 06:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6591.98046875Mb; avail=237962.05859375Mb
2020-10-12 06:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001262
2020-10-12 06:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6591.98046875Mb; avail=237962.05859375Mb
2020-10-12 06:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067952
2020-10-12 06:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6592.0859375Mb; avail=237961.953125Mb
2020-10-12 06:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048853
2020-10-12 06:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118881
2020-10-12 06:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6592.0859375Mb; avail=237961.953125Mb
2020-10-12 06:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6592.0859375Mb; avail=237961.953125Mb
2020-10-12 06:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001118
2020-10-12 06:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6592.0859375Mb; avail=237961.953125Mb
2020-10-12 06:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067230
2020-10-12 06:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6592.28515625Mb; avail=237961.55078125Mb
2020-10-12 06:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049473
2020-10-12 06:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118572
2020-10-12 06:57:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6592.28515625Mb; avail=237961.55078125Mb
2020-10-12 06:57:57 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.913 | nll_loss 5.618 | ppl 49.1 | wps 55805.9 | wpb 2283.9 | bsz 87.3 | num_updates 4584 | best_loss 6.913
2020-10-12 06:57:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 06:58:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 6 @ 4584 updates, score 6.913) (writing took 4.798521374119446 seconds)
2020-10-12 06:58:02 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 06:58:02 | INFO | train | epoch 006 | loss 7.416 | nll_loss 6.274 | ppl 77.4 | wps 23992.8 | ups 3.31 | wpb 7242.1 | bsz 278.7 | num_updates 4584 | lr 0.000186826 | gnorm 1.198 | clip 0 | train_wall 215 | wall 1384
2020-10-12 06:58:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 06:58:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 06:58:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6511.57421875Mb; avail=238042.046875Mb
2020-10-12 06:58:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003492
2020-10-12 06:58:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044792
2020-10-12 06:58:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6517.34765625Mb; avail=238037.0078125Mb
2020-10-12 06:58:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001719
2020-10-12 06:58:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6517.34765625Mb; avail=238037.0078125Mb
2020-10-12 06:58:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.739580
2020-10-12 06:58:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.787025
2020-10-12 06:58:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6511.55859375Mb; avail=238042.7109375Mb
2020-10-12 06:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6511.55859375Mb; avail=238042.7109375Mb
2020-10-12 06:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033213
2020-10-12 06:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6511.55859375Mb; avail=238042.7109375Mb
2020-10-12 06:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001655
2020-10-12 06:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6511.55859375Mb; avail=238042.7109375Mb
2020-10-12 06:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.737332
2020-10-12 06:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.773067
2020-10-12 06:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6510.55859375Mb; avail=238043.921875Mb
2020-10-12 06:58:03 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 06:58:08 | INFO | train_inner | epoch 007:     16 / 764 loss=7.169, nll_loss=5.989, ppl=63.53, wps=19058.9, ups=2.62, wpb=7262.7, bsz=278.2, num_updates=4600, lr=0.000186501, gnorm=1.161, clip=0, train_wall=28, wall=1390
2020-10-12 06:58:37 | INFO | train_inner | epoch 007:    116 / 764 loss=7.084, nll_loss=5.893, ppl=59.42, wps=24940.2, ups=3.45, wpb=7238.4, bsz=268, num_updates=4700, lr=0.000184506, gnorm=1.215, clip=0, train_wall=28, wall=1419
2020-10-12 06:59:06 | INFO | train_inner | epoch 007:    216 / 764 loss=7.026, nll_loss=5.825, ppl=56.69, wps=25325.3, ups=3.4, wpb=7439, bsz=279, num_updates=4800, lr=0.000182574, gnorm=1.186, clip=0, train_wall=29, wall=1449
2020-10-12 06:59:36 | INFO | train_inner | epoch 007:    316 / 764 loss=6.94, nll_loss=5.727, ppl=52.96, wps=24621.9, ups=3.42, wpb=7198.1, bsz=281.7, num_updates=4900, lr=0.000180702, gnorm=1.18, clip=0, train_wall=28, wall=1478
2020-10-12 07:00:05 | INFO | train_inner | epoch 007:    416 / 764 loss=6.918, nll_loss=5.7, ppl=51.97, wps=25211.4, ups=3.44, wpb=7327, bsz=273.9, num_updates=5000, lr=0.000178885, gnorm=1.178, clip=0, train_wall=28, wall=1507
2020-10-12 07:00:33 | INFO | train_inner | epoch 007:    516 / 764 loss=6.822, nll_loss=5.592, ppl=48.23, wps=24637.2, ups=3.48, wpb=7081.5, bsz=307.5, num_updates=5100, lr=0.000177123, gnorm=1.224, clip=0, train_wall=28, wall=1536
2020-10-12 07:01:02 | INFO | train_inner | epoch 007:    616 / 764 loss=6.803, nll_loss=5.568, ppl=47.43, wps=25021.4, ups=3.47, wpb=7220.2, bsz=279.8, num_updates=5200, lr=0.000175412, gnorm=1.093, clip=0, train_wall=28, wall=1564
2020-10-12 07:01:31 | INFO | train_inner | epoch 007:    716 / 764 loss=6.807, nll_loss=5.572, ppl=47.56, wps=25147, ups=3.47, wpb=7237.4, bsz=264.1, num_updates=5300, lr=0.000173749, gnorm=1.161, clip=0, train_wall=28, wall=1593
2020-10-12 07:01:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:01:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6597.06640625Mb; avail=237957.21875Mb
2020-10-12 07:01:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001392
2020-10-12 07:01:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6597.06640625Mb; avail=237957.21875Mb
2020-10-12 07:01:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068711
2020-10-12 07:01:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6598.76953125Mb; avail=237955.515625Mb
2020-10-12 07:01:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050160
2020-10-12 07:01:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121099
2020-10-12 07:01:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6603.0078125Mb; avail=237951.27734375Mb
2020-10-12 07:01:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6607.16015625Mb; avail=237947.125Mb
2020-10-12 07:01:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001157
2020-10-12 07:01:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6607.16015625Mb; avail=237947.125Mb
2020-10-12 07:01:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068435
2020-10-12 07:01:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6598.40234375Mb; avail=237955.8828125Mb
2020-10-12 07:01:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050423
2020-10-12 07:01:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120852
2020-10-12 07:01:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6598.40234375Mb; avail=237955.8828125Mb
2020-10-12 07:01:48 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.517 | nll_loss 5.139 | ppl 35.23 | wps 55816.7 | wpb 2283.9 | bsz 87.3 | num_updates 5348 | best_loss 6.517
2020-10-12 07:01:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:01:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 7 @ 5348 updates, score 6.517) (writing took 4.779158161953092 seconds)
2020-10-12 07:01:53 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 07:01:53 | INFO | train | epoch 007 | loss 6.908 | nll_loss 5.689 | ppl 51.58 | wps 23962.7 | ups 3.31 | wpb 7242.1 | bsz 278.7 | num_updates 5348 | lr 0.000172967 | gnorm 1.174 | clip 0 | train_wall 216 | wall 1615
2020-10-12 07:01:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 07:01:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 07:01:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6523.4140625Mb; avail=238030.765625Mb
2020-10-12 07:01:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003775
2020-10-12 07:01:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.048013
2020-10-12 07:01:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6524.00390625Mb; avail=238030.17578125Mb
2020-10-12 07:01:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001691
2020-10-12 07:01:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6524.00390625Mb; avail=238030.17578125Mb
2020-10-12 07:01:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.754569
2020-10-12 07:01:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.805163
2020-10-12 07:01:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6531.25390625Mb; avail=238022.92578125Mb
2020-10-12 07:01:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6526.33203125Mb; avail=238027.84765625Mb
2020-10-12 07:01:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033169
2020-10-12 07:01:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6523.828125Mb; avail=238030.6328125Mb
2020-10-12 07:01:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001575
2020-10-12 07:01:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6523.828125Mb; avail=238030.6328125Mb
2020-10-12 07:01:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.772473
2020-10-12 07:01:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.808073
2020-10-12 07:01:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6530.10546875Mb; avail=238024.35546875Mb
2020-10-12 07:01:54 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 07:02:09 | INFO | train_inner | epoch 008:     52 / 764 loss=6.693, nll_loss=5.443, ppl=43.5, wps=18580.5, ups=2.6, wpb=7133.2, bsz=273.3, num_updates=5400, lr=0.000172133, gnorm=1.158, clip=0, train_wall=28, wall=1632
2020-10-12 07:02:38 | INFO | train_inner | epoch 008:    152 / 764 loss=6.637, nll_loss=5.378, ppl=41.59, wps=24818.7, ups=3.45, wpb=7190.1, bsz=279.2, num_updates=5500, lr=0.000170561, gnorm=1.186, clip=0, train_wall=28, wall=1661
2020-10-12 07:03:07 | INFO | train_inner | epoch 008:    252 / 764 loss=6.616, nll_loss=5.353, ppl=40.87, wps=24813, ups=3.44, wpb=7209.5, bsz=266.3, num_updates=5600, lr=0.000169031, gnorm=1.079, clip=0, train_wall=28, wall=1690
2020-10-12 07:03:37 | INFO | train_inner | epoch 008:    352 / 764 loss=6.502, nll_loss=5.223, ppl=37.35, wps=24809.7, ups=3.43, wpb=7236.6, bsz=292.8, num_updates=5700, lr=0.000167542, gnorm=1.095, clip=0, train_wall=28, wall=1719
2020-10-12 07:04:06 | INFO | train_inner | epoch 008:    452 / 764 loss=6.511, nll_loss=5.23, ppl=37.54, wps=25163.7, ups=3.39, wpb=7412.6, bsz=278.6, num_updates=5800, lr=0.000166091, gnorm=1.111, clip=0, train_wall=29, wall=1748
2020-10-12 07:04:35 | INFO | train_inner | epoch 008:    552 / 764 loss=6.524, nll_loss=5.244, ppl=37.91, wps=25746.6, ups=3.44, wpb=7478.8, bsz=251.5, num_updates=5900, lr=0.000164677, gnorm=1.112, clip=0, train_wall=28, wall=1777
2020-10-12 07:05:04 | INFO | train_inner | epoch 008:    652 / 764 loss=6.355, nll_loss=5.055, ppl=33.24, wps=24788, ups=3.46, wpb=7169, bsz=311.7, num_updates=6000, lr=0.000163299, gnorm=1.085, clip=0, train_wall=28, wall=1806
2020-10-12 07:05:33 | INFO | train_inner | epoch 008:    752 / 764 loss=6.394, nll_loss=5.097, ppl=34.23, wps=24616.4, ups=3.45, wpb=7130.8, bsz=278.3, num_updates=6100, lr=0.000161955, gnorm=1.094, clip=0, train_wall=28, wall=1835
2020-10-12 07:05:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6628.41796875Mb; avail=237926.0078125Mb
2020-10-12 07:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001372
2020-10-12 07:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6628.41796875Mb; avail=237926.0078125Mb
2020-10-12 07:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069925
2020-10-12 07:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6618.47265625Mb; avail=237935.7421875Mb
2020-10-12 07:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051290
2020-10-12 07:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123389
2020-10-12 07:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6618.47265625Mb; avail=237935.7421875Mb
2020-10-12 07:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6618.47265625Mb; avail=237935.7421875Mb
2020-10-12 07:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001197
2020-10-12 07:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6618.47265625Mb; avail=237935.7421875Mb
2020-10-12 07:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070584
2020-10-12 07:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6618.47265625Mb; avail=237935.7421875Mb
2020-10-12 07:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050102
2020-10-12 07:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122683
2020-10-12 07:05:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6618.47265625Mb; avail=237935.7421875Mb
2020-10-12 07:05:39 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.255 | nll_loss 4.829 | ppl 28.41 | wps 55627.2 | wpb 2283.9 | bsz 87.3 | num_updates 6112 | best_loss 6.255
2020-10-12 07:05:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:05:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 8 @ 6112 updates, score 6.255) (writing took 4.782588406000286 seconds)
2020-10-12 07:05:44 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 07:05:44 | INFO | train | epoch 008 | loss 6.514 | nll_loss 5.236 | ppl 37.68 | wps 23903.8 | ups 3.3 | wpb 7242.1 | bsz 278.7 | num_updates 6112 | lr 0.000161796 | gnorm 1.114 | clip 0 | train_wall 216 | wall 1846
2020-10-12 07:05:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 07:05:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 07:05:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6536.546875Mb; avail=238017.6875Mb
2020-10-12 07:05:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003830
2020-10-12 07:05:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.045363
2020-10-12 07:05:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6540.15625Mb; avail=238014.078125Mb
2020-10-12 07:05:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001638
2020-10-12 07:05:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6541.00390625Mb; avail=238013.96484375Mb
2020-10-12 07:05:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.743434
2020-10-12 07:05:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.791280
2020-10-12 07:05:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6544.44140625Mb; avail=238010.015625Mb
2020-10-12 07:05:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6539.51953125Mb; avail=238014.9375Mb
2020-10-12 07:05:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032887
2020-10-12 07:05:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6539.51953125Mb; avail=238014.9375Mb
2020-10-12 07:05:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001689
2020-10-12 07:05:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6539.51953125Mb; avail=238014.9375Mb
2020-10-12 07:05:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.755244
2020-10-12 07:05:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.790636
2020-10-12 07:05:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6544.59765625Mb; avail=238009.88671875Mb
2020-10-12 07:05:46 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 07:06:11 | INFO | train_inner | epoch 009:     88 / 764 loss=6.28, nll_loss=4.968, ppl=31.29, wps=18718.8, ups=2.61, wpb=7174.1, bsz=279.1, num_updates=6200, lr=0.000160644, gnorm=1.071, clip=0, train_wall=28, wall=1874
2020-10-12 07:06:41 | INFO | train_inner | epoch 009:    188 / 764 loss=6.234, nll_loss=4.915, ppl=30.18, wps=24904.7, ups=3.42, wpb=7271.9, bsz=289.4, num_updates=6300, lr=0.000159364, gnorm=1.055, clip=0, train_wall=28, wall=1903
2020-10-12 07:07:09 | INFO | train_inner | epoch 009:    288 / 764 loss=6.266, nll_loss=4.951, ppl=30.92, wps=24948.9, ups=3.46, wpb=7208.9, bsz=258.7, num_updates=6400, lr=0.000158114, gnorm=1.101, clip=0, train_wall=28, wall=1932
2020-10-12 07:07:38 | INFO | train_inner | epoch 009:    388 / 764 loss=6.243, nll_loss=4.924, ppl=30.35, wps=25127.3, ups=3.47, wpb=7240.3, bsz=272, num_updates=6500, lr=0.000156893, gnorm=1.125, clip=0, train_wall=28, wall=1960
2020-10-12 07:08:07 | INFO | train_inner | epoch 009:    488 / 764 loss=6.217, nll_loss=4.894, ppl=29.73, wps=24829.2, ups=3.48, wpb=7136.2, bsz=284.3, num_updates=6600, lr=0.0001557, gnorm=1.166, clip=0, train_wall=28, wall=1989
2020-10-12 07:08:36 | INFO | train_inner | epoch 009:    588 / 764 loss=6.174, nll_loss=4.844, ppl=28.73, wps=24721.8, ups=3.45, wpb=7172.7, bsz=287.6, num_updates=6700, lr=0.000154533, gnorm=1.089, clip=0, train_wall=28, wall=2018
2020-10-12 07:09:05 | INFO | train_inner | epoch 009:    688 / 764 loss=6.188, nll_loss=4.86, ppl=29.05, wps=25074.7, ups=3.41, wpb=7344.4, bsz=264.3, num_updates=6800, lr=0.000153393, gnorm=1.086, clip=0, train_wall=28, wall=2047
2020-10-12 07:09:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:09:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6698.65234375Mb; avail=237855.77734375Mb
2020-10-12 07:09:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001384
2020-10-12 07:09:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6698.65234375Mb; avail=237855.77734375Mb
2020-10-12 07:09:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068739
2020-10-12 07:09:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6698.07421875Mb; avail=237856.35546875Mb
2020-10-12 07:09:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051545
2020-10-12 07:09:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122485
2020-10-12 07:09:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6698.07421875Mb; avail=237856.35546875Mb
2020-10-12 07:09:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6698.07421875Mb; avail=237856.35546875Mb
2020-10-12 07:09:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001289
2020-10-12 07:09:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6698.07421875Mb; avail=237856.35546875Mb
2020-10-12 07:09:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069337
2020-10-12 07:09:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6698.07421875Mb; avail=237856.35546875Mb
2020-10-12 07:09:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051235
2020-10-12 07:09:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122650
2020-10-12 07:09:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6698.07421875Mb; avail=237856.35546875Mb
2020-10-12 07:09:30 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.037 | nll_loss 4.57 | ppl 23.75 | wps 55852.5 | wpb 2283.9 | bsz 87.3 | num_updates 6876 | best_loss 6.037
2020-10-12 07:09:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:09:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 9 @ 6876 updates, score 6.037) (writing took 4.781023100949824 seconds)
2020-10-12 07:09:35 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 07:09:35 | INFO | train | epoch 009 | loss 6.213 | nll_loss 4.89 | ppl 29.66 | wps 23949.3 | ups 3.31 | wpb 7242.1 | bsz 278.7 | num_updates 6876 | lr 0.000152543 | gnorm 1.1 | clip 0 | train_wall 216 | wall 2077
2020-10-12 07:09:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 07:09:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 07:09:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6554.1640625Mb; avail=238000.19140625Mb
2020-10-12 07:09:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003656
2020-10-12 07:09:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044084
2020-10-12 07:09:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6554.84765625Mb; avail=237999.390625Mb
2020-10-12 07:09:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001549
2020-10-12 07:09:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6554.84765625Mb; avail=237999.390625Mb
2020-10-12 07:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.739196
2020-10-12 07:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.785660
2020-10-12 07:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6563.921875Mb; avail=237990.31640625Mb
2020-10-12 07:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6563.23828125Mb; avail=237991.0Mb
2020-10-12 07:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033210
2020-10-12 07:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6566.41796875Mb; avail=237987.8203125Mb
2020-10-12 07:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001551
2020-10-12 07:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6566.9140625Mb; avail=237987.32421875Mb
2020-10-12 07:09:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.743346
2020-10-12 07:09:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.778916
2020-10-12 07:09:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6561.43359375Mb; avail=237992.8359375Mb
2020-10-12 07:09:37 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 07:09:44 | INFO | train_inner | epoch 010:     24 / 764 loss=6.09, nll_loss=4.75, ppl=26.9, wps=19214.8, ups=2.58, wpb=7437.4, bsz=290.9, num_updates=6900, lr=0.000152277, gnorm=1.102, clip=0, train_wall=29, wall=2086
2020-10-12 07:10:13 | INFO | train_inner | epoch 010:    124 / 764 loss=6.056, nll_loss=4.711, ppl=26.2, wps=24658, ups=3.46, wpb=7125.9, bsz=268.6, num_updates=7000, lr=0.000151186, gnorm=1.109, clip=0, train_wall=28, wall=2115
2020-10-12 07:10:42 | INFO | train_inner | epoch 010:    224 / 764 loss=6.078, nll_loss=4.735, ppl=26.63, wps=25410.5, ups=3.41, wpb=7442.1, bsz=269, num_updates=7100, lr=0.000150117, gnorm=1.123, clip=0, train_wall=29, wall=2144
2020-10-12 07:11:11 | INFO | train_inner | epoch 010:    324 / 764 loss=6.027, nll_loss=4.675, ppl=25.55, wps=24957.2, ups=3.44, wpb=7247.2, bsz=268.3, num_updates=7200, lr=0.000149071, gnorm=1.068, clip=0, train_wall=28, wall=2173
2020-10-12 07:11:40 | INFO | train_inner | epoch 010:    424 / 764 loss=5.958, nll_loss=4.599, ppl=24.23, wps=24856.6, ups=3.43, wpb=7236.6, bsz=299.4, num_updates=7300, lr=0.000148047, gnorm=1.028, clip=0, train_wall=28, wall=2203
2020-10-12 07:12:10 | INFO | train_inner | epoch 010:    524 / 764 loss=5.933, nll_loss=4.569, ppl=23.74, wps=24409.2, ups=3.38, wpb=7218.5, bsz=294.4, num_updates=7400, lr=0.000147043, gnorm=1.07, clip=0, train_wall=29, wall=2232
2020-10-12 07:12:39 | INFO | train_inner | epoch 010:    624 / 764 loss=5.968, nll_loss=4.609, ppl=24.39, wps=24450.7, ups=3.41, wpb=7170, bsz=280.1, num_updates=7500, lr=0.000146059, gnorm=1.09, clip=0, train_wall=28, wall=2261
2020-10-12 07:13:08 | INFO | train_inner | epoch 010:    724 / 764 loss=5.921, nll_loss=4.555, ppl=23.51, wps=24834.2, ups=3.45, wpb=7201.4, bsz=279.8, num_updates=7600, lr=0.000145095, gnorm=1.041, clip=0, train_wall=28, wall=2290
2020-10-12 07:13:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6773.3828125Mb; avail=237780.76953125Mb
2020-10-12 07:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001358
2020-10-12 07:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6773.3828125Mb; avail=237780.76953125Mb
2020-10-12 07:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068556
2020-10-12 07:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6773.98828125Mb; avail=237780.1640625Mb
2020-10-12 07:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049157
2020-10-12 07:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119856
2020-10-12 07:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6783.53125Mb; avail=237770.62109375Mb
2020-10-12 07:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6790.74609375Mb; avail=237763.40625Mb
2020-10-12 07:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001277
2020-10-12 07:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6791.3515625Mb; avail=237762.80078125Mb
2020-10-12 07:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067558
2020-10-12 07:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6786.734375Mb; avail=237767.41796875Mb
2020-10-12 07:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049404
2020-10-12 07:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119019
2020-10-12 07:13:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6777.1171875Mb; avail=237777.03515625Mb
2020-10-12 07:13:23 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.865 | nll_loss 4.371 | ppl 20.7 | wps 55630.5 | wpb 2283.9 | bsz 87.3 | num_updates 7640 | best_loss 5.865
2020-10-12 07:13:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:13:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 10 @ 7640 updates, score 5.865) (writing took 8.488236773060635 seconds)
2020-10-12 07:13:31 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 07:13:31 | INFO | train | epoch 010 | loss 5.991 | nll_loss 4.636 | ppl 24.86 | wps 23424.7 | ups 3.23 | wpb 7242.1 | bsz 278.7 | num_updates 7640 | lr 0.000144715 | gnorm 1.075 | clip 0 | train_wall 217 | wall 2313
2020-10-12 07:13:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 07:13:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 07:13:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6932.7734375Mb; avail=237621.671875Mb
2020-10-12 07:13:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003505
2020-10-12 07:13:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044120
2020-10-12 07:13:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6942.34375Mb; avail=237612.1015625Mb
2020-10-12 07:13:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001599
2020-10-12 07:13:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6942.94921875Mb; avail=237611.49609375Mb
2020-10-12 07:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.740124
2020-10-12 07:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.786741
2020-10-12 07:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6939.359375Mb; avail=237615.5546875Mb
2020-10-12 07:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6934.515625Mb; avail=237620.890625Mb
2020-10-12 07:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033247
2020-10-12 07:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6934.046875Mb; avail=237620.8671875Mb
2020-10-12 07:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001558
2020-10-12 07:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6934.5703125Mb; avail=237620.34375Mb
2020-10-12 07:13:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.745067
2020-10-12 07:13:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.780732
2020-10-12 07:13:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6939.01953125Mb; avail=237615.8984375Mb
2020-10-12 07:13:33 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 07:13:50 | INFO | train_inner | epoch 011:     60 / 764 loss=5.914, nll_loss=4.547, ppl=23.38, wps=17309.3, ups=2.37, wpb=7310.3, bsz=257.6, num_updates=7700, lr=0.00014415, gnorm=1.069, clip=0, train_wall=28, wall=2333
2020-10-12 07:14:19 | INFO | train_inner | epoch 011:    160 / 764 loss=5.802, nll_loss=4.42, ppl=21.4, wps=25053.9, ups=3.45, wpb=7263.9, bsz=289.8, num_updates=7800, lr=0.000143223, gnorm=1.028, clip=0, train_wall=28, wall=2362
2020-10-12 07:14:49 | INFO | train_inner | epoch 011:    260 / 764 loss=5.81, nll_loss=4.429, ppl=21.54, wps=25034.3, ups=3.38, wpb=7404.4, bsz=296.7, num_updates=7900, lr=0.000142314, gnorm=1.076, clip=0, train_wall=29, wall=2391
2020-10-12 07:15:19 | INFO | train_inner | epoch 011:    360 / 764 loss=5.809, nll_loss=4.427, ppl=21.51, wps=24651.2, ups=3.36, wpb=7329.5, bsz=278.1, num_updates=8000, lr=0.000141421, gnorm=1.021, clip=0, train_wall=29, wall=2421
2020-10-12 07:15:48 | INFO | train_inner | epoch 011:    460 / 764 loss=5.844, nll_loss=4.467, ppl=22.11, wps=24699.1, ups=3.47, wpb=7125.4, bsz=262.2, num_updates=8100, lr=0.000140546, gnorm=1.07, clip=0, train_wall=28, wall=2450
2020-10-12 07:16:17 | INFO | train_inner | epoch 011:    560 / 764 loss=5.819, nll_loss=4.438, ppl=21.67, wps=24741.1, ups=3.42, wpb=7231.1, bsz=267.5, num_updates=8200, lr=0.000139686, gnorm=1.058, clip=0, train_wall=28, wall=2479
2020-10-12 07:16:46 | INFO | train_inner | epoch 011:    660 / 764 loss=5.743, nll_loss=4.353, ppl=20.43, wps=24443.1, ups=3.47, wpb=7051.3, bsz=292.2, num_updates=8300, lr=0.000138842, gnorm=1.072, clip=0, train_wall=28, wall=2508
2020-10-12 07:17:14 | INFO | train_inner | epoch 011:    760 / 764 loss=5.78, nll_loss=4.395, ppl=21.03, wps=24971.6, ups=3.47, wpb=7197.8, bsz=273.9, num_updates=8400, lr=0.000138013, gnorm=1.014, clip=0, train_wall=28, wall=2537
2020-10-12 07:17:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7090.9921875Mb; avail=237463.78125Mb
2020-10-12 07:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001382
2020-10-12 07:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.9921875Mb; avail=237463.78125Mb
2020-10-12 07:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067716
2020-10-12 07:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.08984375Mb; avail=237463.68359375Mb
2020-10-12 07:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050467
2020-10-12 07:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120447
2020-10-12 07:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.51171875Mb; avail=237461.26171875Mb
2020-10-12 07:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7095.93359375Mb; avail=237458.83984375Mb
2020-10-12 07:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001328
2020-10-12 07:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.93359375Mb; avail=237458.83984375Mb
2020-10-12 07:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067939
2020-10-12 07:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.25Mb; avail=237454.0Mb
2020-10-12 07:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050380
2020-10-12 07:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120458
2020-10-12 07:17:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.8984375Mb; avail=237463.3515625Mb
2020-10-12 07:17:18 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 5.739 | nll_loss 4.22 | ppl 18.64 | wps 55583.3 | wpb 2283.9 | bsz 87.3 | num_updates 8404 | best_loss 5.739
2020-10-12 07:17:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:17:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 11 @ 8404 updates, score 5.739) (writing took 4.791256631957367 seconds)
2020-10-12 07:17:23 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 07:17:23 | INFO | train | epoch 011 | loss 5.808 | nll_loss 4.426 | ppl 21.5 | wps 23851.4 | ups 3.29 | wpb 7242.1 | bsz 278.7 | num_updates 8404 | lr 0.00013798 | gnorm 1.049 | clip 0 | train_wall 216 | wall 2545
2020-10-12 07:17:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 07:17:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7016.08984375Mb; avail=237538.88671875Mb
2020-10-12 07:17:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004168
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.042418
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7022.09765625Mb; avail=237532.87890625Mb
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001574
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7022.09765625Mb; avail=237532.87890625Mb
2020-10-12 07:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.742584
2020-10-12 07:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.787427
2020-10-12 07:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7031.8125Mb; avail=237522.53515625Mb
2020-10-12 07:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7027.3828125Mb; avail=237526.96484375Mb
2020-10-12 07:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033650
2020-10-12 07:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7027.453125Mb; avail=237526.89453125Mb
2020-10-12 07:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001572
2020-10-12 07:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7027.453125Mb; avail=237526.89453125Mb
2020-10-12 07:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.744615
2020-10-12 07:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.780656
2020-10-12 07:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7033.35546875Mb; avail=237520.9921875Mb
2020-10-12 07:17:25 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 07:17:53 | INFO | train_inner | epoch 012:     96 / 764 loss=5.7, nll_loss=4.303, ppl=19.74, wps=18861.5, ups=2.61, wpb=7237.5, bsz=275, num_updates=8500, lr=0.000137199, gnorm=1.051, clip=0, train_wall=28, wall=2575
2020-10-12 07:18:22 | INFO | train_inner | epoch 012:    196 / 764 loss=5.702, nll_loss=4.305, ppl=19.76, wps=24855.5, ups=3.44, wpb=7222.4, bsz=263.2, num_updates=8600, lr=0.000136399, gnorm=1.062, clip=0, train_wall=28, wall=2604
2020-10-12 07:18:51 | INFO | train_inner | epoch 012:    296 / 764 loss=5.647, nll_loss=4.242, ppl=18.92, wps=24517.5, ups=3.41, wpb=7187.2, bsz=276.2, num_updates=8700, lr=0.000135613, gnorm=1.011, clip=0, train_wall=29, wall=2633
2020-10-12 07:19:20 | INFO | train_inner | epoch 012:    396 / 764 loss=5.676, nll_loss=4.276, ppl=19.38, wps=24679.3, ups=3.43, wpb=7184.8, bsz=271.8, num_updates=8800, lr=0.00013484, gnorm=1.025, clip=0, train_wall=28, wall=2663
2020-10-12 07:19:50 | INFO | train_inner | epoch 012:    496 / 764 loss=5.643, nll_loss=4.238, ppl=18.87, wps=25433.9, ups=3.42, wpb=7429.8, bsz=294, num_updates=8900, lr=0.00013408, gnorm=0.991, clip=0, train_wall=28, wall=2692
2020-10-12 07:20:19 | INFO | train_inner | epoch 012:    596 / 764 loss=5.68, nll_loss=4.28, ppl=19.43, wps=24855.6, ups=3.43, wpb=7241.2, bsz=262.6, num_updates=9000, lr=0.000133333, gnorm=1.033, clip=0, train_wall=28, wall=2721
2020-10-12 07:20:48 | INFO | train_inner | epoch 012:    696 / 764 loss=5.572, nll_loss=4.158, ppl=17.85, wps=24858.1, ups=3.44, wpb=7225, bsz=306.8, num_updates=9100, lr=0.000132599, gnorm=0.992, clip=0, train_wall=28, wall=2750
2020-10-12 07:21:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7109.28515625Mb; avail=237445.3203125Mb
2020-10-12 07:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001392
2020-10-12 07:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7109.28515625Mb; avail=237445.3203125Mb
2020-10-12 07:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069127
2020-10-12 07:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.74609375Mb; avail=237447.52734375Mb
2020-10-12 07:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050436
2020-10-12 07:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121769
2020-10-12 07:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7110.87109375Mb; avail=237443.40234375Mb
2020-10-12 07:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7113.171875Mb; avail=237441.1015625Mb
2020-10-12 07:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001244
2020-10-12 07:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.98828125Mb; avail=237439.28515625Mb
2020-10-12 07:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069380
2020-10-12 07:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.8515625Mb; avail=237447.421875Mb
2020-10-12 07:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050030
2020-10-12 07:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121437
2020-10-12 07:21:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.8515625Mb; avail=237447.421875Mb
2020-10-12 07:21:10 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.631 | nll_loss 4.096 | ppl 17.1 | wps 55708.5 | wpb 2283.9 | bsz 87.3 | num_updates 9168 | best_loss 5.631
2020-10-12 07:21:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:21:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 12 @ 9168 updates, score 5.631) (writing took 4.785892085870728 seconds)
2020-10-12 07:21:15 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 07:21:15 | INFO | train | epoch 012 | loss 5.658 | nll_loss 4.255 | ppl 19.09 | wps 23862.4 | ups 3.29 | wpb 7242.1 | bsz 278.7 | num_updates 9168 | lr 0.000132106 | gnorm 1.023 | clip 0 | train_wall 216 | wall 2777
2020-10-12 07:21:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 07:21:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 07:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7013.421875Mb; avail=237540.83984375Mb
2020-10-12 07:21:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005057
2020-10-12 07:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044683
2020-10-12 07:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7022.50390625Mb; avail=237531.7578125Mb
2020-10-12 07:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001582
2020-10-12 07:21:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7022.50390625Mb; avail=237531.7578125Mb
2020-10-12 07:21:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.737177
2020-10-12 07:21:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.784275
2020-10-12 07:21:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7024.3515625Mb; avail=237530.1328125Mb
2020-10-12 07:21:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7018.9375Mb; avail=237535.546875Mb
2020-10-12 07:21:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032940
2020-10-12 07:21:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7018.9375Mb; avail=237535.546875Mb
2020-10-12 07:21:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001551
2020-10-12 07:21:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7018.9375Mb; avail=237535.546875Mb
2020-10-12 07:21:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.739729
2020-10-12 07:21:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.775044
2020-10-12 07:21:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7034.51171875Mb; avail=237519.828125Mb
2020-10-12 07:21:17 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 07:21:26 | INFO | train_inner | epoch 013:     32 / 764 loss=5.601, nll_loss=4.191, ppl=18.27, wps=18766.8, ups=2.61, wpb=7198.6, bsz=284.6, num_updates=9200, lr=0.000131876, gnorm=1.011, clip=0, train_wall=28, wall=2788
2020-10-12 07:21:55 | INFO | train_inner | epoch 013:    132 / 764 loss=5.538, nll_loss=4.119, ppl=17.37, wps=25187.3, ups=3.44, wpb=7318, bsz=279, num_updates=9300, lr=0.000131165, gnorm=0.996, clip=0, train_wall=28, wall=2817
2020-10-12 07:22:24 | INFO | train_inner | epoch 013:    232 / 764 loss=5.59, nll_loss=4.177, ppl=18.09, wps=24480.7, ups=3.49, wpb=7013.6, bsz=259.9, num_updates=9400, lr=0.000130466, gnorm=1.037, clip=0, train_wall=28, wall=2846
2020-10-12 07:22:53 | INFO | train_inner | epoch 013:    332 / 764 loss=5.514, nll_loss=4.091, ppl=17.05, wps=25276.4, ups=3.41, wpb=7412.5, bsz=293.1, num_updates=9500, lr=0.000129777, gnorm=0.996, clip=0, train_wall=29, wall=2875
2020-10-12 07:23:22 | INFO | train_inner | epoch 013:    432 / 764 loss=5.526, nll_loss=4.104, ppl=17.2, wps=24938.6, ups=3.43, wpb=7263.9, bsz=283.1, num_updates=9600, lr=0.000129099, gnorm=1.041, clip=0, train_wall=28, wall=2904
2020-10-12 07:23:51 | INFO | train_inner | epoch 013:    532 / 764 loss=5.514, nll_loss=4.091, ppl=17.04, wps=24638.6, ups=3.45, wpb=7143.6, bsz=278.3, num_updates=9700, lr=0.000128432, gnorm=1.006, clip=0, train_wall=28, wall=2933
2020-10-12 07:24:20 | INFO | train_inner | epoch 013:    632 / 764 loss=5.542, nll_loss=4.122, ppl=17.41, wps=24900.1, ups=3.44, wpb=7248.9, bsz=264.2, num_updates=9800, lr=0.000127775, gnorm=1.006, clip=0, train_wall=28, wall=2963
2020-10-12 07:24:49 | INFO | train_inner | epoch 013:    732 / 764 loss=5.517, nll_loss=4.095, ppl=17.08, wps=24743.9, ups=3.44, wpb=7195.4, bsz=275.1, num_updates=9900, lr=0.000127128, gnorm=1.023, clip=0, train_wall=28, wall=2992
2020-10-12 07:24:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7127.41015625Mb; avail=237426.56640625Mb
2020-10-12 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001413
2020-10-12 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7127.41015625Mb; avail=237426.56640625Mb
2020-10-12 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070259
2020-10-12 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7127.41015625Mb; avail=237426.56640625Mb
2020-10-12 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050320
2020-10-12 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122788
2020-10-12 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7127.41015625Mb; avail=237426.56640625Mb
2020-10-12 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7127.41015625Mb; avail=237426.56640625Mb
2020-10-12 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001259
2020-10-12 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7127.41015625Mb; avail=237426.56640625Mb
2020-10-12 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070131
2020-10-12 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7127.41015625Mb; avail=237426.56640625Mb
2020-10-12 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050278
2020-10-12 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122430
2020-10-12 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7127.41015625Mb; avail=237426.56640625Mb
2020-10-12 07:25:02 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 5.547 | nll_loss 4.01 | ppl 16.11 | wps 55716.3 | wpb 2283.9 | bsz 87.3 | num_updates 9932 | best_loss 5.547
2020-10-12 07:25:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:25:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 13 @ 9932 updates, score 5.547) (writing took 4.797792376950383 seconds)
2020-10-12 07:25:06 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 07:25:06 | INFO | train | epoch 013 | loss 5.532 | nll_loss 4.111 | ppl 17.28 | wps 23915.6 | ups 3.3 | wpb 7242.1 | bsz 278.7 | num_updates 9932 | lr 0.000126923 | gnorm 1.014 | clip 0 | train_wall 216 | wall 3009
2020-10-12 07:25:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 07:25:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 07:25:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7033.953125Mb; avail=237520.26171875Mb
2020-10-12 07:25:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004499
2020-10-12 07:25:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043872
2020-10-12 07:25:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7038.17578125Mb; avail=237516.0390625Mb
2020-10-12 07:25:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001559
2020-10-12 07:25:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7038.17578125Mb; avail=237516.0390625Mb
2020-10-12 07:25:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.741300
2020-10-12 07:25:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.787578
2020-10-12 07:25:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7043.2421875Mb; avail=237510.97265625Mb
2020-10-12 07:25:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7038.08203125Mb; avail=237516.015625Mb
2020-10-12 07:25:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032982
2020-10-12 07:25:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7038.1796875Mb; avail=237516.0234375Mb
2020-10-12 07:25:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001578
2020-10-12 07:25:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7038.1796875Mb; avail=237516.0234375Mb
2020-10-12 07:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.750894
2020-10-12 07:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.786305
2020-10-12 07:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7045.109375Mb; avail=237509.34375Mb
2020-10-12 07:25:08 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 07:25:28 | INFO | train_inner | epoch 014:     68 / 764 loss=5.445, nll_loss=4.013, ppl=16.15, wps=18926, ups=2.61, wpb=7245.9, bsz=297.4, num_updates=10000, lr=0.000126491, gnorm=1.004, clip=0, train_wall=28, wall=3030
2020-10-12 07:25:57 | INFO | train_inner | epoch 014:    168 / 764 loss=5.456, nll_loss=4.025, ppl=16.28, wps=24997.5, ups=3.46, wpb=7222.3, bsz=259.4, num_updates=10100, lr=0.000125863, gnorm=0.997, clip=0, train_wall=28, wall=3059
2020-10-12 07:26:25 | INFO | train_inner | epoch 014:    268 / 764 loss=5.423, nll_loss=3.987, ppl=15.86, wps=24756.4, ups=3.47, wpb=7136.6, bsz=278.1, num_updates=10200, lr=0.000125245, gnorm=1.011, clip=0, train_wall=28, wall=3088
2020-10-12 07:26:55 | INFO | train_inner | epoch 014:    368 / 764 loss=5.422, nll_loss=3.986, ppl=15.84, wps=25103.3, ups=3.43, wpb=7313.4, bsz=278.5, num_updates=10300, lr=0.000124635, gnorm=0.988, clip=0, train_wall=28, wall=3117
2020-10-12 07:27:24 | INFO | train_inner | epoch 014:    468 / 764 loss=5.396, nll_loss=3.957, ppl=15.53, wps=25212.6, ups=3.42, wpb=7379.8, bsz=294.9, num_updates=10400, lr=0.000124035, gnorm=0.971, clip=0, train_wall=28, wall=3146
2020-10-12 07:27:53 | INFO | train_inner | epoch 014:    568 / 764 loss=5.41, nll_loss=3.973, ppl=15.7, wps=24746.7, ups=3.4, wpb=7268.9, bsz=284.6, num_updates=10500, lr=0.000123443, gnorm=0.976, clip=0, train_wall=29, wall=3175
2020-10-12 07:28:23 | INFO | train_inner | epoch 014:    668 / 764 loss=5.425, nll_loss=3.99, ppl=15.89, wps=24928.4, ups=3.41, wpb=7314.2, bsz=275.1, num_updates=10600, lr=0.000122859, gnorm=1.012, clip=0, train_wall=29, wall=3205
2020-10-12 07:28:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7098.73828125Mb; avail=237455.4921875Mb
2020-10-12 07:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001386
2020-10-12 07:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.73828125Mb; avail=237455.4921875Mb
2020-10-12 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068935
2020-10-12 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.47265625Mb; avail=237455.7578125Mb
2020-10-12 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049937
2020-10-12 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121051
2020-10-12 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.2890625Mb; avail=237453.94140625Mb
2020-10-12 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7103.31640625Mb; avail=237450.9140625Mb
2020-10-12 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001191
2020-10-12 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.31640625Mb; avail=237450.9140625Mb
2020-10-12 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068682
2020-10-12 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.4375Mb; avail=237440.79296875Mb
2020-10-12 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048682
2020-10-12 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119353
2020-10-12 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7108.6171875Mb; avail=237445.61328125Mb
2020-10-12 07:28:53 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 5.468 | nll_loss 3.912 | ppl 15.05 | wps 55655.6 | wpb 2283.9 | bsz 87.3 | num_updates 10696 | best_loss 5.468
2020-10-12 07:28:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:28:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 14 @ 10696 updates, score 5.468) (writing took 4.795315844006836 seconds)
2020-10-12 07:28:58 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 07:28:58 | INFO | train | epoch 014 | loss 5.422 | nll_loss 3.986 | ppl 15.85 | wps 23888.3 | ups 3.3 | wpb 7242.1 | bsz 278.7 | num_updates 10696 | lr 0.000122306 | gnorm 0.995 | clip 0 | train_wall 216 | wall 3240
2020-10-12 07:28:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 07:28:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 07:28:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7003.1640625Mb; avail=237551.0546875Mb
2020-10-12 07:28:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003457
2020-10-12 07:28:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044133
2020-10-12 07:28:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7004.6015625Mb; avail=237550.3515625Mb
2020-10-12 07:28:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001577
2020-10-12 07:28:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7004.6015625Mb; avail=237550.3515625Mb
2020-10-12 07:28:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.743549
2020-10-12 07:28:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.790096
2020-10-12 07:28:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7010.36328125Mb; avail=237544.19921875Mb
2020-10-12 07:28:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7004.94921875Mb; avail=237549.61328125Mb
2020-10-12 07:28:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032987
2020-10-12 07:28:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7004.94921875Mb; avail=237549.61328125Mb
2020-10-12 07:28:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001552
2020-10-12 07:28:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7004.94921875Mb; avail=237549.61328125Mb
2020-10-12 07:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.741869
2020-10-12 07:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.777234
2020-10-12 07:29:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7018.03125Mb; avail=237536.4375Mb
2020-10-12 07:29:00 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 07:29:01 | INFO | train_inner | epoch 015:      4 / 764 loss=5.414, nll_loss=3.978, ppl=15.76, wps=18629.3, ups=2.61, wpb=7151, bsz=276.1, num_updates=10700, lr=0.000122284, gnorm=1.003, clip=0, train_wall=28, wall=3243
2020-10-12 07:29:30 | INFO | train_inner | epoch 015:    104 / 764 loss=5.335, nll_loss=3.889, ppl=14.81, wps=24293.6, ups=3.46, wpb=7026.3, bsz=279.4, num_updates=10800, lr=0.000121716, gnorm=1.046, clip=0, train_wall=28, wall=3272
2020-10-12 07:29:59 | INFO | train_inner | epoch 015:    204 / 764 loss=5.324, nll_loss=3.873, ppl=14.66, wps=25074, ups=3.41, wpb=7346.1, bsz=282.4, num_updates=10900, lr=0.000121157, gnorm=0.948, clip=0, train_wall=29, wall=3301
2020-10-12 07:30:28 | INFO | train_inner | epoch 015:    304 / 764 loss=5.327, nll_loss=3.879, ppl=14.71, wps=25149.3, ups=3.45, wpb=7290.4, bsz=295.9, num_updates=11000, lr=0.000120605, gnorm=0.971, clip=0, train_wall=28, wall=3330
2020-10-12 07:30:57 | INFO | train_inner | epoch 015:    404 / 764 loss=5.314, nll_loss=3.864, ppl=14.56, wps=24582.5, ups=3.41, wpb=7204, bsz=292.2, num_updates=11100, lr=0.00012006, gnorm=0.993, clip=0, train_wall=29, wall=3360
2020-10-12 07:31:27 | INFO | train_inner | epoch 015:    504 / 764 loss=5.322, nll_loss=3.872, ppl=14.64, wps=24862, ups=3.42, wpb=7276.1, bsz=289.9, num_updates=11200, lr=0.000119523, gnorm=0.994, clip=0, train_wall=28, wall=3389
2020-10-12 07:31:56 | INFO | train_inner | epoch 015:    604 / 764 loss=5.35, nll_loss=3.903, ppl=14.96, wps=24796.7, ups=3.44, wpb=7211.3, bsz=251, num_updates=11300, lr=0.000118993, gnorm=0.983, clip=0, train_wall=28, wall=3418
2020-10-12 07:32:25 | INFO | train_inner | epoch 015:    704 / 764 loss=5.339, nll_loss=3.892, ppl=14.85, wps=24999.4, ups=3.47, wpb=7214.7, bsz=267.1, num_updates=11400, lr=0.00011847, gnorm=1.002, clip=0, train_wall=28, wall=3447
2020-10-12 07:32:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:32:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7038.02734375Mb; avail=237515.98828125Mb
2020-10-12 07:32:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001451
2020-10-12 07:32:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7038.02734375Mb; avail=237515.98828125Mb
2020-10-12 07:32:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068419
2020-10-12 07:32:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7038.140625Mb; avail=237515.875Mb
2020-10-12 07:32:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049239
2020-10-12 07:32:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119889
2020-10-12 07:32:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7038.140625Mb; avail=237515.875Mb
2020-10-12 07:32:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7038.140625Mb; avail=237515.875Mb
2020-10-12 07:32:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001087
2020-10-12 07:32:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7038.140625Mb; avail=237515.875Mb
2020-10-12 07:32:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068599
2020-10-12 07:32:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7038.26953125Mb; avail=237515.74609375Mb
2020-10-12 07:32:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049844
2020-10-12 07:32:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120286
2020-10-12 07:32:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7038.27734375Mb; avail=237515.73828125Mb
2020-10-12 07:32:45 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 5.399 | nll_loss 3.839 | ppl 14.31 | wps 55549.6 | wpb 2283.9 | bsz 87.3 | num_updates 11460 | best_loss 5.399
2020-10-12 07:32:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:32:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 15 @ 11460 updates, score 5.399) (writing took 4.790116609074175 seconds)
2020-10-12 07:32:50 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 07:32:50 | INFO | train | epoch 015 | loss 5.33 | nll_loss 3.881 | ppl 14.74 | wps 23882.5 | ups 3.3 | wpb 7242.1 | bsz 278.7 | num_updates 11460 | lr 0.000118159 | gnorm 0.989 | clip 0 | train_wall 216 | wall 3472
2020-10-12 07:32:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 07:32:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 07:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6947.20703125Mb; avail=237607.0390625Mb
2020-10-12 07:32:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003801
2020-10-12 07:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044094
2020-10-12 07:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6947.20703125Mb; avail=237607.0390625Mb
2020-10-12 07:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001724
2020-10-12 07:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6937.85546875Mb; avail=237616.390625Mb
2020-10-12 07:32:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.747654
2020-10-12 07:32:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.794369
2020-10-12 07:32:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6937.953125Mb; avail=237616.29296875Mb
2020-10-12 07:32:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6937.953125Mb; avail=237616.29296875Mb
2020-10-12 07:32:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033157
2020-10-12 07:32:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6937.953125Mb; avail=237616.29296875Mb
2020-10-12 07:32:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001659
2020-10-12 07:32:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6937.953125Mb; avail=237616.29296875Mb
2020-10-12 07:32:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.740641
2020-10-12 07:32:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.776295
2020-10-12 07:32:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6936.890625Mb; avail=237617.34765625Mb
2020-10-12 07:32:51 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 07:33:03 | INFO | train_inner | epoch 016:     40 / 764 loss=5.286, nll_loss=3.832, ppl=14.24, wps=19049.7, ups=2.6, wpb=7313.4, bsz=279.3, num_updates=11500, lr=0.000117954, gnorm=0.971, clip=0, train_wall=28, wall=3485
2020-10-12 07:33:32 | INFO | train_inner | epoch 016:    140 / 764 loss=5.247, nll_loss=3.787, ppl=13.8, wps=25033.8, ups=3.43, wpb=7295.2, bsz=271.4, num_updates=11600, lr=0.000117444, gnorm=0.976, clip=0, train_wall=28, wall=3514
2020-10-12 07:34:01 | INFO | train_inner | epoch 016:    240 / 764 loss=5.23, nll_loss=3.769, ppl=13.63, wps=24773.8, ups=3.46, wpb=7154.7, bsz=293.8, num_updates=11700, lr=0.000116941, gnorm=0.977, clip=0, train_wall=28, wall=3543
2020-10-12 07:34:30 | INFO | train_inner | epoch 016:    340 / 764 loss=5.276, nll_loss=3.819, ppl=14.12, wps=25131.8, ups=3.46, wpb=7258.6, bsz=260.9, num_updates=11800, lr=0.000116445, gnorm=1.011, clip=0, train_wall=28, wall=3572
2020-10-12 07:34:59 | INFO | train_inner | epoch 016:    440 / 764 loss=5.224, nll_loss=3.761, ppl=13.55, wps=25208.5, ups=3.41, wpb=7397.7, bsz=298.6, num_updates=11900, lr=0.000115954, gnorm=0.98, clip=0, train_wall=29, wall=3602
2020-10-12 07:35:29 | INFO | train_inner | epoch 016:    540 / 764 loss=5.247, nll_loss=3.787, ppl=13.8, wps=24976.8, ups=3.41, wpb=7318.5, bsz=287.9, num_updates=12000, lr=0.00011547, gnorm=0.99, clip=0, train_wall=29, wall=3631
2020-10-12 07:35:58 | INFO | train_inner | epoch 016:    640 / 764 loss=5.249, nll_loss=3.79, ppl=13.83, wps=24674.7, ups=3.45, wpb=7144.7, bsz=273.4, num_updates=12100, lr=0.000114992, gnorm=0.999, clip=0, train_wall=28, wall=3660
2020-10-12 07:36:27 | INFO | train_inner | epoch 016:    740 / 764 loss=5.253, nll_loss=3.794, ppl=13.87, wps=24808, ups=3.46, wpb=7176.8, bsz=270.1, num_updates=12200, lr=0.00011452, gnorm=0.971, clip=0, train_wall=28, wall=3689
2020-10-12 07:36:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:36:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6969.73046875Mb; avail=237584.46484375Mb
2020-10-12 07:36:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001382
2020-10-12 07:36:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6969.73046875Mb; avail=237584.46484375Mb
2020-10-12 07:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069437
2020-10-12 07:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6972.953125Mb; avail=237581.2421875Mb
2020-10-12 07:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050449
2020-10-12 07:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122103
2020-10-12 07:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6963.60546875Mb; avail=237590.58984375Mb
2020-10-12 07:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6963.60546875Mb; avail=237590.58984375Mb
2020-10-12 07:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001258
2020-10-12 07:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6963.60546875Mb; avail=237590.58984375Mb
2020-10-12 07:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071287
2020-10-12 07:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6963.60546875Mb; avail=237590.58984375Mb
2020-10-12 07:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050628
2020-10-12 07:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123967
2020-10-12 07:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6963.60546875Mb; avail=237590.58984375Mb
2020-10-12 07:36:36 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 5.36 | nll_loss 3.8 | ppl 13.93 | wps 55871.7 | wpb 2283.9 | bsz 87.3 | num_updates 12224 | best_loss 5.36
2020-10-12 07:36:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:36:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 16 @ 12224 updates, score 5.36) (writing took 4.794834962114692 seconds)
2020-10-12 07:36:41 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 07:36:41 | INFO | train | epoch 016 | loss 5.248 | nll_loss 3.788 | ppl 13.82 | wps 23922.1 | ups 3.3 | wpb 7242.1 | bsz 278.7 | num_updates 12224 | lr 0.000114407 | gnorm 0.987 | clip 0 | train_wall 216 | wall 3703
2020-10-12 07:36:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 07:36:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 07:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6945.4296875Mb; avail=237608.69140625Mb
2020-10-12 07:36:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003578
2020-10-12 07:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.041954
2020-10-12 07:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6944.95703125Mb; avail=237609.1640625Mb
2020-10-12 07:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001691
2020-10-12 07:36:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6944.95703125Mb; avail=237609.1640625Mb
2020-10-12 07:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.741140
2020-10-12 07:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.785655
2020-10-12 07:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6947.33203125Mb; avail=237607.1328125Mb
2020-10-12 07:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6947.33203125Mb; avail=237607.1328125Mb
2020-10-12 07:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033041
2020-10-12 07:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6947.33203125Mb; avail=237607.1328125Mb
2020-10-12 07:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001570
2020-10-12 07:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6947.33203125Mb; avail=237607.1328125Mb
2020-10-12 07:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.743469
2020-10-12 07:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.778896
2020-10-12 07:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6947.62109375Mb; avail=237606.82421875Mb
2020-10-12 07:36:43 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 07:37:05 | INFO | train_inner | epoch 017:     76 / 764 loss=5.222, nll_loss=3.759, ppl=13.54, wps=18914.7, ups=2.61, wpb=7237.5, bsz=266, num_updates=12300, lr=0.000114053, gnorm=0.996, clip=0, train_wall=28, wall=3727
2020-10-12 07:37:34 | INFO | train_inner | epoch 017:    176 / 764 loss=5.187, nll_loss=3.719, ppl=13.17, wps=25016.1, ups=3.46, wpb=7234.1, bsz=266.1, num_updates=12400, lr=0.000113592, gnorm=0.997, clip=0, train_wall=28, wall=3756
2020-10-12 07:38:03 | INFO | train_inner | epoch 017:    276 / 764 loss=5.206, nll_loss=3.741, ppl=13.37, wps=24882.1, ups=3.47, wpb=7172, bsz=255.7, num_updates=12500, lr=0.000113137, gnorm=0.969, clip=0, train_wall=28, wall=3785
2020-10-12 07:38:32 | INFO | train_inner | epoch 017:    376 / 764 loss=5.173, nll_loss=3.703, ppl=13.02, wps=25002.7, ups=3.42, wpb=7300.3, bsz=289.4, num_updates=12600, lr=0.000112687, gnorm=0.977, clip=0, train_wall=28, wall=3814
2020-10-12 07:39:01 | INFO | train_inner | epoch 017:    476 / 764 loss=5.154, nll_loss=3.681, ppl=12.83, wps=24618.9, ups=3.43, wpb=7181.5, bsz=295.9, num_updates=12700, lr=0.000112243, gnorm=0.986, clip=0, train_wall=28, wall=3843
2020-10-12 07:39:30 | INFO | train_inner | epoch 017:    576 / 764 loss=5.163, nll_loss=3.691, ppl=12.91, wps=25112.6, ups=3.41, wpb=7360.9, bsz=289.4, num_updates=12800, lr=0.000111803, gnorm=0.964, clip=0, train_wall=29, wall=3872
2020-10-12 07:39:59 | INFO | train_inner | epoch 017:    676 / 764 loss=5.17, nll_loss=3.699, ppl=12.99, wps=24915.6, ups=3.43, wpb=7260.1, bsz=277.1, num_updates=12900, lr=0.000111369, gnorm=0.995, clip=0, train_wall=28, wall=3902
2020-10-12 07:40:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7037.96484375Mb; avail=237516.0703125Mb
2020-10-12 07:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001409
2020-10-12 07:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7039.17578125Mb; avail=237514.859375Mb
2020-10-12 07:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070272
2020-10-12 07:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7042.921875Mb; avail=237511.11328125Mb
2020-10-12 07:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051544
2020-10-12 07:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124034
2020-10-12 07:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7042.921875Mb; avail=237511.11328125Mb
2020-10-12 07:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7042.921875Mb; avail=237511.11328125Mb
2020-10-12 07:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001205
2020-10-12 07:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7042.921875Mb; avail=237511.11328125Mb
2020-10-12 07:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070020
2020-10-12 07:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7043.01953125Mb; avail=237511.015625Mb
2020-10-12 07:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050841
2020-10-12 07:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122841
2020-10-12 07:40:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7043.01953125Mb; avail=237511.015625Mb
2020-10-12 07:40:28 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 5.317 | nll_loss 3.749 | ppl 13.45 | wps 55855.6 | wpb 2283.9 | bsz 87.3 | num_updates 12988 | best_loss 5.317
2020-10-12 07:40:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:40:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 17 @ 12988 updates, score 5.317) (writing took 4.782340290024877 seconds)
2020-10-12 07:40:32 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 07:40:32 | INFO | train | epoch 017 | loss 5.175 | nll_loss 3.706 | ppl 13.05 | wps 23907.2 | ups 3.3 | wpb 7242.1 | bsz 278.7 | num_updates 12988 | lr 0.000110991 | gnorm 0.983 | clip 0 | train_wall 216 | wall 3935
2020-10-12 07:40:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 07:40:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 07:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6960.859375Mb; avail=237593.3828125Mb
2020-10-12 07:40:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003974
2020-10-12 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043108
2020-10-12 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6962.0546875Mb; avail=237592.1875Mb
2020-10-12 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001634
2020-10-12 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6962.0546875Mb; avail=237592.1875Mb
2020-10-12 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.739466
2020-10-12 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.784993
2020-10-12 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6968.70703125Mb; avail=237586.26953125Mb
2020-10-12 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6965.5859375Mb; avail=237589.390625Mb
2020-10-12 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032969
2020-10-12 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6971.81640625Mb; avail=237583.16015625Mb
2020-10-12 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001729
2020-10-12 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6973.02734375Mb; avail=237581.94921875Mb
2020-10-12 07:40:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.751498
2020-10-12 07:40:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.787038
2020-10-12 07:40:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6968.64453125Mb; avail=237585.82421875Mb
2020-10-12 07:40:34 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 07:40:38 | INFO | train_inner | epoch 018:     12 / 764 loss=5.153, nll_loss=3.681, ppl=12.83, wps=18759, ups=2.61, wpb=7197.1, bsz=278.6, num_updates=13000, lr=0.00011094, gnorm=0.987, clip=0, train_wall=28, wall=3940
2020-10-12 07:41:07 | INFO | train_inner | epoch 018:    112 / 764 loss=5.088, nll_loss=3.607, ppl=12.19, wps=24227, ups=3.47, wpb=6982.3, bsz=278.9, num_updates=13100, lr=0.000110516, gnorm=0.988, clip=0, train_wall=28, wall=3969
2020-10-12 07:41:36 | INFO | train_inner | epoch 018:    212 / 764 loss=5.116, nll_loss=3.639, ppl=12.46, wps=24761.4, ups=3.45, wpb=7175.5, bsz=284.5, num_updates=13200, lr=0.000110096, gnorm=0.974, clip=0, train_wall=28, wall=3998
2020-10-12 07:42:05 | INFO | train_inner | epoch 018:    312 / 764 loss=5.124, nll_loss=3.648, ppl=12.54, wps=24449.2, ups=3.45, wpb=7092.3, bsz=270, num_updates=13300, lr=0.000109682, gnorm=0.983, clip=0, train_wall=28, wall=4027
2020-10-12 07:42:34 | INFO | train_inner | epoch 018:    412 / 764 loss=5.133, nll_loss=3.657, ppl=12.61, wps=25316.1, ups=3.43, wpb=7373, bsz=266.2, num_updates=13400, lr=0.000109272, gnorm=0.973, clip=0, train_wall=28, wall=4056
2020-10-12 07:43:03 | INFO | train_inner | epoch 018:    512 / 764 loss=5.097, nll_loss=3.617, ppl=12.27, wps=25384.2, ups=3.44, wpb=7369.7, bsz=291.3, num_updates=13500, lr=0.000108866, gnorm=0.978, clip=0, train_wall=28, wall=4085
2020-10-12 07:43:32 | INFO | train_inner | epoch 018:    612 / 764 loss=5.107, nll_loss=3.629, ppl=12.37, wps=24860.2, ups=3.46, wpb=7190.8, bsz=278.7, num_updates=13600, lr=0.000108465, gnorm=1.008, clip=0, train_wall=28, wall=4114
2020-10-12 07:44:01 | INFO | train_inner | epoch 018:    712 / 764 loss=5.103, nll_loss=3.623, ppl=12.32, wps=25634.1, ups=3.41, wpb=7524.4, bsz=297.3, num_updates=13700, lr=0.000108069, gnorm=0.94, clip=0, train_wall=29, wall=4143
2020-10-12 07:44:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7067.33203125Mb; avail=237486.76171875Mb
2020-10-12 07:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001360
2020-10-12 07:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7067.33203125Mb; avail=237486.76171875Mb
2020-10-12 07:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069944
2020-10-12 07:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7073.38671875Mb; avail=237480.70703125Mb
2020-10-12 07:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049852
2020-10-12 07:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121961
2020-10-12 07:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.41015625Mb; avail=237478.68359375Mb
2020-10-12 07:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7065.8046875Mb; avail=237488.2890625Mb
2020-10-12 07:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001194
2020-10-12 07:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7065.8046875Mb; avail=237488.2890625Mb
2020-10-12 07:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069688
2020-10-12 07:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7065.64453125Mb; avail=237488.44921875Mb
2020-10-12 07:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049407
2020-10-12 07:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121043
2020-10-12 07:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7065.63671875Mb; avail=237488.45703125Mb
2020-10-12 07:44:19 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 5.28 | nll_loss 3.702 | ppl 13.02 | wps 55722.2 | wpb 2283.9 | bsz 87.3 | num_updates 13752 | best_loss 5.28
2020-10-12 07:44:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:44:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 18 @ 13752 updates, score 5.28) (writing took 4.7914330249186605 seconds)
2020-10-12 07:44:24 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 07:44:24 | INFO | train | epoch 018 | loss 5.111 | nll_loss 3.633 | ppl 12.4 | wps 23929.1 | ups 3.3 | wpb 7242.1 | bsz 278.7 | num_updates 13752 | lr 0.000107864 | gnorm 0.98 | clip 0 | train_wall 216 | wall 4166
2020-10-12 07:44:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 07:44:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 07:44:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6975.37109375Mb; avail=237578.87890625Mb
2020-10-12 07:44:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003546
2020-10-12 07:44:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.042594
2020-10-12 07:44:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6976.56640625Mb; avail=237577.68359375Mb
2020-10-12 07:44:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001577
2020-10-12 07:44:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6976.56640625Mb; avail=237577.68359375Mb
2020-10-12 07:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.737944
2020-10-12 07:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.782968
2020-10-12 07:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6984.26953125Mb; avail=237570.19921875Mb
2020-10-12 07:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6979.34765625Mb; avail=237575.12109375Mb
2020-10-12 07:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033341
2020-10-12 07:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6979.34765625Mb; avail=237575.12109375Mb
2020-10-12 07:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001658
2020-10-12 07:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6979.34765625Mb; avail=237575.12109375Mb
2020-10-12 07:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.750020
2020-10-12 07:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.785836
2020-10-12 07:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6982.6484375Mb; avail=237571.8203125Mb
2020-10-12 07:44:25 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 07:44:39 | INFO | train_inner | epoch 019:     48 / 764 loss=5.071, nll_loss=3.587, ppl=12.01, wps=19037.4, ups=2.6, wpb=7328.7, bsz=268.3, num_updates=13800, lr=0.000107676, gnorm=0.97, clip=0, train_wall=28, wall=4182
2020-10-12 07:45:09 | INFO | train_inner | epoch 019:    148 / 764 loss=5.047, nll_loss=3.56, ppl=11.79, wps=25137.2, ups=3.4, wpb=7386.7, bsz=279.5, num_updates=13900, lr=0.000107288, gnorm=0.97, clip=0, train_wall=29, wall=4211
2020-10-12 07:45:38 | INFO | train_inner | epoch 019:    248 / 764 loss=5.065, nll_loss=3.58, ppl=11.96, wps=24877.8, ups=3.42, wpb=7273.4, bsz=280.1, num_updates=14000, lr=0.000106904, gnorm=0.987, clip=0, train_wall=28, wall=4240
2020-10-12 07:46:07 | INFO | train_inner | epoch 019:    348 / 764 loss=5.089, nll_loss=3.608, ppl=12.19, wps=24443.5, ups=3.43, wpb=7125.5, bsz=257.3, num_updates=14100, lr=0.000106525, gnorm=0.982, clip=0, train_wall=28, wall=4269
2020-10-12 07:46:36 | INFO | train_inner | epoch 019:    448 / 764 loss=5.058, nll_loss=3.572, ppl=11.9, wps=24988, ups=3.42, wpb=7300.8, bsz=285, num_updates=14200, lr=0.000106149, gnorm=0.976, clip=0, train_wall=28, wall=4299
2020-10-12 07:47:06 | INFO | train_inner | epoch 019:    548 / 764 loss=5.028, nll_loss=3.539, ppl=11.62, wps=25000.6, ups=3.44, wpb=7260.4, bsz=288.9, num_updates=14300, lr=0.000105777, gnorm=0.958, clip=0, train_wall=28, wall=4328
2020-10-12 07:47:34 | INFO | train_inner | epoch 019:    648 / 764 loss=5.06, nll_loss=3.575, ppl=11.91, wps=25070.1, ups=3.47, wpb=7231.8, bsz=278.3, num_updates=14400, lr=0.000105409, gnorm=0.987, clip=0, train_wall=28, wall=4357
2020-10-12 07:48:03 | INFO | train_inner | epoch 019:    748 / 764 loss=5.043, nll_loss=3.556, ppl=11.76, wps=24413.4, ups=3.47, wpb=7032.4, bsz=284.2, num_updates=14500, lr=0.000105045, gnorm=0.974, clip=0, train_wall=28, wall=4385
2020-10-12 07:48:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7094.41015625Mb; avail=237459.79296875Mb
2020-10-12 07:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001372
2020-10-12 07:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7094.41015625Mb; avail=237459.79296875Mb
2020-10-12 07:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068612
2020-10-12 07:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.05859375Mb; avail=237469.14453125Mb
2020-10-12 07:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049542
2020-10-12 07:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120340
2020-10-12 07:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.05859375Mb; avail=237469.14453125Mb
2020-10-12 07:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7085.05859375Mb; avail=237469.14453125Mb
2020-10-12 07:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001267
2020-10-12 07:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.05859375Mb; avail=237469.14453125Mb
2020-10-12 07:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069411
2020-10-12 07:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.05859375Mb; avail=237469.14453125Mb
2020-10-12 07:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049936
2020-10-12 07:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121429
2020-10-12 07:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.05859375Mb; avail=237469.14453125Mb
2020-10-12 07:48:11 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 5.221 | nll_loss 3.632 | ppl 12.4 | wps 55926.3 | wpb 2283.9 | bsz 87.3 | num_updates 14516 | best_loss 5.221
2020-10-12 07:48:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:48:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 19 @ 14516 updates, score 5.221) (writing took 4.788005274021998 seconds)
2020-10-12 07:48:15 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 07:48:15 | INFO | train | epoch 019 | loss 5.053 | nll_loss 3.567 | ppl 11.85 | wps 23882.6 | ups 3.3 | wpb 7242.1 | bsz 278.7 | num_updates 14516 | lr 0.000104987 | gnorm 0.974 | clip 0 | train_wall 216 | wall 4398
2020-10-12 07:48:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 07:48:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 07:48:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6993.703125Mb; avail=237560.484375Mb
2020-10-12 07:48:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003626
2020-10-12 07:48:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.042947
2020-10-12 07:48:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6994.15625Mb; avail=237560.296875Mb
2020-10-12 07:48:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001573
2020-10-12 07:48:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6993.90234375Mb; avail=237560.55078125Mb
2020-10-12 07:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.739874
2020-10-12 07:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.785211
2020-10-12 07:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7000.6640625Mb; avail=237553.7890625Mb
2020-10-12 07:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6995.734375Mb; avail=237558.71875Mb
2020-10-12 07:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033244
2020-10-12 07:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6999.94140625Mb; avail=237554.51171875Mb
2020-10-12 07:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001578
2020-10-12 07:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6999.94140625Mb; avail=237554.51171875Mb
2020-10-12 07:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.748894
2020-10-12 07:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.784526
2020-10-12 07:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7000.17578125Mb; avail=237554.2734375Mb
2020-10-12 07:48:17 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 07:48:42 | INFO | train_inner | epoch 020:     84 / 764 loss=4.976, nll_loss=3.48, ppl=11.16, wps=18812.6, ups=2.61, wpb=7219.9, bsz=293.6, num_updates=14600, lr=0.000104685, gnorm=0.975, clip=0, train_wall=28, wall=4424
2020-10-12 07:49:11 | INFO | train_inner | epoch 020:    184 / 764 loss=5.003, nll_loss=3.51, ppl=11.39, wps=25219.3, ups=3.44, wpb=7341.1, bsz=280.5, num_updates=14700, lr=0.000104328, gnorm=0.984, clip=0, train_wall=28, wall=4453
2020-10-12 07:49:40 | INFO | train_inner | epoch 020:    284 / 764 loss=5.018, nll_loss=3.527, ppl=11.53, wps=24954.9, ups=3.43, wpb=7279.2, bsz=264.6, num_updates=14800, lr=0.000103975, gnorm=0.974, clip=0, train_wall=28, wall=4482
2020-10-12 07:50:09 | INFO | train_inner | epoch 020:    384 / 764 loss=5.011, nll_loss=3.519, ppl=11.46, wps=25086.8, ups=3.43, wpb=7321.3, bsz=271.4, num_updates=14900, lr=0.000103626, gnorm=0.967, clip=0, train_wall=28, wall=4511
2020-10-12 07:50:38 | INFO | train_inner | epoch 020:    484 / 764 loss=5.012, nll_loss=3.52, ppl=11.47, wps=24853, ups=3.44, wpb=7223.8, bsz=271.1, num_updates=15000, lr=0.00010328, gnorm=0.989, clip=0, train_wall=28, wall=4540
2020-10-12 07:51:07 | INFO | train_inner | epoch 020:    584 / 764 loss=4.996, nll_loss=3.503, ppl=11.34, wps=24384.6, ups=3.44, wpb=7082.3, bsz=285.3, num_updates=15100, lr=0.000102937, gnorm=0.977, clip=0, train_wall=28, wall=4569
2020-10-12 07:51:36 | INFO | train_inner | epoch 020:    684 / 764 loss=4.989, nll_loss=3.494, ppl=11.26, wps=24525.9, ups=3.4, wpb=7205.3, bsz=284.3, num_updates=15200, lr=0.000102598, gnorm=0.963, clip=0, train_wall=29, wall=4599
2020-10-12 07:52:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7120.8203125Mb; avail=237433.33984375Mb
2020-10-12 07:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001370
2020-10-12 07:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7120.8203125Mb; avail=237433.33984375Mb
2020-10-12 07:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068651
2020-10-12 07:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7120.8203125Mb; avail=237433.33984375Mb
2020-10-12 07:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049732
2020-10-12 07:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120559
2020-10-12 07:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7120.8203125Mb; avail=237433.33984375Mb
2020-10-12 07:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7120.8203125Mb; avail=237433.33984375Mb
2020-10-12 07:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001223
2020-10-12 07:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7120.8203125Mb; avail=237433.33984375Mb
2020-10-12 07:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069534
2020-10-12 07:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7120.8203125Mb; avail=237433.33984375Mb
2020-10-12 07:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049875
2020-10-12 07:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121395
2020-10-12 07:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7120.8203125Mb; avail=237433.33984375Mb
2020-10-12 07:52:02 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 5.203 | nll_loss 3.611 | ppl 12.22 | wps 55746.8 | wpb 2283.9 | bsz 87.3 | num_updates 15280 | best_loss 5.203
2020-10-12 07:52:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:52:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 20 @ 15280 updates, score 5.203) (writing took 4.790105319116265 seconds)
2020-10-12 07:52:07 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 07:52:07 | INFO | train | epoch 020 | loss 5 | nll_loss 3.507 | ppl 11.37 | wps 23856.7 | ups 3.29 | wpb 7242.1 | bsz 278.7 | num_updates 15280 | lr 0.000102329 | gnorm 0.976 | clip 0 | train_wall 216 | wall 4630
2020-10-12 07:52:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 07:52:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 07:52:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7027.515625Mb; avail=237526.69921875Mb
2020-10-12 07:52:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003994
2020-10-12 07:52:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.045180
2020-10-12 07:52:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7028.7109375Mb; avail=237525.50390625Mb
2020-10-12 07:52:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001595
2020-10-12 07:52:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7028.7109375Mb; avail=237525.50390625Mb
2020-10-12 07:52:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.739625
2020-10-12 07:52:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.787238
2020-10-12 07:52:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7034.6015625Mb; avail=237519.61328125Mb
2020-10-12 07:52:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7033.65234375Mb; avail=237520.5625Mb
2020-10-12 07:52:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033474
2020-10-12 07:52:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7038.359375Mb; avail=237515.85546875Mb
2020-10-12 07:52:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001565
2020-10-12 07:52:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7038.359375Mb; avail=237515.85546875Mb
2020-10-12 07:52:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.744372
2020-10-12 07:52:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.780210
2020-10-12 07:52:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7033.51953125Mb; avail=237520.94921875Mb
2020-10-12 07:52:09 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 07:52:15 | INFO | train_inner | epoch 021:     20 / 764 loss=4.999, nll_loss=3.505, ppl=11.35, wps=18864.2, ups=2.6, wpb=7247.2, bsz=269.2, num_updates=15300, lr=0.000102262, gnorm=0.977, clip=0, train_wall=28, wall=4637
2020-10-12 07:52:44 | INFO | train_inner | epoch 021:    120 / 764 loss=4.911, nll_loss=3.405, ppl=10.59, wps=25122.1, ups=3.44, wpb=7295.3, bsz=286.1, num_updates=15400, lr=0.000101929, gnorm=0.976, clip=0, train_wall=28, wall=4666
2020-10-12 07:53:13 | INFO | train_inner | epoch 021:    220 / 764 loss=4.927, nll_loss=3.424, ppl=10.74, wps=25029.2, ups=3.44, wpb=7268, bsz=301.7, num_updates=15500, lr=0.0001016, gnorm=0.952, clip=0, train_wall=28, wall=4695
2020-10-12 07:53:42 | INFO | train_inner | epoch 021:    320 / 764 loss=4.956, nll_loss=3.456, ppl=10.98, wps=25039.3, ups=3.44, wpb=7279.3, bsz=266.9, num_updates=15600, lr=0.000101274, gnorm=0.967, clip=0, train_wall=28, wall=4724
2020-10-12 07:54:11 | INFO | train_inner | epoch 021:    420 / 764 loss=4.954, nll_loss=3.454, ppl=10.96, wps=24424.6, ups=3.47, wpb=7047.9, bsz=272, num_updates=15700, lr=0.000100951, gnorm=0.984, clip=0, train_wall=28, wall=4753
2020-10-12 07:54:40 | INFO | train_inner | epoch 021:    520 / 764 loss=4.964, nll_loss=3.465, ppl=11.05, wps=25126.3, ups=3.44, wpb=7305.6, bsz=271, num_updates=15800, lr=0.000100631, gnorm=0.962, clip=0, train_wall=28, wall=4782
2020-10-12 07:55:09 | INFO | train_inner | epoch 021:    620 / 764 loss=4.955, nll_loss=3.455, ppl=10.97, wps=25175.2, ups=3.45, wpb=7298.9, bsz=291.9, num_updates=15900, lr=0.000100314, gnorm=0.994, clip=0, train_wall=28, wall=4811
2020-10-12 07:55:38 | INFO | train_inner | epoch 021:    720 / 764 loss=4.98, nll_loss=3.482, ppl=11.18, wps=24861.6, ups=3.45, wpb=7211.8, bsz=262.8, num_updates=16000, lr=0.0001, gnorm=0.97, clip=0, train_wall=28, wall=4840
2020-10-12 07:55:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:55:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7119.109375Mb; avail=237434.94921875Mb
2020-10-12 07:55:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001343
2020-10-12 07:55:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7119.109375Mb; avail=237434.94921875Mb
2020-10-12 07:55:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070252
2020-10-12 07:55:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7127.14453125Mb; avail=237427.08984375Mb
2020-10-12 07:55:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050472
2020-10-12 07:55:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122866
2020-10-12 07:55:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7138.578125Mb; avail=237415.65625Mb
2020-10-12 07:55:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7139.16015625Mb; avail=237415.07421875Mb
2020-10-12 07:55:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001277
2020-10-12 07:55:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7139.16015625Mb; avail=237415.07421875Mb
2020-10-12 07:55:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070883
2020-10-12 07:55:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7119.47265625Mb; avail=237434.76171875Mb
2020-10-12 07:55:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049633
2020-10-12 07:55:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122562
2020-10-12 07:55:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7119.47265625Mb; avail=237434.76171875Mb
2020-10-12 07:55:53 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 5.17 | nll_loss 3.571 | ppl 11.88 | wps 55540.9 | wpb 2283.9 | bsz 87.3 | num_updates 16044 | best_loss 5.17
2020-10-12 07:55:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:55:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 21 @ 16044 updates, score 5.17) (writing took 4.799224636051804 seconds)
2020-10-12 07:55:58 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 07:55:58 | INFO | train | epoch 021 | loss 4.951 | nll_loss 3.451 | ppl 10.94 | wps 23950 | ups 3.31 | wpb 7242.1 | bsz 278.7 | num_updates 16044 | lr 9.98628e-05 | gnorm 0.974 | clip 0 | train_wall 216 | wall 4861
2020-10-12 07:55:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 07:55:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 07:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7019.95703125Mb; avail=237534.2578125Mb
2020-10-12 07:55:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003609
2020-10-12 07:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.042461
2020-10-12 07:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7021.15234375Mb; avail=237533.0625Mb
2020-10-12 07:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001581
2020-10-12 07:55:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7021.15234375Mb; avail=237533.0625Mb
2020-10-12 07:55:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.742028
2020-10-12 07:55:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.786894
2020-10-12 07:55:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7027.6484375Mb; avail=237526.828125Mb
2020-10-12 07:55:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7022.7265625Mb; avail=237532.2421875Mb
2020-10-12 07:55:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033104
2020-10-12 07:55:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7021.75Mb; avail=237532.7265625Mb
2020-10-12 07:55:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001574
2020-10-12 07:55:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7021.75Mb; avail=237532.7265625Mb
2020-10-12 07:56:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.748064
2020-10-12 07:56:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.783580
2020-10-12 07:56:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7046.32421875Mb; avail=237508.15234375Mb
2020-10-12 07:56:00 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 07:56:16 | INFO | train_inner | epoch 022:     56 / 764 loss=4.941, nll_loss=3.439, ppl=10.85, wps=19043.7, ups=2.6, wpb=7324.6, bsz=264.9, num_updates=16100, lr=9.9689e-05, gnorm=0.975, clip=0, train_wall=28, wall=4879
2020-10-12 07:56:45 | INFO | train_inner | epoch 022:    156 / 764 loss=4.904, nll_loss=3.397, ppl=10.54, wps=25128.6, ups=3.45, wpb=7286.2, bsz=288.6, num_updates=16200, lr=9.93808e-05, gnorm=0.993, clip=0, train_wall=28, wall=4908
2020-10-12 07:57:14 | INFO | train_inner | epoch 022:    256 / 764 loss=4.908, nll_loss=3.402, ppl=10.57, wps=24485.5, ups=3.45, wpb=7093.7, bsz=268.2, num_updates=16300, lr=9.90755e-05, gnorm=0.956, clip=0, train_wall=28, wall=4937
2020-10-12 07:57:43 | INFO | train_inner | epoch 022:    356 / 764 loss=4.901, nll_loss=3.394, ppl=10.51, wps=24659, ups=3.46, wpb=7129, bsz=281.9, num_updates=16400, lr=9.8773e-05, gnorm=0.978, clip=0, train_wall=28, wall=4966
2020-10-12 07:58:12 | INFO | train_inner | epoch 022:    456 / 764 loss=4.902, nll_loss=3.394, ppl=10.51, wps=24944.2, ups=3.46, wpb=7202.6, bsz=278.3, num_updates=16500, lr=9.84732e-05, gnorm=0.979, clip=0, train_wall=28, wall=4994
2020-10-12 07:58:41 | INFO | train_inner | epoch 022:    556 / 764 loss=4.909, nll_loss=3.403, ppl=10.58, wps=25261.3, ups=3.44, wpb=7332.9, bsz=284, num_updates=16600, lr=9.81761e-05, gnorm=0.947, clip=0, train_wall=28, wall=5023
2020-10-12 07:59:10 | INFO | train_inner | epoch 022:    656 / 764 loss=4.914, nll_loss=3.408, ppl=10.62, wps=24941.4, ups=3.43, wpb=7267.6, bsz=269.8, num_updates=16700, lr=9.78818e-05, gnorm=0.97, clip=0, train_wall=28, wall=5053
2020-10-12 07:59:40 | INFO | train_inner | epoch 022:    756 / 764 loss=4.908, nll_loss=3.401, ppl=10.57, wps=24907.4, ups=3.41, wpb=7307.6, bsz=293.1, num_updates=16800, lr=9.759e-05, gnorm=0.972, clip=0, train_wall=29, wall=5082
2020-10-12 07:59:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7073.82421875Mb; avail=237480.4140625Mb
2020-10-12 07:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001251
2020-10-12 07:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7073.82421875Mb; avail=237480.4140625Mb
2020-10-12 07:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069499
2020-10-12 07:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.05859375Mb; avail=237476.1796875Mb
2020-10-12 07:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051796
2020-10-12 07:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123340
2020-10-12 07:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7068.70703125Mb; avail=237485.53125Mb
2020-10-12 07:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7068.70703125Mb; avail=237485.53125Mb
2020-10-12 07:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001163
2020-10-12 07:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7068.70703125Mb; avail=237485.53125Mb
2020-10-12 07:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068820
2020-10-12 07:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7068.70703125Mb; avail=237485.53125Mb
2020-10-12 07:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050997
2020-10-12 07:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121786
2020-10-12 07:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7068.70703125Mb; avail=237485.53125Mb
2020-10-12 07:59:45 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 5.151 | nll_loss 3.553 | ppl 11.74 | wps 55787.5 | wpb 2283.9 | bsz 87.3 | num_updates 16808 | best_loss 5.151
2020-10-12 07:59:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:59:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 22 @ 16808 updates, score 5.151) (writing took 4.797342975158244 seconds)
2020-10-12 07:59:50 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 07:59:50 | INFO | train | epoch 022 | loss 4.907 | nll_loss 3.4 | ppl 10.56 | wps 23921.7 | ups 3.3 | wpb 7242.1 | bsz 278.7 | num_updates 16808 | lr 9.75668e-05 | gnorm 0.97 | clip 0 | train_wall 216 | wall 5092
2020-10-12 07:59:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 07:59:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 07:59:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6970.7265625Mb; avail=237583.515625Mb
2020-10-12 07:59:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003408
2020-10-12 07:59:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.041682
2020-10-12 07:59:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6970.7265625Mb; avail=237583.515625Mb
2020-10-12 07:59:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001695
2020-10-12 07:59:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6970.7265625Mb; avail=237583.515625Mb
2020-10-12 07:59:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.737877
2020-10-12 07:59:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.782080
2020-10-12 07:59:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6971.9765625Mb; avail=237582.265625Mb
2020-10-12 07:59:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6971.9765625Mb; avail=237582.265625Mb
2020-10-12 07:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033120
2020-10-12 07:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6971.9765625Mb; avail=237582.265625Mb
2020-10-12 07:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001600
2020-10-12 07:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6971.9765625Mb; avail=237582.265625Mb
2020-10-12 07:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.742794
2020-10-12 07:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.778372
2020-10-12 07:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6973.13671875Mb; avail=237581.10546875Mb
2020-10-12 07:59:51 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 08:00:18 | INFO | train_inner | epoch 023:     92 / 764 loss=4.871, nll_loss=3.359, ppl=10.26, wps=18690.8, ups=2.61, wpb=7151.1, bsz=269, num_updates=16900, lr=9.73009e-05, gnorm=0.959, clip=0, train_wall=28, wall=5120
2020-10-12 08:00:47 | INFO | train_inner | epoch 023:    192 / 764 loss=4.865, nll_loss=3.353, ppl=10.21, wps=24684.8, ups=3.49, wpb=7065.8, bsz=262.6, num_updates=17000, lr=9.70143e-05, gnorm=0.963, clip=0, train_wall=28, wall=5149
2020-10-12 08:01:16 | INFO | train_inner | epoch 023:    292 / 764 loss=4.864, nll_loss=3.352, ppl=10.21, wps=24886.9, ups=3.45, wpb=7203.5, bsz=280.2, num_updates=17100, lr=9.67302e-05, gnorm=0.969, clip=0, train_wall=28, wall=5178
2020-10-12 08:01:45 | INFO | train_inner | epoch 023:    392 / 764 loss=4.868, nll_loss=3.355, ppl=10.23, wps=24923.5, ups=3.44, wpb=7239.7, bsz=274.6, num_updates=17200, lr=9.64486e-05, gnorm=0.953, clip=0, train_wall=28, wall=5207
2020-10-12 08:02:14 | INFO | train_inner | epoch 023:    492 / 764 loss=4.862, nll_loss=3.349, ppl=10.19, wps=24695.9, ups=3.44, wpb=7186.1, bsz=277.2, num_updates=17300, lr=9.61694e-05, gnorm=0.971, clip=0, train_wall=28, wall=5236
2020-10-12 08:02:43 | INFO | train_inner | epoch 023:    592 / 764 loss=4.849, nll_loss=3.334, ppl=10.09, wps=25173, ups=3.42, wpb=7367.6, bsz=287.6, num_updates=17400, lr=9.58927e-05, gnorm=0.946, clip=0, train_wall=28, wall=5265
2020-10-12 08:03:12 | INFO | train_inner | epoch 023:    692 / 764 loss=4.863, nll_loss=3.352, ppl=10.21, wps=25134.9, ups=3.39, wpb=7419.4, bsz=303.4, num_updates=17500, lr=9.56183e-05, gnorm=0.972, clip=0, train_wall=29, wall=5295
2020-10-12 08:03:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6967.796875Mb; avail=237586.546875Mb
2020-10-12 08:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001365
2020-10-12 08:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6967.796875Mb; avail=237586.546875Mb
2020-10-12 08:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068428
2020-10-12 08:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6986.4609375Mb; avail=237567.8828125Mb
2020-10-12 08:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049470
2020-10-12 08:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120078
2020-10-12 08:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6986.59375Mb; avail=237567.140625Mb
2020-10-12 08:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6967.90234375Mb; avail=237586.3359375Mb
2020-10-12 08:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001154
2020-10-12 08:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6967.90234375Mb; avail=237586.3359375Mb
2020-10-12 08:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069229
2020-10-12 08:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6967.90234375Mb; avail=237586.3359375Mb
2020-10-12 08:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050051
2020-10-12 08:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121224
2020-10-12 08:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6967.90234375Mb; avail=237586.3359375Mb
2020-10-12 08:03:36 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 5.125 | nll_loss 3.516 | ppl 11.44 | wps 55887.1 | wpb 2283.9 | bsz 87.3 | num_updates 17572 | best_loss 5.125
2020-10-12 08:03:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:03:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 23 @ 17572 updates, score 5.125) (writing took 4.792891060002148 seconds)
2020-10-12 08:03:41 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 08:03:41 | INFO | train | epoch 023 | loss 4.865 | nll_loss 3.353 | ppl 10.22 | wps 23923 | ups 3.3 | wpb 7242.1 | bsz 278.7 | num_updates 17572 | lr 9.54222e-05 | gnorm 0.961 | clip 0 | train_wall 216 | wall 5323
2020-10-12 08:03:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 08:03:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 08:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6890.44921875Mb; avail=237663.78125Mb
2020-10-12 08:03:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003593
2020-10-12 08:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043867
2020-10-12 08:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6890.44921875Mb; avail=237663.78125Mb
2020-10-12 08:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001687
2020-10-12 08:03:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6890.44921875Mb; avail=237663.78125Mb
2020-10-12 08:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.737093
2020-10-12 08:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.783476
2020-10-12 08:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6891.36328125Mb; avail=237663.265625Mb
2020-10-12 08:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6891.36328125Mb; avail=237663.265625Mb
2020-10-12 08:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033107
2020-10-12 08:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6891.36328125Mb; avail=237663.265625Mb
2020-10-12 08:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001601
2020-10-12 08:03:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6891.36328125Mb; avail=237663.265625Mb
2020-10-12 08:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.737628
2020-10-12 08:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.773161
2020-10-12 08:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6907.703125Mb; avail=237646.9921875Mb
2020-10-12 08:03:43 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 08:03:51 | INFO | train_inner | epoch 024:     28 / 764 loss=4.876, nll_loss=3.366, ppl=10.31, wps=18728.2, ups=2.62, wpb=7150, bsz=259.4, num_updates=17600, lr=9.53463e-05, gnorm=0.976, clip=0, train_wall=28, wall=5333
2020-10-12 08:04:20 | INFO | train_inner | epoch 024:    128 / 764 loss=4.83, nll_loss=3.312, ppl=9.93, wps=25019.1, ups=3.45, wpb=7257.7, bsz=261.4, num_updates=17700, lr=9.50765e-05, gnorm=0.977, clip=0, train_wall=28, wall=5362
2020-10-12 08:04:49 | INFO | train_inner | epoch 024:    228 / 764 loss=4.813, nll_loss=3.293, ppl=9.8, wps=24972.8, ups=3.44, wpb=7253.4, bsz=277.7, num_updates=17800, lr=9.48091e-05, gnorm=0.953, clip=0, train_wall=28, wall=5391
2020-10-12 08:05:18 | INFO | train_inner | epoch 024:    328 / 764 loss=4.808, nll_loss=3.289, ppl=9.77, wps=25049.7, ups=3.44, wpb=7283.5, bsz=308.2, num_updates=17900, lr=9.45439e-05, gnorm=0.95, clip=0, train_wall=28, wall=5420
2020-10-12 08:05:47 | INFO | train_inner | epoch 024:    428 / 764 loss=4.838, nll_loss=3.322, ppl=10, wps=25168, ups=3.44, wpb=7316.2, bsz=264.6, num_updates=18000, lr=9.42809e-05, gnorm=0.957, clip=0, train_wall=28, wall=5449
2020-10-12 08:06:16 | INFO | train_inner | epoch 024:    528 / 764 loss=4.843, nll_loss=3.327, ppl=10.04, wps=24978.1, ups=3.46, wpb=7229.1, bsz=275.4, num_updates=18100, lr=9.40201e-05, gnorm=0.987, clip=0, train_wall=28, wall=5478
2020-10-12 08:06:45 | INFO | train_inner | epoch 024:    628 / 764 loss=4.834, nll_loss=3.317, ppl=9.97, wps=25091.2, ups=3.45, wpb=7272.6, bsz=277.4, num_updates=18200, lr=9.37614e-05, gnorm=0.969, clip=0, train_wall=28, wall=5507
2020-10-12 08:07:14 | INFO | train_inner | epoch 024:    728 / 764 loss=4.815, nll_loss=3.296, ppl=9.82, wps=25019.5, ups=3.46, wpb=7229.6, bsz=304.9, num_updates=18300, lr=9.35049e-05, gnorm=0.97, clip=0, train_wall=28, wall=5536
2020-10-12 08:07:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6970.09375Mb; avail=237583.8984375Mb
2020-10-12 08:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001332
2020-10-12 08:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6970.09375Mb; avail=237583.8984375Mb
2020-10-12 08:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069099
2020-10-12 08:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6970.22265625Mb; avail=237584.50390625Mb
2020-10-12 08:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049465
2020-10-12 08:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120669
2020-10-12 08:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6970.0859375Mb; avail=237583.90625Mb
2020-10-12 08:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6971.83984375Mb; avail=237582.15234375Mb
2020-10-12 08:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001168
2020-10-12 08:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6971.83984375Mb; avail=237582.15234375Mb
2020-10-12 08:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067462
2020-10-12 08:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6979.58203125Mb; avail=237574.19140625Mb
2020-10-12 08:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049411
2020-10-12 08:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118790
2020-10-12 08:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6970.33984375Mb; avail=237583.5390625Mb
2020-10-12 08:07:27 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.112 | nll_loss 3.495 | ppl 11.27 | wps 55819.6 | wpb 2283.9 | bsz 87.3 | num_updates 18336 | best_loss 5.112
2020-10-12 08:07:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:07:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 24 @ 18336 updates, score 5.112) (writing took 4.811895946972072 seconds)
2020-10-12 08:07:32 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 08:07:32 | INFO | train | epoch 024 | loss 4.828 | nll_loss 3.31 | ppl 9.92 | wps 23977.7 | ups 3.31 | wpb 7242.1 | bsz 278.7 | num_updates 18336 | lr 9.34131e-05 | gnorm 0.968 | clip 0 | train_wall 215 | wall 5554
2020-10-12 08:07:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 08:07:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 08:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6886.8359375Mb; avail=237667.3828125Mb
2020-10-12 08:07:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003436
2020-10-12 08:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043346
2020-10-12 08:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6886.8359375Mb; avail=237667.3828125Mb
2020-10-12 08:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001728
2020-10-12 08:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6886.8359375Mb; avail=237667.3828125Mb
2020-10-12 08:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.747740
2020-10-12 08:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.793634
2020-10-12 08:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6894.93359375Mb; avail=237659.546875Mb
2020-10-12 08:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6897.3046875Mb; avail=237657.07421875Mb
2020-10-12 08:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033208
2020-10-12 08:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6897.8671875Mb; avail=237656.51171875Mb
2020-10-12 08:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001635
2020-10-12 08:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6897.8671875Mb; avail=237656.51171875Mb
2020-10-12 08:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.755740
2020-10-12 08:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.791417
2020-10-12 08:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6900.8671875Mb; avail=237653.9296875Mb
2020-10-12 08:07:33 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 08:07:52 | INFO | train_inner | epoch 025:     64 / 764 loss=4.78, nll_loss=3.257, ppl=9.56, wps=18754.8, ups=2.61, wpb=7187.6, bsz=286.1, num_updates=18400, lr=9.32505e-05, gnorm=0.962, clip=0, train_wall=28, wall=5574
2020-10-12 08:08:21 | INFO | train_inner | epoch 025:    164 / 764 loss=4.776, nll_loss=3.252, ppl=9.52, wps=25069.1, ups=3.43, wpb=7319.4, bsz=283.9, num_updates=18500, lr=9.29981e-05, gnorm=0.978, clip=0, train_wall=28, wall=5603
2020-10-12 08:08:50 | INFO | train_inner | epoch 025:    264 / 764 loss=4.786, nll_loss=3.263, ppl=9.6, wps=24476.3, ups=3.45, wpb=7090.6, bsz=280.7, num_updates=18600, lr=9.27478e-05, gnorm=0.985, clip=0, train_wall=28, wall=5632
2020-10-12 08:09:20 | INFO | train_inner | epoch 025:    364 / 764 loss=4.756, nll_loss=3.229, ppl=9.38, wps=25415.4, ups=3.39, wpb=7501.1, bsz=308.6, num_updates=18700, lr=9.24995e-05, gnorm=0.946, clip=0, train_wall=29, wall=5662
2020-10-12 08:09:49 | INFO | train_inner | epoch 025:    464 / 764 loss=4.818, nll_loss=3.299, ppl=9.84, wps=24406.2, ups=3.44, wpb=7085.4, bsz=261.9, num_updates=18800, lr=9.22531e-05, gnorm=0.983, clip=0, train_wall=28, wall=5691
2020-10-12 08:10:18 | INFO | train_inner | epoch 025:    564 / 764 loss=4.818, nll_loss=3.299, ppl=9.84, wps=24989.2, ups=3.46, wpb=7216.8, bsz=265, num_updates=18900, lr=9.20087e-05, gnorm=1.001, clip=0, train_wall=28, wall=5720
2020-10-12 08:10:46 | INFO | train_inner | epoch 025:    664 / 764 loss=4.81, nll_loss=3.291, ppl=9.79, wps=25054.9, ups=3.46, wpb=7239.9, bsz=272.7, num_updates=19000, lr=9.17663e-05, gnorm=0.964, clip=0, train_wall=28, wall=5749
2020-10-12 08:11:15 | INFO | train_inner | epoch 025:    764 / 764 loss=4.816, nll_loss=3.297, ppl=9.83, wps=25149.9, ups=3.45, wpb=7288.3, bsz=268.2, num_updates=19100, lr=9.15258e-05, gnorm=0.959, clip=0, train_wall=28, wall=5778
2020-10-12 08:11:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:11:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7048.58203125Mb; avail=237505.4296875Mb
2020-10-12 08:11:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001393
2020-10-12 08:11:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7048.58203125Mb; avail=237505.4296875Mb
2020-10-12 08:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069365
2020-10-12 08:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7053.42578125Mb; avail=237500.5859375Mb
2020-10-12 08:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050138
2020-10-12 08:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121678
2020-10-12 08:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7058.1796875Mb; avail=237495.83203125Mb
2020-10-12 08:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7048.83203125Mb; avail=237505.1796875Mb
2020-10-12 08:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001255
2020-10-12 08:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7048.83203125Mb; avail=237505.1796875Mb
2020-10-12 08:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069358
2020-10-12 08:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7048.83203125Mb; avail=237505.1796875Mb
2020-10-12 08:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049898
2020-10-12 08:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121259
2020-10-12 08:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7048.83203125Mb; avail=237505.1796875Mb
2020-10-12 08:11:18 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.102 | nll_loss 3.491 | ppl 11.24 | wps 55780.3 | wpb 2283.9 | bsz 87.3 | num_updates 19100 | best_loss 5.102
2020-10-12 08:11:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:11:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 25 @ 19100 updates, score 5.102) (writing took 4.786497227149084 seconds)
2020-10-12 08:11:23 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 08:11:23 | INFO | train | epoch 025 | loss 4.792 | nll_loss 3.27 | ppl 9.65 | wps 23910.8 | ups 3.3 | wpb 7242.1 | bsz 278.7 | num_updates 19100 | lr 9.15258e-05 | gnorm 0.972 | clip 0 | train_wall 216 | wall 5785
2020-10-12 08:11:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 08:11:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 08:11:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6961.859375Mb; avail=237592.36328125Mb
2020-10-12 08:11:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003624
2020-10-12 08:11:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043885
2020-10-12 08:11:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6961.73828125Mb; avail=237592.484375Mb
2020-10-12 08:11:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001636
2020-10-12 08:11:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6961.73828125Mb; avail=237592.484375Mb
2020-10-12 08:11:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.743047
2020-10-12 08:11:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.789401
2020-10-12 08:11:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6967.8828125Mb; avail=237586.59765625Mb
2020-10-12 08:11:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6962.9609375Mb; avail=237591.51953125Mb
2020-10-12 08:11:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033199
2020-10-12 08:11:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6962.9609375Mb; avail=237591.51953125Mb
2020-10-12 08:11:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001595
2020-10-12 08:11:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6962.9609375Mb; avail=237591.51953125Mb
2020-10-12 08:11:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.744434
2020-10-12 08:11:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.780033
2020-10-12 08:11:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6968.375Mb; avail=237586.33203125Mb
2020-10-12 08:11:25 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 08:11:54 | INFO | train_inner | epoch 026:    100 / 764 loss=4.749, nll_loss=3.221, ppl=9.32, wps=19036.7, ups=2.6, wpb=7325.1, bsz=276.6, num_updates=19200, lr=9.12871e-05, gnorm=0.965, clip=0, train_wall=28, wall=5816
2020-10-12 08:12:23 | INFO | train_inner | epoch 026:    200 / 764 loss=4.769, nll_loss=3.243, ppl=9.47, wps=25014.2, ups=3.44, wpb=7274.7, bsz=258.6, num_updates=19300, lr=9.10503e-05, gnorm=0.982, clip=0, train_wall=28, wall=5845
2020-10-12 08:12:52 | INFO | train_inner | epoch 026:    300 / 764 loss=4.738, nll_loss=3.208, ppl=9.24, wps=25039.8, ups=3.45, wpb=7261.1, bsz=287.8, num_updates=19400, lr=9.08153e-05, gnorm=0.964, clip=0, train_wall=28, wall=5874
2020-10-12 08:13:21 | INFO | train_inner | epoch 026:    400 / 764 loss=4.767, nll_loss=3.241, ppl=9.45, wps=24576.5, ups=3.46, wpb=7094, bsz=275.1, num_updates=19500, lr=9.05822e-05, gnorm=0.987, clip=0, train_wall=28, wall=5903
2020-10-12 08:13:50 | INFO | train_inner | epoch 026:    500 / 764 loss=4.759, nll_loss=3.232, ppl=9.39, wps=25092.6, ups=3.46, wpb=7250.3, bsz=284.6, num_updates=19600, lr=9.03508e-05, gnorm=0.971, clip=0, train_wall=28, wall=5932
2020-10-12 08:14:19 | INFO | train_inner | epoch 026:    600 / 764 loss=4.761, nll_loss=3.235, ppl=9.42, wps=24622.7, ups=3.48, wpb=7072.8, bsz=288.8, num_updates=19700, lr=9.01212e-05, gnorm=0.985, clip=0, train_wall=28, wall=5961
2020-10-12 08:14:47 | INFO | train_inner | epoch 026:    700 / 764 loss=4.771, nll_loss=3.246, ppl=9.49, wps=24935.1, ups=3.45, wpb=7220.5, bsz=271.4, num_updates=19800, lr=8.98933e-05, gnorm=0.959, clip=0, train_wall=28, wall=5990
2020-10-12 08:15:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:15:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7075.5625Mb; avail=237479.20703125Mb
2020-10-12 08:15:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001371
2020-10-12 08:15:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.5625Mb; avail=237479.20703125Mb
2020-10-12 08:15:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068629
2020-10-12 08:15:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.25Mb; avail=237479.3359375Mb
2020-10-12 08:15:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050157
2020-10-12 08:15:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120951
2020-10-12 08:15:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.2890625Mb; avail=237479.703125Mb
2020-10-12 08:15:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7075.21875Mb; avail=237479.58203125Mb
2020-10-12 08:15:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001211
2020-10-12 08:15:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.21875Mb; avail=237479.58203125Mb
2020-10-12 08:15:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070265
2020-10-12 08:15:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.21875Mb; avail=237479.58203125Mb
2020-10-12 08:15:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050460
2020-10-12 08:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122686
2020-10-12 08:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.21875Mb; avail=237479.58203125Mb
2020-10-12 08:15:09 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.074 | nll_loss 3.457 | ppl 10.98 | wps 55679.7 | wpb 2283.9 | bsz 87.3 | num_updates 19864 | best_loss 5.074
2020-10-12 08:15:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:15:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 26 @ 19864 updates, score 5.074) (writing took 4.805852620862424 seconds)
2020-10-12 08:15:14 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 08:15:14 | INFO | train | epoch 026 | loss 4.759 | nll_loss 3.232 | ppl 9.39 | wps 23976.5 | ups 3.31 | wpb 7242.1 | bsz 278.7 | num_updates 19864 | lr 8.97484e-05 | gnorm 0.971 | clip 0 | train_wall 215 | wall 6016
2020-10-12 08:15:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 08:15:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 08:15:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6984.8671875Mb; avail=237569.38671875Mb
2020-10-12 08:15:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003615
2020-10-12 08:15:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043442
2020-10-12 08:15:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6984.8671875Mb; avail=237569.38671875Mb
2020-10-12 08:15:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001588
2020-10-12 08:15:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6984.8671875Mb; avail=237569.38671875Mb
2020-10-12 08:15:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.756658
2020-10-12 08:15:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.802516
2020-10-12 08:15:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6989.59765625Mb; avail=237564.89453125Mb
2020-10-12 08:15:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6984.67578125Mb; avail=237569.81640625Mb
2020-10-12 08:15:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033117
2020-10-12 08:15:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6984.67578125Mb; avail=237569.81640625Mb
2020-10-12 08:15:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001577
2020-10-12 08:15:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6984.67578125Mb; avail=237569.81640625Mb
2020-10-12 08:15:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.752193
2020-10-12 08:15:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.787697
2020-10-12 08:15:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6989.08984375Mb; avail=237565.38671875Mb
2020-10-12 08:15:15 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 08:15:26 | INFO | train_inner | epoch 027:     36 / 764 loss=4.726, nll_loss=3.194, ppl=9.15, wps=19446.4, ups=2.59, wpb=7508.3, bsz=295.1, num_updates=19900, lr=8.96672e-05, gnorm=0.946, clip=0, train_wall=28, wall=6028
2020-10-12 08:15:55 | INFO | train_inner | epoch 027:    136 / 764 loss=4.738, nll_loss=3.208, ppl=9.24, wps=24821.4, ups=3.47, wpb=7159.4, bsz=248.6, num_updates=20000, lr=8.94427e-05, gnorm=0.978, clip=0, train_wall=28, wall=6057
2020-10-12 08:16:24 | INFO | train_inner | epoch 027:    236 / 764 loss=4.736, nll_loss=3.205, ppl=9.22, wps=24922, ups=3.46, wpb=7195, bsz=276.8, num_updates=20100, lr=8.92199e-05, gnorm=0.976, clip=0, train_wall=28, wall=6086
2020-10-12 08:16:53 | INFO | train_inner | epoch 027:    336 / 764 loss=4.717, nll_loss=3.184, ppl=9.09, wps=25263.8, ups=3.42, wpb=7386.3, bsz=286.5, num_updates=20200, lr=8.89988e-05, gnorm=0.959, clip=0, train_wall=28, wall=6115
2020-10-12 08:17:22 | INFO | train_inner | epoch 027:    436 / 764 loss=4.745, nll_loss=3.216, ppl=9.29, wps=24447.2, ups=3.49, wpb=7012.8, bsz=263.4, num_updates=20300, lr=8.87794e-05, gnorm=0.973, clip=0, train_wall=28, wall=6144
2020-10-12 08:17:51 | INFO | train_inner | epoch 027:    536 / 764 loss=4.734, nll_loss=3.204, ppl=9.21, wps=24966.2, ups=3.48, wpb=7183.7, bsz=269.8, num_updates=20400, lr=8.85615e-05, gnorm=0.974, clip=0, train_wall=28, wall=6173
2020-10-12 08:18:20 | INFO | train_inner | epoch 027:    636 / 764 loss=4.709, nll_loss=3.176, ppl=9.04, wps=25164.9, ups=3.43, wpb=7346.7, bsz=305.1, num_updates=20500, lr=8.83452e-05, gnorm=0.943, clip=0, train_wall=28, wall=6202
2020-10-12 08:18:49 | INFO | train_inner | epoch 027:    736 / 764 loss=4.742, nll_loss=3.212, ppl=9.27, wps=25013.7, ups=3.45, wpb=7250.4, bsz=282.4, num_updates=20600, lr=8.81305e-05, gnorm=0.981, clip=0, train_wall=28, wall=6231
2020-10-12 08:18:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:18:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7091.98828125Mb; avail=237462.0703125Mb
2020-10-12 08:18:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001373
2020-10-12 08:18:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.98828125Mb; avail=237462.0703125Mb
2020-10-12 08:18:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069988
2020-10-12 08:18:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.98828125Mb; avail=237462.0703125Mb
2020-10-12 08:18:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049962
2020-10-12 08:18:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122115
2020-10-12 08:18:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.98828125Mb; avail=237462.0703125Mb
2020-10-12 08:18:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7091.98828125Mb; avail=237462.0703125Mb
2020-10-12 08:18:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001255
2020-10-12 08:18:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.98828125Mb; avail=237462.0703125Mb
2020-10-12 08:18:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068684
2020-10-12 08:18:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.91015625Mb; avail=237461.8828125Mb
2020-10-12 08:18:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049873
2020-10-12 08:18:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120565
2020-10-12 08:18:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.91015625Mb; avail=237461.8828125Mb
2020-10-12 08:19:00 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.051 | nll_loss 3.436 | ppl 10.82 | wps 55826.6 | wpb 2283.9 | bsz 87.3 | num_updates 20628 | best_loss 5.051
2020-10-12 08:19:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:19:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 27 @ 20628 updates, score 5.051) (writing took 4.792670049006119 seconds)
2020-10-12 08:19:05 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 08:19:05 | INFO | train | epoch 027 | loss 4.728 | nll_loss 3.196 | ppl 9.17 | wps 23979.5 | ups 3.31 | wpb 7242.1 | bsz 278.7 | num_updates 20628 | lr 8.80707e-05 | gnorm 0.967 | clip 0 | train_wall 215 | wall 6247
2020-10-12 08:19:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 08:19:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 08:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6995.23828125Mb; avail=237559.0234375Mb
2020-10-12 08:19:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003411
2020-10-12 08:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043013
2020-10-12 08:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6995.23828125Mb; avail=237559.0234375Mb
2020-10-12 08:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001580
2020-10-12 08:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6995.23828125Mb; avail=237559.0234375Mb
2020-10-12 08:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.742405
2020-10-12 08:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.787822
2020-10-12 08:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7001.12109375Mb; avail=237553.140625Mb
2020-10-12 08:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6996.10546875Mb; avail=237558.0625Mb
2020-10-12 08:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033438
2020-10-12 08:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6998.10546875Mb; avail=237556.2890625Mb
2020-10-12 08:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001556
2020-10-12 08:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6998.7109375Mb; avail=237555.68359375Mb
2020-10-12 08:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.747791
2020-10-12 08:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.783620
2020-10-12 08:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7003.4921875Mb; avail=237551.03125Mb
2020-10-12 08:19:06 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 08:19:27 | INFO | train_inner | epoch 028:     72 / 764 loss=4.684, nll_loss=3.147, ppl=8.86, wps=18978.1, ups=2.6, wpb=7287.3, bsz=294.2, num_updates=20700, lr=8.79174e-05, gnorm=0.956, clip=0, train_wall=28, wall=6269
2020-10-12 08:19:56 | INFO | train_inner | epoch 028:    172 / 764 loss=4.683, nll_loss=3.146, ppl=8.85, wps=24715.1, ups=3.47, wpb=7128.6, bsz=274.3, num_updates=20800, lr=8.77058e-05, gnorm=0.976, clip=0, train_wall=28, wall=6298
2020-10-12 08:20:25 | INFO | train_inner | epoch 028:    272 / 764 loss=4.71, nll_loss=3.175, ppl=9.03, wps=25121.9, ups=3.44, wpb=7310.6, bsz=259.1, num_updates=20900, lr=8.74957e-05, gnorm=0.946, clip=0, train_wall=28, wall=6327
2020-10-12 08:20:54 | INFO | train_inner | epoch 028:    372 / 764 loss=4.698, nll_loss=3.162, ppl=8.95, wps=25032.2, ups=3.41, wpb=7337.2, bsz=271.2, num_updates=21000, lr=8.72872e-05, gnorm=0.955, clip=0, train_wall=29, wall=6357
2020-10-12 08:21:23 | INFO | train_inner | epoch 028:    472 / 764 loss=4.676, nll_loss=3.138, ppl=8.8, wps=24857.7, ups=3.45, wpb=7196.3, bsz=296.1, num_updates=21100, lr=8.70801e-05, gnorm=0.973, clip=0, train_wall=28, wall=6386
2020-10-12 08:21:53 | INFO | train_inner | epoch 028:    572 / 764 loss=4.686, nll_loss=3.148, ppl=8.87, wps=25031.5, ups=3.42, wpb=7314.8, bsz=293.1, num_updates=21200, lr=8.68744e-05, gnorm=0.949, clip=0, train_wall=28, wall=6415
2020-10-12 08:22:22 | INFO | train_inner | epoch 028:    672 / 764 loss=4.694, nll_loss=3.159, ppl=8.93, wps=24636.5, ups=3.44, wpb=7171.8, bsz=290.1, num_updates=21300, lr=8.66703e-05, gnorm=0.967, clip=0, train_wall=28, wall=6444
2020-10-12 08:22:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7117.1875Mb; avail=237436.83203125Mb
2020-10-12 08:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001430
2020-10-12 08:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7117.1875Mb; avail=237436.83203125Mb
2020-10-12 08:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069229
2020-10-12 08:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7117.1875Mb; avail=237436.83203125Mb
2020-10-12 08:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051431
2020-10-12 08:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122955
2020-10-12 08:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7117.4296875Mb; avail=237437.32421875Mb
2020-10-12 08:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7117.30078125Mb; avail=237436.71875Mb
2020-10-12 08:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001323
2020-10-12 08:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7117.30078125Mb; avail=237436.71875Mb
2020-10-12 08:22:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069197
2020-10-12 08:22:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7117.30078125Mb; avail=237436.71875Mb
2020-10-12 08:22:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050667
2020-10-12 08:22:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122031
2020-10-12 08:22:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7122.03125Mb; avail=237431.98828125Mb
2020-10-12 08:22:51 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.038 | nll_loss 3.415 | ppl 10.67 | wps 55502.3 | wpb 2283.9 | bsz 87.3 | num_updates 21392 | best_loss 5.038
2020-10-12 08:22:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:22:56 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 28 @ 21392 updates, score 5.038) (writing took 4.788822822039947 seconds)
2020-10-12 08:22:56 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 08:22:56 | INFO | train | epoch 028 | loss 4.696 | nll_loss 3.161 | ppl 8.94 | wps 23916.7 | ups 3.3 | wpb 7242.1 | bsz 278.7 | num_updates 21392 | lr 8.64837e-05 | gnorm 0.964 | clip 0 | train_wall 216 | wall 6478
2020-10-12 08:22:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 08:22:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 08:22:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7025.1171875Mb; avail=237529.140625Mb
2020-10-12 08:22:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004022
2020-10-12 08:22:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.042829
2020-10-12 08:22:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7016.953125Mb; avail=237537.3046875Mb
2020-10-12 08:22:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001570
2020-10-12 08:22:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7016.953125Mb; avail=237537.3046875Mb
2020-10-12 08:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.744135
2020-10-12 08:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.789400
2020-10-12 08:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7024.0Mb; avail=237530.40234375Mb
2020-10-12 08:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7019.078125Mb; avail=237535.32421875Mb
2020-10-12 08:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033260
2020-10-12 08:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7017.2421875Mb; avail=237537.22265625Mb
2020-10-12 08:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001566
2020-10-12 08:22:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7017.2421875Mb; avail=237537.22265625Mb
2020-10-12 08:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.752146
2020-10-12 08:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.787800
2020-10-12 08:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7022.63671875Mb; avail=237531.828125Mb
2020-10-12 08:22:58 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 08:23:00 | INFO | train_inner | epoch 029:      8 / 764 loss=4.736, nll_loss=3.206, ppl=9.23, wps=19070.3, ups=2.6, wpb=7331.3, bsz=266.2, num_updates=21400, lr=8.64675e-05, gnorm=0.975, clip=0, train_wall=28, wall=6482
2020-10-12 08:23:29 | INFO | train_inner | epoch 029:    108 / 764 loss=4.641, nll_loss=3.098, ppl=8.56, wps=25111.5, ups=3.43, wpb=7321.7, bsz=269.7, num_updates=21500, lr=8.62662e-05, gnorm=0.947, clip=0, train_wall=28, wall=6511
2020-10-12 08:23:58 | INFO | train_inner | epoch 029:    208 / 764 loss=4.657, nll_loss=3.116, ppl=8.67, wps=25231.1, ups=3.43, wpb=7364.3, bsz=276.5, num_updates=21600, lr=8.60663e-05, gnorm=0.95, clip=0, train_wall=28, wall=6541
2020-10-12 08:24:27 | INFO | train_inner | epoch 029:    308 / 764 loss=4.686, nll_loss=3.149, ppl=8.87, wps=24010.5, ups=3.47, wpb=6912.9, bsz=270.2, num_updates=21700, lr=8.58678e-05, gnorm=1.006, clip=0, train_wall=28, wall=6569
2020-10-12 08:24:56 | INFO | train_inner | epoch 029:    408 / 764 loss=4.643, nll_loss=3.1, ppl=8.58, wps=24731.8, ups=3.47, wpb=7137.1, bsz=302.8, num_updates=21800, lr=8.56706e-05, gnorm=0.986, clip=0, train_wall=28, wall=6598
2020-10-12 08:25:25 | INFO | train_inner | epoch 029:    508 / 764 loss=4.681, nll_loss=3.143, ppl=8.84, wps=25117, ups=3.41, wpb=7363.8, bsz=280.6, num_updates=21900, lr=8.54748e-05, gnorm=0.954, clip=0, train_wall=29, wall=6628
2020-10-12 08:25:54 | INFO | train_inner | epoch 029:    608 / 764 loss=4.69, nll_loss=3.154, ppl=8.9, wps=24792.4, ups=3.44, wpb=7205.5, bsz=269.9, num_updates=22000, lr=8.52803e-05, gnorm=0.977, clip=0, train_wall=28, wall=6657
2020-10-12 08:26:23 | INFO | train_inner | epoch 029:    708 / 764 loss=4.69, nll_loss=3.152, ppl=8.89, wps=25300.3, ups=3.44, wpb=7346, bsz=269.3, num_updates=22100, lr=8.50871e-05, gnorm=0.966, clip=0, train_wall=28, wall=6686
2020-10-12 08:26:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7118.3671875Mb; avail=237435.6484375Mb
2020-10-12 08:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001369
2020-10-12 08:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7118.3671875Mb; avail=237435.6484375Mb
2020-10-12 08:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068751
2020-10-12 08:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7109.015625Mb; avail=237445.0Mb
2020-10-12 08:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049674
2020-10-12 08:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120578
2020-10-12 08:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7109.015625Mb; avail=237445.0Mb
2020-10-12 08:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7109.015625Mb; avail=237445.0Mb
2020-10-12 08:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001256
2020-10-12 08:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7109.015625Mb; avail=237445.0Mb
2020-10-12 08:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069780
2020-10-12 08:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7109.2578125Mb; avail=237445.4921875Mb
2020-10-12 08:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049505
2020-10-12 08:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121306
2020-10-12 08:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7109.2578125Mb; avail=237445.4921875Mb
2020-10-12 08:26:43 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.029 | nll_loss 3.408 | ppl 10.62 | wps 55728.4 | wpb 2283.9 | bsz 87.3 | num_updates 22156 | best_loss 5.029
2020-10-12 08:26:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:26:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 29 @ 22156 updates, score 5.029) (writing took 4.784894138108939 seconds)
2020-10-12 08:26:47 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 08:26:47 | INFO | train | epoch 029 | loss 4.67 | nll_loss 3.13 | ppl 8.75 | wps 23901.2 | ups 3.3 | wpb 7242.1 | bsz 278.7 | num_updates 22156 | lr 8.49795e-05 | gnorm 0.969 | clip 0 | train_wall 216 | wall 6710
2020-10-12 08:26:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 08:26:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 08:26:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6999.04296875Mb; avail=237555.078125Mb
2020-10-12 08:26:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005118
2020-10-12 08:26:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044673
2020-10-12 08:26:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7005.3125Mb; avail=237549.06640625Mb
2020-10-12 08:26:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001576
2020-10-12 08:26:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7005.3125Mb; avail=237549.06640625Mb
2020-10-12 08:26:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.740110
2020-10-12 08:26:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.787187
2020-10-12 08:26:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7011.3046875Mb; avail=237543.07421875Mb
2020-10-12 08:26:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7006.49609375Mb; avail=237547.8828125Mb
2020-10-12 08:26:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033093
2020-10-12 08:26:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7006.49609375Mb; avail=237547.8828125Mb
2020-10-12 08:26:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001548
2020-10-12 08:26:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7006.49609375Mb; avail=237547.8828125Mb
2020-10-12 08:26:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.745953
2020-10-12 08:26:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.781408
2020-10-12 08:26:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7012.44140625Mb; avail=237542.05078125Mb
2020-10-12 08:26:49 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 08:27:02 | INFO | train_inner | epoch 030:     44 / 764 loss=4.634, nll_loss=3.09, ppl=8.51, wps=18798.7, ups=2.59, wpb=7247.6, bsz=298.2, num_updates=22200, lr=8.48953e-05, gnorm=0.95, clip=0, train_wall=28, wall=6724
2020-10-12 08:27:31 | INFO | train_inner | epoch 030:    144 / 764 loss=4.62, nll_loss=3.075, ppl=8.42, wps=24609.8, ups=3.47, wpb=7101.6, bsz=269.8, num_updates=22300, lr=8.47047e-05, gnorm=0.974, clip=0, train_wall=28, wall=6753
2020-10-12 08:28:00 | INFO | train_inner | epoch 030:    244 / 764 loss=4.666, nll_loss=3.126, ppl=8.73, wps=24622.5, ups=3.47, wpb=7087.9, bsz=267.8, num_updates=22400, lr=8.45154e-05, gnorm=1.012, clip=0, train_wall=28, wall=6782
2020-10-12 08:28:29 | INFO | train_inner | epoch 030:    344 / 764 loss=4.647, nll_loss=3.105, ppl=8.6, wps=24971.3, ups=3.43, wpb=7288.8, bsz=276.6, num_updates=22500, lr=8.43274e-05, gnorm=0.961, clip=0, train_wall=28, wall=6811
2020-10-12 08:28:58 | INFO | train_inner | epoch 030:    444 / 764 loss=4.645, nll_loss=3.102, ppl=8.59, wps=24816.2, ups=3.46, wpb=7162.2, bsz=259.2, num_updates=22600, lr=8.41406e-05, gnorm=0.991, clip=0, train_wall=28, wall=6840
2020-10-12 08:29:27 | INFO | train_inner | epoch 030:    544 / 764 loss=4.656, nll_loss=3.115, ppl=8.66, wps=24811.4, ups=3.41, wpb=7275.4, bsz=271.7, num_updates=22700, lr=8.39551e-05, gnorm=0.972, clip=0, train_wall=29, wall=6869
2020-10-12 08:29:56 | INFO | train_inner | epoch 030:    644 / 764 loss=4.647, nll_loss=3.105, ppl=8.6, wps=25195.2, ups=3.43, wpb=7336.7, bsz=299, num_updates=22800, lr=8.37708e-05, gnorm=0.974, clip=0, train_wall=28, wall=6898
2020-10-12 08:30:25 | INFO | train_inner | epoch 030:    744 / 764 loss=4.66, nll_loss=3.119, ppl=8.69, wps=25356.8, ups=3.42, wpb=7409, bsz=282.7, num_updates=22900, lr=8.35877e-05, gnorm=0.952, clip=0, train_wall=28, wall=6928
2020-10-12 08:30:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:30:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7040.140625Mb; avail=237513.90234375Mb
2020-10-12 08:30:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001354
2020-10-12 08:30:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7040.140625Mb; avail=237513.90234375Mb
2020-10-12 08:30:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069114
2020-10-12 08:30:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7040.140625Mb; avail=237513.90234375Mb
2020-10-12 08:30:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049929
2020-10-12 08:30:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121176
2020-10-12 08:30:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7040.140625Mb; avail=237513.90234375Mb
2020-10-12 08:30:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7040.2421875Mb; avail=237513.78125Mb
2020-10-12 08:30:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001198
2020-10-12 08:30:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7040.2421875Mb; avail=237513.78125Mb
2020-10-12 08:30:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068364
2020-10-12 08:30:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7039.9765625Mb; avail=237514.2734375Mb
2020-10-12 08:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049389
2020-10-12 08:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119708
2020-10-12 08:30:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7039.9765625Mb; avail=237514.2734375Mb
2020-10-12 08:30:34 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.033 | nll_loss 3.41 | ppl 10.63 | wps 55916.8 | wpb 2283.9 | bsz 87.3 | num_updates 22920 | best_loss 5.029
2020-10-12 08:30:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:30:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_last.pt (epoch 30 @ 22920 updates, score 5.033) (writing took 2.5940767771098763 seconds)
2020-10-12 08:30:37 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 08:30:37 | INFO | train | epoch 030 | loss 4.644 | nll_loss 3.101 | ppl 8.58 | wps 24139.5 | ups 3.33 | wpb 7242.1 | bsz 278.7 | num_updates 22920 | lr 8.35512e-05 | gnorm 0.973 | clip 0 | train_wall 216 | wall 6939
2020-10-12 08:30:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 08:30:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 08:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6937.0390625Mb; avail=237617.40625Mb
2020-10-12 08:30:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003957
2020-10-12 08:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043020
2020-10-12 08:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6938.8359375Mb; avail=237615.609375Mb
2020-10-12 08:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001556
2020-10-12 08:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6938.8359375Mb; avail=237615.609375Mb
2020-10-12 08:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.746328
2020-10-12 08:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.791719
2020-10-12 08:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6945.82421875Mb; avail=237608.62109375Mb
2020-10-12 08:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6940.90234375Mb; avail=237613.54296875Mb
2020-10-12 08:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032909
2020-10-12 08:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6940.90234375Mb; avail=237613.54296875Mb
2020-10-12 08:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001548
2020-10-12 08:30:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6940.90234375Mb; avail=237613.54296875Mb
2020-10-12 08:30:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.742646
2020-10-12 08:30:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.777910
2020-10-12 08:30:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6943.91015625Mb; avail=237609.8359375Mb
2020-10-12 08:30:38 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 08:31:02 | INFO | train_inner | epoch 031:     80 / 764 loss=4.583, nll_loss=3.031, ppl=8.18, wps=20342.2, ups=2.75, wpb=7401.2, bsz=290.9, num_updates=23000, lr=8.34058e-05, gnorm=0.946, clip=0, train_wall=28, wall=6964
2020-10-12 08:31:31 | INFO | train_inner | epoch 031:    180 / 764 loss=4.633, nll_loss=3.087, ppl=8.5, wps=24773.2, ups=3.42, wpb=7250.7, bsz=257.8, num_updates=23100, lr=8.3225e-05, gnorm=0.982, clip=0, train_wall=28, wall=6993
2020-10-12 08:32:00 | INFO | train_inner | epoch 031:    280 / 764 loss=4.644, nll_loss=3.101, ppl=8.58, wps=24962.9, ups=3.41, wpb=7329.3, bsz=264.7, num_updates=23200, lr=8.30455e-05, gnorm=0.981, clip=0, train_wall=29, wall=7023
2020-10-12 08:32:29 | INFO | train_inner | epoch 031:    380 / 764 loss=4.633, nll_loss=3.088, ppl=8.51, wps=24552.4, ups=3.45, wpb=7113.8, bsz=266.5, num_updates=23300, lr=8.28671e-05, gnorm=0.981, clip=0, train_wall=28, wall=7052
2020-10-12 08:32:58 | INFO | train_inner | epoch 031:    480 / 764 loss=4.617, nll_loss=3.071, ppl=8.4, wps=24552.8, ups=3.44, wpb=7141.1, bsz=280.6, num_updates=23400, lr=8.26898e-05, gnorm=0.971, clip=0, train_wall=28, wall=7081
2020-10-12 08:33:28 | INFO | train_inner | epoch 031:    580 / 764 loss=4.633, nll_loss=3.087, ppl=8.5, wps=24874, ups=3.43, wpb=7245.5, bsz=260, num_updates=23500, lr=8.25137e-05, gnorm=0.989, clip=0, train_wall=28, wall=7110
2020-10-12 08:33:57 | INFO | train_inner | epoch 031:    680 / 764 loss=4.597, nll_loss=3.048, ppl=8.27, wps=25145.9, ups=3.45, wpb=7293.3, bsz=315.7, num_updates=23600, lr=8.23387e-05, gnorm=0.978, clip=0, train_wall=28, wall=7139
2020-10-12 08:34:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:34:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7050.484375Mb; avail=237503.89453125Mb
2020-10-12 08:34:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001397
2020-10-12 08:34:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7050.484375Mb; avail=237503.89453125Mb
2020-10-12 08:34:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069868
2020-10-12 08:34:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7050.484375Mb; avail=237503.89453125Mb
2020-10-12 08:34:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049706
2020-10-12 08:34:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121752
2020-10-12 08:34:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7050.484375Mb; avail=237503.89453125Mb
2020-10-12 08:34:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7050.484375Mb; avail=237503.89453125Mb
2020-10-12 08:34:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001293
2020-10-12 08:34:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7050.484375Mb; avail=237503.89453125Mb
2020-10-12 08:34:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068973
2020-10-12 08:34:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7050.484375Mb; avail=237503.89453125Mb
2020-10-12 08:34:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049815
2020-10-12 08:34:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120838
2020-10-12 08:34:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7050.484375Mb; avail=237503.89453125Mb
2020-10-12 08:34:24 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 5.002 | nll_loss 3.376 | ppl 10.38 | wps 55735 | wpb 2283.9 | bsz 87.3 | num_updates 23684 | best_loss 5.002
2020-10-12 08:34:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:34:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 31 @ 23684 updates, score 5.002) (writing took 4.792904738103971 seconds)
2020-10-12 08:34:29 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 08:34:29 | INFO | train | epoch 031 | loss 4.619 | nll_loss 3.072 | ppl 8.41 | wps 23860.8 | ups 3.29 | wpb 7242.1 | bsz 278.7 | num_updates 23684 | lr 8.21926e-05 | gnorm 0.973 | clip 0 | train_wall 216 | wall 7171
2020-10-12 08:34:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 08:34:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 08:34:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6943.1328125Mb; avail=237611.08984375Mb
2020-10-12 08:34:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003710
2020-10-12 08:34:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043907
2020-10-12 08:34:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6943.8359375Mb; avail=237610.38671875Mb
2020-10-12 08:34:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001551
2020-10-12 08:34:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6943.8359375Mb; avail=237610.38671875Mb
2020-10-12 08:34:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.756541
2020-10-12 08:34:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.802823
2020-10-12 08:34:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6950.89453125Mb; avail=237603.328125Mb
2020-10-12 08:34:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6946.546875Mb; avail=237607.67578125Mb
2020-10-12 08:34:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033117
2020-10-12 08:34:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6946.546875Mb; avail=237607.67578125Mb
2020-10-12 08:34:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001589
2020-10-12 08:34:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6946.546875Mb; avail=237607.67578125Mb
2020-10-12 08:34:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.898573
2020-10-12 08:34:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.934154
2020-10-12 08:34:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6912.12109375Mb; avail=237642.359375Mb
2020-10-12 08:34:30 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 08:34:35 | INFO | train_inner | epoch 032:     16 / 764 loss=4.594, nll_loss=3.045, ppl=8.25, wps=18691.7, ups=2.6, wpb=7195, bsz=310.4, num_updates=23700, lr=8.21648e-05, gnorm=0.948, clip=0, train_wall=28, wall=7177
2020-10-12 08:35:04 | INFO | train_inner | epoch 032:    116 / 764 loss=4.575, nll_loss=3.022, ppl=8.12, wps=24856.6, ups=3.45, wpb=7214.5, bsz=277.8, num_updates=23800, lr=8.1992e-05, gnorm=0.963, clip=0, train_wall=28, wall=7206
2020-10-12 08:35:33 | INFO | train_inner | epoch 032:    216 / 764 loss=4.589, nll_loss=3.037, ppl=8.21, wps=25271, ups=3.43, wpb=7362.5, bsz=267.8, num_updates=23900, lr=8.18203e-05, gnorm=0.957, clip=0, train_wall=28, wall=7235
2020-10-12 08:36:02 | INFO | train_inner | epoch 032:    316 / 764 loss=4.572, nll_loss=3.019, ppl=8.11, wps=24546, ups=3.46, wpb=7103, bsz=285.2, num_updates=24000, lr=8.16497e-05, gnorm=0.971, clip=0, train_wall=28, wall=7264
2020-10-12 08:36:31 | INFO | train_inner | epoch 032:    416 / 764 loss=4.619, nll_loss=3.071, ppl=8.41, wps=25233.1, ups=3.47, wpb=7277.9, bsz=271.9, num_updates=24100, lr=8.14801e-05, gnorm=0.986, clip=0, train_wall=28, wall=7293
2020-10-12 08:37:00 | INFO | train_inner | epoch 032:    516 / 764 loss=4.599, nll_loss=3.049, ppl=8.28, wps=25363.8, ups=3.44, wpb=7368.8, bsz=281.1, num_updates=24200, lr=8.13116e-05, gnorm=0.941, clip=0, train_wall=28, wall=7322
2020-10-12 08:37:29 | INFO | train_inner | epoch 032:    616 / 764 loss=4.592, nll_loss=3.042, ppl=8.24, wps=24753.3, ups=3.46, wpb=7158, bsz=277.3, num_updates=24300, lr=8.11441e-05, gnorm=0.986, clip=0, train_wall=28, wall=7351
2020-10-12 08:37:58 | INFO | train_inner | epoch 032:    716 / 764 loss=4.63, nll_loss=3.085, ppl=8.48, wps=24688.3, ups=3.46, wpb=7145.3, bsz=256.9, num_updates=24400, lr=8.09776e-05, gnorm=1.001, clip=0, train_wall=28, wall=7380
2020-10-12 08:38:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6934.0234375Mb; avail=237620.0078125Mb
2020-10-12 08:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001557
2020-10-12 08:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6934.0234375Mb; avail=237620.0078125Mb
2020-10-12 08:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069354
2020-10-12 08:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6934.0234375Mb; avail=237620.0078125Mb
2020-10-12 08:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051076
2020-10-12 08:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122821
2020-10-12 08:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6934.0234375Mb; avail=237620.0078125Mb
2020-10-12 08:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6934.0234375Mb; avail=237620.0078125Mb
2020-10-12 08:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001212
2020-10-12 08:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6934.0234375Mb; avail=237620.0078125Mb
2020-10-12 08:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068333
2020-10-12 08:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6935.12109375Mb; avail=237618.91015625Mb
2020-10-12 08:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050294
2020-10-12 08:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120616
2020-10-12 08:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6939.47265625Mb; avail=237614.55859375Mb
2020-10-12 08:38:15 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.005 | nll_loss 3.377 | ppl 10.39 | wps 55850.4 | wpb 2283.9 | bsz 87.3 | num_updates 24448 | best_loss 5.002
2020-10-12 08:38:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:38:17 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_last.pt (epoch 32 @ 24448 updates, score 5.005) (writing took 2.583381161093712 seconds)
2020-10-12 08:38:17 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 08:38:17 | INFO | train | epoch 032 | loss 4.595 | nll_loss 3.045 | ppl 8.25 | wps 24179.3 | ups 3.34 | wpb 7242.1 | bsz 278.7 | num_updates 24448 | lr 8.08981e-05 | gnorm 0.971 | clip 0 | train_wall 216 | wall 7400
2020-10-12 08:38:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 08:38:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 08:38:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6907.39453125Mb; avail=237647.38671875Mb
2020-10-12 08:38:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004113
2020-10-12 08:38:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.042794
2020-10-12 08:38:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6907.85546875Mb; avail=237646.19140625Mb
2020-10-12 08:38:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001722
2020-10-12 08:38:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6907.85546875Mb; avail=237646.19140625Mb
2020-10-12 08:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.735737
2020-10-12 08:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.781079
2020-10-12 08:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6907.26171875Mb; avail=237647.24609375Mb
2020-10-12 08:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6907.26171875Mb; avail=237647.24609375Mb
2020-10-12 08:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033165
2020-10-12 08:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6907.26171875Mb; avail=237647.24609375Mb
2020-10-12 08:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001568
2020-10-12 08:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6907.26171875Mb; avail=237647.24609375Mb
2020-10-12 08:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.741319
2020-10-12 08:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.776877
2020-10-12 08:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6911.2421875Mb; avail=237643.234375Mb
2020-10-12 08:38:19 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 08:38:34 | INFO | train_inner | epoch 033:     52 / 764 loss=4.558, nll_loss=3.003, ppl=8.01, wps=20291.4, ups=2.75, wpb=7368.2, bsz=306.6, num_updates=24500, lr=8.08122e-05, gnorm=0.952, clip=0, train_wall=28, wall=7416
2020-10-12 08:39:03 | INFO | train_inner | epoch 033:    152 / 764 loss=4.563, nll_loss=3.009, ppl=8.05, wps=24705.2, ups=3.44, wpb=7175.5, bsz=269.1, num_updates=24600, lr=8.06478e-05, gnorm=0.962, clip=0, train_wall=28, wall=7446
2020-10-12 08:39:32 | INFO | train_inner | epoch 033:    252 / 764 loss=4.562, nll_loss=3.008, ppl=8.05, wps=25043.9, ups=3.46, wpb=7240.5, bsz=294.5, num_updates=24700, lr=8.04844e-05, gnorm=0.982, clip=0, train_wall=28, wall=7474
2020-10-12 08:40:01 | INFO | train_inner | epoch 033:    352 / 764 loss=4.564, nll_loss=3.01, ppl=8.05, wps=24921.3, ups=3.46, wpb=7199.6, bsz=281, num_updates=24800, lr=8.03219e-05, gnorm=0.97, clip=0, train_wall=28, wall=7503
2020-10-12 08:40:30 | INFO | train_inner | epoch 033:    452 / 764 loss=4.553, nll_loss=2.998, ppl=7.99, wps=24941.2, ups=3.44, wpb=7242.9, bsz=302.6, num_updates=24900, lr=8.01605e-05, gnorm=0.972, clip=0, train_wall=28, wall=7532
2020-10-12 08:40:59 | INFO | train_inner | epoch 033:    552 / 764 loss=4.583, nll_loss=3.031, ppl=8.18, wps=24902.8, ups=3.42, wpb=7275.8, bsz=273.7, num_updates=25000, lr=8e-05, gnorm=0.975, clip=0, train_wall=28, wall=7562
2020-10-12 08:41:28 | INFO | train_inner | epoch 033:    652 / 764 loss=4.586, nll_loss=3.035, ppl=8.2, wps=24395.2, ups=3.46, wpb=7058.6, bsz=277.3, num_updates=25100, lr=7.98405e-05, gnorm=0.988, clip=0, train_wall=28, wall=7591
2020-10-12 08:41:57 | INFO | train_inner | epoch 033:    752 / 764 loss=4.606, nll_loss=3.056, ppl=8.32, wps=25520.5, ups=3.44, wpb=7420.9, bsz=255.3, num_updates=25200, lr=7.96819e-05, gnorm=0.972, clip=0, train_wall=28, wall=7620
2020-10-12 08:42:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6989.2890625Mb; avail=237564.7265625Mb
2020-10-12 08:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001363
2020-10-12 08:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6989.2890625Mb; avail=237564.7265625Mb
2020-10-12 08:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067024
2020-10-12 08:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6989.2890625Mb; avail=237564.7265625Mb
2020-10-12 08:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049656
2020-10-12 08:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118815
2020-10-12 08:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6989.2890625Mb; avail=237564.7265625Mb
2020-10-12 08:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6989.2890625Mb; avail=237564.7265625Mb
2020-10-12 08:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001205
2020-10-12 08:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6989.2890625Mb; avail=237564.7265625Mb
2020-10-12 08:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067700
2020-10-12 08:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6989.2890625Mb; avail=237564.7265625Mb
2020-10-12 08:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049549
2020-10-12 08:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119200
2020-10-12 08:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6989.05859375Mb; avail=237564.95703125Mb
2020-10-12 08:42:04 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 4.99 | nll_loss 3.361 | ppl 10.27 | wps 55851 | wpb 2283.9 | bsz 87.3 | num_updates 25212 | best_loss 4.99
2020-10-12 08:42:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:42:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 33 @ 25212 updates, score 4.99) (writing took 4.7837554649449885 seconds)
2020-10-12 08:42:08 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 08:42:08 | INFO | train | epoch 033 | loss 4.572 | nll_loss 3.019 | ppl 8.11 | wps 23940.3 | ups 3.31 | wpb 7242.1 | bsz 278.7 | num_updates 25212 | lr 7.96629e-05 | gnorm 0.972 | clip 0 | train_wall 216 | wall 7631
2020-10-12 08:42:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 08:42:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 08:42:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6917.85546875Mb; avail=237639.8359375Mb
2020-10-12 08:42:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004503
2020-10-12 08:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043939
2020-10-12 08:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6914.35546875Mb; avail=237639.890625Mb
2020-10-12 08:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001721
2020-10-12 08:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6914.35546875Mb; avail=237639.890625Mb
2020-10-12 08:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.739665
2020-10-12 08:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.786154
2020-10-12 08:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6914.51953125Mb; avail=237639.7265625Mb
2020-10-12 08:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6914.27734375Mb; avail=237639.86328125Mb
2020-10-12 08:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033321
2020-10-12 08:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6914.26171875Mb; avail=237639.87890625Mb
2020-10-12 08:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001576
2020-10-12 08:42:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6914.26171875Mb; avail=237639.87890625Mb
2020-10-12 08:42:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.735558
2020-10-12 08:42:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.771281
2020-10-12 08:42:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6914.8671875Mb; avail=237639.625Mb
2020-10-12 08:42:10 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 08:42:36 | INFO | train_inner | epoch 034:     88 / 764 loss=4.544, nll_loss=2.987, ppl=7.93, wps=19130.1, ups=2.59, wpb=7383, bsz=266.2, num_updates=25300, lr=7.95243e-05, gnorm=0.961, clip=0, train_wall=29, wall=7658
2020-10-12 08:43:05 | INFO | train_inner | epoch 034:    188 / 764 loss=4.533, nll_loss=2.974, ppl=7.86, wps=25235.8, ups=3.44, wpb=7340.8, bsz=289, num_updates=25400, lr=7.93676e-05, gnorm=0.966, clip=0, train_wall=28, wall=7687
2020-10-12 08:43:34 | INFO | train_inner | epoch 034:    288 / 764 loss=4.546, nll_loss=2.989, ppl=7.94, wps=24848.5, ups=3.44, wpb=7219.7, bsz=273.9, num_updates=25500, lr=7.92118e-05, gnorm=0.963, clip=0, train_wall=28, wall=7716
2020-10-12 08:44:03 | INFO | train_inner | epoch 034:    388 / 764 loss=4.547, nll_loss=2.99, ppl=7.95, wps=24957.7, ups=3.41, wpb=7316.6, bsz=287.1, num_updates=25600, lr=7.90569e-05, gnorm=0.969, clip=0, train_wall=29, wall=7746
2020-10-12 08:44:33 | INFO | train_inner | epoch 034:    488 / 764 loss=4.561, nll_loss=3.006, ppl=8.03, wps=24682.4, ups=3.42, wpb=7216.2, bsz=264.8, num_updates=25700, lr=7.8903e-05, gnorm=0.978, clip=0, train_wall=28, wall=7775
2020-10-12 08:45:02 | INFO | train_inner | epoch 034:    588 / 764 loss=4.544, nll_loss=2.989, ppl=7.94, wps=24627, ups=3.44, wpb=7156.4, bsz=305.1, num_updates=25800, lr=7.87499e-05, gnorm=0.985, clip=0, train_wall=28, wall=7804
2020-10-12 08:45:30 | INFO | train_inner | epoch 034:    688 / 764 loss=4.577, nll_loss=3.024, ppl=8.14, wps=24710.6, ups=3.48, wpb=7100.1, bsz=262.6, num_updates=25900, lr=7.85977e-05, gnorm=0.986, clip=0, train_wall=28, wall=7833
2020-10-12 08:45:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:45:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7006.8203125Mb; avail=237547.24609375Mb
2020-10-12 08:45:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001386
2020-10-12 08:45:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7006.8203125Mb; avail=237547.24609375Mb
2020-10-12 08:45:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068974
2020-10-12 08:45:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7006.8203125Mb; avail=237547.24609375Mb
2020-10-12 08:45:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.097981
2020-10-12 08:45:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.169183
2020-10-12 08:45:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7006.8203125Mb; avail=237547.24609375Mb
2020-10-12 08:45:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7006.8203125Mb; avail=237547.24609375Mb
2020-10-12 08:45:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001810
2020-10-12 08:45:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7006.8203125Mb; avail=237547.24609375Mb
2020-10-12 08:45:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.099203
2020-10-12 08:45:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7006.8203125Mb; avail=237547.24609375Mb
2020-10-12 08:45:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054870
2020-10-12 08:45:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.156927
2020-10-12 08:45:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7006.328125Mb; avail=237547.73828125Mb
2020-10-12 08:45:56 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.996 | nll_loss 3.372 | ppl 10.35 | wps 55859.3 | wpb 2283.9 | bsz 87.3 | num_updates 25976 | best_loss 4.99
2020-10-12 08:45:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:45:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_last.pt (epoch 34 @ 25976 updates, score 4.996) (writing took 2.5961112789809704 seconds)
2020-10-12 08:45:58 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 08:45:58 | INFO | train | epoch 034 | loss 4.55 | nll_loss 2.994 | ppl 7.96 | wps 24082.7 | ups 3.33 | wpb 7242.1 | bsz 278.7 | num_updates 25976 | lr 7.84827e-05 | gnorm 0.972 | clip 0 | train_wall 216 | wall 7860
2020-10-12 08:45:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 08:45:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 08:45:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6921.484375Mb; avail=237632.7890625Mb
2020-10-12 08:45:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004457
2020-10-12 08:45:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043479
2020-10-12 08:45:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6931.05078125Mb; avail=237623.22265625Mb
2020-10-12 08:45:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001607
2020-10-12 08:45:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6932.26171875Mb; avail=237622.01171875Mb
2020-10-12 08:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.737003
2020-10-12 08:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.782930
2020-10-12 08:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6923.5390625Mb; avail=237630.98046875Mb
2020-10-12 08:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6924.2734375Mb; avail=237630.98046875Mb
2020-10-12 08:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033085
2020-10-12 08:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6923.5390625Mb; avail=237630.98046875Mb
2020-10-12 08:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001601
2020-10-12 08:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6923.5390625Mb; avail=237630.98046875Mb
2020-10-12 08:46:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.740524
2020-10-12 08:46:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.776038
2020-10-12 08:46:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6996.83203125Mb; avail=237557.55078125Mb
2020-10-12 08:46:00 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 08:46:07 | INFO | train_inner | epoch 035:     24 / 764 loss=4.549, nll_loss=2.993, ppl=7.96, wps=19804.5, ups=2.74, wpb=7214.9, bsz=277.5, num_updates=26000, lr=7.84465e-05, gnorm=0.965, clip=0, train_wall=28, wall=7869
2020-10-12 08:46:36 | INFO | train_inner | epoch 035:    124 / 764 loss=4.501, nll_loss=2.939, ppl=7.67, wps=24668.3, ups=3.46, wpb=7131.5, bsz=287.1, num_updates=26100, lr=7.8296e-05, gnorm=0.973, clip=0, train_wall=28, wall=7898
2020-10-12 08:47:05 | INFO | train_inner | epoch 035:    224 / 764 loss=4.552, nll_loss=2.994, ppl=7.96, wps=25089.3, ups=3.45, wpb=7267.9, bsz=237.6, num_updates=26200, lr=7.81465e-05, gnorm=0.981, clip=0, train_wall=28, wall=7927
2020-10-12 08:47:34 | INFO | train_inner | epoch 035:    324 / 764 loss=4.522, nll_loss=2.962, ppl=7.79, wps=24791.5, ups=3.44, wpb=7215.9, bsz=297.4, num_updates=26300, lr=7.79978e-05, gnorm=0.969, clip=0, train_wall=28, wall=7956
2020-10-12 08:48:03 | INFO | train_inner | epoch 035:    424 / 764 loss=4.521, nll_loss=2.962, ppl=7.79, wps=24970.3, ups=3.44, wpb=7253.9, bsz=284.1, num_updates=26400, lr=7.78499e-05, gnorm=0.97, clip=0, train_wall=28, wall=7985
2020-10-12 08:48:32 | INFO | train_inner | epoch 035:    524 / 764 loss=4.532, nll_loss=2.973, ppl=7.85, wps=24696, ups=3.45, wpb=7149.5, bsz=279.4, num_updates=26500, lr=7.77029e-05, gnorm=0.981, clip=0, train_wall=28, wall=8014
2020-10-12 08:49:01 | INFO | train_inner | epoch 035:    624 / 764 loss=4.551, nll_loss=2.993, ppl=7.96, wps=25493.9, ups=3.45, wpb=7390.5, bsz=265.7, num_updates=26600, lr=7.75567e-05, gnorm=0.961, clip=0, train_wall=28, wall=8043
2020-10-12 08:49:30 | INFO | train_inner | epoch 035:    724 / 764 loss=4.514, nll_loss=2.953, ppl=7.74, wps=25299.7, ups=3.44, wpb=7346.6, bsz=303.2, num_updates=26700, lr=7.74113e-05, gnorm=0.983, clip=0, train_wall=28, wall=8072
2020-10-12 08:49:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:49:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7096.10546875Mb; avail=237458.80859375Mb
2020-10-12 08:49:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001359
2020-10-12 08:49:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.10546875Mb; avail=237458.80859375Mb
2020-10-12 08:49:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069309
2020-10-12 08:49:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.9765625Mb; avail=237458.203125Mb
2020-10-12 08:49:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049885
2020-10-12 08:49:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121327
2020-10-12 08:49:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.9765625Mb; avail=237458.203125Mb
2020-10-12 08:49:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7095.9765625Mb; avail=237458.203125Mb
2020-10-12 08:49:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001193
2020-10-12 08:49:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.9765625Mb; avail=237458.203125Mb
2020-10-12 08:49:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068757
2020-10-12 08:49:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.9765625Mb; avail=237458.203125Mb
2020-10-12 08:49:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049834
2020-10-12 08:49:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120530
2020-10-12 08:49:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.9765625Mb; avail=237458.203125Mb
2020-10-12 08:49:44 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.979 | nll_loss 3.348 | ppl 10.18 | wps 55872 | wpb 2283.9 | bsz 87.3 | num_updates 26740 | best_loss 4.979
2020-10-12 08:49:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:49:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 35 @ 26740 updates, score 4.979) (writing took 4.785307897953317 seconds)
2020-10-12 08:49:49 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 08:49:49 | INFO | train | epoch 035 | loss 4.53 | nll_loss 2.97 | ppl 7.84 | wps 23968.9 | ups 3.31 | wpb 7242.1 | bsz 278.7 | num_updates 26740 | lr 7.73534e-05 | gnorm 0.975 | clip 0 | train_wall 215 | wall 8091
2020-10-12 08:49:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 08:49:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 08:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7001.828125Mb; avail=237552.41015625Mb
2020-10-12 08:49:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004109
2020-10-12 08:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043180
2020-10-12 08:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7002.97265625Mb; avail=237551.265625Mb
2020-10-12 08:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001595
2020-10-12 08:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7002.97265625Mb; avail=237551.265625Mb
2020-10-12 08:49:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.741830
2020-10-12 08:49:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.787383
2020-10-12 08:49:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7008.79296875Mb; avail=237545.89453125Mb
2020-10-12 08:49:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7004.36328125Mb; avail=237550.32421875Mb
2020-10-12 08:49:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032932
2020-10-12 08:49:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7003.62890625Mb; avail=237550.32421875Mb
2020-10-12 08:49:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001622
2020-10-12 08:49:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7003.62890625Mb; avail=237550.32421875Mb
2020-10-12 08:49:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.743150
2020-10-12 08:49:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.778532
2020-10-12 08:49:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7008.66015625Mb; avail=237546.02734375Mb
2020-10-12 08:49:51 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 08:50:08 | INFO | train_inner | epoch 036:     60 / 764 loss=4.519, nll_loss=2.959, ppl=7.77, wps=18644.3, ups=2.62, wpb=7104.3, bsz=278.8, num_updates=26800, lr=7.72667e-05, gnorm=0.982, clip=0, train_wall=28, wall=8110
2020-10-12 08:50:37 | INFO | train_inner | epoch 036:    160 / 764 loss=4.489, nll_loss=2.924, ppl=7.59, wps=25251.7, ups=3.47, wpb=7286.5, bsz=284.7, num_updates=26900, lr=7.7123e-05, gnorm=0.966, clip=0, train_wall=28, wall=8139
2020-10-12 08:51:06 | INFO | train_inner | epoch 036:    260 / 764 loss=4.514, nll_loss=2.951, ppl=7.73, wps=25161.9, ups=3.43, wpb=7333.7, bsz=248.8, num_updates=27000, lr=7.698e-05, gnorm=0.965, clip=0, train_wall=28, wall=8168
2020-10-12 08:51:35 | INFO | train_inner | epoch 036:    360 / 764 loss=4.51, nll_loss=2.949, ppl=7.72, wps=24891.2, ups=3.46, wpb=7192.7, bsz=274.6, num_updates=27100, lr=7.68379e-05, gnorm=0.97, clip=0, train_wall=28, wall=8197
2020-10-12 08:52:04 | INFO | train_inner | epoch 036:    460 / 764 loss=4.516, nll_loss=2.954, ppl=7.75, wps=25249.1, ups=3.41, wpb=7393.7, bsz=291.7, num_updates=27200, lr=7.66965e-05, gnorm=0.966, clip=0, train_wall=28, wall=8226
2020-10-12 08:52:33 | INFO | train_inner | epoch 036:    560 / 764 loss=4.517, nll_loss=2.956, ppl=7.76, wps=24867.9, ups=3.43, wpb=7245.8, bsz=279.1, num_updates=27300, lr=7.65559e-05, gnorm=0.996, clip=0, train_wall=28, wall=8256
2020-10-12 08:53:02 | INFO | train_inner | epoch 036:    660 / 764 loss=4.503, nll_loss=2.942, ppl=7.68, wps=24229.7, ups=3.48, wpb=6969.3, bsz=280.2, num_updates=27400, lr=7.64161e-05, gnorm=1, clip=0, train_wall=28, wall=8284
2020-10-12 08:53:31 | INFO | train_inner | epoch 036:    760 / 764 loss=4.538, nll_loss=2.979, ppl=7.89, wps=25250.8, ups=3.42, wpb=7373.4, bsz=282.8, num_updates=27500, lr=7.6277e-05, gnorm=0.97, clip=0, train_wall=28, wall=8314
2020-10-12 08:53:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:53:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7110.14453125Mb; avail=237444.62890625Mb
2020-10-12 08:53:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001365
2020-10-12 08:53:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7110.75Mb; avail=237444.0234375Mb
2020-10-12 08:53:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069069
2020-10-12 08:53:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7119.34375Mb; avail=237435.3125Mb
2020-10-12 08:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049878
2020-10-12 08:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121105
2020-10-12 08:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.484375Mb; avail=237440.76953125Mb
2020-10-12 08:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7115.30078125Mb; avail=237438.953125Mb
2020-10-12 08:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001239
2020-10-12 08:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7115.90625Mb; avail=237438.34765625Mb
2020-10-12 08:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067936
2020-10-12 08:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7110.0625Mb; avail=237444.19140625Mb
2020-10-12 08:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049466
2020-10-12 08:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119411
2020-10-12 08:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7110.0625Mb; avail=237444.19140625Mb
2020-10-12 08:53:35 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 4.968 | nll_loss 3.337 | ppl 10.11 | wps 55834.2 | wpb 2283.9 | bsz 87.3 | num_updates 27504 | best_loss 4.968
2020-10-12 08:53:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:53:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 36 @ 27504 updates, score 4.968) (writing took 4.784724296070635 seconds)
2020-10-12 08:53:40 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 08:53:40 | INFO | train | epoch 036 | loss 4.51 | nll_loss 2.948 | ppl 7.71 | wps 23955.3 | ups 3.31 | wpb 7242.1 | bsz 278.7 | num_updates 27504 | lr 7.62715e-05 | gnorm 0.976 | clip 0 | train_wall 216 | wall 8322
2020-10-12 08:53:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 08:53:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 08:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7011.40234375Mb; avail=237542.828125Mb
2020-10-12 08:53:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003531
2020-10-12 08:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043049
2020-10-12 08:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7011.7578125Mb; avail=237542.47265625Mb
2020-10-12 08:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001566
2020-10-12 08:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7011.7578125Mb; avail=237542.47265625Mb
2020-10-12 08:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.739437
2020-10-12 08:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.784907
2020-10-12 08:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7017.89453125Mb; avail=237536.3671875Mb
2020-10-12 08:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7012.97265625Mb; avail=237541.2890625Mb
2020-10-12 08:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033108
2020-10-12 08:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7012.97265625Mb; avail=237541.2890625Mb
2020-10-12 08:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001559
2020-10-12 08:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7012.97265625Mb; avail=237541.2890625Mb
2020-10-12 08:53:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.740985
2020-10-12 08:53:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.776476
2020-10-12 08:53:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7029.546875Mb; avail=237524.9453125Mb
2020-10-12 08:53:42 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 08:54:10 | INFO | train_inner | epoch 037:     96 / 764 loss=4.442, nll_loss=2.871, ppl=7.32, wps=18978.7, ups=2.61, wpb=7265, bsz=298.3, num_updates=27600, lr=7.61387e-05, gnorm=0.957, clip=0, train_wall=28, wall=8352
2020-10-12 08:54:39 | INFO | train_inner | epoch 037:    196 / 764 loss=4.486, nll_loss=2.92, ppl=7.57, wps=24759.7, ups=3.43, wpb=7209.9, bsz=276.8, num_updates=27700, lr=7.60011e-05, gnorm=0.995, clip=0, train_wall=28, wall=8381
2020-10-12 08:55:08 | INFO | train_inner | epoch 037:    296 / 764 loss=4.458, nll_loss=2.889, ppl=7.41, wps=24781.9, ups=3.45, wpb=7179.5, bsz=290, num_updates=27800, lr=7.58643e-05, gnorm=0.971, clip=0, train_wall=28, wall=8410
2020-10-12 08:55:37 | INFO | train_inner | epoch 037:    396 / 764 loss=4.49, nll_loss=2.924, ppl=7.59, wps=24899.4, ups=3.46, wpb=7189.9, bsz=276.5, num_updates=27900, lr=7.57282e-05, gnorm=0.977, clip=0, train_wall=28, wall=8439
2020-10-12 08:56:06 | INFO | train_inner | epoch 037:    496 / 764 loss=4.492, nll_loss=2.928, ppl=7.61, wps=24925.7, ups=3.44, wpb=7247.3, bsz=283.7, num_updates=28000, lr=7.55929e-05, gnorm=0.975, clip=0, train_wall=28, wall=8468
2020-10-12 08:56:35 | INFO | train_inner | epoch 037:    596 / 764 loss=4.517, nll_loss=2.955, ppl=7.76, wps=25046.5, ups=3.42, wpb=7318.7, bsz=275.8, num_updates=28100, lr=7.54583e-05, gnorm=0.973, clip=0, train_wall=28, wall=8497
2020-10-12 08:57:04 | INFO | train_inner | epoch 037:    696 / 764 loss=4.518, nll_loss=2.957, ppl=7.77, wps=24961.2, ups=3.42, wpb=7297.1, bsz=273.9, num_updates=28200, lr=7.53244e-05, gnorm=0.979, clip=0, train_wall=28, wall=8526
2020-10-12 08:57:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:57:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7130.6953125Mb; avail=237423.55859375Mb
2020-10-12 08:57:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001374
2020-10-12 08:57:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7130.6953125Mb; avail=237423.55859375Mb
2020-10-12 08:57:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070964
2020-10-12 08:57:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7130.6953125Mb; avail=237423.55859375Mb
2020-10-12 08:57:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049108
2020-10-12 08:57:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122207
2020-10-12 08:57:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7134.328125Mb; avail=237419.92578125Mb
2020-10-12 08:57:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7136.64453125Mb; avail=237417.609375Mb
2020-10-12 08:57:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001244
2020-10-12 08:57:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7136.64453125Mb; avail=237417.609375Mb
2020-10-12 08:57:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068303
2020-10-12 08:57:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7136.86328125Mb; avail=237417.390625Mb
2020-10-12 08:57:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049691
2020-10-12 08:57:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120012
2020-10-12 08:57:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7141.58203125Mb; avail=237412.671875Mb
2020-10-12 08:57:26 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 4.957 | nll_loss 3.324 | ppl 10.02 | wps 55759 | wpb 2283.9 | bsz 87.3 | num_updates 28268 | best_loss 4.957
2020-10-12 08:57:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:57:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 37 @ 28268 updates, score 4.957) (writing took 4.789762559812516 seconds)
2020-10-12 08:57:31 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 08:57:31 | INFO | train | epoch 037 | loss 4.491 | nll_loss 2.927 | ppl 7.6 | wps 23925.9 | ups 3.3 | wpb 7242.1 | bsz 278.7 | num_updates 28268 | lr 7.52337e-05 | gnorm 0.976 | clip 0 | train_wall 216 | wall 8553
2020-10-12 08:57:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 08:57:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 08:57:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7031.75390625Mb; avail=237522.7578125Mb
2020-10-12 08:57:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003453
2020-10-12 08:57:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043404
2020-10-12 08:57:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7031.75390625Mb; avail=237522.7578125Mb
2020-10-12 08:57:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001547
2020-10-12 08:57:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7031.75390625Mb; avail=237522.7578125Mb
2020-10-12 08:57:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.747504
2020-10-12 08:57:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.793280
2020-10-12 08:57:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7038.72265625Mb; avail=237515.7890625Mb
2020-10-12 08:57:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7033.6796875Mb; avail=237520.83203125Mb
2020-10-12 08:57:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032992
2020-10-12 08:57:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7033.33984375Mb; avail=237521.171875Mb
2020-10-12 08:57:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001602
2020-10-12 08:57:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7033.33984375Mb; avail=237521.171875Mb
2020-10-12 08:57:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.740141
2020-10-12 08:57:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.775550
2020-10-12 08:57:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7051.30078125Mb; avail=237503.72265625Mb
2020-10-12 08:57:33 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 08:57:43 | INFO | train_inner | epoch 038:     32 / 764 loss=4.507, nll_loss=2.944, ppl=7.7, wps=18445.1, ups=2.57, wpb=7178.8, bsz=259.4, num_updates=28300, lr=7.51912e-05, gnorm=0.976, clip=0, train_wall=29, wall=8565
2020-10-12 08:58:12 | INFO | train_inner | epoch 038:    132 / 764 loss=4.432, nll_loss=2.859, ppl=7.26, wps=24600.4, ups=3.43, wpb=7163.4, bsz=281, num_updates=28400, lr=7.50587e-05, gnorm=0.97, clip=0, train_wall=28, wall=8594
2020-10-12 08:58:41 | INFO | train_inner | epoch 038:    232 / 764 loss=4.454, nll_loss=2.884, ppl=7.38, wps=25171.3, ups=3.43, wpb=7348.7, bsz=285.4, num_updates=28500, lr=7.49269e-05, gnorm=0.975, clip=0, train_wall=28, wall=8624
2020-10-12 08:59:10 | INFO | train_inner | epoch 038:    332 / 764 loss=4.48, nll_loss=2.914, ppl=7.54, wps=25125.1, ups=3.44, wpb=7300.7, bsz=277.9, num_updates=28600, lr=7.47958e-05, gnorm=0.985, clip=0, train_wall=28, wall=8653
2020-10-12 08:59:39 | INFO | train_inner | epoch 038:    432 / 764 loss=4.492, nll_loss=2.926, ppl=7.6, wps=25018.8, ups=3.46, wpb=7235, bsz=269.7, num_updates=28700, lr=7.46653e-05, gnorm=0.983, clip=0, train_wall=28, wall=8682
2020-10-12 09:00:08 | INFO | train_inner | epoch 038:    532 / 764 loss=4.47, nll_loss=2.903, ppl=7.48, wps=25066.1, ups=3.45, wpb=7266.1, bsz=286.6, num_updates=28800, lr=7.45356e-05, gnorm=0.952, clip=0, train_wall=28, wall=8710
2020-10-12 09:00:37 | INFO | train_inner | epoch 038:    632 / 764 loss=4.495, nll_loss=2.93, ppl=7.62, wps=24380.8, ups=3.47, wpb=7032, bsz=274.4, num_updates=28900, lr=7.44065e-05, gnorm=0.99, clip=0, train_wall=28, wall=8739
2020-10-12 09:01:06 | INFO | train_inner | epoch 038:    732 / 764 loss=4.503, nll_loss=2.939, ppl=7.67, wps=25287, ups=3.44, wpb=7355.5, bsz=273.3, num_updates=29000, lr=7.42781e-05, gnorm=0.981, clip=0, train_wall=28, wall=8768
2020-10-12 09:01:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 09:01:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7045.33203125Mb; avail=237508.6484375Mb
2020-10-12 09:01:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001362
2020-10-12 09:01:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7045.33203125Mb; avail=237508.6484375Mb
2020-10-12 09:01:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068640
2020-10-12 09:01:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7045.33203125Mb; avail=237508.6484375Mb
2020-10-12 09:01:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049234
2020-10-12 09:01:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120008
2020-10-12 09:01:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7045.33203125Mb; avail=237508.6484375Mb
2020-10-12 09:01:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7045.33203125Mb; avail=237508.6484375Mb
2020-10-12 09:01:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001180
2020-10-12 09:01:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7045.33203125Mb; avail=237508.6484375Mb
2020-10-12 09:01:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069805
2020-10-12 09:01:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7045.33203125Mb; avail=237508.6484375Mb
2020-10-12 09:01:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048866
2020-10-12 09:01:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120595
2020-10-12 09:01:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7045.4453125Mb; avail=237508.53515625Mb
2020-10-12 09:01:18 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 4.948 | nll_loss 3.313 | ppl 9.94 | wps 55563 | wpb 2283.9 | bsz 87.3 | num_updates 29032 | best_loss 4.948
2020-10-12 09:01:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 09:01:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 38 @ 29032 updates, score 4.948) (writing took 4.781841330928728 seconds)
2020-10-12 09:01:23 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 09:01:23 | INFO | train | epoch 038 | loss 4.473 | nll_loss 2.906 | ppl 7.49 | wps 23855.7 | ups 3.29 | wpb 7242.1 | bsz 278.7 | num_updates 29032 | lr 7.42372e-05 | gnorm 0.976 | clip 0 | train_wall 217 | wall 8785
2020-10-12 09:01:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 09:01:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 09:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6943.11328125Mb; avail=237611.14453125Mb
2020-10-12 09:01:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003717
2020-10-12 09:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044344
2020-10-12 09:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6943.11328125Mb; avail=237611.14453125Mb
2020-10-12 09:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001564
2020-10-12 09:01:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6943.11328125Mb; avail=237611.14453125Mb
2020-10-12 09:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.856470
2020-10-12 09:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.903191
2020-10-12 09:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6950.9765625Mb; avail=237604.015625Mb
2020-10-12 09:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6950.1796875Mb; avail=237604.8125Mb
2020-10-12 09:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033072
2020-10-12 09:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6955.21875Mb; avail=237599.7734375Mb
2020-10-12 09:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001585
2020-10-12 09:01:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6955.21875Mb; avail=237599.7734375Mb
2020-10-12 09:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.746608
2020-10-12 09:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.782092
2020-10-12 09:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6949.2421875Mb; avail=237604.94140625Mb
2020-10-12 09:01:25 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 09:01:45 | INFO | train_inner | epoch 039:     68 / 764 loss=4.462, nll_loss=2.892, ppl=7.42, wps=19173.7, ups=2.58, wpb=7422.2, bsz=257.8, num_updates=29100, lr=7.41504e-05, gnorm=0.976, clip=0, train_wall=29, wall=8807
2020-10-12 09:02:14 | INFO | train_inner | epoch 039:    168 / 764 loss=4.447, nll_loss=2.877, ppl=7.34, wps=24942, ups=3.43, wpb=7271.3, bsz=279.3, num_updates=29200, lr=7.40233e-05, gnorm=0.975, clip=0, train_wall=28, wall=8836
2020-10-12 09:02:43 | INFO | train_inner | epoch 039:    268 / 764 loss=4.428, nll_loss=2.854, ppl=7.23, wps=24960.8, ups=3.44, wpb=7251.6, bsz=278.4, num_updates=29300, lr=7.38969e-05, gnorm=0.975, clip=0, train_wall=28, wall=8865
2020-10-12 09:03:12 | INFO | train_inner | epoch 039:    368 / 764 loss=4.441, nll_loss=2.87, ppl=7.31, wps=24966, ups=3.44, wpb=7264, bsz=282.2, num_updates=29400, lr=7.37711e-05, gnorm=0.975, clip=0, train_wall=28, wall=8894
2020-10-12 09:03:41 | INFO | train_inner | epoch 039:    468 / 764 loss=4.464, nll_loss=2.895, ppl=7.44, wps=24858.1, ups=3.46, wpb=7185.5, bsz=278.4, num_updates=29500, lr=7.3646e-05, gnorm=0.982, clip=0, train_wall=28, wall=8923
2020-10-12 09:04:10 | INFO | train_inner | epoch 039:    568 / 764 loss=4.475, nll_loss=2.908, ppl=7.51, wps=24717.6, ups=3.48, wpb=7105.2, bsz=279.4, num_updates=29600, lr=7.35215e-05, gnorm=1.022, clip=0, train_wall=28, wall=8952
2020-10-12 09:04:39 | INFO | train_inner | epoch 039:    668 / 764 loss=4.457, nll_loss=2.888, ppl=7.4, wps=25079.8, ups=3.44, wpb=7293.5, bsz=302, num_updates=29700, lr=7.33976e-05, gnorm=0.989, clip=0, train_wall=28, wall=8981
2020-10-12 09:05:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 09:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7062.17578125Mb; avail=237491.80859375Mb
2020-10-12 09:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001412
2020-10-12 09:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7062.17578125Mb; avail=237491.80859375Mb
2020-10-12 09:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070380
2020-10-12 09:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7062.17578125Mb; avail=237491.80859375Mb
2020-10-12 09:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050717
2020-10-12 09:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123313
2020-10-12 09:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7062.17578125Mb; avail=237491.80859375Mb
2020-10-12 09:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7062.17578125Mb; avail=237491.80859375Mb
2020-10-12 09:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001228
2020-10-12 09:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7062.17578125Mb; avail=237491.80859375Mb
2020-10-12 09:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069221
2020-10-12 09:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7062.17578125Mb; avail=237491.80859375Mb
2020-10-12 09:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049984
2020-10-12 09:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121201
2020-10-12 09:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7062.17578125Mb; avail=237491.80859375Mb
2020-10-12 09:05:09 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 4.944 | nll_loss 3.31 | ppl 9.92 | wps 55578.3 | wpb 2283.9 | bsz 87.3 | num_updates 29796 | best_loss 4.944
2020-10-12 09:05:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 09:05:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 39 @ 29796 updates, score 4.944) (writing took 4.804471671814099 seconds)
2020-10-12 09:05:14 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 09:05:14 | INFO | train | epoch 039 | loss 4.456 | nll_loss 2.886 | ppl 7.39 | wps 23951.6 | ups 3.31 | wpb 7242.1 | bsz 278.7 | num_updates 29796 | lr 7.32792e-05 | gnorm 0.986 | clip 0 | train_wall 215 | wall 9016
2020-10-12 09:05:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 09:05:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 09:05:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6960.37890625Mb; avail=237593.78125Mb
2020-10-12 09:05:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003453
2020-10-12 09:05:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043303
2020-10-12 09:05:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6960.0859375Mb; avail=237594.07421875Mb
2020-10-12 09:05:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001558
2020-10-12 09:05:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6960.0859375Mb; avail=237594.07421875Mb
2020-10-12 09:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.868622
2020-10-12 09:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.914342
2020-10-12 09:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6940.6484375Mb; avail=237613.7578125Mb
2020-10-12 09:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6935.50390625Mb; avail=237618.90234375Mb
2020-10-12 09:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033094
2020-10-12 09:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6935.125Mb; avail=237619.28125Mb
2020-10-12 09:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001574
2020-10-12 09:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6935.125Mb; avail=237619.28125Mb
2020-10-12 09:05:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.738870
2020-10-12 09:05:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.774333
2020-10-12 09:05:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6934.40234375Mb; avail=237619.40234375Mb
2020-10-12 09:05:16 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 09:05:17 | INFO | train_inner | epoch 040:      4 / 764 loss=4.478, nll_loss=2.91, ppl=7.52, wps=18753.2, ups=2.61, wpb=7189.7, bsz=273.1, num_updates=29800, lr=7.32743e-05, gnorm=0.99, clip=0, train_wall=28, wall=9020
2020-10-12 09:05:46 | INFO | train_inner | epoch 040:    104 / 764 loss=4.422, nll_loss=2.848, ppl=7.2, wps=24850.3, ups=3.46, wpb=7186.5, bsz=282.6, num_updates=29900, lr=7.31517e-05, gnorm=0.979, clip=0, train_wall=28, wall=9048
2020-10-12 09:06:15 | INFO | train_inner | epoch 040:    204 / 764 loss=4.457, nll_loss=2.886, ppl=7.39, wps=25283.3, ups=3.44, wpb=7352, bsz=254.9, num_updates=30000, lr=7.30297e-05, gnorm=0.99, clip=0, train_wall=28, wall=9078
2020-10-12 09:06:44 | INFO | train_inner | epoch 040:    304 / 764 loss=4.444, nll_loss=2.872, ppl=7.32, wps=25231.6, ups=3.43, wpb=7345.9, bsz=273.6, num_updates=30100, lr=7.29083e-05, gnorm=0.979, clip=0, train_wall=28, wall=9107
2020-10-12 09:07:13 | INFO | train_inner | epoch 040:    404 / 764 loss=4.387, nll_loss=2.809, ppl=7.01, wps=25019.7, ups=3.45, wpb=7244.9, bsz=324, num_updates=30200, lr=7.27875e-05, gnorm=0.969, clip=0, train_wall=28, wall=9136
2020-10-12 09:07:42 | INFO | train_inner | epoch 040:    504 / 764 loss=4.447, nll_loss=2.876, ppl=7.34, wps=24907.7, ups=3.44, wpb=7242, bsz=291.2, num_updates=30300, lr=7.26672e-05, gnorm=0.995, clip=0, train_wall=28, wall=9165
2020-10-12 09:08:12 | INFO | train_inner | epoch 040:    604 / 764 loss=4.457, nll_loss=2.887, ppl=7.4, wps=25052.7, ups=3.43, wpb=7302.4, bsz=270.7, num_updates=30400, lr=7.25476e-05, gnorm=0.985, clip=0, train_wall=28, wall=9194
2020-10-12 09:08:40 | INFO | train_inner | epoch 040:    704 / 764 loss=4.45, nll_loss=2.879, ppl=7.36, wps=24420.6, ups=3.48, wpb=7011.4, bsz=257.1, num_updates=30500, lr=7.24286e-05, gnorm=1.002, clip=0, train_wall=28, wall=9223
2020-10-12 09:08:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 09:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6963.5390625Mb; avail=237591.15234375Mb
2020-10-12 09:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001551
2020-10-12 09:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6963.5390625Mb; avail=237591.15234375Mb
2020-10-12 09:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068864
2020-10-12 09:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6963.5390625Mb; avail=237591.15234375Mb
2020-10-12 09:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050107
2020-10-12 09:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121304
2020-10-12 09:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6963.5390625Mb; avail=237591.15234375Mb
2020-10-12 09:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6967.05859375Mb; avail=237587.6328125Mb
2020-10-12 09:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001170
2020-10-12 09:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6967.6640625Mb; avail=237587.02734375Mb
2020-10-12 09:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068153
2020-10-12 09:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6973.7578125Mb; avail=237580.4375Mb
2020-10-12 09:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049886
2020-10-12 09:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119962
2020-10-12 09:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6964.92578125Mb; avail=237589.765625Mb
2020-10-12 09:09:00 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 4.939 | nll_loss 3.303 | ppl 9.87 | wps 55685.6 | wpb 2283.9 | bsz 87.3 | num_updates 30560 | best_loss 4.939
2020-10-12 09:09:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 09:09:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/O2M/checkpoint_best.pt (epoch 40 @ 30560 updates, score 4.939) (writing took 4.790061722975224 seconds)
2020-10-12 09:09:05 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 09:09:05 | INFO | train | epoch 040 | loss 4.44 | nll_loss 2.868 | ppl 7.3 | wps 23945.6 | ups 3.31 | wpb 7242.1 | bsz 278.7 | num_updates 30560 | lr 7.23575e-05 | gnorm 0.985 | clip 0 | train_wall 216 | wall 9247
2020-10-12 09:09:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 09:09:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 09:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6941.2578125Mb; avail=237612.93359375Mb
2020-10-12 09:09:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003395
2020-10-12 09:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.041443
2020-10-12 09:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6941.2578125Mb; avail=237612.93359375Mb
2020-10-12 09:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001723
2020-10-12 09:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6941.2578125Mb; avail=237612.93359375Mb
2020-10-12 09:09:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.745374
2020-10-12 09:09:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.789368
2020-10-12 09:09:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6941.59375Mb; avail=237612.59765625Mb
2020-10-12 09:09:06 | INFO | fairseq_cli.train | done training in 9247.4 seconds
