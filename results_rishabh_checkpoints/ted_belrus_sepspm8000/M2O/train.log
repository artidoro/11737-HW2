2020-10-12 06:33:45 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_belrus_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='bel-eng,rus-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_belrus_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-12 06:33:45 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-12 06:33:45 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['bel', 'eng', 'rus']
2020-10-12 06:33:45 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bel] dictionary: 21851 types
2020-10-12 06:33:45 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 21851 types
2020-10-12 06:33:45 | INFO | fairseq.data.multilingual.multilingual_data_manager | [rus] dictionary: 21851 types
2020-10-12 06:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-12 06:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=1269.7890625Mb; avail=243309.68359375Mb
2020-10-12 06:33:45 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 06:33:45 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:bel-eng': 1, 'main:rus-eng': 1}
2020-10-12 06:33:45 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-12 06:33:45 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belrus_sepspm8000/M2O/valid.bel-eng.bel
2020-10-12 06:33:45 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belrus_sepspm8000/M2O/valid.bel-eng.eng
2020-10-12 06:33:45 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrus_sepspm8000/M2O/ valid bel-eng 248 examples
2020-10-12 06:33:45 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:rus-eng src_langtok: None; tgt_langtok: None
2020-10-12 06:33:45 | INFO | fairseq.data.data_utils | loaded 4814 examples from: fairseq/data-bin/ted_belrus_sepspm8000/M2O/valid.rus-eng.rus
2020-10-12 06:33:45 | INFO | fairseq.data.data_utils | loaded 4814 examples from: fairseq/data-bin/ted_belrus_sepspm8000/M2O/valid.rus-eng.eng
2020-10-12 06:33:45 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrus_sepspm8000/M2O/ valid rus-eng 4814 examples
2020-10-12 06:33:46 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21851, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21851, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=21851, bias=False)
  )
)
2020-10-12 06:33:46 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-12 06:33:46 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-12 06:33:46 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-12 06:33:46 | INFO | fairseq_cli.train | num. model params: 42731008 (num. trained: 42731008)
2020-10-12 06:33:49 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-12 06:33:49 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-12 06:33:49 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 06:33:49 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-12 06:33:49 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-12 06:33:49 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-12 06:33:49 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-12 06:33:49 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_last.pt
2020-10-12 06:33:49 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-12 06:33:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-12 06:33:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3561.83984375Mb; avail=241007.13671875Mb
2020-10-12 06:33:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-12 06:33:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:bel-eng': 1, 'main:rus-eng': 1}
2020-10-12 06:33:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-12 06:33:49 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belrus_sepspm8000/M2O/train.bel-eng.bel
2020-10-12 06:33:49 | INFO | fairseq.data.data_utils | loaded 4509 examples from: fairseq/data-bin/ted_belrus_sepspm8000/M2O/train.bel-eng.eng
2020-10-12 06:33:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrus_sepspm8000/M2O/ train bel-eng 4509 examples
2020-10-12 06:33:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:rus-eng src_langtok: None; tgt_langtok: None
2020-10-12 06:33:49 | INFO | fairseq.data.data_utils | loaded 208397 examples from: fairseq/data-bin/ted_belrus_sepspm8000/M2O/train.rus-eng.rus
2020-10-12 06:33:49 | INFO | fairseq.data.data_utils | loaded 208397 examples from: fairseq/data-bin/ted_belrus_sepspm8000/M2O/train.rus-eng.eng
2020-10-12 06:33:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belrus_sepspm8000/M2O/ train rus-eng 208397 examples
2020-10-12 06:33:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:bel-eng', 4509), ('main:rus-eng', 208397)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-12 06:33:49 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-12 06:33:49 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 212906
2020-10-12 06:33:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 212906; virtual dataset size 212906
2020-10-12 06:33:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-12 06:33:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:bel-eng': 4509, 'main:rus-eng': 208397}; raw total size: 212906
2020-10-12 06:33:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:bel-eng': 4509, 'main:rus-eng': 208397}; resampled total size: 212906
2020-10-12 06:33:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-12 06:33:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.022509
2020-10-12 06:33:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=3562.24609375Mb; avail=241006.73046875Mb
2020-10-12 06:33:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004327
2020-10-12 06:33:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043449
2020-10-12 06:33:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3571.875Mb; avail=240997.1015625Mb
2020-10-12 06:33:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001701
2020-10-12 06:33:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3572.48046875Mb; avail=240996.49609375Mb
2020-10-12 06:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.775722
2020-10-12 06:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.821636
2020-10-12 06:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3572.9375Mb; avail=240996.02734375Mb
2020-10-12 06:33:50 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-12 06:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=3569.60546875Mb; avail=240999.359375Mb
2020-10-12 06:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033656
2020-10-12 06:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3569.60546875Mb; avail=240999.359375Mb
2020-10-12 06:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001632
2020-10-12 06:33:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3569.60546875Mb; avail=240999.359375Mb
2020-10-12 06:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.758745
2020-10-12 06:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.794751
2020-10-12 06:33:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3575.5078125Mb; avail=240993.46875Mb
2020-10-12 06:33:51 | INFO | fairseq.trainer | begin training epoch 1
2020-10-12 06:34:27 | INFO | train_inner | epoch 001:    100 / 753 loss=14.121, nll_loss=14.007, ppl=16468.6, wps=25638.9, ups=3.34, wpb=7681.5, bsz=283.9, num_updates=100, lr=5.0975e-06, gnorm=4.821, clip=0, train_wall=35, wall=38
2020-10-12 06:34:57 | INFO | train_inner | epoch 001:    200 / 753 loss=12.402, nll_loss=12.088, ppl=4354.38, wps=25057.4, ups=3.37, wpb=7444.2, bsz=292.6, num_updates=200, lr=1.0095e-05, gnorm=2.31, clip=0, train_wall=28, wall=68
2020-10-12 06:35:27 | INFO | train_inner | epoch 001:    300 / 753 loss=11.619, nll_loss=11.216, ppl=2378.47, wps=25771.7, ups=3.32, wpb=7757.3, bsz=296.7, num_updates=300, lr=1.50925e-05, gnorm=1.781, clip=0, train_wall=29, wall=98
2020-10-12 06:35:57 | INFO | train_inner | epoch 001:    400 / 753 loss=10.612, nll_loss=10.073, ppl=1077.27, wps=25289.1, ups=3.35, wpb=7556, bsz=282.6, num_updates=400, lr=2.009e-05, gnorm=2.066, clip=0, train_wall=28, wall=128
2020-10-12 06:36:27 | INFO | train_inner | epoch 001:    500 / 753 loss=9.694, nll_loss=8.988, ppl=507.72, wps=25696.4, ups=3.35, wpb=7668.5, bsz=254, num_updates=500, lr=2.50875e-05, gnorm=1.406, clip=0, train_wall=28, wall=157
2020-10-12 06:36:57 | INFO | train_inner | epoch 001:    600 / 753 loss=9.38, nll_loss=8.589, ppl=385, wps=25252.4, ups=3.31, wpb=7630.1, bsz=299.4, num_updates=600, lr=3.0085e-05, gnorm=1.397, clip=0, train_wall=29, wall=188
2020-10-12 06:37:27 | INFO | train_inner | epoch 001:    700 / 753 loss=9.198, nll_loss=8.356, ppl=327.67, wps=25066.5, ups=3.33, wpb=7537, bsz=277.7, num_updates=700, lr=3.50825e-05, gnorm=1.463, clip=0, train_wall=29, wall=218
2020-10-12 06:37:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 06:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6693.421875Mb; avail=237860.6015625Mb
2020-10-12 06:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001307
2020-10-12 06:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6693.421875Mb; avail=237860.6015625Mb
2020-10-12 06:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070408
2020-10-12 06:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6696.35546875Mb; avail=237857.66796875Mb
2020-10-12 06:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052089
2020-10-12 06:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124585
2020-10-12 06:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6700.59375Mb; avail=237853.4296875Mb
2020-10-12 06:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6703.56640625Mb; avail=237850.45703125Mb
2020-10-12 06:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001193
2020-10-12 06:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6703.56640625Mb; avail=237850.45703125Mb
2020-10-12 06:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070777
2020-10-12 06:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6694.21484375Mb; avail=237859.80859375Mb
2020-10-12 06:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052395
2020-10-12 06:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125148
2020-10-12 06:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6695.91796875Mb; avail=237858.10546875Mb
/home/ubuntu/courses/fairseq/fairseq/utils.py:340: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2020-10-12 06:37:46 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.827 | nll_loss 7.876 | ppl 234.89 | wps 55111 | wpb 2261.4 | bsz 84.4 | num_updates 753
2020-10-12 06:37:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 06:37:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 1 @ 753 updates, score 8.827) (writing took 1.6745214690454304 seconds)
2020-10-12 06:37:48 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-12 06:37:48 | INFO | train | epoch 001 | loss 10.873 | nll_loss 10.32 | ppl 1277.9 | wps 24896.5 | ups 3.27 | wpb 7619.3 | bsz 282.7 | num_updates 753 | lr 3.77312e-05 | gnorm 2.134 | clip 0 | train_wall 221 | wall 238
2020-10-12 06:37:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-12 06:37:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-12 06:37:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6543.4375Mb; avail=238010.171875Mb
2020-10-12 06:37:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004862
2020-10-12 06:37:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044123
2020-10-12 06:37:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6553.68359375Mb; avail=238000.53125Mb
2020-10-12 06:37:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001538
2020-10-12 06:37:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6553.68359375Mb; avail=238000.53125Mb
2020-10-12 06:37:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.771172
2020-10-12 06:37:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.817636
2020-10-12 06:37:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6554.30078125Mb; avail=237999.9609375Mb
2020-10-12 06:37:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6549.9609375Mb; avail=238004.30078125Mb
2020-10-12 06:37:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032993
2020-10-12 06:37:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6549.6953125Mb; avail=238004.79296875Mb
2020-10-12 06:37:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001537
2020-10-12 06:37:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6549.6953125Mb; avail=238004.79296875Mb
2020-10-12 06:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.773766
2020-10-12 06:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.809081
2020-10-12 06:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6554.53125Mb; avail=237999.8359375Mb
2020-10-12 06:37:49 | INFO | fairseq.trainer | begin training epoch 2
2020-10-12 06:38:03 | INFO | train_inner | epoch 002:     47 / 753 loss=9.115, nll_loss=8.25, ppl=304.53, wps=20820, ups=2.74, wpb=7601.7, bsz=263.6, num_updates=800, lr=4.008e-05, gnorm=1.561, clip=0, train_wall=29, wall=254
2020-10-12 06:38:34 | INFO | train_inner | epoch 002:    147 / 753 loss=8.902, nll_loss=8.008, ppl=257.37, wps=24756, ups=3.29, wpb=7523.7, bsz=300.3, num_updates=900, lr=4.50775e-05, gnorm=1.443, clip=0, train_wall=29, wall=285
2020-10-12 06:39:04 | INFO | train_inner | epoch 002:    247 / 753 loss=8.774, nll_loss=7.862, ppl=232.63, wps=25595.5, ups=3.27, wpb=7827.7, bsz=295.4, num_updates=1000, lr=5.0075e-05, gnorm=1.292, clip=0, train_wall=29, wall=315
2020-10-12 06:39:35 | INFO | train_inner | epoch 002:    347 / 753 loss=8.675, nll_loss=7.748, ppl=214.91, wps=25291.1, ups=3.31, wpb=7641.6, bsz=281.3, num_updates=1100, lr=5.50725e-05, gnorm=1.619, clip=0, train_wall=29, wall=345
2020-10-12 06:40:05 | INFO | train_inner | epoch 002:    447 / 753 loss=8.5, nll_loss=7.55, ppl=187.37, wps=25005.1, ups=3.33, wpb=7519.3, bsz=252.7, num_updates=1200, lr=6.007e-05, gnorm=1.249, clip=0, train_wall=29, wall=376
2020-10-12 06:40:35 | INFO | train_inner | epoch 002:    547 / 753 loss=8.323, nll_loss=7.349, ppl=163.06, wps=25676.4, ups=3.27, wpb=7851.9, bsz=315.4, num_updates=1300, lr=6.50675e-05, gnorm=1.415, clip=0, train_wall=29, wall=406
2020-10-12 06:41:05 | INFO | train_inner | epoch 002:    647 / 753 loss=8.263, nll_loss=7.278, ppl=155.22, wps=24756.6, ups=3.34, wpb=7421.9, bsz=275.4, num_updates=1400, lr=7.0065e-05, gnorm=1.401, clip=0, train_wall=29, wall=436
2020-10-12 06:41:35 | INFO | train_inner | epoch 002:    747 / 753 loss=8.128, nll_loss=7.124, ppl=139.46, wps=25130.2, ups=3.31, wpb=7592.1, bsz=268.1, num_updates=1500, lr=7.50625e-05, gnorm=1.401, clip=0, train_wall=29, wall=466
2020-10-12 06:41:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 06:41:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6618.3203125Mb; avail=237935.60546875Mb
2020-10-12 06:41:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001349
2020-10-12 06:41:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6618.3203125Mb; avail=237935.60546875Mb
2020-10-12 06:41:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070655
2020-10-12 06:41:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6617.6953125Mb; avail=237936.08984375Mb
2020-10-12 06:41:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051621
2020-10-12 06:41:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124399
2020-10-12 06:41:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6617.76171875Mb; avail=237936.45703125Mb
2020-10-12 06:41:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6617.765625Mb; avail=237936.49609375Mb
2020-10-12 06:41:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001176
2020-10-12 06:41:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6617.765625Mb; avail=237936.49609375Mb
2020-10-12 06:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070427
2020-10-12 06:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6617.765625Mb; avail=237936.49609375Mb
2020-10-12 06:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052031
2020-10-12 06:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124386
2020-10-12 06:41:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6617.765625Mb; avail=237936.49609375Mb
2020-10-12 06:41:40 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.865 | nll_loss 6.788 | ppl 110.53 | wps 55007 | wpb 2261.4 | bsz 84.4 | num_updates 1506 | best_loss 7.865
2020-10-12 06:41:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 06:41:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 2 @ 1506 updates, score 7.865) (writing took 4.781230163061991 seconds)
2020-10-12 06:41:45 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-12 06:41:45 | INFO | train | epoch 002 | loss 8.541 | nll_loss 7.596 | ppl 193.53 | wps 24166.9 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 1506 | lr 7.53624e-05 | gnorm 1.412 | clip 0 | train_wall 217 | wall 476
2020-10-12 06:41:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-12 06:41:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-12 06:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6541.1875Mb; avail=238013.046875Mb
2020-10-12 06:41:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004112
2020-10-12 06:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043444
2020-10-12 06:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6543.59375Mb; avail=238010.640625Mb
2020-10-12 06:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001551
2020-10-12 06:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6543.59375Mb; avail=238010.640625Mb
2020-10-12 06:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.767249
2020-10-12 06:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.813053
2020-10-12 06:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6550.88671875Mb; avail=238003.60546875Mb
2020-10-12 06:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6546.5703125Mb; avail=238007.921875Mb
2020-10-12 06:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032869
2020-10-12 06:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6546.5703125Mb; avail=238007.921875Mb
2020-10-12 06:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001547
2020-10-12 06:41:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6546.5703125Mb; avail=238007.921875Mb
2020-10-12 06:41:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.769261
2020-10-12 06:41:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.804505
2020-10-12 06:41:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6550.203125Mb; avail=238004.296875Mb
2020-10-12 06:41:47 | INFO | fairseq.trainer | begin training epoch 3
2020-10-12 06:42:15 | INFO | train_inner | epoch 003:     94 / 753 loss=8.107, nll_loss=7.1, ppl=137.19, wps=19057, ups=2.53, wpb=7539.6, bsz=271.5, num_updates=1600, lr=8.006e-05, gnorm=1.293, clip=0, train_wall=29, wall=506
2020-10-12 06:42:45 | INFO | train_inner | epoch 003:    194 / 753 loss=7.966, nll_loss=6.938, ppl=122.58, wps=25250.7, ups=3.3, wpb=7650.4, bsz=293.4, num_updates=1700, lr=8.50575e-05, gnorm=1.352, clip=0, train_wall=29, wall=536
2020-10-12 06:43:16 | INFO | train_inner | epoch 003:    294 / 753 loss=7.992, nll_loss=6.967, ppl=125.09, wps=25144.9, ups=3.3, wpb=7621.8, bsz=270, num_updates=1800, lr=9.0055e-05, gnorm=1.292, clip=0, train_wall=29, wall=566
2020-10-12 06:43:46 | INFO | train_inner | epoch 003:    394 / 753 loss=7.903, nll_loss=6.865, ppl=116.53, wps=25200.7, ups=3.32, wpb=7587.8, bsz=266.2, num_updates=1900, lr=9.50525e-05, gnorm=1.245, clip=0, train_wall=29, wall=597
2020-10-12 06:44:16 | INFO | train_inner | epoch 003:    494 / 753 loss=7.837, nll_loss=6.791, ppl=110.71, wps=24934.4, ups=3.33, wpb=7489, bsz=292.2, num_updates=2000, lr=0.00010005, gnorm=1.213, clip=0, train_wall=29, wall=627
2020-10-12 06:44:46 | INFO | train_inner | epoch 003:    594 / 753 loss=7.76, nll_loss=6.701, ppl=104.07, wps=25042.5, ups=3.3, wpb=7586.9, bsz=287.4, num_updates=2100, lr=0.000105048, gnorm=1.244, clip=0, train_wall=29, wall=657
2020-10-12 06:45:16 | INFO | train_inner | epoch 003:    694 / 753 loss=7.601, nll_loss=6.519, ppl=91.72, wps=25585.6, ups=3.29, wpb=7771.1, bsz=295.2, num_updates=2200, lr=0.000110045, gnorm=1.176, clip=0, train_wall=29, wall=687
2020-10-12 06:45:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 06:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6642.01953125Mb; avail=237911.9921875Mb
2020-10-12 06:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001350
2020-10-12 06:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6642.01953125Mb; avail=237911.9921875Mb
2020-10-12 06:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070519
2020-10-12 06:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6641.97265625Mb; avail=237911.5078125Mb
2020-10-12 06:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052070
2020-10-12 06:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124737
2020-10-12 06:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6641.94921875Mb; avail=237911.66796875Mb
2020-10-12 06:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6641.94921875Mb; avail=237911.66796875Mb
2020-10-12 06:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001164
2020-10-12 06:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6641.94921875Mb; avail=237911.66796875Mb
2020-10-12 06:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070103
2020-10-12 06:45:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6641.94921875Mb; avail=237911.66796875Mb
2020-10-12 06:45:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051769
2020-10-12 06:45:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123796
2020-10-12 06:45:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6641.94921875Mb; avail=237911.66796875Mb
2020-10-12 06:45:37 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.335 | nll_loss 6.175 | ppl 72.25 | wps 54957 | wpb 2261.4 | bsz 84.4 | num_updates 2259 | best_loss 7.335
2020-10-12 06:45:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 06:45:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 3 @ 2259 updates, score 7.335) (writing took 4.78296944196336 seconds)
2020-10-12 06:45:42 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-12 06:45:42 | INFO | train | epoch 003 | loss 7.851 | nll_loss 6.806 | ppl 111.91 | wps 24211.9 | ups 3.18 | wpb 7619.3 | bsz 282.7 | num_updates 2259 | lr 0.000112994 | gnorm 1.251 | clip 0 | train_wall 217 | wall 713
2020-10-12 06:45:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-12 06:45:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-12 06:45:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6561.45703125Mb; avail=237992.6796875Mb
2020-10-12 06:45:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004127
2020-10-12 06:45:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.042793
2020-10-12 06:45:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6562.765625Mb; avail=237991.37109375Mb
2020-10-12 06:45:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001554
2020-10-12 06:45:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6562.765625Mb; avail=237991.37109375Mb
2020-10-12 06:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.779206
2020-10-12 06:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.824360
2020-10-12 06:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6568.91015625Mb; avail=237985.59765625Mb
2020-10-12 06:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6567.5078125Mb; avail=237987.0Mb
2020-10-12 06:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033311
2020-10-12 06:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6569.82421875Mb; avail=237984.68359375Mb
2020-10-12 06:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001545
2020-10-12 06:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6570.4140625Mb; avail=237984.09375Mb
2020-10-12 06:45:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.767258
2020-10-12 06:45:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.802921
2020-10-12 06:45:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6573.59375Mb; avail=237980.91796875Mb
2020-10-12 06:45:44 | INFO | fairseq.trainer | begin training epoch 4
2020-10-12 06:45:56 | INFO | train_inner | epoch 004:     41 / 753 loss=7.576, nll_loss=6.492, ppl=89.98, wps=19437.1, ups=2.52, wpb=7698.9, bsz=276.1, num_updates=2300, lr=0.000115043, gnorm=1.14, clip=0, train_wall=29, wall=727
2020-10-12 06:46:27 | INFO | train_inner | epoch 004:    141 / 753 loss=7.521, nll_loss=6.429, ppl=86.15, wps=25270.2, ups=3.28, wpb=7699.9, bsz=309.1, num_updates=2400, lr=0.00012004, gnorm=1.099, clip=0, train_wall=29, wall=757
2020-10-12 06:46:57 | INFO | train_inner | epoch 004:    241 / 753 loss=7.55, nll_loss=6.46, ppl=88.07, wps=25017.9, ups=3.33, wpb=7519.1, bsz=267.4, num_updates=2500, lr=0.000125037, gnorm=1.231, clip=0, train_wall=29, wall=787
2020-10-12 06:47:27 | INFO | train_inner | epoch 004:    341 / 753 loss=7.513, nll_loss=6.418, ppl=85.48, wps=24917.9, ups=3.28, wpb=7603, bsz=293.8, num_updates=2600, lr=0.000130035, gnorm=1.154, clip=0, train_wall=29, wall=818
2020-10-12 06:47:57 | INFO | train_inner | epoch 004:    441 / 753 loss=7.384, nll_loss=6.27, ppl=77.15, wps=25164, ups=3.32, wpb=7569.1, bsz=268.6, num_updates=2700, lr=0.000135032, gnorm=1.076, clip=0, train_wall=29, wall=848
2020-10-12 06:48:27 | INFO | train_inner | epoch 004:    541 / 753 loss=7.401, nll_loss=6.289, ppl=78.19, wps=25243.4, ups=3.3, wpb=7650.6, bsz=277, num_updates=2800, lr=0.00014003, gnorm=1.115, clip=0, train_wall=29, wall=878
2020-10-12 06:48:58 | INFO | train_inner | epoch 004:    641 / 753 loss=7.294, nll_loss=6.167, ppl=71.86, wps=25122.2, ups=3.28, wpb=7652.7, bsz=296.6, num_updates=2900, lr=0.000145028, gnorm=1.12, clip=0, train_wall=29, wall=909
2020-10-12 06:49:28 | INFO | train_inner | epoch 004:    741 / 753 loss=7.263, nll_loss=6.131, ppl=70.11, wps=25340.2, ups=3.3, wpb=7674.7, bsz=280.6, num_updates=3000, lr=0.000150025, gnorm=1.03, clip=0, train_wall=29, wall=939
2020-10-12 06:49:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 06:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6663.26171875Mb; avail=237890.73828125Mb
2020-10-12 06:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001404
2020-10-12 06:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6663.26171875Mb; avail=237890.73828125Mb
2020-10-12 06:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071438
2020-10-12 06:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6663.26171875Mb; avail=237890.73828125Mb
2020-10-12 06:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052431
2020-10-12 06:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126071
2020-10-12 06:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6663.26171875Mb; avail=237890.73828125Mb
2020-10-12 06:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6663.26171875Mb; avail=237890.73828125Mb
2020-10-12 06:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001192
2020-10-12 06:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6663.26171875Mb; avail=237890.73828125Mb
2020-10-12 06:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071317
2020-10-12 06:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6663.40234375Mb; avail=237890.59765625Mb
2020-10-12 06:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052902
2020-10-12 06:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126188
2020-10-12 06:49:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6662.91015625Mb; avail=237891.08984375Mb
2020-10-12 06:49:35 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.953 | nll_loss 5.715 | ppl 52.52 | wps 53794 | wpb 2261.4 | bsz 84.4 | num_updates 3012 | best_loss 6.953
2020-10-12 06:49:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 06:49:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 4 @ 3012 updates, score 6.953) (writing took 4.802165851928294 seconds)
2020-10-12 06:49:40 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-12 06:49:40 | INFO | train | epoch 004 | loss 7.426 | nll_loss 6.318 | ppl 79.79 | wps 24135.3 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 3012 | lr 0.000150625 | gnorm 1.118 | clip 0 | train_wall 217 | wall 950
2020-10-12 06:49:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-12 06:49:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-12 06:49:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6577.046875Mb; avail=237977.21484375Mb
2020-10-12 06:49:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004897
2020-10-12 06:49:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044387
2020-10-12 06:49:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6582.72265625Mb; avail=237971.5390625Mb
2020-10-12 06:49:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001561
2020-10-12 06:49:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6582.72265625Mb; avail=237971.5390625Mb
2020-10-12 06:49:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.779347
2020-10-12 06:49:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.826103
2020-10-12 06:49:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6599.07421875Mb; avail=237955.40625Mb
2020-10-12 06:49:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6585.41796875Mb; avail=237969.5546875Mb
2020-10-12 06:49:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034003
2020-10-12 06:49:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6585.50390625Mb; avail=237968.9765625Mb
2020-10-12 06:49:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001584
2020-10-12 06:49:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6585.50390625Mb; avail=237968.9765625Mb
2020-10-12 06:49:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.777903
2020-10-12 06:49:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.814309
2020-10-12 06:49:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6591.47265625Mb; avail=237963.11328125Mb
2020-10-12 06:49:41 | INFO | fairseq.trainer | begin training epoch 5
2020-10-12 06:50:08 | INFO | train_inner | epoch 005:     88 / 753 loss=7.086, nll_loss=5.931, ppl=61, wps=19235.2, ups=2.5, wpb=7708.5, bsz=299.1, num_updates=3100, lr=0.000155023, gnorm=1.076, clip=0, train_wall=29, wall=979
2020-10-12 06:50:39 | INFO | train_inner | epoch 005:    188 / 753 loss=7.177, nll_loss=6.034, ppl=65.53, wps=25068, ups=3.3, wpb=7603.1, bsz=278.2, num_updates=3200, lr=0.00016002, gnorm=1.049, clip=0, train_wall=29, wall=1009
2020-10-12 06:51:09 | INFO | train_inner | epoch 005:    288 / 753 loss=7.026, nll_loss=5.861, ppl=58.13, wps=25206.4, ups=3.3, wpb=7648.7, bsz=291.4, num_updates=3300, lr=0.000165018, gnorm=1.102, clip=0, train_wall=29, wall=1040
2020-10-12 06:51:39 | INFO | train_inner | epoch 005:    388 / 753 loss=7.017, nll_loss=5.849, ppl=57.66, wps=25274.5, ups=3.3, wpb=7656.8, bsz=285.7, num_updates=3400, lr=0.000170015, gnorm=1.08, clip=0, train_wall=29, wall=1070
2020-10-12 06:52:09 | INFO | train_inner | epoch 005:    488 / 753 loss=7.066, nll_loss=5.905, ppl=59.94, wps=24874.4, ups=3.31, wpb=7518.8, bsz=263.9, num_updates=3500, lr=0.000175013, gnorm=1.089, clip=0, train_wall=29, wall=1100
2020-10-12 06:52:40 | INFO | train_inner | epoch 005:    588 / 753 loss=6.939, nll_loss=5.759, ppl=54.16, wps=25400.2, ups=3.26, wpb=7789.2, bsz=289.2, num_updates=3600, lr=0.00018001, gnorm=1.074, clip=0, train_wall=29, wall=1131
2020-10-12 06:53:10 | INFO | train_inner | epoch 005:    688 / 753 loss=6.824, nll_loss=5.629, ppl=49.49, wps=24863.5, ups=3.34, wpb=7440, bsz=266.6, num_updates=3700, lr=0.000185008, gnorm=1.065, clip=0, train_wall=29, wall=1161
2020-10-12 06:53:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 06:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6686.69140625Mb; avail=237867.3359375Mb
2020-10-12 06:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001389
2020-10-12 06:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6686.69140625Mb; avail=237867.3359375Mb
2020-10-12 06:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070477
2020-10-12 06:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6686.69140625Mb; avail=237867.3359375Mb
2020-10-12 06:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052709
2020-10-12 06:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125342
2020-10-12 06:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6686.69140625Mb; avail=237867.3359375Mb
2020-10-12 06:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6686.69140625Mb; avail=237867.3359375Mb
2020-10-12 06:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001229
2020-10-12 06:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6686.69140625Mb; avail=237867.3359375Mb
2020-10-12 06:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069883
2020-10-12 06:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6686.93359375Mb; avail=237867.828125Mb
2020-10-12 06:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052464
2020-10-12 06:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124322
2020-10-12 06:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6687.42578125Mb; avail=237867.3359375Mb
2020-10-12 06:53:33 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.514 | nll_loss 5.204 | ppl 36.87 | wps 55186.3 | wpb 2261.4 | bsz 84.4 | num_updates 3765 | best_loss 6.514
2020-10-12 06:53:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 06:53:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 5 @ 3765 updates, score 6.514) (writing took 4.781722811050713 seconds)
2020-10-12 06:53:37 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-12 06:53:37 | INFO | train | epoch 005 | loss 7.004 | nll_loss 5.835 | ppl 57.08 | wps 24132.1 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 3765 | lr 0.000188256 | gnorm 1.082 | clip 0 | train_wall 218 | wall 1188
2020-10-12 06:53:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-12 06:53:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-12 06:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6610.1171875Mb; avail=237944.171875Mb
2020-10-12 06:53:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004115
2020-10-12 06:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043614
2020-10-12 06:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6602.56640625Mb; avail=237951.72265625Mb
2020-10-12 06:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001586
2020-10-12 06:53:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6602.56640625Mb; avail=237951.72265625Mb
2020-10-12 06:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.769612
2020-10-12 06:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.815625
2020-10-12 06:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6619.33984375Mb; avail=237934.94921875Mb
2020-10-12 06:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6584.41796875Mb; avail=237969.83984375Mb
2020-10-12 06:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033730
2020-10-12 06:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6583.90234375Mb; avail=237970.35546875Mb
2020-10-12 06:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001596
2020-10-12 06:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6583.90234375Mb; avail=237970.35546875Mb
2020-10-12 06:53:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.769034
2020-10-12 06:53:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.805170
2020-10-12 06:53:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6583.87890625Mb; avail=237971.328125Mb
2020-10-12 06:53:39 | INFO | fairseq.trainer | begin training epoch 6
2020-10-12 06:53:50 | INFO | train_inner | epoch 006:     35 / 753 loss=6.849, nll_loss=5.658, ppl=50.48, wps=18978.9, ups=2.52, wpb=7540.9, bsz=273, num_updates=3800, lr=0.000190005, gnorm=1.123, clip=0, train_wall=29, wall=1201
2020-10-12 06:54:20 | INFO | train_inner | epoch 006:    135 / 753 loss=6.668, nll_loss=5.452, ppl=43.78, wps=25466.4, ups=3.3, wpb=7717.6, bsz=293.5, num_updates=3900, lr=0.000195003, gnorm=1.059, clip=0, train_wall=29, wall=1231
2020-10-12 06:54:51 | INFO | train_inner | epoch 006:    235 / 753 loss=6.657, nll_loss=5.438, ppl=43.35, wps=24970.7, ups=3.28, wpb=7606.2, bsz=298.2, num_updates=4000, lr=0.0002, gnorm=1.122, clip=0, train_wall=29, wall=1261
2020-10-12 06:55:21 | INFO | train_inner | epoch 006:    335 / 753 loss=6.657, nll_loss=5.436, ppl=43.28, wps=25110.7, ups=3.3, wpb=7618.9, bsz=283.6, num_updates=4100, lr=0.000197546, gnorm=1.102, clip=0, train_wall=29, wall=1292
2020-10-12 06:55:51 | INFO | train_inner | epoch 006:    435 / 753 loss=6.535, nll_loss=5.298, ppl=39.34, wps=25457.8, ups=3.31, wpb=7684.3, bsz=280.2, num_updates=4200, lr=0.00019518, gnorm=1.022, clip=0, train_wall=29, wall=1322
2020-10-12 06:56:21 | INFO | train_inner | epoch 006:    535 / 753 loss=6.559, nll_loss=5.324, ppl=40.06, wps=24988.4, ups=3.33, wpb=7514.3, bsz=260.7, num_updates=4300, lr=0.000192897, gnorm=1.096, clip=0, train_wall=29, wall=1352
2020-10-12 06:56:51 | INFO | train_inner | epoch 006:    635 / 753 loss=6.432, nll_loss=5.179, ppl=36.23, wps=25000.4, ups=3.31, wpb=7552.5, bsz=295.6, num_updates=4400, lr=0.000190693, gnorm=1.093, clip=0, train_wall=29, wall=1382
2020-10-12 06:57:22 | INFO | train_inner | epoch 006:    735 / 753 loss=6.372, nll_loss=5.111, ppl=34.56, wps=25343, ups=3.29, wpb=7694.2, bsz=272.8, num_updates=4500, lr=0.000188562, gnorm=1.015, clip=0, train_wall=29, wall=1413
2020-10-12 06:57:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6592.8046875Mb; avail=237961.19140625Mb
2020-10-12 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001505
2020-10-12 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6592.8046875Mb; avail=237961.19140625Mb
2020-10-12 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070201
2020-10-12 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6592.8046875Mb; avail=237961.19140625Mb
2020-10-12 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052788
2020-10-12 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125285
2020-10-12 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6592.8046875Mb; avail=237961.19140625Mb
2020-10-12 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6592.8046875Mb; avail=237961.19140625Mb
2020-10-12 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001097
2020-10-12 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6592.8046875Mb; avail=237961.19140625Mb
2020-10-12 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071018
2020-10-12 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6592.8046875Mb; avail=237961.19140625Mb
2020-10-12 06:57:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052824
2020-10-12 06:57:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125717
2020-10-12 06:57:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6592.8046875Mb; avail=237961.19140625Mb
2020-10-12 06:57:30 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.115 | nll_loss 4.738 | ppl 26.68 | wps 55154.8 | wpb 2261.4 | bsz 84.4 | num_updates 4518 | best_loss 6.115
2020-10-12 06:57:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 06:57:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 6 @ 4518 updates, score 6.115) (writing took 4.793736263876781 seconds)
2020-10-12 06:57:35 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-12 06:57:35 | INFO | train | epoch 006 | loss 6.565 | nll_loss 5.332 | ppl 40.29 | wps 24150.9 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 4518 | lr 0.000188186 | gnorm 1.077 | clip 0 | train_wall 217 | wall 1426
2020-10-12 06:57:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-12 06:57:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-12 06:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6563.7109375Mb; avail=237990.5703125Mb
2020-10-12 06:57:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003478
2020-10-12 06:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043869
2020-10-12 06:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6563.7109375Mb; avail=237990.5703125Mb
2020-10-12 06:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001679
2020-10-12 06:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6563.7109375Mb; avail=237990.5703125Mb
2020-10-12 06:57:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.775598
2020-10-12 06:57:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.821954
2020-10-12 06:57:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6571.9453125Mb; avail=237982.3359375Mb
2020-10-12 06:57:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6574.0859375Mb; avail=237980.6875Mb
2020-10-12 06:57:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033351
2020-10-12 06:57:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6573.69921875Mb; avail=237980.58203125Mb
2020-10-12 06:57:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001570
2020-10-12 06:57:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6573.69921875Mb; avail=237980.58203125Mb
2020-10-12 06:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.771302
2020-10-12 06:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.807039
2020-10-12 06:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6567.890625Mb; avail=237986.58203125Mb
2020-10-12 06:57:37 | INFO | fairseq.trainer | begin training epoch 7
2020-10-12 06:58:01 | INFO | train_inner | epoch 007:     82 / 753 loss=6.374, nll_loss=5.113, ppl=34.61, wps=18859.8, ups=2.52, wpb=7473.1, bsz=263, num_updates=4600, lr=0.000186501, gnorm=1.058, clip=0, train_wall=29, wall=1452
2020-10-12 06:58:32 | INFO | train_inner | epoch 007:    182 / 753 loss=6.253, nll_loss=4.975, ppl=31.45, wps=25444.6, ups=3.3, wpb=7711, bsz=274.9, num_updates=4700, lr=0.000184506, gnorm=1.011, clip=0, train_wall=29, wall=1483
2020-10-12 06:59:02 | INFO | train_inner | epoch 007:    282 / 753 loss=6.261, nll_loss=4.984, ppl=31.65, wps=24755.3, ups=3.32, wpb=7459.1, bsz=289.5, num_updates=4800, lr=0.000182574, gnorm=1.103, clip=0, train_wall=29, wall=1513
2020-10-12 06:59:32 | INFO | train_inner | epoch 007:    382 / 753 loss=6.181, nll_loss=4.891, ppl=29.68, wps=25251.1, ups=3.3, wpb=7651.3, bsz=277.8, num_updates=4900, lr=0.000180702, gnorm=1.005, clip=0, train_wall=29, wall=1543
2020-10-12 07:00:03 | INFO | train_inner | epoch 007:    482 / 753 loss=6.108, nll_loss=4.809, ppl=28.02, wps=24892.2, ups=3.27, wpb=7604.2, bsz=326.9, num_updates=5000, lr=0.000178885, gnorm=1.109, clip=0, train_wall=29, wall=1573
2020-10-12 07:00:34 | INFO | train_inner | epoch 007:    582 / 753 loss=6.122, nll_loss=4.822, ppl=28.28, wps=24784.7, ups=3.2, wpb=7749.1, bsz=278.5, num_updates=5100, lr=0.000177123, gnorm=1.023, clip=0, train_wall=30, wall=1605
2020-10-12 07:01:04 | INFO | train_inner | epoch 007:    682 / 753 loss=6.039, nll_loss=4.728, ppl=26.51, wps=25325.4, ups=3.28, wpb=7710.1, bsz=280.4, num_updates=5200, lr=0.000175412, gnorm=0.974, clip=0, train_wall=29, wall=1635
2020-10-12 07:01:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6648.9140625Mb; avail=237905.31640625Mb
2020-10-12 07:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001389
2020-10-12 07:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6648.9140625Mb; avail=237905.31640625Mb
2020-10-12 07:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071995
2020-10-12 07:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6648.9140625Mb; avail=237905.31640625Mb
2020-10-12 07:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053347
2020-10-12 07:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127522
2020-10-12 07:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6649.02734375Mb; avail=237905.203125Mb
2020-10-12 07:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6650.3515625Mb; avail=237903.87890625Mb
2020-10-12 07:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001159
2020-10-12 07:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6650.3515625Mb; avail=237903.87890625Mb
2020-10-12 07:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071059
2020-10-12 07:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6658.71484375Mb; avail=237895.515625Mb
2020-10-12 07:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053733
2020-10-12 07:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126720
2020-10-12 07:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6648.87109375Mb; avail=237905.359375Mb
2020-10-12 07:01:29 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.773 | nll_loss 4.319 | ppl 19.96 | wps 55021.5 | wpb 2261.4 | bsz 84.4 | num_updates 5271 | best_loss 5.773
2020-10-12 07:01:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:01:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 7 @ 5271 updates, score 5.773) (writing took 4.7845692140981555 seconds)
2020-10-12 07:01:33 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-12 07:01:33 | INFO | train | epoch 007 | loss 6.167 | nll_loss 4.875 | ppl 29.35 | wps 24052.9 | ups 3.16 | wpb 7619.3 | bsz 282.7 | num_updates 5271 | lr 0.000174226 | gnorm 1.033 | clip 0 | train_wall 218 | wall 1664
2020-10-12 07:01:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-12 07:01:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-12 07:01:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6562.44921875Mb; avail=237991.7578125Mb
2020-10-12 07:01:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003728
2020-10-12 07:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044823
2020-10-12 07:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6563.63671875Mb; avail=237990.5703125Mb
2020-10-12 07:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001686
2020-10-12 07:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6563.63671875Mb; avail=237990.5703125Mb
2020-10-12 07:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.799772
2020-10-12 07:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.847135
2020-10-12 07:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6569.59375Mb; avail=237984.61328125Mb
2020-10-12 07:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6565.42578125Mb; avail=237988.921875Mb
2020-10-12 07:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033248
2020-10-12 07:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6565.1875Mb; avail=237989.03515625Mb
2020-10-12 07:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001632
2020-10-12 07:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6565.1875Mb; avail=237989.03515625Mb
2020-10-12 07:01:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.785882
2020-10-12 07:01:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.821601
2020-10-12 07:01:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6570.3984375Mb; avail=237984.078125Mb
2020-10-12 07:01:35 | INFO | fairseq.trainer | begin training epoch 8
2020-10-12 07:01:44 | INFO | train_inner | epoch 008:     29 / 753 loss=5.989, nll_loss=4.671, ppl=25.48, wps=19144.8, ups=2.52, wpb=7599.6, bsz=279.5, num_updates=5300, lr=0.000173749, gnorm=1.024, clip=0, train_wall=29, wall=1675
2020-10-12 07:02:14 | INFO | train_inner | epoch 008:    129 / 753 loss=5.939, nll_loss=4.614, ppl=24.49, wps=25675.7, ups=3.3, wpb=7785.9, bsz=285.8, num_updates=5400, lr=0.000172133, gnorm=1.016, clip=0, train_wall=29, wall=1705
2020-10-12 07:02:45 | INFO | train_inner | epoch 008:    229 / 753 loss=5.936, nll_loss=4.609, ppl=24.41, wps=25287.8, ups=3.31, wpb=7648.7, bsz=270, num_updates=5500, lr=0.000170561, gnorm=0.961, clip=0, train_wall=29, wall=1735
2020-10-12 07:03:15 | INFO | train_inner | epoch 008:    329 / 753 loss=5.891, nll_loss=4.558, ppl=23.55, wps=25002.4, ups=3.31, wpb=7549.9, bsz=280.6, num_updates=5600, lr=0.000169031, gnorm=1.004, clip=0, train_wall=29, wall=1766
2020-10-12 07:03:46 | INFO | train_inner | epoch 008:    429 / 753 loss=5.846, nll_loss=4.506, ppl=22.73, wps=25128.2, ups=3.26, wpb=7707.2, bsz=293.6, num_updates=5700, lr=0.000167542, gnorm=0.966, clip=0, train_wall=29, wall=1796
2020-10-12 07:04:16 | INFO | train_inner | epoch 008:    529 / 753 loss=5.849, nll_loss=4.509, ppl=22.77, wps=25132.4, ups=3.33, wpb=7553.1, bsz=266.1, num_updates=5800, lr=0.000166091, gnorm=0.998, clip=0, train_wall=29, wall=1826
2020-10-12 07:04:46 | INFO | train_inner | epoch 008:    629 / 753 loss=5.701, nll_loss=4.341, ppl=20.27, wps=25020.3, ups=3.31, wpb=7559.3, bsz=298.7, num_updates=5900, lr=0.000164677, gnorm=0.919, clip=0, train_wall=29, wall=1857
2020-10-12 07:05:16 | INFO | train_inner | epoch 008:    729 / 753 loss=5.748, nll_loss=4.393, ppl=21.01, wps=25107, ups=3.32, wpb=7564.3, bsz=280.6, num_updates=6000, lr=0.000163299, gnorm=1.015, clip=0, train_wall=29, wall=1887
2020-10-12 07:05:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6672.24609375Mb; avail=237881.7734375Mb
2020-10-12 07:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001343
2020-10-12 07:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6672.24609375Mb; avail=237881.7734375Mb
2020-10-12 07:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071851
2020-10-12 07:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6672.23828125Mb; avail=237881.78125Mb
2020-10-12 07:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052211
2020-10-12 07:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126233
2020-10-12 07:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6671.953125Mb; avail=237881.66015625Mb
2020-10-12 07:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6672.0859375Mb; avail=237881.41796875Mb
2020-10-12 07:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001175
2020-10-12 07:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6672.0859375Mb; avail=237881.41796875Mb
2020-10-12 07:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070291
2020-10-12 07:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6672.2109375Mb; avail=237882.015625Mb
2020-10-12 07:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052435
2020-10-12 07:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124681
2020-10-12 07:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6672.4765625Mb; avail=237881.75Mb
2020-10-12 07:05:26 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.53 | nll_loss 4.044 | ppl 16.5 | wps 55125.2 | wpb 2261.4 | bsz 84.4 | num_updates 6024 | best_loss 5.53
2020-10-12 07:05:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:05:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 8 @ 6024 updates, score 5.53) (writing took 4.798567431047559 seconds)
2020-10-12 07:05:31 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-12 07:05:31 | INFO | train | epoch 008 | loss 5.844 | nll_loss 4.504 | ppl 22.68 | wps 24168.9 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 6024 | lr 0.000162974 | gnorm 0.985 | clip 0 | train_wall 217 | wall 1902
2020-10-12 07:05:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-12 07:05:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-12 07:05:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6589.91796875Mb; avail=237964.3359375Mb
2020-10-12 07:05:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003678
2020-10-12 07:05:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044303
2020-10-12 07:05:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6591.12109375Mb; avail=237963.1328125Mb
2020-10-12 07:05:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001556
2020-10-12 07:05:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6591.12109375Mb; avail=237963.1328125Mb
2020-10-12 07:05:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.776161
2020-10-12 07:05:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.822783
2020-10-12 07:05:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6595.62890625Mb; avail=237958.94921875Mb
2020-10-12 07:05:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6594.9453125Mb; avail=237960.125Mb
2020-10-12 07:05:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033057
2020-10-12 07:05:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6597.359375Mb; avail=237957.21875Mb
2020-10-12 07:05:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001558
2020-10-12 07:05:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6599.78125Mb; avail=237954.796875Mb
2020-10-12 07:05:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.777065
2020-10-12 07:05:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.812499
2020-10-12 07:05:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6596.546875Mb; avail=237957.96484375Mb
2020-10-12 07:05:33 | INFO | fairseq.trainer | begin training epoch 9
2020-10-12 07:05:56 | INFO | train_inner | epoch 009:     76 / 753 loss=5.665, nll_loss=4.3, ppl=19.69, wps=19056.5, ups=2.52, wpb=7569.1, bsz=280.8, num_updates=6100, lr=0.000161955, gnorm=0.937, clip=0, train_wall=29, wall=1926
2020-10-12 07:06:26 | INFO | train_inner | epoch 009:    176 / 753 loss=5.636, nll_loss=4.266, ppl=19.24, wps=25005.2, ups=3.3, wpb=7577.3, bsz=300.7, num_updates=6200, lr=0.000160644, gnorm=0.959, clip=0, train_wall=29, wall=1957
2020-10-12 07:06:56 | INFO | train_inner | epoch 009:    276 / 753 loss=5.687, nll_loss=4.322, ppl=20, wps=25139.4, ups=3.33, wpb=7545.8, bsz=244.2, num_updates=6300, lr=0.000159364, gnorm=0.941, clip=0, train_wall=29, wall=1987
2020-10-12 07:07:26 | INFO | train_inner | epoch 009:    376 / 753 loss=5.581, nll_loss=4.202, ppl=18.4, wps=25251.4, ups=3.28, wpb=7702.4, bsz=279.4, num_updates=6400, lr=0.000158114, gnorm=0.937, clip=0, train_wall=29, wall=2017
2020-10-12 07:07:57 | INFO | train_inner | epoch 009:    476 / 753 loss=5.533, nll_loss=4.147, ppl=17.71, wps=25416.1, ups=3.29, wpb=7726.9, bsz=295.8, num_updates=6500, lr=0.000156893, gnorm=0.89, clip=0, train_wall=29, wall=2048
2020-10-12 07:08:27 | INFO | train_inner | epoch 009:    576 / 753 loss=5.534, nll_loss=4.148, ppl=17.73, wps=25312.9, ups=3.3, wpb=7665.6, bsz=303.1, num_updates=6600, lr=0.0001557, gnorm=0.959, clip=0, train_wall=29, wall=2078
2020-10-12 07:08:57 | INFO | train_inner | epoch 009:    676 / 753 loss=5.58, nll_loss=4.199, ppl=18.37, wps=25040.6, ups=3.32, wpb=7544.3, bsz=272, num_updates=6700, lr=0.000154533, gnorm=0.987, clip=0, train_wall=29, wall=2108
2020-10-12 07:09:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6693.1015625Mb; avail=237861.078125Mb
2020-10-12 07:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001386
2020-10-12 07:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6693.1015625Mb; avail=237861.078125Mb
2020-10-12 07:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069999
2020-10-12 07:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6693.1015625Mb; avail=237861.078125Mb
2020-10-12 07:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052697
2020-10-12 07:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124876
2020-10-12 07:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6693.1015625Mb; avail=237861.078125Mb
2020-10-12 07:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6693.1015625Mb; avail=237861.078125Mb
2020-10-12 07:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001246
2020-10-12 07:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6693.1015625Mb; avail=237861.078125Mb
2020-10-12 07:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071093
2020-10-12 07:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6693.1015625Mb; avail=237861.078125Mb
2020-10-12 07:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053223
2020-10-12 07:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126331
2020-10-12 07:09:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6693.1015625Mb; avail=237861.078125Mb
2020-10-12 07:09:23 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.365 | nll_loss 3.844 | ppl 14.36 | wps 55185.8 | wpb 2261.4 | bsz 84.4 | num_updates 6777 | best_loss 5.365
2020-10-12 07:09:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:09:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 9 @ 6777 updates, score 5.365) (writing took 4.7812850209884346 seconds)
2020-10-12 07:09:28 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-12 07:09:28 | INFO | train | epoch 009 | loss 5.591 | nll_loss 4.213 | ppl 18.55 | wps 24169.1 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 6777 | lr 0.000153653 | gnorm 0.942 | clip 0 | train_wall 217 | wall 2139
2020-10-12 07:09:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-12 07:09:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-12 07:09:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6634.984375Mb; avail=237919.40234375Mb
2020-10-12 07:09:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003554
2020-10-12 07:09:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043450
2020-10-12 07:09:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6626.91015625Mb; avail=237927.4765625Mb
2020-10-12 07:09:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001577
2020-10-12 07:09:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6626.91015625Mb; avail=237927.4765625Mb
2020-10-12 07:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.769369
2020-10-12 07:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.815167
2020-10-12 07:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6633.55859375Mb; avail=237920.89453125Mb
2020-10-12 07:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6631.1328125Mb; avail=237923.3203125Mb
2020-10-12 07:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033069
2020-10-12 07:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6634.765625Mb; avail=237919.6875Mb
2020-10-12 07:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001563
2020-10-12 07:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6634.765625Mb; avail=237919.6875Mb
2020-10-12 07:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.772556
2020-10-12 07:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.807999
2020-10-12 07:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6633.76953125Mb; avail=237920.68359375Mb
2020-10-12 07:09:30 | INFO | fairseq.trainer | begin training epoch 10
2020-10-12 07:09:37 | INFO | train_inner | epoch 010:     23 / 753 loss=5.518, nll_loss=4.129, ppl=17.49, wps=18848.6, ups=2.52, wpb=7479.2, bsz=287.5, num_updates=6800, lr=0.000153393, gnorm=0.928, clip=0, train_wall=29, wall=2148
2020-10-12 07:10:07 | INFO | train_inner | epoch 010:    123 / 753 loss=5.483, nll_loss=4.089, ppl=17.01, wps=24893.9, ups=3.34, wpb=7458.9, bsz=256.9, num_updates=6900, lr=0.000152277, gnorm=0.946, clip=0, train_wall=29, wall=2178
2020-10-12 07:10:37 | INFO | train_inner | epoch 010:    223 / 753 loss=5.439, nll_loss=4.039, ppl=16.43, wps=25112.8, ups=3.3, wpb=7603.6, bsz=274.4, num_updates=7000, lr=0.000151186, gnorm=0.894, clip=0, train_wall=29, wall=2208
2020-10-12 07:11:08 | INFO | train_inner | epoch 010:    323 / 753 loss=5.436, nll_loss=4.034, ppl=16.38, wps=25399.8, ups=3.27, wpb=7768.8, bsz=287.5, num_updates=7100, lr=0.000150117, gnorm=0.907, clip=0, train_wall=29, wall=2239
2020-10-12 07:11:38 | INFO | train_inner | epoch 010:    423 / 753 loss=5.362, nll_loss=3.95, ppl=15.46, wps=25581.8, ups=3.29, wpb=7775, bsz=308.6, num_updates=7200, lr=0.000149071, gnorm=0.926, clip=0, train_wall=29, wall=2269
2020-10-12 07:12:09 | INFO | train_inner | epoch 010:    523 / 753 loss=5.329, nll_loss=3.913, ppl=15.06, wps=24721, ups=3.26, wpb=7583.7, bsz=306, num_updates=7300, lr=0.000148047, gnorm=0.91, clip=0, train_wall=29, wall=2300
2020-10-12 07:12:40 | INFO | train_inner | epoch 010:    623 / 753 loss=5.415, nll_loss=4.009, ppl=16.1, wps=24631.6, ups=3.25, wpb=7584.7, bsz=282.8, num_updates=7400, lr=0.000147043, gnorm=0.906, clip=0, train_wall=29, wall=2330
2020-10-12 07:13:10 | INFO | train_inner | epoch 010:    723 / 753 loss=5.355, nll_loss=3.942, ppl=15.37, wps=25292.8, ups=3.3, wpb=7660.6, bsz=274.5, num_updates=7500, lr=0.000146059, gnorm=0.898, clip=0, train_wall=29, wall=2361
2020-10-12 07:13:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6742.5859375Mb; avail=237811.42578125Mb
2020-10-12 07:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001404
2020-10-12 07:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6742.5859375Mb; avail=237811.42578125Mb
2020-10-12 07:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070205
2020-10-12 07:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6741.6640625Mb; avail=237813.08203125Mb
2020-10-12 07:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052439
2020-10-12 07:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124827
2020-10-12 07:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6741.5703125Mb; avail=237813.17578125Mb
2020-10-12 07:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6742.7734375Mb; avail=237811.97265625Mb
2020-10-12 07:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001181
2020-10-12 07:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6743.31640625Mb; avail=237811.4296875Mb
2020-10-12 07:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069579
2020-10-12 07:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6741.73046875Mb; avail=237813.015625Mb
2020-10-12 07:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052401
2020-10-12 07:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123912
2020-10-12 07:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6741.41015625Mb; avail=237813.57421875Mb
2020-10-12 07:13:22 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.246 | nll_loss 3.704 | ppl 13.03 | wps 55172.2 | wpb 2261.4 | bsz 84.4 | num_updates 7530 | best_loss 5.246
2020-10-12 07:13:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:13:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 10 @ 7530 updates, score 5.246) (writing took 7.37079920200631 seconds)
2020-10-12 07:13:31 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-12 07:13:31 | INFO | train | epoch 010 | loss 5.402 | nll_loss 3.996 | ppl 15.96 | wps 23607 | ups 3.1 | wpb 7619.3 | bsz 282.7 | num_updates 7530 | lr 0.000145768 | gnorm 0.912 | clip 0 | train_wall 218 | wall 2382
2020-10-12 07:13:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-12 07:13:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-12 07:13:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6932.7734375Mb; avail=237621.671875Mb
2020-10-12 07:13:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003963
2020-10-12 07:13:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.041724
2020-10-12 07:13:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6942.34375Mb; avail=237612.1015625Mb
2020-10-12 07:13:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001646
2020-10-12 07:13:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6942.94921875Mb; avail=237611.49609375Mb
2020-10-12 07:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.768446
2020-10-12 07:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.812679
2020-10-12 07:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6939.4375Mb; avail=237615.4765625Mb
2020-10-12 07:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6929.6796875Mb; avail=237625.234375Mb
2020-10-12 07:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033721
2020-10-12 07:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6928.6953125Mb; avail=237626.21875Mb
2020-10-12 07:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001652
2020-10-12 07:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6928.6953125Mb; avail=237626.21875Mb
2020-10-12 07:13:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.783349
2020-10-12 07:13:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.819559
2020-10-12 07:13:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6946.23046875Mb; avail=237608.64453125Mb
2020-10-12 07:13:33 | INFO | fairseq.trainer | begin training epoch 11
2020-10-12 07:13:54 | INFO | train_inner | epoch 011:     70 / 753 loss=5.329, nll_loss=3.914, ppl=15.07, wps=16949.8, ups=2.26, wpb=7494.3, bsz=254.3, num_updates=7600, lr=0.000145095, gnorm=0.942, clip=0, train_wall=29, wall=2405
2020-10-12 07:14:24 | INFO | train_inner | epoch 011:    170 / 753 loss=5.244, nll_loss=3.816, ppl=14.09, wps=24886.9, ups=3.3, wpb=7551.6, bsz=307.9, num_updates=7700, lr=0.00014415, gnorm=0.883, clip=0, train_wall=29, wall=2435
2020-10-12 07:14:55 | INFO | train_inner | epoch 011:    270 / 753 loss=5.263, nll_loss=3.837, ppl=14.29, wps=24686.1, ups=3.26, wpb=7573.8, bsz=292.2, num_updates=7800, lr=0.000143223, gnorm=0.92, clip=0, train_wall=29, wall=2466
2020-10-12 07:15:26 | INFO | train_inner | epoch 011:    370 / 753 loss=5.303, nll_loss=3.881, ppl=14.73, wps=24719.4, ups=3.23, wpb=7645.9, bsz=284.3, num_updates=7900, lr=0.000142314, gnorm=0.883, clip=0, train_wall=29, wall=2497
2020-10-12 07:15:57 | INFO | train_inner | epoch 011:    470 / 753 loss=5.261, nll_loss=3.834, ppl=14.26, wps=24988.1, ups=3.24, wpb=7712.6, bsz=265.5, num_updates=8000, lr=0.000141421, gnorm=0.895, clip=0, train_wall=29, wall=2528
2020-10-12 07:16:28 | INFO | train_inner | epoch 011:    570 / 753 loss=5.27, nll_loss=3.844, ppl=14.36, wps=25143.6, ups=3.27, wpb=7691.7, bsz=289.3, num_updates=8100, lr=0.000140546, gnorm=0.905, clip=0, train_wall=29, wall=2558
2020-10-12 07:16:58 | INFO | train_inner | epoch 011:    670 / 753 loss=5.217, nll_loss=3.784, ppl=13.77, wps=25349.2, ups=3.3, wpb=7672.5, bsz=279, num_updates=8200, lr=0.000139686, gnorm=0.929, clip=0, train_wall=29, wall=2589
2020-10-12 07:17:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7105.015625Mb; avail=237449.9609375Mb
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001406
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7105.015625Mb; avail=237449.9609375Mb
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070911
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7094.6796875Mb; avail=237460.296875Mb
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053619
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126805
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7094.6796875Mb; avail=237460.296875Mb
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7094.6796875Mb; avail=237460.296875Mb
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001249
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7094.6796875Mb; avail=237460.296875Mb
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071432
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7094.44140625Mb; avail=237460.53515625Mb
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053759
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127202
2020-10-12 07:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7094.43359375Mb; avail=237460.54296875Mb
2020-10-12 07:17:26 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 5.149 | nll_loss 3.592 | ppl 12.05 | wps 54911.3 | wpb 2261.4 | bsz 84.4 | num_updates 8283 | best_loss 5.149
2020-10-12 07:17:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:17:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 11 @ 8283 updates, score 5.149) (writing took 4.875938636017963 seconds)
2020-10-12 07:17:31 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-12 07:17:31 | INFO | train | epoch 011 | loss 5.26 | nll_loss 3.833 | ppl 14.26 | wps 23962.5 | ups 3.14 | wpb 7619.3 | bsz 282.7 | num_updates 8283 | lr 0.000138984 | gnorm 0.905 | clip 0 | train_wall 218 | wall 2622
2020-10-12 07:17:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-12 07:17:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-12 07:17:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6943.328125Mb; avail=237610.90625Mb
2020-10-12 07:17:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003462
2020-10-12 07:17:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043646
2020-10-12 07:17:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6948.32421875Mb; avail=237605.91015625Mb
2020-10-12 07:17:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001580
2020-10-12 07:17:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6948.32421875Mb; avail=237605.91015625Mb
2020-10-12 07:17:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.769585
2020-10-12 07:17:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.815620
2020-10-12 07:17:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6953.28515625Mb; avail=237601.19140625Mb
2020-10-12 07:17:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6939.62890625Mb; avail=237615.33984375Mb
2020-10-12 07:17:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033536
2020-10-12 07:17:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6939.13671875Mb; avail=237615.33984375Mb
2020-10-12 07:17:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001574
2020-10-12 07:17:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6939.13671875Mb; avail=237615.33984375Mb
2020-10-12 07:17:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.780321
2020-10-12 07:17:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.816230
2020-10-12 07:17:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6944.453125Mb; avail=237610.3203125Mb
2020-10-12 07:17:32 | INFO | fairseq.trainer | begin training epoch 12
2020-10-12 07:17:38 | INFO | train_inner | epoch 012:     17 / 753 loss=5.189, nll_loss=3.752, ppl=13.48, wps=19191.9, ups=2.5, wpb=7665.3, bsz=297.6, num_updates=8300, lr=0.000138842, gnorm=0.876, clip=0, train_wall=29, wall=2629
2020-10-12 07:18:08 | INFO | train_inner | epoch 012:    117 / 753 loss=5.198, nll_loss=3.763, ppl=13.57, wps=25057.8, ups=3.32, wpb=7554.1, bsz=245.8, num_updates=8400, lr=0.000138013, gnorm=0.902, clip=0, train_wall=29, wall=2659
2020-10-12 07:18:38 | INFO | train_inner | epoch 012:    217 / 753 loss=5.145, nll_loss=3.703, ppl=13.02, wps=24874.4, ups=3.32, wpb=7484.8, bsz=271.1, num_updates=8500, lr=0.000137199, gnorm=0.891, clip=0, train_wall=29, wall=2689
2020-10-12 07:19:08 | INFO | train_inner | epoch 012:    317 / 753 loss=5.212, nll_loss=3.777, ppl=13.71, wps=24963.8, ups=3.31, wpb=7542, bsz=273.4, num_updates=8600, lr=0.000136399, gnorm=0.912, clip=0, train_wall=29, wall=2719
2020-10-12 07:19:38 | INFO | train_inner | epoch 012:    417 / 753 loss=5.079, nll_loss=3.628, ppl=12.36, wps=25215.4, ups=3.33, wpb=7576.7, bsz=292.2, num_updates=8700, lr=0.000135613, gnorm=0.854, clip=0, train_wall=29, wall=2749
2020-10-12 07:20:09 | INFO | train_inner | epoch 012:    517 / 753 loss=5.15, nll_loss=3.707, ppl=13.06, wps=25205.3, ups=3.27, wpb=7697.1, bsz=290.9, num_updates=8800, lr=0.00013484, gnorm=0.91, clip=0, train_wall=29, wall=2780
2020-10-12 07:20:40 | INFO | train_inner | epoch 012:    617 / 753 loss=5.104, nll_loss=3.656, ppl=12.6, wps=25471.4, ups=3.26, wpb=7824.2, bsz=295.4, num_updates=8900, lr=0.00013408, gnorm=0.855, clip=0, train_wall=29, wall=2810
2020-10-12 07:21:10 | INFO | train_inner | epoch 012:    717 / 753 loss=5.129, nll_loss=3.683, ppl=12.85, wps=25336.2, ups=3.3, wpb=7680.5, bsz=286.9, num_updates=9000, lr=0.000133333, gnorm=0.868, clip=0, train_wall=29, wall=2841
2020-10-12 07:21:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:21:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7052.45703125Mb; avail=237501.44921875Mb
2020-10-12 07:21:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001373
2020-10-12 07:21:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7052.45703125Mb; avail=237501.44921875Mb
2020-10-12 07:21:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070202
2020-10-12 07:21:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7058.3984375Mb; avail=237495.5078125Mb
2020-10-12 07:21:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053442
2020-10-12 07:21:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125827
2020-10-12 07:21:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7049.40234375Mb; avail=237505.23828125Mb
2020-10-12 07:21:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7049.40234375Mb; avail=237505.23828125Mb
2020-10-12 07:21:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001277
2020-10-12 07:21:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7049.40234375Mb; avail=237505.23828125Mb
2020-10-12 07:21:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072089
2020-10-12 07:21:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7049.40234375Mb; avail=237505.23828125Mb
2020-10-12 07:21:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052885
2020-10-12 07:21:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127012
2020-10-12 07:21:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7049.40234375Mb; avail=237505.23828125Mb
2020-10-12 07:21:24 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.059 | nll_loss 3.486 | ppl 11.2 | wps 54690.1 | wpb 2261.4 | bsz 84.4 | num_updates 9036 | best_loss 5.059
2020-10-12 07:21:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:21:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 12 @ 9036 updates, score 5.059) (writing took 4.859196825884283 seconds)
2020-10-12 07:21:29 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-12 07:21:29 | INFO | train | epoch 012 | loss 5.138 | nll_loss 3.694 | ppl 12.94 | wps 24117.8 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 9036 | lr 0.000133067 | gnorm 0.884 | clip 0 | train_wall 217 | wall 2859
2020-10-12 07:21:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-12 07:21:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-12 07:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6957.71484375Mb; avail=237596.484375Mb
2020-10-12 07:21:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003987
2020-10-12 07:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043226
2020-10-12 07:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6957.15234375Mb; avail=237597.046875Mb
2020-10-12 07:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001553
2020-10-12 07:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6957.7578125Mb; avail=237596.44140625Mb
2020-10-12 07:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.776465
2020-10-12 07:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.822057
2020-10-12 07:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6962.67578125Mb; avail=237591.5234375Mb
2020-10-12 07:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6957.9921875Mb; avail=237596.45703125Mb
2020-10-12 07:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033073
2020-10-12 07:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6957.9921875Mb; avail=237596.45703125Mb
2020-10-12 07:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001569
2020-10-12 07:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6957.9921875Mb; avail=237596.45703125Mb
2020-10-12 07:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.781987
2020-10-12 07:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.817453
2020-10-12 07:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6963.6328125Mb; avail=237590.859375Mb
2020-10-12 07:21:30 | INFO | fairseq.trainer | begin training epoch 13
2020-10-12 07:21:50 | INFO | train_inner | epoch 013:     64 / 753 loss=5.068, nll_loss=3.614, ppl=12.24, wps=19150.4, ups=2.5, wpb=7646.2, bsz=280.6, num_updates=9100, lr=0.000132599, gnorm=0.888, clip=0, train_wall=29, wall=2881
2020-10-12 07:22:20 | INFO | train_inner | epoch 013:    164 / 753 loss=5.045, nll_loss=3.589, ppl=12.03, wps=25366.8, ups=3.29, wpb=7702.5, bsz=294.4, num_updates=9200, lr=0.000131876, gnorm=0.858, clip=0, train_wall=29, wall=2911
2020-10-12 07:22:50 | INFO | train_inner | epoch 013:    264 / 753 loss=5.01, nll_loss=3.549, ppl=11.7, wps=25327.7, ups=3.32, wpb=7619.8, bsz=275.8, num_updates=9300, lr=0.000131165, gnorm=0.861, clip=0, train_wall=29, wall=2941
2020-10-12 07:23:20 | INFO | train_inner | epoch 013:    364 / 753 loss=5.076, nll_loss=3.622, ppl=12.31, wps=24975, ups=3.33, wpb=7495.2, bsz=276.8, num_updates=9400, lr=0.000130466, gnorm=0.882, clip=0, train_wall=29, wall=2971
2020-10-12 07:23:50 | INFO | train_inner | epoch 013:    464 / 753 loss=5.01, nll_loss=3.549, ppl=11.7, wps=24893, ups=3.33, wpb=7477.7, bsz=274.4, num_updates=9500, lr=0.000129777, gnorm=0.896, clip=0, train_wall=29, wall=3001
2020-10-12 07:24:21 | INFO | train_inner | epoch 013:    564 / 753 loss=5.051, nll_loss=3.594, ppl=12.08, wps=25224.9, ups=3.3, wpb=7632.8, bsz=271.9, num_updates=9600, lr=0.000129099, gnorm=0.846, clip=0, train_wall=29, wall=3031
2020-10-12 07:24:51 | INFO | train_inner | epoch 013:    664 / 753 loss=5.027, nll_loss=3.567, ppl=11.85, wps=25230.9, ups=3.29, wpb=7679.3, bsz=290.3, num_updates=9700, lr=0.000128432, gnorm=0.853, clip=0, train_wall=29, wall=3062
2020-10-12 07:25:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7070.14453125Mb; avail=237483.859375Mb
2020-10-12 07:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001345
2020-10-12 07:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7070.14453125Mb; avail=237483.859375Mb
2020-10-12 07:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070840
2020-10-12 07:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7070.7421875Mb; avail=237483.26171875Mb
2020-10-12 07:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053553
2020-10-12 07:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126526
2020-10-12 07:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7070.859375Mb; avail=237483.37890625Mb
2020-10-12 07:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7070.859375Mb; avail=237483.37890625Mb
2020-10-12 07:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001206
2020-10-12 07:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7070.859375Mb; avail=237483.37890625Mb
2020-10-12 07:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072701
2020-10-12 07:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7070.64453125Mb; avail=237483.4921875Mb
2020-10-12 07:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053289
2020-10-12 07:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127951
2020-10-12 07:25:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7070.50390625Mb; avail=237483.7265625Mb
2020-10-12 07:25:21 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 4.991 | nll_loss 3.404 | ppl 10.59 | wps 54830.5 | wpb 2261.4 | bsz 84.4 | num_updates 9789 | best_loss 4.991
2020-10-12 07:25:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:25:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 13 @ 9789 updates, score 4.991) (writing took 4.867318413918838 seconds)
2020-10-12 07:25:26 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-12 07:25:26 | INFO | train | epoch 013 | loss 5.038 | nll_loss 3.579 | ppl 11.95 | wps 24165.5 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 9789 | lr 0.000127847 | gnorm 0.866 | clip 0 | train_wall 217 | wall 3097
2020-10-12 07:25:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-12 07:25:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-12 07:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6976.10546875Mb; avail=237578.140625Mb
2020-10-12 07:25:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003940
2020-10-12 07:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.041258
2020-10-12 07:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6976.10546875Mb; avail=237578.140625Mb
2020-10-12 07:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001542
2020-10-12 07:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6976.10546875Mb; avail=237578.140625Mb
2020-10-12 07:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.774782
2020-10-12 07:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.818396
2020-10-12 07:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6982.4140625Mb; avail=237572.05859375Mb
2020-10-12 07:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6977.4921875Mb; avail=237577.47265625Mb
2020-10-12 07:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033185
2020-10-12 07:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6977.0Mb; avail=237577.47265625Mb
2020-10-12 07:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001553
2020-10-12 07:25:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6977.0Mb; avail=237577.47265625Mb
2020-10-12 07:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.770021
2020-10-12 07:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.805580
2020-10-12 07:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6989.97265625Mb; avail=237564.53125Mb
2020-10-12 07:25:28 | INFO | fairseq.trainer | begin training epoch 14
2020-10-12 07:25:31 | INFO | train_inner | epoch 014:     11 / 753 loss=5.01, nll_loss=3.547, ppl=11.69, wps=18758.1, ups=2.48, wpb=7574.3, bsz=310.6, num_updates=9800, lr=0.000127775, gnorm=0.861, clip=0, train_wall=29, wall=3102
2020-10-12 07:26:01 | INFO | train_inner | epoch 014:    111 / 753 loss=4.943, nll_loss=3.472, ppl=11.1, wps=25057.3, ups=3.33, wpb=7534.3, bsz=277.6, num_updates=9900, lr=0.000127128, gnorm=0.871, clip=0, train_wall=29, wall=3132
2020-10-12 07:26:32 | INFO | train_inner | epoch 014:    211 / 753 loss=4.955, nll_loss=3.485, ppl=11.2, wps=25378.9, ups=3.3, wpb=7699.5, bsz=282.8, num_updates=10000, lr=0.000126491, gnorm=0.839, clip=0, train_wall=29, wall=3163
2020-10-12 07:27:02 | INFO | train_inner | epoch 014:    311 / 753 loss=4.958, nll_loss=3.488, ppl=11.22, wps=25378.5, ups=3.31, wpb=7671.3, bsz=279.5, num_updates=10100, lr=0.000125863, gnorm=0.845, clip=0, train_wall=29, wall=3193
2020-10-12 07:27:33 | INFO | train_inner | epoch 014:    411 / 753 loss=4.96, nll_loss=3.491, ppl=11.24, wps=25114.5, ups=3.27, wpb=7670, bsz=282.1, num_updates=10200, lr=0.000125245, gnorm=0.86, clip=0, train_wall=29, wall=3223
2020-10-12 07:28:03 | INFO | train_inner | epoch 014:    511 / 753 loss=4.973, nll_loss=3.506, ppl=11.36, wps=25071.4, ups=3.31, wpb=7578.8, bsz=277.3, num_updates=10300, lr=0.000124635, gnorm=0.88, clip=0, train_wall=29, wall=3254
2020-10-12 07:28:33 | INFO | train_inner | epoch 014:    611 / 753 loss=4.945, nll_loss=3.473, ppl=11.11, wps=25358.6, ups=3.31, wpb=7650, bsz=289, num_updates=10400, lr=0.000124035, gnorm=0.871, clip=0, train_wall=29, wall=3284
2020-10-12 07:29:03 | INFO | train_inner | epoch 014:    711 / 753 loss=4.95, nll_loss=3.48, ppl=11.16, wps=25438.2, ups=3.32, wpb=7654, bsz=285.4, num_updates=10500, lr=0.000123443, gnorm=0.87, clip=0, train_wall=29, wall=3314
2020-10-12 07:29:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:29:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7106.30078125Mb; avail=237447.9296875Mb
2020-10-12 07:29:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001261
2020-10-12 07:29:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.30078125Mb; avail=237447.9296875Mb
2020-10-12 07:29:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071160
2020-10-12 07:29:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.30078125Mb; avail=237447.9296875Mb
2020-10-12 07:29:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053371
2020-10-12 07:29:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126587
2020-10-12 07:29:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.30078125Mb; avail=237447.9296875Mb
2020-10-12 07:29:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7106.30078125Mb; avail=237447.9296875Mb
2020-10-12 07:29:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001147
2020-10-12 07:29:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.30078125Mb; avail=237447.9296875Mb
2020-10-12 07:29:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071170
2020-10-12 07:29:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.30078125Mb; avail=237447.9296875Mb
2020-10-12 07:29:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053039
2020-10-12 07:29:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126108
2020-10-12 07:29:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.30078125Mb; avail=237447.9296875Mb
2020-10-12 07:29:18 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 4.966 | nll_loss 3.385 | ppl 10.45 | wps 54577.2 | wpb 2261.4 | bsz 84.4 | num_updates 10542 | best_loss 4.966
2020-10-12 07:29:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:29:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 14 @ 10542 updates, score 4.966) (writing took 4.866845827084035 seconds)
2020-10-12 07:29:23 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-12 07:29:23 | INFO | train | epoch 014 | loss 4.953 | nll_loss 3.483 | ppl 11.18 | wps 24165.3 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 10542 | lr 0.000123197 | gnorm 0.867 | clip 0 | train_wall 217 | wall 3334
2020-10-12 07:29:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-12 07:29:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-12 07:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7018.8671875Mb; avail=237535.61328125Mb
2020-10-12 07:29:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003380
2020-10-12 07:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043679
2020-10-12 07:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7009.03125Mb; avail=237545.44921875Mb
2020-10-12 07:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001704
2020-10-12 07:29:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7009.03125Mb; avail=237545.44921875Mb
2020-10-12 07:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.767858
2020-10-12 07:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.814088
2020-10-12 07:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7008.98046875Mb; avail=237545.58984375Mb
2020-10-12 07:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7010.28125Mb; avail=237544.2890625Mb
2020-10-12 07:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033908
2020-10-12 07:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7013.9140625Mb; avail=237540.65625Mb
2020-10-12 07:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001608
2020-10-12 07:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7014.05859375Mb; avail=237540.51171875Mb
2020-10-12 07:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.772945
2020-10-12 07:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.809313
2020-10-12 07:29:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6961.00390625Mb; avail=237593.4609375Mb
2020-10-12 07:29:25 | INFO | fairseq.trainer | begin training epoch 15
2020-10-12 07:29:43 | INFO | train_inner | epoch 015:     58 / 753 loss=4.901, nll_loss=3.424, ppl=10.73, wps=19065.9, ups=2.51, wpb=7592.9, bsz=278.1, num_updates=10600, lr=0.000122859, gnorm=0.891, clip=0, train_wall=29, wall=3354
2020-10-12 07:30:13 | INFO | train_inner | epoch 015:    158 / 753 loss=4.842, nll_loss=3.358, ppl=10.25, wps=25055.7, ups=3.3, wpb=7594.3, bsz=299.8, num_updates=10700, lr=0.000122284, gnorm=0.824, clip=0, train_wall=29, wall=3384
2020-10-12 07:30:43 | INFO | train_inner | epoch 015:    258 / 753 loss=4.847, nll_loss=3.363, ppl=10.29, wps=25345.1, ups=3.33, wpb=7613.8, bsz=280.8, num_updates=10800, lr=0.000121716, gnorm=0.852, clip=0, train_wall=29, wall=3414
2020-10-12 07:31:14 | INFO | train_inner | epoch 015:    358 / 753 loss=4.872, nll_loss=3.391, ppl=10.49, wps=25168.4, ups=3.28, wpb=7667.7, bsz=306.7, num_updates=10900, lr=0.000121157, gnorm=0.839, clip=0, train_wall=29, wall=3444
2020-10-12 07:31:44 | INFO | train_inner | epoch 015:    458 / 753 loss=4.871, nll_loss=3.389, ppl=10.48, wps=25292.2, ups=3.3, wpb=7671.8, bsz=284.9, num_updates=11000, lr=0.000120605, gnorm=0.841, clip=0, train_wall=29, wall=3475
2020-10-12 07:32:14 | INFO | train_inner | epoch 015:    558 / 753 loss=4.919, nll_loss=3.443, ppl=10.88, wps=24900.9, ups=3.31, wpb=7528, bsz=260.2, num_updates=11100, lr=0.00012006, gnorm=0.85, clip=0, train_wall=29, wall=3505
2020-10-12 07:32:45 | INFO | train_inner | epoch 015:    658 / 753 loss=4.878, nll_loss=3.397, ppl=10.53, wps=25279, ups=3.3, wpb=7659.2, bsz=280.4, num_updates=11200, lr=0.000119523, gnorm=0.845, clip=0, train_wall=29, wall=3535
2020-10-12 07:33:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:33:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6965.61328125Mb; avail=237588.8125Mb
2020-10-12 07:33:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001352
2020-10-12 07:33:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6965.61328125Mb; avail=237588.8125Mb
2020-10-12 07:33:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070861
2020-10-12 07:33:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6971.94921875Mb; avail=237582.2734375Mb
2020-10-12 07:33:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052145
2020-10-12 07:33:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125137
2020-10-12 07:33:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6962.59765625Mb; avail=237591.625Mb
2020-10-12 07:33:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6964.4140625Mb; avail=237589.80859375Mb
2020-10-12 07:33:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001227
2020-10-12 07:33:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6964.4140625Mb; avail=237589.80859375Mb
2020-10-12 07:33:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069801
2020-10-12 07:33:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6972.5703125Mb; avail=237581.65234375Mb
2020-10-12 07:33:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052431
2020-10-12 07:33:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124226
2020-10-12 07:33:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6963.23828125Mb; avail=237590.984375Mb
2020-10-12 07:33:16 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 4.889 | nll_loss 3.295 | ppl 9.82 | wps 54861.3 | wpb 2261.4 | bsz 84.4 | num_updates 11295 | best_loss 4.889
2020-10-12 07:33:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:33:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 15 @ 11295 updates, score 4.889) (writing took 4.86353568197228 seconds)
2020-10-12 07:33:21 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-12 07:33:21 | INFO | train | epoch 015 | loss 4.876 | nll_loss 3.396 | ppl 10.52 | wps 24153.9 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 11295 | lr 0.000119019 | gnorm 0.844 | clip 0 | train_wall 217 | wall 3572
2020-10-12 07:33:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-12 07:33:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-12 07:33:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6884.6171875Mb; avail=237669.5625Mb
2020-10-12 07:33:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003447
2020-10-12 07:33:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044188
2020-10-12 07:33:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6884.6171875Mb; avail=237669.5625Mb
2020-10-12 07:33:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001689
2020-10-12 07:33:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6884.6171875Mb; avail=237669.5625Mb
2020-10-12 07:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.791952
2020-10-12 07:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.838644
2020-10-12 07:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6886.19140625Mb; avail=237667.98828125Mb
2020-10-12 07:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6886.19140625Mb; avail=237667.98828125Mb
2020-10-12 07:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033325
2020-10-12 07:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6886.19140625Mb; avail=237667.98828125Mb
2020-10-12 07:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001604
2020-10-12 07:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6886.19140625Mb; avail=237667.98828125Mb
2020-10-12 07:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.769699
2020-10-12 07:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.805437
2020-10-12 07:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6886.34765625Mb; avail=237668.11328125Mb
2020-10-12 07:33:23 | INFO | fairseq.trainer | begin training epoch 16
2020-10-12 07:33:24 | INFO | train_inner | epoch 016:      5 / 753 loss=4.901, nll_loss=3.423, ppl=10.73, wps=18988.2, ups=2.51, wpb=7554.5, bsz=264.3, num_updates=11300, lr=0.000118993, gnorm=0.854, clip=0, train_wall=29, wall=3575
2020-10-12 07:33:55 | INFO | train_inner | epoch 016:    105 / 753 loss=4.8, nll_loss=3.309, ppl=9.91, wps=24865.7, ups=3.3, wpb=7545.8, bsz=288, num_updates=11400, lr=0.00011847, gnorm=0.84, clip=0, train_wall=29, wall=3605
2020-10-12 07:34:25 | INFO | train_inner | epoch 016:    205 / 753 loss=4.839, nll_loss=3.353, ppl=10.22, wps=24958, ups=3.32, wpb=7513, bsz=264.6, num_updates=11500, lr=0.000117954, gnorm=0.854, clip=0, train_wall=29, wall=3636
2020-10-12 07:34:55 | INFO | train_inner | epoch 016:    305 / 753 loss=4.82, nll_loss=3.332, ppl=10.07, wps=25236.3, ups=3.3, wpb=7657.1, bsz=286.6, num_updates=11600, lr=0.000117444, gnorm=0.874, clip=0, train_wall=29, wall=3666
2020-10-12 07:35:25 | INFO | train_inner | epoch 016:    405 / 753 loss=4.787, nll_loss=3.294, ppl=9.81, wps=25459.2, ups=3.29, wpb=7738.6, bsz=296.6, num_updates=11700, lr=0.000116941, gnorm=0.832, clip=0, train_wall=29, wall=3696
2020-10-12 07:35:56 | INFO | train_inner | epoch 016:    505 / 753 loss=4.802, nll_loss=3.312, ppl=9.93, wps=25056.6, ups=3.32, wpb=7538.4, bsz=282.4, num_updates=11800, lr=0.000116445, gnorm=0.838, clip=0, train_wall=29, wall=3726
2020-10-12 07:36:26 | INFO | train_inner | epoch 016:    605 / 753 loss=4.82, nll_loss=3.331, ppl=10.06, wps=25250.9, ups=3.28, wpb=7694.8, bsz=286.4, num_updates=11900, lr=0.000115954, gnorm=0.872, clip=0, train_wall=29, wall=3757
2020-10-12 07:36:57 | INFO | train_inner | epoch 016:    705 / 753 loss=4.807, nll_loss=3.316, ppl=9.96, wps=25218.3, ups=3.27, wpb=7713.7, bsz=293.1, num_updates=12000, lr=0.00011547, gnorm=0.838, clip=0, train_wall=29, wall=3787
2020-10-12 07:37:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6971.359375Mb; avail=237582.53125Mb
2020-10-12 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001387
2020-10-12 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6971.359375Mb; avail=237582.53125Mb
2020-10-12 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070430
2020-10-12 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6971.359375Mb; avail=237582.53125Mb
2020-10-12 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053391
2020-10-12 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125988
2020-10-12 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6970.921875Mb; avail=237582.96875Mb
2020-10-12 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6970.921875Mb; avail=237582.96875Mb
2020-10-12 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001176
2020-10-12 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6970.921875Mb; avail=237582.96875Mb
2020-10-12 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070173
2020-10-12 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6970.921875Mb; avail=237582.96875Mb
2020-10-12 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053094
2020-10-12 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125194
2020-10-12 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6970.921875Mb; avail=237582.96875Mb
2020-10-12 07:37:14 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.844 | nll_loss 3.245 | ppl 9.48 | wps 54795.6 | wpb 2261.4 | bsz 84.4 | num_updates 12048 | best_loss 4.844
2020-10-12 07:37:14 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:37:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 16 @ 12048 updates, score 4.844) (writing took 4.866695492062718 seconds)
2020-10-12 07:37:19 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-12 07:37:19 | INFO | train | epoch 016 | loss 4.813 | nll_loss 3.324 | ppl 10.01 | wps 24117.7 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 12048 | lr 0.00011524 | gnorm 0.852 | clip 0 | train_wall 217 | wall 3810
2020-10-12 07:37:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-12 07:37:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-12 07:37:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6886.5390625Mb; avail=237667.8984375Mb
2020-10-12 07:37:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003407
2020-10-12 07:37:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044372
2020-10-12 07:37:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6886.5390625Mb; avail=237667.8984375Mb
2020-10-12 07:37:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001719
2020-10-12 07:37:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6886.5390625Mb; avail=237667.8984375Mb
2020-10-12 07:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.795813
2020-10-12 07:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.842718
2020-10-12 07:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6891.234375Mb; avail=237663.2109375Mb
2020-10-12 07:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6891.28515625Mb; avail=237663.16015625Mb
2020-10-12 07:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033663
2020-10-12 07:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6896.125Mb; avail=237658.3203125Mb
2020-10-12 07:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001689
2020-10-12 07:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6896.73046875Mb; avail=237657.71484375Mb
2020-10-12 07:37:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.796444
2020-10-12 07:37:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.832627
2020-10-12 07:37:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6891.1328125Mb; avail=237663.44140625Mb
2020-10-12 07:37:21 | INFO | fairseq.trainer | begin training epoch 17
2020-10-12 07:37:36 | INFO | train_inner | epoch 017:     52 / 753 loss=4.826, nll_loss=3.339, ppl=10.12, wps=19160.8, ups=2.52, wpb=7616.2, bsz=251.5, num_updates=12100, lr=0.000114992, gnorm=0.849, clip=0, train_wall=29, wall=3827
2020-10-12 07:38:06 | INFO | train_inner | epoch 017:    152 / 753 loss=4.72, nll_loss=3.22, ppl=9.32, wps=24906.3, ups=3.35, wpb=7424.3, bsz=265.8, num_updates=12200, lr=0.00011452, gnorm=0.872, clip=0, train_wall=28, wall=3857
2020-10-12 07:38:37 | INFO | train_inner | epoch 017:    252 / 753 loss=4.791, nll_loss=3.298, ppl=9.83, wps=25439.7, ups=3.29, wpb=7743.4, bsz=264.1, num_updates=12300, lr=0.000114053, gnorm=0.833, clip=0, train_wall=29, wall=3887
2020-10-12 07:39:07 | INFO | train_inner | epoch 017:    352 / 753 loss=4.784, nll_loss=3.29, ppl=9.78, wps=25225.3, ups=3.29, wpb=7673.6, bsz=289.7, num_updates=12400, lr=0.000113592, gnorm=0.844, clip=0, train_wall=29, wall=3918
2020-10-12 07:39:37 | INFO | train_inner | epoch 017:    452 / 753 loss=4.713, nll_loss=3.211, ppl=9.26, wps=25092.3, ups=3.33, wpb=7529.9, bsz=300.7, num_updates=12500, lr=0.000113137, gnorm=0.849, clip=0, train_wall=29, wall=3948
2020-10-12 07:40:08 | INFO | train_inner | epoch 017:    552 / 753 loss=4.747, nll_loss=3.248, ppl=9.5, wps=25116.7, ups=3.28, wpb=7652.5, bsz=300.1, num_updates=12600, lr=0.000112687, gnorm=0.845, clip=0, train_wall=29, wall=3978
2020-10-12 07:40:38 | INFO | train_inner | epoch 017:    652 / 753 loss=4.763, nll_loss=3.267, ppl=9.62, wps=25055.4, ups=3.29, wpb=7622.9, bsz=278.9, num_updates=12700, lr=0.000112243, gnorm=0.841, clip=0, train_wall=29, wall=4009
2020-10-12 07:41:08 | INFO | train_inner | epoch 017:    752 / 753 loss=4.722, nll_loss=3.22, ppl=9.32, wps=25128.5, ups=3.3, wpb=7623.7, bsz=297, num_updates=12800, lr=0.000111803, gnorm=0.847, clip=0, train_wall=29, wall=4039
2020-10-12 07:41:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6992.25Mb; avail=237561.96875Mb
2020-10-12 07:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001388
2020-10-12 07:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6992.25Mb; avail=237561.96875Mb
2020-10-12 07:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071065
2020-10-12 07:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6992.25Mb; avail=237561.96875Mb
2020-10-12 07:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053902
2020-10-12 07:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127134
2020-10-12 07:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6992.25Mb; avail=237561.96875Mb
2020-10-12 07:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6992.25Mb; avail=237561.96875Mb
2020-10-12 07:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001179
2020-10-12 07:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6992.25Mb; avail=237561.96875Mb
2020-10-12 07:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071450
2020-10-12 07:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6992.25Mb; avail=237561.96875Mb
2020-10-12 07:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052916
2020-10-12 07:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126286
2020-10-12 07:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6992.25Mb; avail=237561.96875Mb
2020-10-12 07:41:11 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.798 | nll_loss 3.196 | ppl 9.16 | wps 54612.4 | wpb 2261.4 | bsz 84.4 | num_updates 12801 | best_loss 4.798
2020-10-12 07:41:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:41:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 17 @ 12801 updates, score 4.798) (writing took 4.8614972189534456 seconds)
2020-10-12 07:41:16 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-12 07:41:16 | INFO | train | epoch 017 | loss 4.753 | nll_loss 3.255 | ppl 9.55 | wps 24152.6 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 12801 | lr 0.000111799 | gnorm 0.846 | clip 0 | train_wall 217 | wall 4047
2020-10-12 07:41:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-12 07:41:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-12 07:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6902.5859375Mb; avail=237651.6328125Mb
2020-10-12 07:41:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003380
2020-10-12 07:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044130
2020-10-12 07:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6902.7421875Mb; avail=237651.609375Mb
2020-10-12 07:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001671
2020-10-12 07:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6902.7421875Mb; avail=237651.609375Mb
2020-10-12 07:41:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.773486
2020-10-12 07:41:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.820083
2020-10-12 07:41:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6909.9140625Mb; avail=237644.57421875Mb
2020-10-12 07:41:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6911.44140625Mb; avail=237643.046875Mb
2020-10-12 07:41:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033406
2020-10-12 07:41:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6915.2109375Mb; avail=237639.27734375Mb
2020-10-12 07:41:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001620
2020-10-12 07:41:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6915.2109375Mb; avail=237640.75390625Mb
2020-10-12 07:41:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.768068
2020-10-12 07:41:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.803916
2020-10-12 07:41:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6907.0078125Mb; avail=237647.453125Mb
2020-10-12 07:41:18 | INFO | fairseq.trainer | begin training epoch 18
2020-10-12 07:41:48 | INFO | train_inner | epoch 018:     99 / 753 loss=4.682, nll_loss=3.175, ppl=9.03, wps=19215, ups=2.52, wpb=7628.7, bsz=279.2, num_updates=12900, lr=0.000111369, gnorm=0.808, clip=0, train_wall=29, wall=4079
2020-10-12 07:42:18 | INFO | train_inner | epoch 018:    199 / 753 loss=4.685, nll_loss=3.179, ppl=9.06, wps=25214.4, ups=3.29, wpb=7655.1, bsz=297.3, num_updates=13000, lr=0.00011094, gnorm=0.836, clip=0, train_wall=29, wall=4109
2020-10-12 07:42:49 | INFO | train_inner | epoch 018:    299 / 753 loss=4.701, nll_loss=3.195, ppl=9.16, wps=25272.2, ups=3.29, wpb=7679.1, bsz=294.2, num_updates=13100, lr=0.000110516, gnorm=0.842, clip=0, train_wall=29, wall=4140
2020-10-12 07:43:19 | INFO | train_inner | epoch 018:    399 / 753 loss=4.724, nll_loss=3.222, ppl=9.33, wps=25007.8, ups=3.3, wpb=7573.3, bsz=269.3, num_updates=13200, lr=0.000110096, gnorm=0.854, clip=0, train_wall=29, wall=4170
2020-10-12 07:43:50 | INFO | train_inner | epoch 018:    499 / 753 loss=4.677, nll_loss=3.17, ppl=9, wps=25362.6, ups=3.26, wpb=7786.5, bsz=290.6, num_updates=13300, lr=0.000109682, gnorm=0.818, clip=0, train_wall=29, wall=4201
2020-10-12 07:44:20 | INFO | train_inner | epoch 018:    599 / 753 loss=4.711, nll_loss=3.209, ppl=9.25, wps=25001.5, ups=3.34, wpb=7490.4, bsz=263.2, num_updates=13400, lr=0.000109272, gnorm=0.853, clip=0, train_wall=29, wall=4231
2020-10-12 07:44:50 | INFO | train_inner | epoch 018:    699 / 753 loss=4.698, nll_loss=3.192, ppl=9.14, wps=25140.8, ups=3.3, wpb=7623.2, bsz=304, num_updates=13500, lr=0.000108866, gnorm=0.837, clip=0, train_wall=29, wall=4261
2020-10-12 07:45:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7010.6328125Mb; avail=237543.359375Mb
2020-10-12 07:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001362
2020-10-12 07:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7010.6328125Mb; avail=237543.359375Mb
2020-10-12 07:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070222
2020-10-12 07:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7010.34765625Mb; avail=237543.859375Mb
2020-10-12 07:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052655
2020-10-12 07:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125021
2020-10-12 07:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7010.34765625Mb; avail=237543.859375Mb
2020-10-12 07:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7010.34765625Mb; avail=237543.859375Mb
2020-10-12 07:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001160
2020-10-12 07:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7010.34765625Mb; avail=237543.859375Mb
2020-10-12 07:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070929
2020-10-12 07:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7010.20703125Mb; avail=237544.09375Mb
2020-10-12 07:45:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052287
2020-10-12 07:45:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125126
2020-10-12 07:45:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7010.20703125Mb; avail=237544.09375Mb
2020-10-12 07:45:09 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.774 | nll_loss 3.166 | ppl 8.98 | wps 54856.5 | wpb 2261.4 | bsz 84.4 | num_updates 13554 | best_loss 4.774
2020-10-12 07:45:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:45:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 18 @ 13554 updates, score 4.774) (writing took 4.857242200057954 seconds)
2020-10-12 07:45:14 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-12 07:45:14 | INFO | train | epoch 018 | loss 4.699 | nll_loss 3.194 | ppl 9.15 | wps 24142.9 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 13554 | lr 0.000108649 | gnorm 0.84 | clip 0 | train_wall 217 | wall 4285
2020-10-12 07:45:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-12 07:45:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-12 07:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6926.3671875Mb; avail=237627.2421875Mb
2020-10-12 07:45:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003377
2020-10-12 07:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043855
2020-10-12 07:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6927.4765625Mb; avail=237626.73828125Mb
2020-10-12 07:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001570
2020-10-12 07:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6926.984375Mb; avail=237627.23046875Mb
2020-10-12 07:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.782822
2020-10-12 07:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.829080
2020-10-12 07:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6923.609375Mb; avail=237630.83984375Mb
2020-10-12 07:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6918.6875Mb; avail=237635.76171875Mb
2020-10-12 07:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033332
2020-10-12 07:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6918.6875Mb; avail=237635.76171875Mb
2020-10-12 07:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001562
2020-10-12 07:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6918.6875Mb; avail=237635.76171875Mb
2020-10-12 07:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.770031
2020-10-12 07:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.805736
2020-10-12 07:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6923.18359375Mb; avail=237631.50390625Mb
2020-10-12 07:45:16 | INFO | fairseq.trainer | begin training epoch 19
2020-10-12 07:45:30 | INFO | train_inner | epoch 019:     46 / 753 loss=4.692, nll_loss=3.186, ppl=9.1, wps=18798.4, ups=2.52, wpb=7458.8, bsz=255.6, num_updates=13600, lr=0.000108465, gnorm=0.88, clip=0, train_wall=29, wall=4301
2020-10-12 07:46:00 | INFO | train_inner | epoch 019:    146 / 753 loss=4.628, nll_loss=3.114, ppl=8.66, wps=24902.3, ups=3.31, wpb=7515.8, bsz=278.7, num_updates=13700, lr=0.000108069, gnorm=0.829, clip=0, train_wall=29, wall=4331
2020-10-12 07:46:30 | INFO | train_inner | epoch 019:    246 / 753 loss=4.649, nll_loss=3.137, ppl=8.79, wps=25257.2, ups=3.27, wpb=7733.1, bsz=294.7, num_updates=13800, lr=0.000107676, gnorm=0.826, clip=0, train_wall=29, wall=4361
2020-10-12 07:47:01 | INFO | train_inner | epoch 019:    346 / 753 loss=4.652, nll_loss=3.14, ppl=8.82, wps=25476, ups=3.29, wpb=7737.2, bsz=279.8, num_updates=13900, lr=0.000107288, gnorm=0.819, clip=0, train_wall=29, wall=4392
2020-10-12 07:47:31 | INFO | train_inner | epoch 019:    446 / 753 loss=4.677, nll_loss=3.169, ppl=9, wps=24728.2, ups=3.3, wpb=7496.2, bsz=279, num_updates=14000, lr=0.000106904, gnorm=0.851, clip=0, train_wall=29, wall=4422
2020-10-12 07:48:02 | INFO | train_inner | epoch 019:    546 / 753 loss=4.635, nll_loss=3.122, ppl=8.7, wps=25318.8, ups=3.3, wpb=7683.6, bsz=288.7, num_updates=14100, lr=0.000106525, gnorm=0.843, clip=0, train_wall=29, wall=4452
2020-10-12 07:48:32 | INFO | train_inner | epoch 019:    646 / 753 loss=4.672, nll_loss=3.163, ppl=8.96, wps=25129.2, ups=3.27, wpb=7680.8, bsz=293, num_updates=14200, lr=0.000106149, gnorm=0.856, clip=0, train_wall=29, wall=4483
2020-10-12 07:49:02 | INFO | train_inner | epoch 019:    746 / 753 loss=4.644, nll_loss=3.133, ppl=8.77, wps=25276.7, ups=3.35, wpb=7555.8, bsz=273.4, num_updates=14300, lr=0.000105777, gnorm=0.848, clip=0, train_wall=29, wall=4513
2020-10-12 07:49:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:49:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7026.6875Mb; avail=237527.3203125Mb
2020-10-12 07:49:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001385
2020-10-12 07:49:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7026.6875Mb; avail=237527.3203125Mb
2020-10-12 07:49:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070899
2020-10-12 07:49:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7026.6640625Mb; avail=237527.34375Mb
2020-10-12 07:49:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052243
2020-10-12 07:49:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125334
2020-10-12 07:49:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7026.3125Mb; avail=237527.6953125Mb
2020-10-12 07:49:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7026.3046875Mb; avail=237527.703125Mb
2020-10-12 07:49:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001196
2020-10-12 07:49:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7026.3046875Mb; avail=237527.703125Mb
2020-10-12 07:49:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069815
2020-10-12 07:49:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7026.5234375Mb; avail=237527.484375Mb
2020-10-12 07:49:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051826
2020-10-12 07:49:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123620
2020-10-12 07:49:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7026.26171875Mb; avail=237527.8359375Mb
2020-10-12 07:49:07 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.751 | nll_loss 3.143 | ppl 8.83 | wps 54620.6 | wpb 2261.4 | bsz 84.4 | num_updates 14307 | best_loss 4.751
2020-10-12 07:49:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:49:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 19 @ 14307 updates, score 4.751) (writing took 4.864801387069747 seconds)
2020-10-12 07:49:12 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-12 07:49:12 | INFO | train | epoch 019 | loss 4.651 | nll_loss 3.139 | ppl 8.81 | wps 24119.8 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 14307 | lr 0.000105751 | gnorm 0.84 | clip 0 | train_wall 217 | wall 4523
2020-10-12 07:49:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-12 07:49:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-12 07:49:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6981.4609375Mb; avail=237572.94921875Mb
2020-10-12 07:49:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003398
2020-10-12 07:49:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.045048
2020-10-12 07:49:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6986.796875Mb; avail=237567.61328125Mb
2020-10-12 07:49:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001791
2020-10-12 07:49:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6986.796875Mb; avail=237567.61328125Mb
2020-10-12 07:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.776787
2020-10-12 07:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.824571
2020-10-12 07:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7004.57421875Mb; avail=237549.875Mb
2020-10-12 07:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6999.65234375Mb; avail=237554.796875Mb
2020-10-12 07:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033154
2020-10-12 07:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6999.65234375Mb; avail=237554.796875Mb
2020-10-12 07:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001550
2020-10-12 07:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6999.65234375Mb; avail=237554.796875Mb
2020-10-12 07:49:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.767756
2020-10-12 07:49:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.803310
2020-10-12 07:49:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7013.5703125Mb; avail=237540.91796875Mb
2020-10-12 07:49:14 | INFO | fairseq.trainer | begin training epoch 20
2020-10-12 07:49:41 | INFO | train_inner | epoch 020:     93 / 753 loss=4.59, nll_loss=3.071, ppl=8.4, wps=18820.7, ups=2.53, wpb=7432.2, bsz=283.2, num_updates=14400, lr=0.000105409, gnorm=0.838, clip=0, train_wall=28, wall=4552
2020-10-12 07:50:12 | INFO | train_inner | epoch 020:    193 / 753 loss=4.589, nll_loss=3.07, ppl=8.4, wps=25296.3, ups=3.28, wpb=7709.2, bsz=289.8, num_updates=14500, lr=0.000105045, gnorm=0.838, clip=0, train_wall=29, wall=4583
2020-10-12 07:50:42 | INFO | train_inner | epoch 020:    293 / 753 loss=4.622, nll_loss=3.106, ppl=8.61, wps=25052.3, ups=3.28, wpb=7648.5, bsz=268.6, num_updates=14600, lr=0.000104685, gnorm=0.841, clip=0, train_wall=29, wall=4613
2020-10-12 07:51:13 | INFO | train_inner | epoch 020:    393 / 753 loss=4.626, nll_loss=3.111, ppl=8.64, wps=24956.1, ups=3.3, wpb=7573.1, bsz=284.5, num_updates=14700, lr=0.000104328, gnorm=0.853, clip=0, train_wall=29, wall=4644
2020-10-12 07:51:44 | INFO | train_inner | epoch 020:    493 / 753 loss=4.611, nll_loss=3.093, ppl=8.53, wps=24832.1, ups=3.25, wpb=7642.6, bsz=282, num_updates=14800, lr=0.000103975, gnorm=0.827, clip=0, train_wall=29, wall=4674
2020-10-12 07:52:14 | INFO | train_inner | epoch 020:    593 / 753 loss=4.596, nll_loss=3.077, ppl=8.44, wps=25196.7, ups=3.31, wpb=7610.4, bsz=295.4, num_updates=14900, lr=0.000103626, gnorm=0.846, clip=0, train_wall=29, wall=4705
2020-10-12 07:52:44 | INFO | train_inner | epoch 020:    693 / 753 loss=4.603, nll_loss=3.085, ppl=8.49, wps=25362.3, ups=3.28, wpb=7744, bsz=293, num_updates=15000, lr=0.00010328, gnorm=0.835, clip=0, train_wall=29, wall=4735
2020-10-12 07:53:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:53:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7131.7890625Mb; avail=237423.140625Mb
2020-10-12 07:53:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001363
2020-10-12 07:53:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7131.7890625Mb; avail=237423.140625Mb
2020-10-12 07:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070944
2020-10-12 07:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7130.125Mb; avail=237424.09375Mb
2020-10-12 07:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052255
2020-10-12 07:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125339
2020-10-12 07:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7130.125Mb; avail=237424.09375Mb
2020-10-12 07:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7130.125Mb; avail=237424.09375Mb
2020-10-12 07:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001180
2020-10-12 07:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7130.125Mb; avail=237424.09375Mb
2020-10-12 07:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070763
2020-10-12 07:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7130.125Mb; avail=237424.09375Mb
2020-10-12 07:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052735
2020-10-12 07:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125413
2020-10-12 07:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7130.125Mb; avail=237424.09375Mb
2020-10-12 07:53:05 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.716 | nll_loss 3.1 | ppl 8.58 | wps 54683.7 | wpb 2261.4 | bsz 84.4 | num_updates 15060 | best_loss 4.716
2020-10-12 07:53:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:53:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 20 @ 15060 updates, score 4.716) (writing took 4.874090889003128 seconds)
2020-10-12 07:53:10 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-12 07:53:10 | INFO | train | epoch 020 | loss 4.608 | nll_loss 3.09 | ppl 8.52 | wps 24066 | ups 3.16 | wpb 7619.3 | bsz 282.7 | num_updates 15060 | lr 0.000103074 | gnorm 0.84 | clip 0 | train_wall 218 | wall 4761
2020-10-12 07:53:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-12 07:53:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-12 07:53:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7033.62890625Mb; avail=237520.0625Mb
2020-10-12 07:53:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003384
2020-10-12 07:53:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043629
2020-10-12 07:53:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7034.25Mb; avail=237520.17578125Mb
2020-10-12 07:53:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001549
2020-10-12 07:53:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7034.25Mb; avail=237520.17578125Mb
2020-10-12 07:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.772312
2020-10-12 07:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.818286
2020-10-12 07:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7039.48046875Mb; avail=237514.96484375Mb
2020-10-12 07:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7034.21484375Mb; avail=237520.72265625Mb
2020-10-12 07:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033236
2020-10-12 07:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7037.9609375Mb; avail=237516.484375Mb
2020-10-12 07:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001595
2020-10-12 07:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7038.203125Mb; avail=237516.9765625Mb
2020-10-12 07:53:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.771496
2020-10-12 07:53:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.807160
2020-10-12 07:53:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7039.06640625Mb; avail=237515.39453125Mb
2020-10-12 07:53:12 | INFO | fairseq.trainer | begin training epoch 21
2020-10-12 07:53:24 | INFO | train_inner | epoch 021:     40 / 753 loss=4.634, nll_loss=3.12, ppl=8.69, wps=18945.5, ups=2.5, wpb=7570.4, bsz=247.9, num_updates=15100, lr=0.000102937, gnorm=0.851, clip=0, train_wall=29, wall=4775
2020-10-12 07:53:55 | INFO | train_inner | epoch 021:    140 / 753 loss=4.536, nll_loss=3.01, ppl=8.05, wps=25398.5, ups=3.29, wpb=7718, bsz=306.2, num_updates=15200, lr=0.000102598, gnorm=0.806, clip=0, train_wall=29, wall=4806
2020-10-12 07:54:25 | INFO | train_inner | epoch 021:    240 / 753 loss=4.556, nll_loss=3.032, ppl=8.18, wps=25007.7, ups=3.31, wpb=7553.8, bsz=286.7, num_updates=15300, lr=0.000102262, gnorm=0.844, clip=0, train_wall=29, wall=4836
2020-10-12 07:54:55 | INFO | train_inner | epoch 021:    340 / 753 loss=4.564, nll_loss=3.042, ppl=8.23, wps=25054.9, ups=3.32, wpb=7539.1, bsz=268.8, num_updates=15400, lr=0.000101929, gnorm=0.849, clip=0, train_wall=29, wall=4866
2020-10-12 07:55:25 | INFO | train_inner | epoch 021:    440 / 753 loss=4.569, nll_loss=3.046, ppl=8.26, wps=25536.3, ups=3.3, wpb=7748.5, bsz=284.8, num_updates=15500, lr=0.0001016, gnorm=0.826, clip=0, train_wall=29, wall=4896
2020-10-12 07:55:55 | INFO | train_inner | epoch 021:    540 / 753 loss=4.609, nll_loss=3.092, ppl=8.53, wps=24893.3, ups=3.32, wpb=7487.8, bsz=265.8, num_updates=15600, lr=0.000101274, gnorm=0.863, clip=0, train_wall=29, wall=4926
2020-10-12 07:56:26 | INFO | train_inner | epoch 021:    640 / 753 loss=4.572, nll_loss=3.049, ppl=8.27, wps=25101.6, ups=3.29, wpb=7637.2, bsz=289.4, num_updates=15700, lr=0.000100951, gnorm=0.84, clip=0, train_wall=29, wall=4957
2020-10-12 07:56:56 | INFO | train_inner | epoch 021:    740 / 753 loss=4.556, nll_loss=3.032, ppl=8.18, wps=25203.4, ups=3.28, wpb=7679.9, bsz=292.7, num_updates=15800, lr=0.000100631, gnorm=0.822, clip=0, train_wall=29, wall=4987
2020-10-12 07:57:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 07:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7095.20703125Mb; avail=237459.01953125Mb
2020-10-12 07:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001400
2020-10-12 07:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.20703125Mb; avail=237459.01953125Mb
2020-10-12 07:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072093
2020-10-12 07:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.20703125Mb; avail=237459.01953125Mb
2020-10-12 07:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052469
2020-10-12 07:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126739
2020-10-12 07:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.20703125Mb; avail=237459.01953125Mb
2020-10-12 07:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7095.20703125Mb; avail=237459.01953125Mb
2020-10-12 07:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001196
2020-10-12 07:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.20703125Mb; avail=237459.01953125Mb
2020-10-12 07:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070831
2020-10-12 07:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.20703125Mb; avail=237459.01953125Mb
2020-10-12 07:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052418
2020-10-12 07:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125198
2020-10-12 07:57:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.20703125Mb; avail=237459.01953125Mb
2020-10-12 07:57:03 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 4.696 | nll_loss 3.084 | ppl 8.48 | wps 54887.4 | wpb 2261.4 | bsz 84.4 | num_updates 15813 | best_loss 4.696
2020-10-12 07:57:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 07:57:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 21 @ 15813 updates, score 4.696) (writing took 4.855437421007082 seconds)
2020-10-12 07:57:08 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-12 07:57:08 | INFO | train | epoch 021 | loss 4.568 | nll_loss 3.045 | ppl 8.25 | wps 24144.2 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 15813 | lr 0.00010059 | gnorm 0.838 | clip 0 | train_wall 217 | wall 4999
2020-10-12 07:57:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-12 07:57:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-12 07:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6998.890625Mb; avail=237555.37109375Mb
2020-10-12 07:57:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003375
2020-10-12 07:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043368
2020-10-12 07:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6998.890625Mb; avail=237555.37109375Mb
2020-10-12 07:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001569
2020-10-12 07:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6998.890625Mb; avail=237555.37109375Mb
2020-10-12 07:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.785698
2020-10-12 07:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.831408
2020-10-12 07:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7002.79296875Mb; avail=237551.72265625Mb
2020-10-12 07:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6997.87109375Mb; avail=237557.13671875Mb
2020-10-12 07:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032967
2020-10-12 07:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6997.37890625Mb; avail=237557.13671875Mb
2020-10-12 07:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001574
2020-10-12 07:57:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6997.37890625Mb; avail=237557.13671875Mb
2020-10-12 07:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.780146
2020-10-12 07:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.815508
2020-10-12 07:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7002.7734375Mb; avail=237551.59765625Mb
2020-10-12 07:57:10 | INFO | fairseq.trainer | begin training epoch 22
2020-10-12 07:57:36 | INFO | train_inner | epoch 022:     87 / 753 loss=4.53, nll_loss=3.002, ppl=8.01, wps=18862, ups=2.51, wpb=7523, bsz=276.7, num_updates=15900, lr=0.000100314, gnorm=0.845, clip=0, train_wall=29, wall=5027
2020-10-12 07:58:07 | INFO | train_inner | epoch 022:    187 / 753 loss=4.49, nll_loss=2.957, ppl=7.77, wps=25327.4, ups=3.29, wpb=7703.3, bsz=299, num_updates=16000, lr=0.0001, gnorm=0.818, clip=0, train_wall=29, wall=5057
2020-10-12 07:58:37 | INFO | train_inner | epoch 022:    287 / 753 loss=4.55, nll_loss=3.023, ppl=8.13, wps=25523.9, ups=3.32, wpb=7687.9, bsz=272.4, num_updates=16100, lr=9.9689e-05, gnorm=0.825, clip=0, train_wall=29, wall=5088
2020-10-12 07:59:07 | INFO | train_inner | epoch 022:    387 / 753 loss=4.518, nll_loss=2.988, ppl=7.94, wps=25367.6, ups=3.3, wpb=7692.5, bsz=291, num_updates=16200, lr=9.93808e-05, gnorm=0.824, clip=0, train_wall=29, wall=5118
2020-10-12 07:59:37 | INFO | train_inner | epoch 022:    487 / 753 loss=4.524, nll_loss=2.996, ppl=7.98, wps=25421.9, ups=3.33, wpb=7638.4, bsz=264.7, num_updates=16300, lr=9.90755e-05, gnorm=0.831, clip=0, train_wall=29, wall=5148
2020-10-12 08:00:07 | INFO | train_inner | epoch 022:    587 / 753 loss=4.54, nll_loss=3.014, ppl=8.08, wps=24956.7, ups=3.32, wpb=7518.8, bsz=287.4, num_updates=16400, lr=9.8773e-05, gnorm=0.843, clip=0, train_wall=29, wall=5178
2020-10-12 08:00:37 | INFO | train_inner | epoch 022:    687 / 753 loss=4.548, nll_loss=3.022, ppl=8.12, wps=25166.3, ups=3.31, wpb=7595.4, bsz=286.3, num_updates=16500, lr=9.84732e-05, gnorm=0.843, clip=0, train_wall=29, wall=5208
2020-10-12 08:00:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:00:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6992.859375Mb; avail=237561.17578125Mb
2020-10-12 08:00:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001267
2020-10-12 08:00:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6992.859375Mb; avail=237561.17578125Mb
2020-10-12 08:00:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071328
2020-10-12 08:00:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6997.09765625Mb; avail=237556.9375Mb
2020-10-12 08:00:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052680
2020-10-12 08:00:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126077
2020-10-12 08:00:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7005.375Mb; avail=237548.66015625Mb
2020-10-12 08:00:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7007.6875Mb; avail=237546.34765625Mb
2020-10-12 08:00:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001190
2020-10-12 08:00:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7007.6875Mb; avail=237546.34765625Mb
2020-10-12 08:00:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071480
2020-10-12 08:00:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6994.38671875Mb; avail=237559.6484375Mb
2020-10-12 08:00:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052739
2020-10-12 08:00:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126183
2020-10-12 08:00:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6993.89453125Mb; avail=237560.140625Mb
2020-10-12 08:01:00 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 4.682 | nll_loss 3.067 | ppl 8.38 | wps 54886.9 | wpb 2261.4 | bsz 84.4 | num_updates 16566 | best_loss 4.682
2020-10-12 08:01:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:01:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 22 @ 16566 updates, score 4.682) (writing took 4.865462107118219 seconds)
2020-10-12 08:01:05 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-12 08:01:05 | INFO | train | epoch 022 | loss 4.529 | nll_loss 3.001 | ppl 8.01 | wps 24185.6 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 16566 | lr 9.82768e-05 | gnorm 0.834 | clip 0 | train_wall 217 | wall 5236
2020-10-12 08:01:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-12 08:01:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-12 08:01:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6893.578125Mb; avail=237661.4140625Mb
2020-10-12 08:01:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003368
2020-10-12 08:01:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043631
2020-10-12 08:01:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6893.25390625Mb; avail=237661.73828125Mb
2020-10-12 08:01:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001722
2020-10-12 08:01:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6893.25390625Mb; avail=237661.73828125Mb
2020-10-12 08:01:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.784961
2020-10-12 08:01:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.831131
2020-10-12 08:01:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6893.6953125Mb; avail=237660.99609375Mb
2020-10-12 08:01:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6892.9609375Mb; avail=237660.99609375Mb
2020-10-12 08:01:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033335
2020-10-12 08:01:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6892.96875Mb; avail=237660.98828125Mb
2020-10-12 08:01:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001570
2020-10-12 08:01:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6892.96875Mb; avail=237660.98828125Mb
2020-10-12 08:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.766870
2020-10-12 08:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.802577
2020-10-12 08:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6893.69921875Mb; avail=237660.78125Mb
2020-10-12 08:01:07 | INFO | fairseq.trainer | begin training epoch 23
2020-10-12 08:01:17 | INFO | train_inner | epoch 023:     34 / 753 loss=4.498, nll_loss=2.967, ppl=7.82, wps=19161, ups=2.51, wpb=7625.2, bsz=294.6, num_updates=16600, lr=9.81761e-05, gnorm=0.842, clip=0, train_wall=29, wall=5248
2020-10-12 08:01:48 | INFO | train_inner | epoch 023:    134 / 753 loss=4.5, nll_loss=2.968, ppl=7.82, wps=25206.1, ups=3.3, wpb=7644.5, bsz=275.9, num_updates=16700, lr=9.78818e-05, gnorm=0.828, clip=0, train_wall=29, wall=5278
2020-10-12 08:02:18 | INFO | train_inner | epoch 023:    234 / 753 loss=4.482, nll_loss=2.947, ppl=7.71, wps=25003.9, ups=3.31, wpb=7560.9, bsz=290.3, num_updates=16800, lr=9.759e-05, gnorm=0.837, clip=0, train_wall=29, wall=5309
2020-10-12 08:02:48 | INFO | train_inner | epoch 023:    334 / 753 loss=4.488, nll_loss=2.954, ppl=7.75, wps=25432.2, ups=3.3, wpb=7704.8, bsz=268.6, num_updates=16900, lr=9.73009e-05, gnorm=0.812, clip=0, train_wall=29, wall=5339
2020-10-12 08:03:18 | INFO | train_inner | epoch 023:    434 / 753 loss=4.508, nll_loss=2.977, ppl=7.87, wps=24954.1, ups=3.33, wpb=7487.6, bsz=269.8, num_updates=17000, lr=9.70143e-05, gnorm=0.875, clip=0, train_wall=29, wall=5369
2020-10-12 08:03:48 | INFO | train_inner | epoch 023:    534 / 753 loss=4.476, nll_loss=2.942, ppl=7.68, wps=25151, ups=3.3, wpb=7629.8, bsz=286.3, num_updates=17100, lr=9.67302e-05, gnorm=0.835, clip=0, train_wall=29, wall=5399
2020-10-12 08:04:18 | INFO | train_inner | epoch 023:    634 / 753 loss=4.506, nll_loss=2.976, ppl=7.87, wps=25189.7, ups=3.32, wpb=7577.7, bsz=272.2, num_updates=17200, lr=9.64486e-05, gnorm=0.839, clip=0, train_wall=29, wall=5429
2020-10-12 08:04:49 | INFO | train_inner | epoch 023:    734 / 753 loss=4.516, nll_loss=2.986, ppl=7.92, wps=25087.6, ups=3.29, wpb=7631.1, bsz=311.6, num_updates=17300, lr=9.61694e-05, gnorm=0.863, clip=0, train_wall=29, wall=5460
2020-10-12 08:04:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6966.390625Mb; avail=237587.80859375Mb
2020-10-12 08:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001372
2020-10-12 08:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6966.390625Mb; avail=237587.80859375Mb
2020-10-12 08:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069656
2020-10-12 08:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6966.390625Mb; avail=237587.80859375Mb
2020-10-12 08:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052149
2020-10-12 08:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123965
2020-10-12 08:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6966.390625Mb; avail=237587.80859375Mb
2020-10-12 08:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6966.390625Mb; avail=237587.80859375Mb
2020-10-12 08:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001124
2020-10-12 08:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6966.390625Mb; avail=237587.80859375Mb
2020-10-12 08:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071372
2020-10-12 08:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6966.390625Mb; avail=237587.80859375Mb
2020-10-12 08:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051907
2020-10-12 08:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125136
2020-10-12 08:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6965.65625Mb; avail=237587.80859375Mb
2020-10-12 08:04:58 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 4.661 | nll_loss 3.042 | ppl 8.24 | wps 54705.6 | wpb 2261.4 | bsz 84.4 | num_updates 17319 | best_loss 4.661
2020-10-12 08:04:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:05:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 23 @ 17319 updates, score 4.661) (writing took 4.876777581172064 seconds)
2020-10-12 08:05:03 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-12 08:05:03 | INFO | train | epoch 023 | loss 4.496 | nll_loss 2.964 | ppl 7.8 | wps 24167 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 17319 | lr 9.61166e-05 | gnorm 0.841 | clip 0 | train_wall 217 | wall 5473
2020-10-12 08:05:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-12 08:05:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-12 08:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6899.8984375Mb; avail=237654.33203125Mb
2020-10-12 08:05:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003700
2020-10-12 08:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044180
2020-10-12 08:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6890.546875Mb; avail=237663.68359375Mb
2020-10-12 08:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001690
2020-10-12 08:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6890.546875Mb; avail=237663.68359375Mb
2020-10-12 08:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.775599
2020-10-12 08:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.822298
2020-10-12 08:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6890.50390625Mb; avail=237663.94921875Mb
2020-10-12 08:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6894.49609375Mb; avail=237660.01171875Mb
2020-10-12 08:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033721
2020-10-12 08:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6899.09765625Mb; avail=237655.41015625Mb
2020-10-12 08:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001636
2020-10-12 08:05:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6899.09765625Mb; avail=237655.41015625Mb
2020-10-12 08:05:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.764677
2020-10-12 08:05:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.800916
2020-10-12 08:05:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6891.2421875Mb; avail=237663.39453125Mb
2020-10-12 08:05:04 | INFO | fairseq.trainer | begin training epoch 24
2020-10-12 08:05:29 | INFO | train_inner | epoch 024:     81 / 753 loss=4.48, nll_loss=2.945, ppl=7.7, wps=19059.4, ups=2.52, wpb=7560.8, bsz=252.2, num_updates=17400, lr=9.58927e-05, gnorm=0.836, clip=0, train_wall=29, wall=5499
2020-10-12 08:05:59 | INFO | train_inner | epoch 024:    181 / 753 loss=4.45, nll_loss=2.911, ppl=7.52, wps=25220.2, ups=3.3, wpb=7646.9, bsz=279.5, num_updates=17500, lr=9.56183e-05, gnorm=0.844, clip=0, train_wall=29, wall=5530
2020-10-12 08:06:29 | INFO | train_inner | epoch 024:    281 / 753 loss=4.473, nll_loss=2.937, ppl=7.66, wps=24812.1, ups=3.29, wpb=7535.8, bsz=284.7, num_updates=17600, lr=9.53463e-05, gnorm=0.857, clip=0, train_wall=29, wall=5560
2020-10-12 08:07:00 | INFO | train_inner | epoch 024:    381 / 753 loss=4.443, nll_loss=2.904, ppl=7.48, wps=25335.5, ups=3.3, wpb=7675.5, bsz=277.5, num_updates=17700, lr=9.50765e-05, gnorm=0.826, clip=0, train_wall=29, wall=5590
2020-10-12 08:07:30 | INFO | train_inner | epoch 024:    481 / 753 loss=4.466, nll_loss=2.929, ppl=7.62, wps=25698.3, ups=3.31, wpb=7775.1, bsz=274.1, num_updates=17800, lr=9.48091e-05, gnorm=0.813, clip=0, train_wall=29, wall=5621
2020-10-12 08:08:00 | INFO | train_inner | epoch 024:    581 / 753 loss=4.48, nll_loss=2.944, ppl=7.7, wps=25539.6, ups=3.3, wpb=7737.7, bsz=293.6, num_updates=17900, lr=9.45439e-05, gnorm=0.839, clip=0, train_wall=29, wall=5651
2020-10-12 08:08:30 | INFO | train_inner | epoch 024:    681 / 753 loss=4.467, nll_loss=2.931, ppl=7.62, wps=25274.5, ups=3.3, wpb=7649.5, bsz=303.8, num_updates=18000, lr=9.42809e-05, gnorm=0.839, clip=0, train_wall=29, wall=5681
2020-10-12 08:08:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6974.9375Mb; avail=237579.1171875Mb
2020-10-12 08:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001371
2020-10-12 08:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6974.9375Mb; avail=237579.1171875Mb
2020-10-12 08:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070087
2020-10-12 08:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6984.3984375Mb; avail=237569.65625Mb
2020-10-12 08:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052493
2020-10-12 08:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124758
2020-10-12 08:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6975.078125Mb; avail=237578.9765625Mb
2020-10-12 08:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6975.078125Mb; avail=237578.9765625Mb
2020-10-12 08:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001200
2020-10-12 08:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6975.078125Mb; avail=237578.9765625Mb
2020-10-12 08:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069241
2020-10-12 08:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6975.078125Mb; avail=237578.9765625Mb
2020-10-12 08:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051683
2020-10-12 08:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122896
2020-10-12 08:08:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6975.3203125Mb; avail=237579.46875Mb
2020-10-12 08:08:55 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 4.643 | nll_loss 3.025 | ppl 8.14 | wps 54929.9 | wpb 2261.4 | bsz 84.4 | num_updates 18072 | best_loss 4.643
2020-10-12 08:08:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:09:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 24 @ 18072 updates, score 4.643) (writing took 4.860943028936163 seconds)
2020-10-12 08:09:00 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-12 08:09:00 | INFO | train | epoch 024 | loss 4.463 | nll_loss 2.926 | ppl 7.6 | wps 24187.3 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 18072 | lr 9.40929e-05 | gnorm 0.84 | clip 0 | train_wall 217 | wall 5711
2020-10-12 08:09:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-12 08:09:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-12 08:09:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6893.03515625Mb; avail=237661.1953125Mb
2020-10-12 08:09:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003700
2020-10-12 08:09:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043981
2020-10-12 08:09:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6893.03515625Mb; avail=237661.1953125Mb
2020-10-12 08:09:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001713
2020-10-12 08:09:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6893.03515625Mb; avail=237661.1953125Mb
2020-10-12 08:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.772679
2020-10-12 08:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.819195
2020-10-12 08:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6897.0Mb; avail=237657.48046875Mb
2020-10-12 08:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6894.5390625Mb; avail=237659.94140625Mb
2020-10-12 08:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033292
2020-10-12 08:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6894.5390625Mb; avail=237659.94140625Mb
2020-10-12 08:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001570
2020-10-12 08:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6894.5390625Mb; avail=237659.94140625Mb
2020-10-12 08:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.769601
2020-10-12 08:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.805286
2020-10-12 08:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6896.453125Mb; avail=237658.046875Mb
2020-10-12 08:09:01 | INFO | fairseq.trainer | begin training epoch 25
2020-10-12 08:09:10 | INFO | train_inner | epoch 025:     28 / 753 loss=4.452, nll_loss=2.914, ppl=7.54, wps=18729.2, ups=2.52, wpb=7440.9, bsz=299.6, num_updates=18100, lr=9.40201e-05, gnorm=0.86, clip=0, train_wall=29, wall=5721
2020-10-12 08:09:40 | INFO | train_inner | epoch 025:    128 / 753 loss=4.412, nll_loss=2.868, ppl=7.3, wps=25060.8, ups=3.31, wpb=7581.4, bsz=284.6, num_updates=18200, lr=9.37614e-05, gnorm=0.827, clip=0, train_wall=29, wall=5751
2020-10-12 08:10:11 | INFO | train_inner | epoch 025:    228 / 753 loss=4.451, nll_loss=2.911, ppl=7.52, wps=25028.3, ups=3.31, wpb=7555.9, bsz=280.7, num_updates=18300, lr=9.35049e-05, gnorm=0.867, clip=0, train_wall=29, wall=5781
2020-10-12 08:10:41 | INFO | train_inner | epoch 025:    328 / 753 loss=4.432, nll_loss=2.889, ppl=7.41, wps=25433.2, ups=3.29, wpb=7726.9, bsz=296, num_updates=18400, lr=9.32505e-05, gnorm=0.836, clip=0, train_wall=29, wall=5812
2020-10-12 08:11:11 | INFO | train_inner | epoch 025:    428 / 753 loss=4.405, nll_loss=2.861, ppl=7.27, wps=24893.1, ups=3.32, wpb=7499.6, bsz=276.2, num_updates=18500, lr=9.29981e-05, gnorm=0.838, clip=0, train_wall=29, wall=5842
2020-10-12 08:11:41 | INFO | train_inner | epoch 025:    528 / 753 loss=4.425, nll_loss=2.883, ppl=7.38, wps=25381.9, ups=3.29, wpb=7721.5, bsz=279.4, num_updates=18600, lr=9.27478e-05, gnorm=0.828, clip=0, train_wall=29, wall=5872
2020-10-12 08:12:12 | INFO | train_inner | epoch 025:    628 / 753 loss=4.46, nll_loss=2.922, ppl=7.58, wps=25239.4, ups=3.32, wpb=7612.9, bsz=276.6, num_updates=18700, lr=9.24995e-05, gnorm=0.852, clip=0, train_wall=29, wall=5902
2020-10-12 08:12:42 | INFO | train_inner | epoch 025:    728 / 753 loss=4.438, nll_loss=2.897, ppl=7.45, wps=25253.1, ups=3.28, wpb=7700.6, bsz=290.9, num_updates=18800, lr=9.22531e-05, gnorm=0.834, clip=0, train_wall=29, wall=5933
2020-10-12 08:12:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6995.578125Mb; avail=237558.3515625Mb
2020-10-12 08:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001373
2020-10-12 08:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6995.578125Mb; avail=237558.3515625Mb
2020-10-12 08:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071266
2020-10-12 08:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6995.578125Mb; avail=237558.3515625Mb
2020-10-12 08:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052027
2020-10-12 08:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125437
2020-10-12 08:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6995.578125Mb; avail=237558.3515625Mb
2020-10-12 08:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6995.578125Mb; avail=237558.3515625Mb
2020-10-12 08:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001156
2020-10-12 08:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6995.578125Mb; avail=237558.3515625Mb
2020-10-12 08:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070938
2020-10-12 08:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6995.578125Mb; avail=237558.3515625Mb
2020-10-12 08:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052763
2020-10-12 08:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125591
2020-10-12 08:12:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6995.578125Mb; avail=237558.3515625Mb
2020-10-12 08:12:52 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 4.63 | nll_loss 3.004 | ppl 8.02 | wps 54810.6 | wpb 2261.4 | bsz 84.4 | num_updates 18825 | best_loss 4.63
2020-10-12 08:12:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:12:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 25 @ 18825 updates, score 4.63) (writing took 4.866815649904311 seconds)
2020-10-12 08:12:57 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-12 08:12:57 | INFO | train | epoch 025 | loss 4.433 | nll_loss 2.892 | ppl 7.42 | wps 24141.8 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 18825 | lr 9.21918e-05 | gnorm 0.841 | clip 0 | train_wall 217 | wall 5948
2020-10-12 08:12:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-12 08:12:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-12 08:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6907.328125Mb; avail=237647.109375Mb
2020-10-12 08:12:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003699
2020-10-12 08:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043894
2020-10-12 08:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6907.5234375Mb; avail=237647.1640625Mb
2020-10-12 08:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001603
2020-10-12 08:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6907.5234375Mb; avail=237647.1640625Mb
2020-10-12 08:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.770833
2020-10-12 08:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.817100
2020-10-12 08:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6913.68359375Mb; avail=237641.00390625Mb
2020-10-12 08:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6918.44921875Mb; avail=237636.73046875Mb
2020-10-12 08:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033476
2020-10-12 08:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6927.3125Mb; avail=237627.375Mb
2020-10-12 08:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001591
2020-10-12 08:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6927.3125Mb; avail=237627.375Mb
2020-10-12 08:12:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.769687
2020-10-12 08:12:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.805598
2020-10-12 08:12:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6911.15234375Mb; avail=237643.3515625Mb
2020-10-12 08:12:59 | INFO | fairseq.trainer | begin training epoch 26
2020-10-12 08:13:22 | INFO | train_inner | epoch 026:     75 / 753 loss=4.404, nll_loss=2.859, ppl=7.26, wps=18948.8, ups=2.52, wpb=7526.8, bsz=270.2, num_updates=18900, lr=9.20087e-05, gnorm=0.839, clip=0, train_wall=29, wall=5973
2020-10-12 08:13:52 | INFO | train_inner | epoch 026:    175 / 753 loss=4.421, nll_loss=2.878, ppl=7.35, wps=25019.6, ups=3.29, wpb=7599.6, bsz=265.5, num_updates=19000, lr=9.17663e-05, gnorm=0.843, clip=0, train_wall=29, wall=6003
2020-10-12 08:14:22 | INFO | train_inner | epoch 026:    275 / 753 loss=4.416, nll_loss=2.871, ppl=7.32, wps=25030.9, ups=3.31, wpb=7563.3, bsz=285.7, num_updates=19100, lr=9.15258e-05, gnorm=0.851, clip=0, train_wall=29, wall=6033
2020-10-12 08:14:52 | INFO | train_inner | epoch 026:    375 / 753 loss=4.346, nll_loss=2.794, ppl=6.94, wps=25440.6, ups=3.33, wpb=7635.3, bsz=289.9, num_updates=19200, lr=9.12871e-05, gnorm=0.829, clip=0, train_wall=29, wall=6063
2020-10-12 08:15:23 | INFO | train_inner | epoch 026:    475 / 753 loss=4.423, nll_loss=2.881, ppl=7.36, wps=25488.2, ups=3.31, wpb=7694.8, bsz=272.9, num_updates=19300, lr=9.10503e-05, gnorm=0.843, clip=0, train_wall=29, wall=6094
2020-10-12 08:15:53 | INFO | train_inner | epoch 026:    575 / 753 loss=4.424, nll_loss=2.881, ppl=7.37, wps=24890.6, ups=3.31, wpb=7515.5, bsz=289.8, num_updates=19400, lr=9.08153e-05, gnorm=0.872, clip=0, train_wall=29, wall=6124
2020-10-12 08:16:23 | INFO | train_inner | epoch 026:    675 / 753 loss=4.404, nll_loss=2.859, ppl=7.26, wps=25456.5, ups=3.3, wpb=7716.8, bsz=292.8, num_updates=19500, lr=9.05822e-05, gnorm=0.84, clip=0, train_wall=29, wall=6154
2020-10-12 08:16:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7015.33984375Mb; avail=237538.58984375Mb
2020-10-12 08:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001340
2020-10-12 08:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7015.33984375Mb; avail=237538.58984375Mb
2020-10-12 08:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075780
2020-10-12 08:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7015.33984375Mb; avail=237538.58984375Mb
2020-10-12 08:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051935
2020-10-12 08:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130039
2020-10-12 08:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7015.83203125Mb; avail=237538.09765625Mb
2020-10-12 08:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7015.83203125Mb; avail=237538.09765625Mb
2020-10-12 08:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001148
2020-10-12 08:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7015.83203125Mb; avail=237538.09765625Mb
2020-10-12 08:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070138
2020-10-12 08:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7019.46484375Mb; avail=237534.46484375Mb
2020-10-12 08:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056932
2020-10-12 08:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128970
2020-10-12 08:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7025.8984375Mb; avail=237528.03125Mb
2020-10-12 08:16:50 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 4.627 | nll_loss 3 | ppl 8 | wps 54946.6 | wpb 2261.4 | bsz 84.4 | num_updates 19578 | best_loss 4.627
2020-10-12 08:16:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:16:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 26 @ 19578 updates, score 4.627) (writing took 4.860854527913034 seconds)
2020-10-12 08:16:55 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-12 08:16:55 | INFO | train | epoch 026 | loss 4.405 | nll_loss 2.86 | ppl 7.26 | wps 24177.4 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 19578 | lr 9.04015e-05 | gnorm 0.844 | clip 0 | train_wall 217 | wall 6186
2020-10-12 08:16:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-12 08:16:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-12 08:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6922.46484375Mb; avail=237631.7890625Mb
2020-10-12 08:16:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003726
2020-10-12 08:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044284
2020-10-12 08:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6921.97265625Mb; avail=237632.28125Mb
2020-10-12 08:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001607
2020-10-12 08:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6921.97265625Mb; avail=237632.28125Mb
2020-10-12 08:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.778489
2020-10-12 08:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.825206
2020-10-12 08:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6927.1015625Mb; avail=237627.38671875Mb
2020-10-12 08:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6922.29296875Mb; avail=237632.1953125Mb
2020-10-12 08:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033157
2020-10-12 08:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6924.109375Mb; avail=237630.37890625Mb
2020-10-12 08:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001568
2020-10-12 08:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6924.109375Mb; avail=237630.37890625Mb
2020-10-12 08:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.776215
2020-10-12 08:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.811752
2020-10-12 08:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6927.6640625Mb; avail=237626.82421875Mb
2020-10-12 08:16:56 | INFO | fairseq.trainer | begin training epoch 27
2020-10-12 08:17:03 | INFO | train_inner | epoch 027:     22 / 753 loss=4.43, nll_loss=2.888, ppl=7.4, wps=19043.5, ups=2.5, wpb=7614, bsz=273, num_updates=19600, lr=9.03508e-05, gnorm=0.842, clip=0, train_wall=29, wall=6194
2020-10-12 08:17:33 | INFO | train_inner | epoch 027:    122 / 753 loss=4.381, nll_loss=2.832, ppl=7.12, wps=25024.2, ups=3.33, wpb=7522, bsz=268.6, num_updates=19700, lr=9.01212e-05, gnorm=0.866, clip=0, train_wall=29, wall=6224
2020-10-12 08:18:04 | INFO | train_inner | epoch 027:    222 / 753 loss=4.365, nll_loss=2.815, ppl=7.04, wps=25325.7, ups=3.3, wpb=7684, bsz=292, num_updates=19800, lr=8.98933e-05, gnorm=0.825, clip=0, train_wall=29, wall=6254
2020-10-12 08:18:34 | INFO | train_inner | epoch 027:    322 / 753 loss=4.401, nll_loss=2.855, ppl=7.23, wps=25392.4, ups=3.29, wpb=7711.8, bsz=275.9, num_updates=19900, lr=8.96672e-05, gnorm=0.848, clip=0, train_wall=29, wall=6285
2020-10-12 08:19:04 | INFO | train_inner | epoch 027:    422 / 753 loss=4.378, nll_loss=2.828, ppl=7.1, wps=25386.5, ups=3.29, wpb=7707.3, bsz=280.8, num_updates=20000, lr=8.94427e-05, gnorm=0.849, clip=0, train_wall=29, wall=6315
2020-10-12 08:19:34 | INFO | train_inner | epoch 027:    522 / 753 loss=4.386, nll_loss=2.838, ppl=7.15, wps=24493.5, ups=3.31, wpb=7395.7, bsz=281.7, num_updates=20100, lr=8.92199e-05, gnorm=0.856, clip=0, train_wall=29, wall=6345
2020-10-12 08:20:05 | INFO | train_inner | epoch 027:    622 / 753 loss=4.374, nll_loss=2.824, ppl=7.08, wps=25384.2, ups=3.29, wpb=7706.8, bsz=302.1, num_updates=20200, lr=8.89988e-05, gnorm=0.828, clip=0, train_wall=29, wall=6376
2020-10-12 08:20:35 | INFO | train_inner | epoch 027:    722 / 753 loss=4.35, nll_loss=2.799, ppl=6.96, wps=25413.9, ups=3.33, wpb=7626.7, bsz=283.8, num_updates=20300, lr=8.87794e-05, gnorm=0.823, clip=0, train_wall=29, wall=6406
2020-10-12 08:20:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7029.640625Mb; avail=237524.37890625Mb
2020-10-12 08:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001354
2020-10-12 08:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7029.640625Mb; avail=237524.37890625Mb
2020-10-12 08:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068740
2020-10-12 08:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7032.078125Mb; avail=237521.94140625Mb
2020-10-12 08:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052425
2020-10-12 08:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123292
2020-10-12 08:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7035.734375Mb; avail=237518.28515625Mb
2020-10-12 08:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7038.76171875Mb; avail=237515.2578125Mb
2020-10-12 08:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001216
2020-10-12 08:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7038.76171875Mb; avail=237515.2578125Mb
2020-10-12 08:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070458
2020-10-12 08:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7029.95703125Mb; avail=237524.0625Mb
2020-10-12 08:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052179
2020-10-12 08:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124589
2020-10-12 08:20:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7034.94140625Mb; avail=237519.34765625Mb
2020-10-12 08:20:47 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 4.605 | nll_loss 2.979 | ppl 7.88 | wps 54733.3 | wpb 2261.4 | bsz 84.4 | num_updates 20331 | best_loss 4.605
2020-10-12 08:20:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:20:52 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 27 @ 20331 updates, score 4.605) (writing took 4.8648309810087085 seconds)
2020-10-12 08:20:52 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-12 08:20:52 | INFO | train | epoch 027 | loss 4.379 | nll_loss 2.83 | ppl 7.11 | wps 24182.3 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 20331 | lr 8.87116e-05 | gnorm 0.844 | clip 0 | train_wall 217 | wall 6423
2020-10-12 08:20:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-12 08:20:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-12 08:20:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6935.09375Mb; avail=237619.1875Mb
2020-10-12 08:20:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003647
2020-10-12 08:20:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044067
2020-10-12 08:20:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6957.49609375Mb; avail=237596.78515625Mb
2020-10-12 08:20:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001691
2020-10-12 08:20:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6959.91796875Mb; avail=237594.36328125Mb
2020-10-12 08:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.783910
2020-10-12 08:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.830609
2020-10-12 08:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7013.5234375Mb; avail=237540.96484375Mb
2020-10-12 08:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7009.09375Mb; avail=237545.39453125Mb
2020-10-12 08:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033309
2020-10-12 08:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7009.09375Mb; avail=237545.39453125Mb
2020-10-12 08:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001574
2020-10-12 08:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7009.09375Mb; avail=237545.39453125Mb
2020-10-12 08:20:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.769920
2020-10-12 08:20:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.805613
2020-10-12 08:20:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7014.53515625Mb; avail=237539.95703125Mb
2020-10-12 08:20:54 | INFO | fairseq.trainer | begin training epoch 28
2020-10-12 08:21:14 | INFO | train_inner | epoch 028:     69 / 753 loss=4.359, nll_loss=2.807, ppl=7, wps=18807.4, ups=2.53, wpb=7427.2, bsz=293.4, num_updates=20400, lr=8.85615e-05, gnorm=0.872, clip=0, train_wall=28, wall=6445
2020-10-12 08:21:45 | INFO | train_inner | epoch 028:    169 / 753 loss=4.351, nll_loss=2.799, ppl=6.96, wps=25094.5, ups=3.31, wpb=7590.8, bsz=276.9, num_updates=20500, lr=8.83452e-05, gnorm=0.843, clip=0, train_wall=29, wall=6475
2020-10-12 08:22:15 | INFO | train_inner | epoch 028:    269 / 753 loss=4.355, nll_loss=2.802, ppl=6.97, wps=25246.5, ups=3.31, wpb=7626.2, bsz=265.5, num_updates=20600, lr=8.81305e-05, gnorm=0.844, clip=0, train_wall=29, wall=6506
2020-10-12 08:22:45 | INFO | train_inner | epoch 028:    369 / 753 loss=4.372, nll_loss=2.822, ppl=7.07, wps=25299.6, ups=3.31, wpb=7652.6, bsz=274.1, num_updates=20700, lr=8.79174e-05, gnorm=0.839, clip=0, train_wall=29, wall=6536
2020-10-12 08:23:16 | INFO | train_inner | epoch 028:    469 / 753 loss=4.337, nll_loss=2.783, ppl=6.88, wps=25470.8, ups=3.26, wpb=7805.5, bsz=299.7, num_updates=20800, lr=8.77058e-05, gnorm=0.844, clip=0, train_wall=29, wall=6567
2020-10-12 08:23:46 | INFO | train_inner | epoch 028:    569 / 753 loss=4.352, nll_loss=2.8, ppl=6.97, wps=25363.4, ups=3.27, wpb=7753.9, bsz=304.9, num_updates=20900, lr=8.74957e-05, gnorm=0.849, clip=0, train_wall=29, wall=6597
2020-10-12 08:24:16 | INFO | train_inner | epoch 028:    669 / 753 loss=4.37, nll_loss=2.821, ppl=7.06, wps=24979.4, ups=3.33, wpb=7490.6, bsz=267, num_updates=21000, lr=8.72872e-05, gnorm=0.847, clip=0, train_wall=29, wall=6627
2020-10-12 08:24:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:24:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7127.203125Mb; avail=237426.99609375Mb
2020-10-12 08:24:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001357
2020-10-12 08:24:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7127.203125Mb; avail=237426.99609375Mb
2020-10-12 08:24:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072088
2020-10-12 08:24:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7127.203125Mb; avail=237426.99609375Mb
2020-10-12 08:24:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051982
2020-10-12 08:24:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126215
2020-10-12 08:24:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7127.203125Mb; avail=237426.99609375Mb
2020-10-12 08:24:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7127.203125Mb; avail=237426.99609375Mb
2020-10-12 08:24:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001181
2020-10-12 08:24:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7127.203125Mb; avail=237426.99609375Mb
2020-10-12 08:24:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070312
2020-10-12 08:24:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7127.203125Mb; avail=237426.99609375Mb
2020-10-12 08:24:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051678
2020-10-12 08:24:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123917
2020-10-12 08:24:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7127.203125Mb; avail=237426.99609375Mb
2020-10-12 08:24:44 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 4.588 | nll_loss 2.958 | ppl 7.77 | wps 54754.8 | wpb 2261.4 | bsz 84.4 | num_updates 21084 | best_loss 4.588
2020-10-12 08:24:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:24:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 28 @ 21084 updates, score 4.588) (writing took 4.856518446933478 seconds)
2020-10-12 08:24:49 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-12 08:24:49 | INFO | train | epoch 028 | loss 4.355 | nll_loss 2.803 | ppl 6.98 | wps 24163.1 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 21084 | lr 8.71131e-05 | gnorm 0.847 | clip 0 | train_wall 217 | wall 6660
2020-10-12 08:24:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-12 08:24:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-12 08:24:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7044.37890625Mb; avail=237509.890625Mb
2020-10-12 08:24:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003479
2020-10-12 08:24:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043681
2020-10-12 08:24:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7045.21484375Mb; avail=237509.3046875Mb
2020-10-12 08:24:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001546
2020-10-12 08:24:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7045.7109375Mb; avail=237508.80859375Mb
2020-10-12 08:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.779941
2020-10-12 08:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.825987
2020-10-12 08:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7032.046875Mb; avail=237522.47265625Mb
2020-10-12 08:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7027.73046875Mb; avail=237526.7890625Mb
2020-10-12 08:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033078
2020-10-12 08:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7027.73046875Mb; avail=237526.7890625Mb
2020-10-12 08:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001574
2020-10-12 08:24:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7027.73046875Mb; avail=237526.7890625Mb
2020-10-12 08:24:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.775459
2020-10-12 08:24:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.810923
2020-10-12 08:24:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7033.015625Mb; avail=237521.50390625Mb
2020-10-12 08:24:51 | INFO | fairseq.trainer | begin training epoch 29
2020-10-12 08:24:56 | INFO | train_inner | epoch 029:     16 / 753 loss=4.349, nll_loss=2.796, ppl=6.95, wps=19051.5, ups=2.49, wpb=7655.6, bsz=295.3, num_updates=21100, lr=8.70801e-05, gnorm=0.843, clip=0, train_wall=29, wall=6667
2020-10-12 08:25:27 | INFO | train_inner | epoch 029:    116 / 753 loss=4.326, nll_loss=2.769, ppl=6.82, wps=25264.2, ups=3.31, wpb=7624.6, bsz=279.4, num_updates=21200, lr=8.68744e-05, gnorm=0.834, clip=0, train_wall=29, wall=6697
2020-10-12 08:25:57 | INFO | train_inner | epoch 029:    216 / 753 loss=4.322, nll_loss=2.765, ppl=6.8, wps=24829.4, ups=3.33, wpb=7459.8, bsz=272.4, num_updates=21300, lr=8.66703e-05, gnorm=0.854, clip=0, train_wall=29, wall=6728
2020-10-12 08:26:27 | INFO | train_inner | epoch 029:    316 / 753 loss=4.305, nll_loss=2.747, ppl=6.71, wps=25312.4, ups=3.31, wpb=7654.3, bsz=281, num_updates=21400, lr=8.64675e-05, gnorm=0.827, clip=0, train_wall=29, wall=6758
2020-10-12 08:26:57 | INFO | train_inner | epoch 029:    416 / 753 loss=4.31, nll_loss=2.752, ppl=6.74, wps=25310.2, ups=3.28, wpb=7720.5, bsz=313.3, num_updates=21500, lr=8.62662e-05, gnorm=0.843, clip=0, train_wall=29, wall=6788
2020-10-12 08:27:27 | INFO | train_inner | epoch 029:    516 / 753 loss=4.335, nll_loss=2.78, ppl=6.87, wps=25220.6, ups=3.33, wpb=7567, bsz=277, num_updates=21600, lr=8.60663e-05, gnorm=0.863, clip=0, train_wall=29, wall=6818
2020-10-12 08:27:58 | INFO | train_inner | epoch 029:    616 / 753 loss=4.351, nll_loss=2.797, ppl=6.95, wps=25097.9, ups=3.32, wpb=7567.3, bsz=276.8, num_updates=21700, lr=8.58678e-05, gnorm=0.857, clip=0, train_wall=29, wall=6848
2020-10-12 08:28:28 | INFO | train_inner | epoch 029:    716 / 753 loss=4.358, nll_loss=2.805, ppl=6.99, wps=25424.1, ups=3.29, wpb=7725.5, bsz=275.6, num_updates=21800, lr=8.56706e-05, gnorm=0.845, clip=0, train_wall=29, wall=6879
2020-10-12 08:28:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7114.5390625Mb; avail=237439.515625Mb
2020-10-12 08:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001262
2020-10-12 08:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.5390625Mb; avail=237439.515625Mb
2020-10-12 08:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069680
2020-10-12 08:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.30078125Mb; avail=237439.75390625Mb
2020-10-12 08:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052138
2020-10-12 08:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123868
2020-10-12 08:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.02734375Mb; avail=237440.02734375Mb
2020-10-12 08:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7114.02734375Mb; avail=237440.02734375Mb
2020-10-12 08:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001088
2020-10-12 08:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.02734375Mb; avail=237440.02734375Mb
2020-10-12 08:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071652
2020-10-12 08:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.203125Mb; avail=237440.8515625Mb
2020-10-12 08:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052013
2020-10-12 08:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125525
2020-10-12 08:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.33203125Mb; avail=237440.72265625Mb
2020-10-12 08:28:42 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 4.583 | nll_loss 2.956 | ppl 7.76 | wps 54768.8 | wpb 2261.4 | bsz 84.4 | num_updates 21837 | best_loss 4.583
2020-10-12 08:28:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:28:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 29 @ 21837 updates, score 4.583) (writing took 4.86989886010997 seconds)
2020-10-12 08:28:47 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-12 08:28:47 | INFO | train | epoch 029 | loss 4.329 | nll_loss 2.774 | ppl 6.84 | wps 24166.6 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 21837 | lr 8.5598e-05 | gnorm 0.847 | clip 0 | train_wall 217 | wall 6898
2020-10-12 08:28:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-12 08:28:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-12 08:28:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7022.9609375Mb; avail=237531.296875Mb
2020-10-12 08:28:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003671
2020-10-12 08:28:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043717
2020-10-12 08:28:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7013.609375Mb; avail=237540.6484375Mb
2020-10-12 08:28:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001692
2020-10-12 08:28:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7013.609375Mb; avail=237540.6484375Mb
2020-10-12 08:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.775045
2020-10-12 08:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.821245
2020-10-12 08:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7012.66796875Mb; avail=237541.82421875Mb
2020-10-12 08:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7012.66796875Mb; avail=237541.82421875Mb
2020-10-12 08:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033033
2020-10-12 08:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7012.66796875Mb; avail=237541.82421875Mb
2020-10-12 08:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001608
2020-10-12 08:28:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7012.66796875Mb; avail=237541.82421875Mb
2020-10-12 08:28:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.770482
2020-10-12 08:28:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.805921
2020-10-12 08:28:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7012.5859375Mb; avail=237541.8828125Mb
2020-10-12 08:28:49 | INFO | fairseq.trainer | begin training epoch 30
2020-10-12 08:29:08 | INFO | train_inner | epoch 030:     63 / 753 loss=4.335, nll_loss=2.78, ppl=6.87, wps=19235.9, ups=2.52, wpb=7640, bsz=255.2, num_updates=21900, lr=8.54748e-05, gnorm=0.846, clip=0, train_wall=29, wall=6919
2020-10-12 08:29:38 | INFO | train_inner | epoch 030:    163 / 753 loss=4.278, nll_loss=2.716, ppl=6.57, wps=24869.8, ups=3.33, wpb=7477.8, bsz=277.4, num_updates=22000, lr=8.52803e-05, gnorm=0.847, clip=0, train_wall=29, wall=6949
2020-10-12 08:30:08 | INFO | train_inner | epoch 030:    263 / 753 loss=4.314, nll_loss=2.756, ppl=6.75, wps=25106.2, ups=3.27, wpb=7670.2, bsz=310.6, num_updates=22100, lr=8.50871e-05, gnorm=0.844, clip=0, train_wall=29, wall=6979
2020-10-12 08:30:38 | INFO | train_inner | epoch 030:    363 / 753 loss=4.288, nll_loss=2.726, ppl=6.62, wps=25191.2, ups=3.31, wpb=7607.2, bsz=287, num_updates=22200, lr=8.48953e-05, gnorm=0.832, clip=0, train_wall=29, wall=7009
2020-10-12 08:31:09 | INFO | train_inner | epoch 030:    463 / 753 loss=4.336, nll_loss=2.78, ppl=6.87, wps=25220.4, ups=3.3, wpb=7647.8, bsz=265.6, num_updates=22300, lr=8.47047e-05, gnorm=0.855, clip=0, train_wall=29, wall=7040
2020-10-12 08:31:39 | INFO | train_inner | epoch 030:    563 / 753 loss=4.283, nll_loss=2.723, ppl=6.6, wps=25335.7, ups=3.3, wpb=7669.1, bsz=298.8, num_updates=22400, lr=8.45154e-05, gnorm=0.845, clip=0, train_wall=29, wall=7070
2020-10-12 08:32:09 | INFO | train_inner | epoch 030:    663 / 753 loss=4.327, nll_loss=2.77, ppl=6.82, wps=25014.5, ups=3.31, wpb=7563.5, bsz=274.8, num_updates=22500, lr=8.43274e-05, gnorm=0.851, clip=0, train_wall=29, wall=7100
2020-10-12 08:32:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:32:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7104.76171875Mb; avail=237449.49609375Mb
2020-10-12 08:32:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001401
2020-10-12 08:32:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7105.3671875Mb; avail=237448.890625Mb
2020-10-12 08:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070358
2020-10-12 08:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7111.30859375Mb; avail=237442.94921875Mb
2020-10-12 08:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052218
2020-10-12 08:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124810
2020-10-12 08:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.44921875Mb; avail=237451.80859375Mb
2020-10-12 08:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7102.44921875Mb; avail=237451.80859375Mb
2020-10-12 08:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001148
2020-10-12 08:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.44921875Mb; avail=237451.80859375Mb
2020-10-12 08:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071115
2020-10-12 08:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.44921875Mb; avail=237451.80859375Mb
2020-10-12 08:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052361
2020-10-12 08:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125385
2020-10-12 08:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.44921875Mb; avail=237451.80859375Mb
2020-10-12 08:32:39 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 4.58 | nll_loss 2.95 | ppl 7.73 | wps 54725.3 | wpb 2261.4 | bsz 84.4 | num_updates 22590 | best_loss 4.58
2020-10-12 08:32:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:32:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 30 @ 22590 updates, score 4.58) (writing took 4.866860147099942 seconds)
2020-10-12 08:32:44 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-12 08:32:44 | INFO | train | epoch 030 | loss 4.307 | nll_loss 2.748 | ppl 6.72 | wps 24160.7 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 22590 | lr 8.41593e-05 | gnorm 0.847 | clip 0 | train_wall 217 | wall 7135
2020-10-12 08:32:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-12 08:32:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-12 08:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7026.81640625Mb; avail=237527.4296875Mb
2020-10-12 08:32:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003687
2020-10-12 08:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043661
2020-10-12 08:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7028.515625Mb; avail=237525.61328125Mb
2020-10-12 08:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001720
2020-10-12 08:32:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7028.515625Mb; avail=237525.61328125Mb
2020-10-12 08:32:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.786071
2020-10-12 08:32:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.832269
2020-10-12 08:32:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7025.359375Mb; avail=237529.15234375Mb
2020-10-12 08:32:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7030.78515625Mb; avail=237523.7265625Mb
2020-10-12 08:32:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033456
2020-10-12 08:32:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7035.5234375Mb; avail=237518.98828125Mb
2020-10-12 08:32:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001574
2020-10-12 08:32:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7035.5234375Mb; avail=237518.98828125Mb
2020-10-12 08:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.771682
2020-10-12 08:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.807517
2020-10-12 08:32:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7025.859375Mb; avail=237528.64453125Mb
2020-10-12 08:32:46 | INFO | fairseq.trainer | begin training epoch 31
2020-10-12 08:32:49 | INFO | train_inner | epoch 031:     10 / 753 loss=4.299, nll_loss=2.74, ppl=6.68, wps=19228.7, ups=2.51, wpb=7656.3, bsz=286.6, num_updates=22600, lr=8.41406e-05, gnorm=0.863, clip=0, train_wall=29, wall=7140
2020-10-12 08:33:19 | INFO | train_inner | epoch 031:    110 / 753 loss=4.275, nll_loss=2.712, ppl=6.55, wps=25104.4, ups=3.33, wpb=7529.5, bsz=278.6, num_updates=22700, lr=8.39551e-05, gnorm=0.869, clip=0, train_wall=29, wall=7170
2020-10-12 08:33:49 | INFO | train_inner | epoch 031:    210 / 753 loss=4.285, nll_loss=2.722, ppl=6.6, wps=25320.7, ups=3.31, wpb=7654.2, bsz=268.1, num_updates=22800, lr=8.37708e-05, gnorm=0.845, clip=0, train_wall=29, wall=7200
2020-10-12 08:34:20 | INFO | train_inner | epoch 031:    310 / 753 loss=4.275, nll_loss=2.712, ppl=6.55, wps=25177.7, ups=3.31, wpb=7607.2, bsz=274.2, num_updates=22900, lr=8.35877e-05, gnorm=0.844, clip=0, train_wall=29, wall=7230
2020-10-12 08:34:50 | INFO | train_inner | epoch 031:    410 / 753 loss=4.29, nll_loss=2.729, ppl=6.63, wps=25725.3, ups=3.28, wpb=7835.8, bsz=282.8, num_updates=23000, lr=8.34058e-05, gnorm=0.843, clip=0, train_wall=29, wall=7261
2020-10-12 08:35:20 | INFO | train_inner | epoch 031:    510 / 753 loss=4.299, nll_loss=2.739, ppl=6.68, wps=24893.4, ups=3.32, wpb=7506.6, bsz=278.3, num_updates=23100, lr=8.3225e-05, gnorm=0.857, clip=0, train_wall=29, wall=7291
2020-10-12 08:35:50 | INFO | train_inner | epoch 031:    610 / 753 loss=4.318, nll_loss=2.76, ppl=6.77, wps=24621, ups=3.33, wpb=7390.2, bsz=265.1, num_updates=23200, lr=8.30455e-05, gnorm=0.891, clip=0, train_wall=29, wall=7321
2020-10-12 08:36:21 | INFO | train_inner | epoch 031:    710 / 753 loss=4.284, nll_loss=2.722, ppl=6.6, wps=24947.9, ups=3.25, wpb=7668.1, bsz=315.2, num_updates=23300, lr=8.28671e-05, gnorm=0.871, clip=0, train_wall=29, wall=7352
2020-10-12 08:36:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6990.65234375Mb; avail=237563.38671875Mb
2020-10-12 08:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001366
2020-10-12 08:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6990.65234375Mb; avail=237563.38671875Mb
2020-10-12 08:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069835
2020-10-12 08:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6990.65234375Mb; avail=237563.38671875Mb
2020-10-12 08:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052823
2020-10-12 08:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124784
2020-10-12 08:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6990.65234375Mb; avail=237563.38671875Mb
2020-10-12 08:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6990.65234375Mb; avail=237563.38671875Mb
2020-10-12 08:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001148
2020-10-12 08:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6990.65234375Mb; avail=237563.38671875Mb
2020-10-12 08:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070288
2020-10-12 08:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6990.64453125Mb; avail=237563.39453125Mb
2020-10-12 08:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052577
2020-10-12 08:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124757
2020-10-12 08:36:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6990.16015625Mb; avail=237563.87890625Mb
2020-10-12 08:36:37 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 4.558 | nll_loss 2.929 | ppl 7.62 | wps 54297.3 | wpb 2261.4 | bsz 84.4 | num_updates 23343 | best_loss 4.558
2020-10-12 08:36:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:36:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 31 @ 23343 updates, score 4.558) (writing took 4.854883267078549 seconds)
2020-10-12 08:36:42 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-12 08:36:42 | INFO | train | epoch 031 | loss 4.287 | nll_loss 2.725 | ppl 6.61 | wps 24149.4 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 23343 | lr 8.27907e-05 | gnorm 0.856 | clip 0 | train_wall 217 | wall 7373
2020-10-12 08:36:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-12 08:36:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-12 08:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6907.54296875Mb; avail=237646.1171875Mb
2020-10-12 08:36:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003674
2020-10-12 08:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043673
2020-10-12 08:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6908.1484375Mb; avail=237645.51171875Mb
2020-10-12 08:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001692
2020-10-12 08:36:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6908.1484375Mb; avail=237645.51171875Mb
2020-10-12 08:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.775421
2020-10-12 08:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.821548
2020-10-12 08:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6908.8359375Mb; avail=237644.82421875Mb
2020-10-12 08:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6908.94921875Mb; avail=237644.7109375Mb
2020-10-12 08:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032996
2020-10-12 08:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6908.94921875Mb; avail=237644.7109375Mb
2020-10-12 08:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001616
2020-10-12 08:36:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6908.94921875Mb; avail=237644.7109375Mb
2020-10-12 08:36:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.771221
2020-10-12 08:36:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.806651
2020-10-12 08:36:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6909.453125Mb; avail=237645.05859375Mb
2020-10-12 08:36:44 | INFO | fairseq.trainer | begin training epoch 32
2020-10-12 08:37:01 | INFO | train_inner | epoch 032:     57 / 753 loss=4.261, nll_loss=2.696, ppl=6.48, wps=19465.7, ups=2.5, wpb=7787.2, bsz=292.6, num_updates=23400, lr=8.26898e-05, gnorm=0.815, clip=0, train_wall=29, wall=7392
2020-10-12 08:37:31 | INFO | train_inner | epoch 032:    157 / 753 loss=4.23, nll_loss=2.66, ppl=6.32, wps=25264.7, ups=3.3, wpb=7662.2, bsz=296.4, num_updates=23500, lr=8.25137e-05, gnorm=0.834, clip=0, train_wall=29, wall=7422
2020-10-12 08:38:01 | INFO | train_inner | epoch 032:    257 / 753 loss=4.25, nll_loss=2.684, ppl=6.43, wps=25084.6, ups=3.33, wpb=7536.7, bsz=291.4, num_updates=23600, lr=8.23387e-05, gnorm=0.865, clip=0, train_wall=29, wall=7452
2020-10-12 08:38:31 | INFO | train_inner | epoch 032:    357 / 753 loss=4.282, nll_loss=2.719, ppl=6.58, wps=25497.5, ups=3.32, wpb=7690.2, bsz=273.4, num_updates=23700, lr=8.21648e-05, gnorm=0.853, clip=0, train_wall=29, wall=7482
2020-10-12 08:39:02 | INFO | train_inner | epoch 032:    457 / 753 loss=4.263, nll_loss=2.697, ppl=6.48, wps=24886.7, ups=3.3, wpb=7535.8, bsz=285.5, num_updates=23800, lr=8.1992e-05, gnorm=0.868, clip=0, train_wall=29, wall=7513
2020-10-12 08:39:32 | INFO | train_inner | epoch 032:    557 / 753 loss=4.285, nll_loss=2.723, ppl=6.6, wps=24968.1, ups=3.3, wpb=7555.8, bsz=259, num_updates=23900, lr=8.18203e-05, gnorm=0.86, clip=0, train_wall=29, wall=7543
2020-10-12 08:40:02 | INFO | train_inner | epoch 032:    657 / 753 loss=4.274, nll_loss=2.712, ppl=6.55, wps=25132.3, ups=3.3, wpb=7619.7, bsz=283.8, num_updates=24000, lr=8.16497e-05, gnorm=0.859, clip=0, train_wall=29, wall=7573
2020-10-12 08:40:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:40:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7000.828125Mb; avail=237553.625Mb
2020-10-12 08:40:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001367
2020-10-12 08:40:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7000.828125Mb; avail=237553.625Mb
2020-10-12 08:40:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070737
2020-10-12 08:40:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7007.86328125Mb; avail=237546.58984375Mb
2020-10-12 08:40:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052490
2020-10-12 08:40:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125393
2020-10-12 08:40:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6998.01953125Mb; avail=237556.43359375Mb
2020-10-12 08:40:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6998.01953125Mb; avail=237556.43359375Mb
2020-10-12 08:40:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001206
2020-10-12 08:40:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6998.01953125Mb; avail=237556.43359375Mb
2020-10-12 08:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071529
2020-10-12 08:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7000.43359375Mb; avail=237554.01953125Mb
2020-10-12 08:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052440
2020-10-12 08:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125927
2020-10-12 08:40:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7007.578125Mb; avail=237546.875Mb
2020-10-12 08:40:34 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 4.556 | nll_loss 2.925 | ppl 7.59 | wps 54799.7 | wpb 2261.4 | bsz 84.4 | num_updates 24096 | best_loss 4.556
2020-10-12 08:40:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:40:39 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 32 @ 24096 updates, score 4.556) (writing took 4.87391108321026 seconds)
2020-10-12 08:40:39 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-12 08:40:39 | INFO | train | epoch 032 | loss 4.265 | nll_loss 2.7 | ppl 6.5 | wps 24180.1 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 24096 | lr 8.14868e-05 | gnorm 0.852 | clip 0 | train_wall 217 | wall 7610
2020-10-12 08:40:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-12 08:40:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-12 08:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6904.54296875Mb; avail=237649.72265625Mb
2020-10-12 08:40:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003673
2020-10-12 08:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043534
2020-10-12 08:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6906.95703125Mb; avail=237647.30859375Mb
2020-10-12 08:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001583
2020-10-12 08:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6906.95703125Mb; avail=237647.30859375Mb
2020-10-12 08:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.774389
2020-10-12 08:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.820327
2020-10-12 08:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6910.87890625Mb; avail=237643.60546875Mb
2020-10-12 08:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6906.0703125Mb; avail=237648.4140625Mb
2020-10-12 08:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033685
2020-10-12 08:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6906.0703125Mb; avail=237648.4140625Mb
2020-10-12 08:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001556
2020-10-12 08:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6906.0703125Mb; avail=237648.4140625Mb
2020-10-12 08:40:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.775203
2020-10-12 08:40:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.811267
2020-10-12 08:40:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6910.59765625Mb; avail=237643.8828125Mb
2020-10-12 08:40:41 | INFO | fairseq.trainer | begin training epoch 33
2020-10-12 08:40:42 | INFO | train_inner | epoch 033:      4 / 753 loss=4.266, nll_loss=2.702, ppl=6.51, wps=19403.5, ups=2.51, wpb=7729.1, bsz=298.2, num_updates=24100, lr=8.14801e-05, gnorm=0.842, clip=0, train_wall=29, wall=7613
2020-10-12 08:41:12 | INFO | train_inner | epoch 033:    104 / 753 loss=4.222, nll_loss=2.652, ppl=6.28, wps=25098.5, ups=3.32, wpb=7570, bsz=295.8, num_updates=24200, lr=8.13116e-05, gnorm=0.86, clip=0, train_wall=29, wall=7643
2020-10-12 08:41:43 | INFO | train_inner | epoch 033:    204 / 753 loss=4.239, nll_loss=2.671, ppl=6.37, wps=25085, ups=3.31, wpb=7574.5, bsz=273.2, num_updates=24300, lr=8.11441e-05, gnorm=0.854, clip=0, train_wall=29, wall=7673
2020-10-12 08:42:13 | INFO | train_inner | epoch 033:    304 / 753 loss=4.249, nll_loss=2.682, ppl=6.42, wps=25349.9, ups=3.28, wpb=7735.9, bsz=285.4, num_updates=24400, lr=8.09776e-05, gnorm=0.846, clip=0, train_wall=29, wall=7704
2020-10-12 08:42:43 | INFO | train_inner | epoch 033:    404 / 753 loss=4.242, nll_loss=2.674, ppl=6.38, wps=25242.1, ups=3.32, wpb=7603.3, bsz=297, num_updates=24500, lr=8.08122e-05, gnorm=0.86, clip=0, train_wall=29, wall=7734
2020-10-12 08:43:13 | INFO | train_inner | epoch 033:    504 / 753 loss=4.243, nll_loss=2.675, ppl=6.39, wps=25143.9, ups=3.34, wpb=7525.3, bsz=271.1, num_updates=24600, lr=8.06478e-05, gnorm=0.863, clip=0, train_wall=29, wall=7764
2020-10-12 08:43:43 | INFO | train_inner | epoch 033:    604 / 753 loss=4.233, nll_loss=2.664, ppl=6.34, wps=25052.2, ups=3.3, wpb=7595.3, bsz=300, num_updates=24700, lr=8.04844e-05, gnorm=0.852, clip=0, train_wall=29, wall=7794
2020-10-12 08:44:14 | INFO | train_inner | epoch 033:    704 / 753 loss=4.263, nll_loss=2.698, ppl=6.49, wps=25380.4, ups=3.28, wpb=7733.4, bsz=279.3, num_updates=24800, lr=8.03219e-05, gnorm=0.844, clip=0, train_wall=29, wall=7825
2020-10-12 08:44:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:44:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7007.58203125Mb; avail=237546.65234375Mb
2020-10-12 08:44:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001329
2020-10-12 08:44:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7007.58203125Mb; avail=237546.65234375Mb
2020-10-12 08:44:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070886
2020-10-12 08:44:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7007.58203125Mb; avail=237546.65234375Mb
2020-10-12 08:44:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051959
2020-10-12 08:44:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124927
2020-10-12 08:44:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7007.58203125Mb; avail=237546.65234375Mb
2020-10-12 08:44:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7007.58203125Mb; avail=237546.65234375Mb
2020-10-12 08:44:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001198
2020-10-12 08:44:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7007.58203125Mb; avail=237546.65234375Mb
2020-10-12 08:44:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071015
2020-10-12 08:44:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7007.58203125Mb; avail=237546.65234375Mb
2020-10-12 08:44:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051262
2020-10-12 08:44:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124210
2020-10-12 08:44:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7007.58203125Mb; avail=237546.65234375Mb
2020-10-12 08:44:31 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 4.545 | nll_loss 2.91 | ppl 7.52 | wps 54596.7 | wpb 2261.4 | bsz 84.4 | num_updates 24849 | best_loss 4.545
2020-10-12 08:44:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:44:36 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 33 @ 24849 updates, score 4.545) (writing took 4.863235624041408 seconds)
2020-10-12 08:44:36 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-12 08:44:36 | INFO | train | epoch 033 | loss 4.246 | nll_loss 2.678 | ppl 6.4 | wps 24183.7 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 24849 | lr 8.02427e-05 | gnorm 0.855 | clip 0 | train_wall 217 | wall 7847
2020-10-12 08:44:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-12 08:44:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-12 08:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6912.20703125Mb; avail=237642.2734375Mb
2020-10-12 08:44:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003965
2020-10-12 08:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043801
2020-10-12 08:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6912.20703125Mb; avail=237642.2734375Mb
2020-10-12 08:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001548
2020-10-12 08:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6912.20703125Mb; avail=237642.2734375Mb
2020-10-12 08:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.781313
2020-10-12 08:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.827450
2020-10-12 08:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6918.5859375Mb; avail=237635.74609375Mb
2020-10-12 08:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6914.23046875Mb; avail=237640.1015625Mb
2020-10-12 08:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033411
2020-10-12 08:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6914.23046875Mb; avail=237640.1015625Mb
2020-10-12 08:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001593
2020-10-12 08:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6914.23046875Mb; avail=237640.1015625Mb
2020-10-12 08:44:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.771994
2020-10-12 08:44:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.807793
2020-10-12 08:44:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6918.34765625Mb; avail=237636.44140625Mb
2020-10-12 08:44:38 | INFO | fairseq.trainer | begin training epoch 34
2020-10-12 08:44:54 | INFO | train_inner | epoch 034:     51 / 753 loss=4.266, nll_loss=2.7, ppl=6.5, wps=19231.6, ups=2.51, wpb=7648, bsz=259, num_updates=24900, lr=8.01605e-05, gnorm=0.851, clip=0, train_wall=29, wall=7864
2020-10-12 08:45:24 | INFO | train_inner | epoch 034:    151 / 753 loss=4.218, nll_loss=2.647, ppl=6.26, wps=25425.8, ups=3.28, wpb=7751.9, bsz=281.3, num_updates=25000, lr=8e-05, gnorm=0.858, clip=0, train_wall=29, wall=7895
2020-10-12 08:45:55 | INFO | train_inner | epoch 034:    251 / 753 loss=4.248, nll_loss=2.679, ppl=6.41, wps=25132.4, ups=3.29, wpb=7634.3, bsz=270.5, num_updates=25100, lr=7.98405e-05, gnorm=0.872, clip=0, train_wall=29, wall=7925
2020-10-12 08:46:25 | INFO | train_inner | epoch 034:    351 / 753 loss=4.214, nll_loss=2.643, ppl=6.24, wps=24835, ups=3.29, wpb=7551.1, bsz=284.4, num_updates=25200, lr=7.96819e-05, gnorm=0.87, clip=0, train_wall=29, wall=7956
2020-10-12 08:46:55 | INFO | train_inner | epoch 034:    451 / 753 loss=4.234, nll_loss=2.665, ppl=6.34, wps=25058.5, ups=3.34, wpb=7505.8, bsz=268.3, num_updates=25300, lr=7.95243e-05, gnorm=0.859, clip=0, train_wall=29, wall=7986
2020-10-12 08:47:25 | INFO | train_inner | epoch 034:    551 / 753 loss=4.209, nll_loss=2.637, ppl=6.22, wps=25332.5, ups=3.28, wpb=7726.2, bsz=310, num_updates=25400, lr=7.93676e-05, gnorm=0.84, clip=0, train_wall=29, wall=8016
2020-10-12 08:47:55 | INFO | train_inner | epoch 034:    651 / 753 loss=4.238, nll_loss=2.669, ppl=6.36, wps=24961.9, ups=3.34, wpb=7481.3, bsz=294.2, num_updates=25500, lr=7.92118e-05, gnorm=0.873, clip=0, train_wall=29, wall=8046
2020-10-12 08:48:25 | INFO | train_inner | epoch 034:    751 / 753 loss=4.233, nll_loss=2.664, ppl=6.34, wps=25383.2, ups=3.32, wpb=7649.9, bsz=271.9, num_updates=25600, lr=7.90569e-05, gnorm=0.848, clip=0, train_wall=29, wall=8076
2020-10-12 08:48:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7083.13671875Mb; avail=237470.91796875Mb
2020-10-12 08:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001358
2020-10-12 08:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7083.13671875Mb; avail=237470.91796875Mb
2020-10-12 08:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070354
2020-10-12 08:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7082.359375Mb; avail=237471.6953125Mb
2020-10-12 08:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052619
2020-10-12 08:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125127
2020-10-12 08:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7081.875Mb; avail=237472.1796875Mb
2020-10-12 08:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7081.875Mb; avail=237472.1796875Mb
2020-10-12 08:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001208
2020-10-12 08:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7081.875Mb; avail=237472.1796875Mb
2020-10-12 08:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070324
2020-10-12 08:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7082.48046875Mb; avail=237471.6953125Mb
2020-10-12 08:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053525
2020-10-12 08:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125829
2020-10-12 08:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.5625Mb; avail=237462.61328125Mb
2020-10-12 08:48:29 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.55 | nll_loss 2.915 | ppl 7.54 | wps 54810.6 | wpb 2261.4 | bsz 84.4 | num_updates 25602 | best_loss 4.545
2020-10-12 08:48:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:48:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_last.pt (epoch 34 @ 25602 updates, score 4.55) (writing took 2.639557912014425 seconds)
2020-10-12 08:48:32 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-12 08:48:32 | INFO | train | epoch 034 | loss 4.228 | nll_loss 2.658 | ppl 6.31 | wps 24385.5 | ups 3.2 | wpb 7619.3 | bsz 282.7 | num_updates 25602 | lr 7.90539e-05 | gnorm 0.858 | clip 0 | train_wall 217 | wall 8082
2020-10-12 08:48:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-12 08:48:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-12 08:48:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6987.6171875Mb; avail=237566.62109375Mb
2020-10-12 08:48:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003800
2020-10-12 08:48:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043485
2020-10-12 08:48:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6987.6171875Mb; avail=237566.62109375Mb
2020-10-12 08:48:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001551
2020-10-12 08:48:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6987.6171875Mb; avail=237566.62109375Mb
2020-10-12 08:48:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.783730
2020-10-12 08:48:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.829567
2020-10-12 08:48:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6993.55859375Mb; avail=237560.6796875Mb
2020-10-12 08:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6988.14453125Mb; avail=237566.09375Mb
2020-10-12 08:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033215
2020-10-12 08:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6988.14453125Mb; avail=237566.09375Mb
2020-10-12 08:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001560
2020-10-12 08:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6988.14453125Mb; avail=237566.09375Mb
2020-10-12 08:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.770712
2020-10-12 08:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.806338
2020-10-12 08:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6993.2890625Mb; avail=237560.94921875Mb
2020-10-12 08:48:33 | INFO | fairseq.trainer | begin training epoch 35
2020-10-12 08:49:03 | INFO | train_inner | epoch 035:     98 / 753 loss=4.169, nll_loss=2.591, ppl=6.03, wps=20293.9, ups=2.67, wpb=7611.2, bsz=276.8, num_updates=25700, lr=7.8903e-05, gnorm=0.838, clip=0, train_wall=29, wall=8114
2020-10-12 08:49:33 | INFO | train_inner | epoch 035:    198 / 753 loss=4.22, nll_loss=2.647, ppl=6.26, wps=24880.3, ups=3.3, wpb=7541.6, bsz=275.5, num_updates=25800, lr=7.87499e-05, gnorm=0.861, clip=0, train_wall=29, wall=8144
2020-10-12 08:50:04 | INFO | train_inner | epoch 035:    298 / 753 loss=4.221, nll_loss=2.649, ppl=6.27, wps=25186.3, ups=3.3, wpb=7626, bsz=275.6, num_updates=25900, lr=7.85977e-05, gnorm=0.878, clip=0, train_wall=29, wall=8174
2020-10-12 08:50:34 | INFO | train_inner | epoch 035:    398 / 753 loss=4.193, nll_loss=2.619, ppl=6.14, wps=25230.5, ups=3.3, wpb=7642, bsz=290.3, num_updates=26000, lr=7.84465e-05, gnorm=0.851, clip=0, train_wall=29, wall=8205
2020-10-12 08:51:04 | INFO | train_inner | epoch 035:    498 / 753 loss=4.197, nll_loss=2.625, ppl=6.17, wps=25055.7, ups=3.34, wpb=7500.2, bsz=274.1, num_updates=26100, lr=7.8296e-05, gnorm=0.865, clip=0, train_wall=29, wall=8235
2020-10-12 08:51:34 | INFO | train_inner | epoch 035:    598 / 753 loss=4.213, nll_loss=2.641, ppl=6.24, wps=25272.8, ups=3.31, wpb=7626.4, bsz=276, num_updates=26200, lr=7.81465e-05, gnorm=0.854, clip=0, train_wall=29, wall=8265
2020-10-12 08:52:04 | INFO | train_inner | epoch 035:    698 / 753 loss=4.237, nll_loss=2.667, ppl=6.35, wps=25271.9, ups=3.29, wpb=7674.8, bsz=299.4, num_updates=26300, lr=7.79978e-05, gnorm=0.854, clip=0, train_wall=29, wall=8295
2020-10-12 08:52:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7120.8828125Mb; avail=237433.89453125Mb
2020-10-12 08:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002024
2020-10-12 08:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7120.8828125Mb; avail=237433.89453125Mb
2020-10-12 08:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.093899
2020-10-12 08:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7111.50390625Mb; avail=237443.2734375Mb
2020-10-12 08:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052958
2020-10-12 08:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.149935
2020-10-12 08:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7111.50390625Mb; avail=237443.2734375Mb
2020-10-12 08:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7111.50390625Mb; avail=237443.2734375Mb
2020-10-12 08:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001240
2020-10-12 08:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7111.50390625Mb; avail=237443.2734375Mb
2020-10-12 08:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072734
2020-10-12 08:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7111.50390625Mb; avail=237443.2734375Mb
2020-10-12 08:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052528
2020-10-12 08:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127235
2020-10-12 08:52:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7111.01171875Mb; avail=237443.765625Mb
2020-10-12 08:52:24 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.553 | nll_loss 2.917 | ppl 7.55 | wps 54569.5 | wpb 2261.4 | bsz 84.4 | num_updates 26355 | best_loss 4.545
2020-10-12 08:52:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:52:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_last.pt (epoch 35 @ 26355 updates, score 4.553) (writing took 2.621386236976832 seconds)
2020-10-12 08:52:27 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-12 08:52:27 | INFO | train | epoch 035 | loss 4.209 | nll_loss 2.637 | ppl 6.22 | wps 24398.2 | ups 3.2 | wpb 7619.3 | bsz 282.7 | num_updates 26355 | lr 7.79163e-05 | gnorm 0.858 | clip 0 | train_wall 217 | wall 8318
2020-10-12 08:52:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-12 08:52:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-12 08:52:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7007.59765625Mb; avail=237546.3984375Mb
2020-10-12 08:52:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003500
2020-10-12 08:52:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043168
2020-10-12 08:52:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7006.61328125Mb; avail=237547.3828125Mb
2020-10-12 08:52:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001554
2020-10-12 08:52:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7007.10546875Mb; avail=237546.890625Mb
2020-10-12 08:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.779663
2020-10-12 08:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.825205
2020-10-12 08:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7012.4140625Mb; avail=237541.58203125Mb
2020-10-12 08:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7007.60546875Mb; avail=237546.390625Mb
2020-10-12 08:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033041
2020-10-12 08:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7007.60546875Mb; avail=237546.390625Mb
2020-10-12 08:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001581
2020-10-12 08:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7007.60546875Mb; avail=237546.390625Mb
2020-10-12 08:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.766239
2020-10-12 08:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.801676
2020-10-12 08:52:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7011.08984375Mb; avail=237543.39453125Mb
2020-10-12 08:52:28 | INFO | fairseq.trainer | begin training epoch 36
2020-10-12 08:52:42 | INFO | train_inner | epoch 036:     45 / 753 loss=4.227, nll_loss=2.655, ppl=6.3, wps=20503.3, ups=2.64, wpb=7770.8, bsz=281.4, num_updates=26400, lr=7.78499e-05, gnorm=0.855, clip=0, train_wall=29, wall=8333
2020-10-12 08:53:13 | INFO | train_inner | epoch 036:    145 / 753 loss=4.15, nll_loss=2.57, ppl=5.94, wps=25139.7, ups=3.3, wpb=7617.9, bsz=301.5, num_updates=26500, lr=7.77029e-05, gnorm=0.855, clip=0, train_wall=29, wall=8363
2020-10-12 08:53:43 | INFO | train_inner | epoch 036:    245 / 753 loss=4.187, nll_loss=2.612, ppl=6.11, wps=25257.1, ups=3.31, wpb=7622.2, bsz=264.7, num_updates=26600, lr=7.75567e-05, gnorm=0.846, clip=0, train_wall=29, wall=8394
2020-10-12 08:54:13 | INFO | train_inner | epoch 036:    345 / 753 loss=4.195, nll_loss=2.619, ppl=6.15, wps=24886.4, ups=3.31, wpb=7525.4, bsz=280.6, num_updates=26700, lr=7.74113e-05, gnorm=0.885, clip=0, train_wall=29, wall=8424
2020-10-12 08:54:43 | INFO | train_inner | epoch 036:    445 / 753 loss=4.171, nll_loss=2.593, ppl=6.03, wps=25351.2, ups=3.28, wpb=7726.6, bsz=297.1, num_updates=26800, lr=7.72667e-05, gnorm=0.856, clip=0, train_wall=29, wall=8454
2020-10-12 08:55:14 | INFO | train_inner | epoch 036:    545 / 753 loss=4.211, nll_loss=2.639, ppl=6.23, wps=24998.4, ups=3.31, wpb=7553.5, bsz=269.8, num_updates=26900, lr=7.7123e-05, gnorm=0.858, clip=0, train_wall=29, wall=8485
2020-10-12 08:55:44 | INFO | train_inner | epoch 036:    645 / 753 loss=4.212, nll_loss=2.639, ppl=6.23, wps=25106.4, ups=3.3, wpb=7598.4, bsz=285.3, num_updates=27000, lr=7.698e-05, gnorm=0.862, clip=0, train_wall=29, wall=8515
2020-10-12 08:56:14 | INFO | train_inner | epoch 036:    745 / 753 loss=4.218, nll_loss=2.646, ppl=6.26, wps=25149.1, ups=3.31, wpb=7595.6, bsz=289.5, num_updates=27100, lr=7.68379e-05, gnorm=0.868, clip=0, train_wall=29, wall=8545
2020-10-12 08:56:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 08:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7122.328125Mb; avail=237431.578125Mb
2020-10-12 08:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001363
2020-10-12 08:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7122.328125Mb; avail=237431.578125Mb
2020-10-12 08:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073271
2020-10-12 08:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7121.26953125Mb; avail=237432.9609375Mb
2020-10-12 08:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052403
2020-10-12 08:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127843
2020-10-12 08:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7121.26953125Mb; avail=237432.9609375Mb
2020-10-12 08:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7121.26953125Mb; avail=237432.9609375Mb
2020-10-12 08:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001250
2020-10-12 08:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7121.26953125Mb; avail=237432.9609375Mb
2020-10-12 08:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070714
2020-10-12 08:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7121.26953125Mb; avail=237432.9609375Mb
2020-10-12 08:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052092
2020-10-12 08:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124797
2020-10-12 08:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7121.26953125Mb; avail=237432.9609375Mb
2020-10-12 08:56:19 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 4.533 | nll_loss 2.895 | ppl 7.44 | wps 54884.2 | wpb 2261.4 | bsz 84.4 | num_updates 27108 | best_loss 4.533
2020-10-12 08:56:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 08:56:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 36 @ 27108 updates, score 4.533) (writing took 4.8555659260600805 seconds)
2020-10-12 08:56:24 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-12 08:56:24 | INFO | train | epoch 036 | loss 4.192 | nll_loss 2.617 | ppl 6.14 | wps 24144.2 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 27108 | lr 7.68265e-05 | gnorm 0.86 | clip 0 | train_wall 217 | wall 8555
2020-10-12 08:56:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-12 08:56:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-12 08:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7018.75Mb; avail=237535.50390625Mb
2020-10-12 08:56:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003514
2020-10-12 08:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043272
2020-10-12 08:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7019.15234375Mb; avail=237535.140625Mb
2020-10-12 08:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001546
2020-10-12 08:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7019.15234375Mb; avail=237535.140625Mb
2020-10-12 08:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.775285
2020-10-12 08:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.820868
2020-10-12 08:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7024.046875Mb; avail=237531.19140625Mb
2020-10-12 08:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7022.13671875Mb; avail=237532.3671875Mb
2020-10-12 08:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033291
2020-10-12 08:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7025.6484375Mb; avail=237528.85546875Mb
2020-10-12 08:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001541
2020-10-12 08:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7025.6484375Mb; avail=237528.85546875Mb
2020-10-12 08:56:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.771999
2020-10-12 08:56:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.807657
2020-10-12 08:56:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7024.49609375Mb; avail=237530.01171875Mb
2020-10-12 08:56:26 | INFO | fairseq.trainer | begin training epoch 37
2020-10-12 08:56:54 | INFO | train_inner | epoch 037:     92 / 753 loss=4.139, nll_loss=2.559, ppl=5.89, wps=19187.4, ups=2.5, wpb=7677.5, bsz=282.4, num_updates=27200, lr=7.66965e-05, gnorm=0.849, clip=0, train_wall=29, wall=8585
2020-10-12 08:57:24 | INFO | train_inner | epoch 037:    192 / 753 loss=4.167, nll_loss=2.587, ppl=6.01, wps=25177, ups=3.3, wpb=7629.6, bsz=277, num_updates=27300, lr=7.65559e-05, gnorm=0.869, clip=0, train_wall=29, wall=8615
2020-10-12 08:57:55 | INFO | train_inner | epoch 037:    292 / 753 loss=4.159, nll_loss=2.58, ppl=5.98, wps=24628.8, ups=3.25, wpb=7574.9, bsz=300.2, num_updates=27400, lr=7.64161e-05, gnorm=0.86, clip=0, train_wall=29, wall=8646
2020-10-12 08:58:25 | INFO | train_inner | epoch 037:    392 / 753 loss=4.177, nll_loss=2.599, ppl=6.06, wps=25189.8, ups=3.32, wpb=7576.6, bsz=287.7, num_updates=27500, lr=7.6277e-05, gnorm=0.86, clip=0, train_wall=29, wall=8676
2020-10-12 08:58:56 | INFO | train_inner | epoch 037:    492 / 753 loss=4.169, nll_loss=2.591, ppl=6.03, wps=25597.8, ups=3.31, wpb=7737, bsz=283.5, num_updates=27600, lr=7.61387e-05, gnorm=0.835, clip=0, train_wall=29, wall=8706
2020-10-12 08:59:26 | INFO | train_inner | epoch 037:    592 / 753 loss=4.207, nll_loss=2.633, ppl=6.2, wps=24866.3, ups=3.31, wpb=7519.1, bsz=282.5, num_updates=27700, lr=7.60011e-05, gnorm=0.892, clip=0, train_wall=29, wall=8737
2020-10-12 08:59:56 | INFO | train_inner | epoch 037:    692 / 753 loss=4.202, nll_loss=2.627, ppl=6.18, wps=25308.4, ups=3.28, wpb=7714.7, bsz=281.1, num_updates=27800, lr=7.58643e-05, gnorm=0.855, clip=0, train_wall=29, wall=8767
2020-10-12 09:00:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 09:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7138.32421875Mb; avail=237415.84765625Mb
2020-10-12 09:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001295
2020-10-12 09:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7138.32421875Mb; avail=237415.84765625Mb
2020-10-12 09:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072204
2020-10-12 09:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7128.97265625Mb; avail=237425.19921875Mb
2020-10-12 09:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053890
2020-10-12 09:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128181
2020-10-12 09:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7128.97265625Mb; avail=237425.19921875Mb
2020-10-12 09:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7128.97265625Mb; avail=237425.19921875Mb
2020-10-12 09:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001148
2020-10-12 09:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7128.97265625Mb; avail=237425.19921875Mb
2020-10-12 09:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070827
2020-10-12 09:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7128.97265625Mb; avail=237425.19921875Mb
2020-10-12 09:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054437
2020-10-12 09:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127146
2020-10-12 09:00:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7128.97265625Mb; avail=237425.19921875Mb
2020-10-12 09:00:17 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 4.535 | nll_loss 2.898 | ppl 7.45 | wps 54628.1 | wpb 2261.4 | bsz 84.4 | num_updates 27861 | best_loss 4.533
2020-10-12 09:00:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 09:00:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_last.pt (epoch 37 @ 27861 updates, score 4.535) (writing took 2.6373190528247505 seconds)
2020-10-12 09:00:20 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-12 09:00:20 | INFO | train | epoch 037 | loss 4.176 | nll_loss 2.599 | ppl 6.06 | wps 24332.9 | ups 3.19 | wpb 7619.3 | bsz 282.7 | num_updates 27861 | lr 7.57812e-05 | gnorm 0.863 | clip 0 | train_wall 218 | wall 8791
2020-10-12 09:00:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-12 09:00:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-12 09:00:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7034.17578125Mb; avail=237520.7578125Mb
2020-10-12 09:00:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003956
2020-10-12 09:00:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.041840
2020-10-12 09:00:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7024.72265625Mb; avail=237530.2109375Mb
2020-10-12 09:00:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001743
2020-10-12 09:00:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7024.72265625Mb; avail=237530.2109375Mb
2020-10-12 09:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.768582
2020-10-12 09:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.812980
2020-10-12 09:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7022.875Mb; avail=237531.75Mb
2020-10-12 09:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7028.81640625Mb; avail=237525.80859375Mb
2020-10-12 09:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033368
2020-10-12 09:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7033.0Mb; avail=237521.625Mb
2020-10-12 09:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001581
2020-10-12 09:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7033.0Mb; avail=237521.625Mb
2020-10-12 09:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.768894
2020-10-12 09:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.804697
2020-10-12 09:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7022.12109375Mb; avail=237532.32421875Mb
2020-10-12 09:00:22 | INFO | fairseq.trainer | begin training epoch 38
2020-10-12 09:00:34 | INFO | train_inner | epoch 038:     39 / 753 loss=4.173, nll_loss=2.595, ppl=6.04, wps=20074.8, ups=2.65, wpb=7569.7, bsz=277.8, num_updates=27900, lr=7.57282e-05, gnorm=0.871, clip=0, train_wall=29, wall=8805
2020-10-12 09:01:04 | INFO | train_inner | epoch 038:    139 / 753 loss=4.149, nll_loss=2.567, ppl=5.93, wps=25258, ups=3.28, wpb=7708.7, bsz=286, num_updates=28000, lr=7.55929e-05, gnorm=0.857, clip=0, train_wall=29, wall=8835
2020-10-12 09:01:35 | INFO | train_inner | epoch 038:    239 / 753 loss=4.163, nll_loss=2.584, ppl=6, wps=24998.6, ups=3.27, wpb=7635.1, bsz=291.6, num_updates=28100, lr=7.54583e-05, gnorm=0.875, clip=0, train_wall=29, wall=8866
2020-10-12 09:02:05 | INFO | train_inner | epoch 038:    339 / 753 loss=4.143, nll_loss=2.562, ppl=5.91, wps=25154.8, ups=3.32, wpb=7570.4, bsz=270.2, num_updates=28200, lr=7.53244e-05, gnorm=0.858, clip=0, train_wall=29, wall=8896
2020-10-12 09:02:36 | INFO | train_inner | epoch 038:    439 / 753 loss=4.165, nll_loss=2.585, ppl=6, wps=24963.7, ups=3.29, wpb=7594.8, bsz=283.4, num_updates=28300, lr=7.51912e-05, gnorm=0.861, clip=0, train_wall=29, wall=8926
2020-10-12 09:03:06 | INFO | train_inner | epoch 038:    539 / 753 loss=4.158, nll_loss=2.578, ppl=5.97, wps=25033.1, ups=3.3, wpb=7577.8, bsz=282.5, num_updates=28400, lr=7.50587e-05, gnorm=0.852, clip=0, train_wall=29, wall=8957
2020-10-12 09:03:37 | INFO | train_inner | epoch 038:    639 / 753 loss=4.168, nll_loss=2.589, ppl=6.02, wps=24974.1, ups=3.26, wpb=7665.8, bsz=294.8, num_updates=28500, lr=7.49269e-05, gnorm=0.87, clip=0, train_wall=29, wall=8987
2020-10-12 09:04:06 | INFO | train_inner | epoch 038:    739 / 753 loss=4.18, nll_loss=2.604, ppl=6.08, wps=25282.6, ups=3.34, wpb=7565.4, bsz=259.5, num_updates=28600, lr=7.47958e-05, gnorm=0.886, clip=0, train_wall=29, wall=9017
2020-10-12 09:04:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 09:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7113.46484375Mb; avail=237440.60546875Mb
2020-10-12 09:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001395
2020-10-12 09:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.46484375Mb; avail=237440.60546875Mb
2020-10-12 09:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071563
2020-10-12 09:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.46484375Mb; avail=237440.60546875Mb
2020-10-12 09:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053136
2020-10-12 09:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126920
2020-10-12 09:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7117.6953125Mb; avail=237436.375Mb
2020-10-12 09:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7119.390625Mb; avail=237434.6796875Mb
2020-10-12 09:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001215
2020-10-12 09:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7119.390625Mb; avail=237434.6796875Mb
2020-10-12 09:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070851
2020-10-12 09:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.67578125Mb; avail=237440.39453125Mb
2020-10-12 09:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053133
2020-10-12 09:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125958
2020-10-12 09:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.67578125Mb; avail=237440.39453125Mb
2020-10-12 09:04:14 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 4.53 | nll_loss 2.895 | ppl 7.44 | wps 54601.7 | wpb 2261.4 | bsz 84.4 | num_updates 28614 | best_loss 4.53
2020-10-12 09:04:14 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 09:04:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 38 @ 28614 updates, score 4.53) (writing took 4.851171150105074 seconds)
2020-10-12 09:04:18 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-12 09:04:18 | INFO | train | epoch 038 | loss 4.16 | nll_loss 2.58 | ppl 5.98 | wps 24081.8 | ups 3.16 | wpb 7619.3 | bsz 282.7 | num_updates 28614 | lr 7.47775e-05 | gnorm 0.865 | clip 0 | train_wall 218 | wall 9029
2020-10-12 09:04:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-12 09:04:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-12 09:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7041.765625Mb; avail=237512.65234375Mb
2020-10-12 09:04:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003763
2020-10-12 09:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.041940
2020-10-12 09:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7045.32421875Mb; avail=237509.09375Mb
2020-10-12 09:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001693
2020-10-12 09:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7045.32421875Mb; avail=237509.09375Mb
2020-10-12 09:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.770092
2020-10-12 09:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.814523
2020-10-12 09:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7036.63671875Mb; avail=237517.78125Mb
2020-10-12 09:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7036.7890625Mb; avail=237517.7734375Mb
2020-10-12 09:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033246
2020-10-12 09:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7036.5546875Mb; avail=237517.88671875Mb
2020-10-12 09:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001569
2020-10-12 09:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7036.5546875Mb; avail=237517.88671875Mb
2020-10-12 09:04:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.765557
2020-10-12 09:04:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.801176
2020-10-12 09:04:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7036.4921875Mb; avail=237517.94140625Mb
2020-10-12 09:04:20 | INFO | fairseq.trainer | begin training epoch 39
2020-10-12 09:04:46 | INFO | train_inner | epoch 039:     86 / 753 loss=4.151, nll_loss=2.57, ppl=5.94, wps=19157.5, ups=2.53, wpb=7578.9, bsz=264.7, num_updates=28700, lr=7.46653e-05, gnorm=0.877, clip=0, train_wall=29, wall=9057
2020-10-12 09:05:16 | INFO | train_inner | epoch 039:    186 / 753 loss=4.15, nll_loss=2.568, ppl=5.93, wps=25338.5, ups=3.28, wpb=7724.8, bsz=279.1, num_updates=28800, lr=7.45356e-05, gnorm=0.851, clip=0, train_wall=29, wall=9087
2020-10-12 09:05:47 | INFO | train_inner | epoch 039:    286 / 753 loss=4.151, nll_loss=2.569, ppl=5.93, wps=25081, ups=3.31, wpb=7580.7, bsz=270.9, num_updates=28900, lr=7.44065e-05, gnorm=0.871, clip=0, train_wall=29, wall=9118
2020-10-12 09:06:17 | INFO | train_inner | epoch 039:    386 / 753 loss=4.129, nll_loss=2.545, ppl=5.84, wps=25121.8, ups=3.28, wpb=7668.1, bsz=284.9, num_updates=29000, lr=7.42781e-05, gnorm=0.858, clip=0, train_wall=29, wall=9148
2020-10-12 09:06:48 | INFO | train_inner | epoch 039:    486 / 753 loss=4.132, nll_loss=2.55, ppl=5.86, wps=25118.5, ups=3.28, wpb=7660.7, bsz=302.5, num_updates=29100, lr=7.41504e-05, gnorm=0.864, clip=0, train_wall=29, wall=9179
2020-10-12 09:07:18 | INFO | train_inner | epoch 039:    586 / 753 loss=4.149, nll_loss=2.567, ppl=5.93, wps=25058.2, ups=3.25, wpb=7706.8, bsz=295.5, num_updates=29200, lr=7.40233e-05, gnorm=0.853, clip=0, train_wall=29, wall=9209
2020-10-12 09:07:49 | INFO | train_inner | epoch 039:    686 / 753 loss=4.144, nll_loss=2.563, ppl=5.91, wps=24744.2, ups=3.31, wpb=7471.1, bsz=291.1, num_updates=29300, lr=7.38969e-05, gnorm=0.886, clip=0, train_wall=29, wall=9240
2020-10-12 09:08:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 09:08:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7021.41796875Mb; avail=237532.515625Mb
2020-10-12 09:08:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001380
2020-10-12 09:08:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7021.41796875Mb; avail=237532.515625Mb
2020-10-12 09:08:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070978
2020-10-12 09:08:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7021.41796875Mb; avail=237532.515625Mb
2020-10-12 09:08:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052068
2020-10-12 09:08:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125215
2020-10-12 09:08:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7021.41796875Mb; avail=237532.515625Mb
2020-10-12 09:08:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7023.35546875Mb; avail=237530.578125Mb
2020-10-12 09:08:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001169
2020-10-12 09:08:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7023.35546875Mb; avail=237530.578125Mb
2020-10-12 09:08:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072059
2020-10-12 09:08:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7031.2265625Mb; avail=237522.70703125Mb
2020-10-12 09:08:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052138
2020-10-12 09:08:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126106
2020-10-12 09:08:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7022.39453125Mb; avail=237531.5390625Mb
2020-10-12 09:08:12 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 4.525 | nll_loss 2.886 | ppl 7.39 | wps 54724.4 | wpb 2261.4 | bsz 84.4 | num_updates 29367 | best_loss 4.525
2020-10-12 09:08:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 09:08:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 39 @ 29367 updates, score 4.525) (writing took 4.864351505180821 seconds)
2020-10-12 09:08:17 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-12 09:08:17 | INFO | train | epoch 039 | loss 4.145 | nll_loss 2.563 | ppl 5.91 | wps 24099.5 | ups 3.16 | wpb 7619.3 | bsz 282.7 | num_updates 29367 | lr 7.38125e-05 | gnorm 0.867 | clip 0 | train_wall 218 | wall 9267
2020-10-12 09:08:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-12 09:08:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-12 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6938.18359375Mb; avail=237615.8828125Mb
2020-10-12 09:08:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003805
2020-10-12 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.041706
2020-10-12 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6938.18359375Mb; avail=237615.8828125Mb
2020-10-12 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001713
2020-10-12 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6938.18359375Mb; avail=237615.8828125Mb
2020-10-12 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.765979
2020-10-12 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.810196
2020-10-12 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6946.33203125Mb; avail=237608.09375Mb
2020-10-12 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6936.984375Mb; avail=237617.44140625Mb
2020-10-12 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033322
2020-10-12 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6937.11328125Mb; avail=237617.3125Mb
2020-10-12 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001723
2020-10-12 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6937.11328125Mb; avail=237617.3125Mb
2020-10-12 09:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.763250
2020-10-12 09:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.799095
2020-10-12 09:08:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6936.80078125Mb; avail=237617.625Mb
2020-10-12 09:08:18 | INFO | fairseq.trainer | begin training epoch 40
2020-10-12 09:08:28 | INFO | train_inner | epoch 040:     33 / 753 loss=4.153, nll_loss=2.571, ppl=5.94, wps=19064.9, ups=2.53, wpb=7546.3, bsz=274.2, num_updates=29400, lr=7.37711e-05, gnorm=0.877, clip=0, train_wall=29, wall=9279
2020-10-12 09:08:59 | INFO | train_inner | epoch 040:    133 / 753 loss=4.109, nll_loss=2.522, ppl=5.74, wps=25308, ups=3.3, wpb=7657.5, bsz=269.1, num_updates=29500, lr=7.3646e-05, gnorm=0.852, clip=0, train_wall=29, wall=9309
2020-10-12 09:09:29 | INFO | train_inner | epoch 040:    233 / 753 loss=4.14, nll_loss=2.557, ppl=5.88, wps=25441.4, ups=3.29, wpb=7738.1, bsz=265.4, num_updates=29600, lr=7.35215e-05, gnorm=0.858, clip=0, train_wall=29, wall=9340
2020-10-12 09:09:59 | INFO | train_inner | epoch 040:    333 / 753 loss=4.129, nll_loss=2.544, ppl=5.83, wps=24771, ups=3.27, wpb=7567.1, bsz=281.9, num_updates=29700, lr=7.33976e-05, gnorm=0.866, clip=0, train_wall=29, wall=9370
2020-10-12 09:10:30 | INFO | train_inner | epoch 040:    433 / 753 loss=4.109, nll_loss=2.524, ppl=5.75, wps=25334.9, ups=3.31, wpb=7664.3, bsz=307.3, num_updates=29800, lr=7.32743e-05, gnorm=0.868, clip=0, train_wall=29, wall=9401
2020-10-12 09:11:00 | INFO | train_inner | epoch 040:    533 / 753 loss=4.117, nll_loss=2.532, ppl=5.78, wps=25302.1, ups=3.31, wpb=7643, bsz=312, num_updates=29900, lr=7.31517e-05, gnorm=0.861, clip=0, train_wall=29, wall=9431
2020-10-12 09:11:30 | INFO | train_inner | epoch 040:    633 / 753 loss=4.132, nll_loss=2.55, ppl=5.86, wps=24955.3, ups=3.35, wpb=7458.1, bsz=271.8, num_updates=30000, lr=7.30297e-05, gnorm=0.891, clip=0, train_wall=28, wall=9461
2020-10-12 09:12:00 | INFO | train_inner | epoch 040:    733 / 753 loss=4.164, nll_loss=2.583, ppl=5.99, wps=25294.8, ups=3.3, wpb=7666.4, bsz=276.5, num_updates=30100, lr=7.29083e-05, gnorm=0.877, clip=0, train_wall=29, wall=9491
2020-10-12 09:12:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-12 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=4249.9609375Mb; avail=240316.453125Mb
2020-10-12 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001374
2020-10-12 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=4249.76171875Mb; avail=240316.65234375Mb
2020-10-12 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069921
2020-10-12 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=4254.61328125Mb; avail=240311.80078125Mb
2020-10-12 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052320
2020-10-12 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124405
2020-10-12 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=4259.86328125Mb; avail=240306.55078125Mb
2020-10-12 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=4259.37109375Mb; avail=240307.04296875Mb
2020-10-12 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001104
2020-10-12 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=4250.01953125Mb; avail=240316.39453125Mb
2020-10-12 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070139
2020-10-12 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=4250.01171875Mb; avail=240316.40234375Mb
2020-10-12 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053315
2020-10-12 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125287
2020-10-12 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=4250.01171875Mb; avail=240316.40234375Mb
2020-10-12 09:12:09 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 4.519 | nll_loss 2.884 | ppl 7.38 | wps 54942.9 | wpb 2261.4 | bsz 84.4 | num_updates 30120 | best_loss 4.519
2020-10-12 09:12:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-12 09:12:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belrus_sepspm8000/M2O/checkpoint_best.pt (epoch 40 @ 30120 updates, score 4.519) (writing took 4.866796882124618 seconds)
2020-10-12 09:12:14 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-12 09:12:14 | INFO | train | epoch 040 | loss 4.13 | nll_loss 2.546 | ppl 5.84 | wps 24172.9 | ups 3.17 | wpb 7619.3 | bsz 282.7 | num_updates 30120 | lr 7.28841e-05 | gnorm 0.868 | clip 0 | train_wall 217 | wall 9505
2020-10-12 09:12:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-12 09:12:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-12 09:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=4160.56640625Mb; avail=240406.0625Mb
2020-10-12 09:12:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003804
2020-10-12 09:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.041596
2020-10-12 09:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=4160.56640625Mb; avail=240406.0625Mb
2020-10-12 09:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001600
2020-10-12 09:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=4160.56640625Mb; avail=240406.0625Mb
2020-10-12 09:12:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.771052
2020-10-12 09:12:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.815024
2020-10-12 09:12:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=4174.2578125Mb; avail=240392.6171875Mb
2020-10-12 09:12:15 | INFO | fairseq_cli.train | done training in 9504.6 seconds
