2020-10-11 01:22:21 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_aze_spm8000/aze_eng/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=80, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='fairseq/checkpoints/ted_aze_spm8000/aze_eng/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-11 01:22:21 | INFO | fairseq.tasks.translation | [aze] dictionary: 7728 types
2020-10-11 01:22:21 | INFO | fairseq.tasks.translation | [eng] dictionary: 7728 types
2020-10-11 01:22:21 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_aze_spm8000/aze_eng/valid.aze-eng.aze
2020-10-11 01:22:21 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_aze_spm8000/aze_eng/valid.aze-eng.eng
2020-10-11 01:22:21 | INFO | fairseq.tasks.translation | fairseq/data-bin/ted_aze_spm8000/aze_eng/ valid aze-eng 671 examples
2020-10-11 01:22:22 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(7728, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(7728, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=7728, bias=False)
  )
)
2020-10-11 01:22:22 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2020-10-11 01:22:22 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-11 01:22:22 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-11 01:22:22 | INFO | fairseq_cli.train | num. model params: 35500032 (num. trained: 35500032)
2020-10-11 01:23:00 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_aze_spm8000/aze_eng/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=80, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='fairseq/checkpoints/ted_aze_spm8000/aze_eng/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-11 01:23:00 | INFO | fairseq.tasks.translation | [aze] dictionary: 7728 types
2020-10-11 01:23:00 | INFO | fairseq.tasks.translation | [eng] dictionary: 7728 types
2020-10-11 01:23:00 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_aze_spm8000/aze_eng/valid.aze-eng.aze
2020-10-11 01:23:00 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_aze_spm8000/aze_eng/valid.aze-eng.eng
2020-10-11 01:23:00 | INFO | fairseq.tasks.translation | fairseq/data-bin/ted_aze_spm8000/aze_eng/ valid aze-eng 671 examples
2020-10-11 01:23:02 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(7728, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(7728, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=7728, bias=False)
  )
)
2020-10-11 01:23:02 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2020-10-11 01:23:02 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-11 01:23:02 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-11 01:23:02 | INFO | fairseq_cli.train | num. model params: 35500032 (num. trained: 35500032)
2020-10-11 01:23:24 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-11 01:23:24 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-11 01:23:24 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-11 01:23:24 | INFO | fairseq.utils | rank   0: capabilities =  5.2  ; total memory = 11.927 GB ; name = GeForce GTX TITAN X                     
2020-10-11 01:23:24 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-11 01:23:24 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-11 01:23:24 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-11 01:23:24 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt
2020-10-11 01:23:24 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-11 01:23:24 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_aze_spm8000/aze_eng/train.aze-eng.aze
2020-10-11 01:23:25 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_aze_spm8000/aze_eng/train.aze-eng.eng
2020-10-11 01:23:25 | INFO | fairseq.tasks.translation | fairseq/data-bin/ted_aze_spm8000/aze_eng/ train aze-eng 5946 examples
2020-10-11 01:23:25 | INFO | fairseq.trainer | begin training epoch 1
2020-10-11 01:23:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
/usr1/home/rjoshi2/envs/torch160/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2020-10-11 01:23:43 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.774 | nll_loss 12.679 | ppl 6558.22 | wps 25929.8 | wpb 1984.1 | bsz 74.6 | num_updates 33
2020-10-11 01:23:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:23:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 1 @ 33 updates, score 12.774) (writing took 5.1259923577308655 seconds)
2020-10-11 01:23:48 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-11 01:23:48 | INFO | train | epoch 001 | loss 13.355 | nll_loss 13.325 | ppl 10258.8 | wps 6990.7 | ups 1.43 | wpb 4899.7 | bsz 180.2 | num_updates 33 | lr 1.74918e-06 | gnorm 6.147 | clip 0 | train_wall 17 | wall 25
2020-10-11 01:23:48 | INFO | fairseq.trainer | begin training epoch 2
2020-10-11 01:24:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:24:06 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.641 | nll_loss 11.409 | ppl 2719.25 | wps 26008.2 | wpb 1984.1 | bsz 74.6 | num_updates 66 | best_loss 11.641
2020-10-11 01:24:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:24:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 2 @ 66 updates, score 11.641) (writing took 8.05046233534813 seconds)
2020-10-11 01:24:15 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-11 01:24:15 | INFO | train | epoch 002 | loss 12.488 | nll_loss 12.36 | ppl 5257.52 | wps 6206.6 | ups 1.27 | wpb 4899.7 | bsz 180.2 | num_updates 66 | lr 3.39835e-06 | gnorm 4.619 | clip 0 | train_wall 17 | wall 51
2020-10-11 01:24:15 | INFO | fairseq.trainer | begin training epoch 3
2020-10-11 01:24:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:24:33 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 11.156 | nll_loss 10.849 | ppl 1845.02 | wps 26240.9 | wpb 1984.1 | bsz 74.6 | num_updates 99 | best_loss 11.156
2020-10-11 01:24:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:24:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 3 @ 99 updates, score 11.156) (writing took 5.782289255410433 seconds)
2020-10-11 01:24:38 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-11 01:24:38 | INFO | train | epoch 003 | loss 11.735 | nll_loss 11.514 | ppl 2925.05 | wps 6754.4 | ups 1.38 | wpb 4899.7 | bsz 180.2 | num_updates 99 | lr 5.04753e-06 | gnorm 2.681 | clip 0 | train_wall 17 | wall 75
2020-10-11 01:24:38 | INFO | fairseq.trainer | begin training epoch 4
2020-10-11 01:24:39 | INFO | train_inner | epoch 004:      1 / 33 loss=12.516, nll_loss=12.388, ppl=5360.47, wps=6633.8, ups=1.35, wpb=4897.3, bsz=179.4, num_updates=100, lr=5.0975e-06, gnorm=4.459, clip=0, train_wall=52, wall=75
2020-10-11 01:24:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:24:56 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.859 | nll_loss 10.515 | ppl 1462.82 | wps 26091.4 | wpb 1984.1 | bsz 74.6 | num_updates 132 | best_loss 10.859
2020-10-11 01:24:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:25:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 4 @ 132 updates, score 10.859) (writing took 8.046756871044636 seconds)
2020-10-11 01:25:05 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-11 01:25:05 | INFO | train | epoch 004 | loss 11.335 | nll_loss 11.065 | ppl 2142.87 | wps 6195.3 | ups 1.26 | wpb 4899.7 | bsz 180.2 | num_updates 132 | lr 6.6967e-06 | gnorm 1.922 | clip 0 | train_wall 17 | wall 101
2020-10-11 01:25:05 | INFO | fairseq.trainer | begin training epoch 5
2020-10-11 01:25:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:25:23 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.632 | nll_loss 10.262 | ppl 1227.59 | wps 26053.2 | wpb 1984.1 | bsz 74.6 | num_updates 165 | best_loss 10.632
2020-10-11 01:25:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:25:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 5 @ 165 updates, score 10.632) (writing took 6.852152734994888 seconds)
2020-10-11 01:25:30 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-11 01:25:30 | INFO | train | epoch 005 | loss 11.073 | nll_loss 10.771 | ppl 1746.85 | wps 6458 | ups 1.32 | wpb 4899.7 | bsz 180.2 | num_updates 165 | lr 8.34588e-06 | gnorm 1.628 | clip 0 | train_wall 17 | wall 126
2020-10-11 01:25:30 | INFO | fairseq.trainer | begin training epoch 6
2020-10-11 01:25:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:25:48 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 10.449 | nll_loss 10.055 | ppl 1063.49 | wps 26016.3 | wpb 1984.1 | bsz 74.6 | num_updates 198 | best_loss 10.449
2020-10-11 01:25:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:25:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 6 @ 198 updates, score 10.449) (writing took 6.022707358002663 seconds)
2020-10-11 01:25:54 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-11 01:25:54 | INFO | train | epoch 006 | loss 10.843 | nll_loss 10.513 | ppl 1461.72 | wps 6668.3 | ups 1.36 | wpb 4899.7 | bsz 180.2 | num_updates 198 | lr 9.99505e-06 | gnorm 1.448 | clip 0 | train_wall 17 | wall 150
2020-10-11 01:25:54 | INFO | fairseq.trainer | begin training epoch 7
2020-10-11 01:25:55 | INFO | train_inner | epoch 007:      2 / 33 loss=11.073, nll_loss=10.771, ppl=1747.3, wps=6446.1, ups=1.32, wpb=4892.5, bsz=181.5, num_updates=200, lr=1.0095e-05, gnorm=1.654, clip=0, train_wall=52, wall=151
2020-10-11 01:26:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:26:12 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 10.279 | nll_loss 9.859 | ppl 928.59 | wps 26031.4 | wpb 1984.1 | bsz 74.6 | num_updates 231 | best_loss 10.279
2020-10-11 01:26:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:26:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 7 @ 231 updates, score 10.279) (writing took 7.69868627935648 seconds)
2020-10-11 01:26:20 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-11 01:26:20 | INFO | train | epoch 007 | loss 10.627 | nll_loss 10.272 | ppl 1236.26 | wps 6247.2 | ups 1.28 | wpb 4899.7 | bsz 180.2 | num_updates 231 | lr 1.16442e-05 | gnorm 1.313 | clip 0 | train_wall 17 | wall 176
2020-10-11 01:26:20 | INFO | fairseq.trainer | begin training epoch 8
2020-10-11 01:26:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:26:38 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.06 | nll_loss 9.609 | ppl 781.07 | wps 25941.7 | wpb 1984.1 | bsz 74.6 | num_updates 264 | best_loss 10.06
2020-10-11 01:26:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:26:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 8 @ 264 updates, score 10.06) (writing took 5.567539643496275 seconds)
2020-10-11 01:26:44 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-11 01:26:44 | INFO | train | epoch 008 | loss 10.412 | nll_loss 10.03 | ppl 1045.35 | wps 6763.5 | ups 1.38 | wpb 4899.7 | bsz 180.2 | num_updates 264 | lr 1.32934e-05 | gnorm 1.207 | clip 0 | train_wall 17 | wall 200
2020-10-11 01:26:44 | INFO | fairseq.trainer | begin training epoch 9
2020-10-11 01:27:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:27:02 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 9.788 | nll_loss 9.3 | ppl 630.28 | wps 23924.3 | wpb 1984.1 | bsz 74.6 | num_updates 297 | best_loss 9.788
2020-10-11 01:27:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:27:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 9 @ 297 updates, score 9.788) (writing took 7.784187439829111 seconds)
2020-10-11 01:27:10 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-11 01:27:10 | INFO | train | epoch 009 | loss 10.177 | nll_loss 9.763 | ppl 869.15 | wps 6203.7 | ups 1.27 | wpb 4899.7 | bsz 180.2 | num_updates 297 | lr 1.49426e-05 | gnorm 1.303 | clip 0 | train_wall 17 | wall 226
2020-10-11 01:27:10 | INFO | fairseq.trainer | begin training epoch 10
2020-10-11 01:27:12 | INFO | train_inner | epoch 010:      3 / 33 loss=10.386, nll_loss=10, ppl=1023.91, wps=6431.9, ups=1.31, wpb=4923.7, bsz=181, num_updates=300, lr=1.50925e-05, gnorm=1.285, clip=0, train_wall=52, wall=228
2020-10-11 01:27:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:27:28 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 9.494 | nll_loss 8.945 | ppl 492.88 | wps 25881.2 | wpb 1984.1 | bsz 74.6 | num_updates 330 | best_loss 9.494
2020-10-11 01:27:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:27:36 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 10 @ 330 updates, score 9.494) (writing took 8.023398093879223 seconds)
2020-10-11 01:27:36 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-11 01:27:36 | INFO | train | epoch 010 | loss 9.898 | nll_loss 9.446 | ppl 697.65 | wps 6142.6 | ups 1.25 | wpb 4899.7 | bsz 180.2 | num_updates 330 | lr 1.65917e-05 | gnorm 1.55 | clip 0 | train_wall 17 | wall 252
2020-10-11 01:27:36 | INFO | fairseq.trainer | begin training epoch 11
2020-10-11 01:27:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:27:54 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 9.151 | nll_loss 8.544 | ppl 373.14 | wps 26121.7 | wpb 1984.1 | bsz 74.6 | num_updates 363 | best_loss 9.151
2020-10-11 01:27:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:28:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 11 @ 363 updates, score 9.151) (writing took 7.945253111422062 seconds)
2020-10-11 01:28:02 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-11 01:28:02 | INFO | train | epoch 011 | loss 9.585 | nll_loss 9.083 | ppl 542.15 | wps 6175.1 | ups 1.26 | wpb 4899.7 | bsz 180.2 | num_updates 363 | lr 1.82409e-05 | gnorm 1.642 | clip 0 | train_wall 17 | wall 278
2020-10-11 01:28:02 | INFO | fairseq.trainer | begin training epoch 12
2020-10-11 01:28:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:28:20 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.977 | nll_loss 8.314 | ppl 318.32 | wps 25875.8 | wpb 1984.1 | bsz 74.6 | num_updates 396 | best_loss 8.977
2020-10-11 01:28:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:28:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 12 @ 396 updates, score 8.977) (writing took 7.967273823916912 seconds)
2020-10-11 01:28:28 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-11 01:28:28 | INFO | train | epoch 012 | loss 9.324 | nll_loss 8.772 | ppl 437 | wps 6179.2 | ups 1.26 | wpb 4899.7 | bsz 180.2 | num_updates 396 | lr 1.98901e-05 | gnorm 1.124 | clip 0 | train_wall 17 | wall 305
2020-10-11 01:28:28 | INFO | fairseq.trainer | begin training epoch 13
2020-10-11 01:28:31 | INFO | train_inner | epoch 013:      4 / 33 loss=9.569, nll_loss=9.061, ppl=534.22, wps=6200.3, ups=1.27, wpb=4901.3, bsz=178.6, num_updates=400, lr=2.009e-05, gnorm=1.416, clip=0, train_wall=52, wall=307
2020-10-11 01:28:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:28:47 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.838 | nll_loss 8.134 | ppl 280.91 | wps 25930.9 | wpb 1984.1 | bsz 74.6 | num_updates 429 | best_loss 8.838
2020-10-11 01:28:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:28:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 13 @ 429 updates, score 8.838) (writing took 7.8022154569625854 seconds)
2020-10-11 01:28:54 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-11 01:28:54 | INFO | train | epoch 013 | loss 9.141 | nll_loss 8.55 | ppl 374.78 | wps 6225.8 | ups 1.27 | wpb 4899.7 | bsz 180.2 | num_updates 429 | lr 2.15393e-05 | gnorm 1.093 | clip 0 | train_wall 17 | wall 331
2020-10-11 01:28:54 | INFO | fairseq.trainer | begin training epoch 14
2020-10-11 01:29:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:29:12 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.729 | nll_loss 8.012 | ppl 258.2 | wps 25921.5 | wpb 1984.1 | bsz 74.6 | num_updates 462 | best_loss 8.729
2020-10-11 01:29:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:29:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 14 @ 462 updates, score 8.729) (writing took 6.755462024360895 seconds)
2020-10-11 01:29:19 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-11 01:29:19 | INFO | train | epoch 014 | loss 9 | nll_loss 8.378 | ppl 332.75 | wps 6600.2 | ups 1.35 | wpb 4899.7 | bsz 180.2 | num_updates 462 | lr 2.31884e-05 | gnorm 1.092 | clip 0 | train_wall 17 | wall 355
2020-10-11 01:29:19 | INFO | fairseq.trainer | begin training epoch 15
2020-10-11 01:29:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:29:37 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.634 | nll_loss 7.897 | ppl 238.29 | wps 25030 | wpb 1984.1 | bsz 74.6 | num_updates 495 | best_loss 8.634
2020-10-11 01:29:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:29:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 15 @ 495 updates, score 8.634) (writing took 7.074778165668249 seconds)
2020-10-11 01:29:44 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-11 01:29:44 | INFO | train | epoch 015 | loss 8.905 | nll_loss 8.261 | ppl 306.8 | wps 6399.7 | ups 1.31 | wpb 4899.7 | bsz 180.2 | num_updates 495 | lr 2.48376e-05 | gnorm 1.061 | clip 0 | train_wall 17 | wall 380
2020-10-11 01:29:44 | INFO | fairseq.trainer | begin training epoch 16
2020-10-11 01:29:47 | INFO | train_inner | epoch 016:      5 / 33 loss=9.011, nll_loss=8.39, ppl=335.52, wps=6409.4, ups=1.31, wpb=4889.9, bsz=180.7, num_updates=500, lr=2.50875e-05, gnorm=1.084, clip=0, train_wall=52, wall=383
2020-10-11 01:30:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:30:02 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.572 | nll_loss 7.818 | ppl 225.7 | wps 26111.6 | wpb 1984.1 | bsz 74.6 | num_updates 528 | best_loss 8.572
2020-10-11 01:30:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:30:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 16 @ 528 updates, score 8.572) (writing took 7.8188640885055065 seconds)
2020-10-11 01:30:10 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-11 01:30:10 | INFO | train | epoch 016 | loss 8.83 | nll_loss 8.166 | ppl 287.31 | wps 6332.9 | ups 1.29 | wpb 4899.7 | bsz 180.2 | num_updates 528 | lr 2.64868e-05 | gnorm 1.037 | clip 0 | train_wall 17 | wall 406
2020-10-11 01:30:10 | INFO | fairseq.trainer | begin training epoch 17
2020-10-11 01:30:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:30:28 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 8.555 | nll_loss 7.794 | ppl 221.95 | wps 25884.4 | wpb 1984.1 | bsz 74.6 | num_updates 561 | best_loss 8.555
2020-10-11 01:30:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:30:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 17 @ 561 updates, score 8.555) (writing took 4.112266983836889 seconds)
2020-10-11 01:30:32 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-11 01:30:32 | INFO | train | epoch 017 | loss 8.789 | nll_loss 8.111 | ppl 276.45 | wps 7245.5 | ups 1.48 | wpb 4899.7 | bsz 180.2 | num_updates 561 | lr 2.8136e-05 | gnorm 1.61 | clip 0 | train_wall 17 | wall 428
2020-10-11 01:30:32 | INFO | fairseq.trainer | begin training epoch 18
2020-10-11 01:30:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:30:50 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 8.504 | nll_loss 7.723 | ppl 211.34 | wps 24362.9 | wpb 1984.1 | bsz 74.6 | num_updates 594 | best_loss 8.504
2020-10-11 01:30:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:31:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 18 @ 594 updates, score 8.504) (writing took 11.597577769309282 seconds)
2020-10-11 01:31:02 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-11 01:31:02 | INFO | train | epoch 018 | loss 8.736 | nll_loss 8.045 | ppl 264.08 | wps 5365.7 | ups 1.1 | wpb 4899.7 | bsz 180.2 | num_updates 594 | lr 2.97852e-05 | gnorm 1.323 | clip 0 | train_wall 17 | wall 458
2020-10-11 01:31:02 | INFO | fairseq.trainer | begin training epoch 19
2020-10-11 01:31:05 | INFO | train_inner | epoch 019:      6 / 33 loss=8.759, nll_loss=8.076, ppl=269.92, wps=6243, ups=1.27, wpb=4903.2, bsz=181.8, num_updates=600, lr=3.0085e-05, gnorm=1.327, clip=0, train_wall=52, wall=462
2020-10-11 01:31:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:31:21 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 8.468 | nll_loss 7.684 | ppl 205.64 | wps 25965.7 | wpb 1984.1 | bsz 74.6 | num_updates 627 | best_loss 8.468
2020-10-11 01:31:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:31:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 19 @ 627 updates, score 8.468) (writing took 12.105945888906717 seconds)
2020-10-11 01:31:33 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-11 01:31:33 | INFO | train | epoch 019 | loss 8.683 | nll_loss 7.98 | ppl 252.47 | wps 5283.1 | ups 1.08 | wpb 4899.7 | bsz 180.2 | num_updates 627 | lr 3.14343e-05 | gnorm 1.124 | clip 0 | train_wall 17 | wall 489
2020-10-11 01:31:33 | INFO | fairseq.trainer | begin training epoch 20
2020-10-11 01:31:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:31:50 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 8.419 | nll_loss 7.615 | ppl 196 | wps 25256.9 | wpb 1984.1 | bsz 74.6 | num_updates 660 | best_loss 8.419
2020-10-11 01:31:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:32:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 20 @ 660 updates, score 8.419) (writing took 11.960771650075912 seconds)
2020-10-11 01:32:02 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-11 01:32:02 | INFO | train | epoch 020 | loss 8.641 | nll_loss 7.929 | ppl 243.79 | wps 5483.7 | ups 1.12 | wpb 4899.7 | bsz 180.2 | num_updates 660 | lr 3.30835e-05 | gnorm 1.397 | clip 0 | train_wall 17 | wall 518
2020-10-11 01:32:02 | INFO | fairseq.trainer | begin training epoch 21
2020-10-11 01:32:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:32:21 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 8.351 | nll_loss 7.545 | ppl 186.81 | wps 26003.1 | wpb 1984.1 | bsz 74.6 | num_updates 693 | best_loss 8.351
2020-10-11 01:32:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:32:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 21 @ 693 updates, score 8.351) (writing took 11.254276130348444 seconds)
2020-10-11 01:32:32 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-11 01:32:32 | INFO | train | epoch 021 | loss 8.587 | nll_loss 7.865 | ppl 233.13 | wps 5445 | ups 1.11 | wpb 4899.7 | bsz 180.2 | num_updates 693 | lr 3.47327e-05 | gnorm 1.103 | clip 0 | train_wall 17 | wall 548
2020-10-11 01:32:32 | INFO | fairseq.trainer | begin training epoch 22
2020-10-11 01:32:36 | INFO | train_inner | epoch 022:      7 / 33 loss=8.64, nll_loss=7.927, ppl=243.45, wps=5435.9, ups=1.11, wpb=4906.2, bsz=179, num_updates=700, lr=3.50825e-05, gnorm=1.205, clip=0, train_wall=52, wall=552
2020-10-11 01:32:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:32:50 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 8.305 | nll_loss 7.484 | ppl 178.96 | wps 25929.4 | wpb 1984.1 | bsz 74.6 | num_updates 726 | best_loss 8.305
2020-10-11 01:32:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:32:59 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 22 @ 726 updates, score 8.305) (writing took 8.962073232978582 seconds)
2020-10-11 01:32:59 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-11 01:32:59 | INFO | train | epoch 022 | loss 8.539 | nll_loss 7.809 | ppl 224.3 | wps 5917.9 | ups 1.21 | wpb 4899.7 | bsz 180.2 | num_updates 726 | lr 3.63819e-05 | gnorm 1.215 | clip 0 | train_wall 17 | wall 575
2020-10-11 01:32:59 | INFO | fairseq.trainer | begin training epoch 23
2020-10-11 01:33:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:33:40 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 8.27 | nll_loss 7.442 | ppl 173.9 | wps 11835.4 | wpb 1984.1 | bsz 74.6 | num_updates 759 | best_loss 8.27
2020-10-11 01:33:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:33:52 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 23 @ 759 updates, score 8.27) (writing took 11.53463951125741 seconds)
2020-10-11 01:33:52 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-11 01:33:52 | INFO | train | epoch 023 | loss 8.493 | nll_loss 7.755 | ppl 216 | wps 3067.6 | ups 0.63 | wpb 4899.7 | bsz 180.2 | num_updates 759 | lr 3.8031e-05 | gnorm 1.495 | clip 0 | train_wall 39 | wall 628
2020-10-11 01:33:52 | INFO | fairseq.trainer | begin training epoch 24
2020-10-11 01:34:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:34:32 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.21 | nll_loss 7.375 | ppl 166.03 | wps 11934.9 | wpb 1984.1 | bsz 74.6 | num_updates 792 | best_loss 8.21
2020-10-11 01:34:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:34:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 24 @ 792 updates, score 8.21) (writing took 11.543889369815588 seconds)
2020-10-11 01:34:44 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-11 01:34:44 | INFO | train | epoch 024 | loss 8.444 | nll_loss 7.698 | ppl 207.71 | wps 3108.4 | ups 0.63 | wpb 4899.7 | bsz 180.2 | num_updates 792 | lr 3.96802e-05 | gnorm 1.504 | clip 0 | train_wall 39 | wall 680
2020-10-11 01:34:44 | INFO | fairseq.trainer | begin training epoch 25
2020-10-11 01:34:53 | INFO | train_inner | epoch 025:      8 / 33 loss=8.478, nll_loss=7.738, ppl=213.45, wps=3538.5, ups=0.73, wpb=4869.8, bsz=181.1, num_updates=800, lr=4.008e-05, gnorm=1.406, clip=0, train_wall=101, wall=690
2020-10-11 01:35:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:35:25 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.174 | nll_loss 7.33 | ppl 160.91 | wps 12165.4 | wpb 1984.1 | bsz 74.6 | num_updates 825 | best_loss 8.174
2020-10-11 01:35:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:35:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 25 @ 825 updates, score 8.174) (writing took 7.791098732501268 seconds)
2020-10-11 01:35:32 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-11 01:35:32 | INFO | train | epoch 025 | loss 8.377 | nll_loss 7.623 | ppl 197.13 | wps 3337.4 | ups 0.68 | wpb 4899.7 | bsz 180.2 | num_updates 825 | lr 4.13294e-05 | gnorm 1.259 | clip 0 | train_wall 39 | wall 729
2020-10-11 01:35:32 | INFO | fairseq.trainer | begin training epoch 26
2020-10-11 01:36:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:36:13 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.123 | nll_loss 7.266 | ppl 153.92 | wps 11990.8 | wpb 1984.1 | bsz 74.6 | num_updates 858 | best_loss 8.123
2020-10-11 01:36:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:36:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 26 @ 858 updates, score 8.123) (writing took 9.259900528937578 seconds)
2020-10-11 01:36:22 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-11 01:36:22 | INFO | train | epoch 026 | loss 8.32 | nll_loss 7.559 | ppl 188.51 | wps 3265.3 | ups 0.67 | wpb 4899.7 | bsz 180.2 | num_updates 858 | lr 4.29786e-05 | gnorm 1.229 | clip 0 | train_wall 39 | wall 778
2020-10-11 01:36:22 | INFO | fairseq.trainer | begin training epoch 27
2020-10-11 01:37:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:37:02 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.058 | nll_loss 7.202 | ppl 147.28 | wps 12316.1 | wpb 1984.1 | bsz 74.6 | num_updates 891 | best_loss 8.058
2020-10-11 01:37:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:37:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 27 @ 891 updates, score 8.058) (writing took 7.11679607257247 seconds)
2020-10-11 01:37:09 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-11 01:37:09 | INFO | train | epoch 027 | loss 8.268 | nll_loss 7.498 | ppl 180.78 | wps 3407 | ups 0.7 | wpb 4899.7 | bsz 180.2 | num_updates 891 | lr 4.46277e-05 | gnorm 1.565 | clip 0 | train_wall 39 | wall 826
2020-10-11 01:37:09 | INFO | fairseq.trainer | begin training epoch 28
2020-10-11 01:37:20 | INFO | train_inner | epoch 028:      9 / 33 loss=8.32, nll_loss=7.558, ppl=188.41, wps=3359.2, ups=0.68, wpb=4937, bsz=176.4, num_updates=900, lr=4.50775e-05, gnorm=1.365, clip=0, train_wall=118, wall=837
2020-10-11 01:37:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:37:49 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.007 | nll_loss 7.146 | ppl 141.61 | wps 12371.6 | wpb 1984.1 | bsz 74.6 | num_updates 924 | best_loss 8.007
2020-10-11 01:37:49 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:37:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 28 @ 924 updates, score 8.007) (writing took 4.2796160243451595 seconds)
2020-10-11 01:37:54 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-11 01:37:54 | INFO | train | epoch 028 | loss 8.202 | nll_loss 7.424 | ppl 171.71 | wps 3650.7 | ups 0.75 | wpb 4899.7 | bsz 180.2 | num_updates 924 | lr 4.62769e-05 | gnorm 1.403 | clip 0 | train_wall 38 | wall 870
2020-10-11 01:37:54 | INFO | fairseq.trainer | begin training epoch 29
2020-10-11 01:38:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:38:34 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.99 | nll_loss 7.107 | ppl 137.88 | wps 11962.8 | wpb 1984.1 | bsz 74.6 | num_updates 957 | best_loss 7.99
2020-10-11 01:38:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:38:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 29 @ 957 updates, score 7.99) (writing took 9.223851025104523 seconds)
2020-10-11 01:38:43 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-11 01:38:43 | INFO | train | epoch 029 | loss 8.149 | nll_loss 7.362 | ppl 164.49 | wps 3277.9 | ups 0.67 | wpb 4899.7 | bsz 180.2 | num_updates 957 | lr 4.79261e-05 | gnorm 1.535 | clip 0 | train_wall 38 | wall 919
2020-10-11 01:38:43 | INFO | fairseq.trainer | begin training epoch 30
2020-10-11 01:39:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:39:23 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.945 | nll_loss 7.056 | ppl 133.04 | wps 12135.5 | wpb 1984.1 | bsz 74.6 | num_updates 990 | best_loss 7.945
2020-10-11 01:39:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:39:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 30 @ 990 updates, score 7.945) (writing took 6.932274512946606 seconds)
2020-10-11 01:39:30 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-11 01:39:30 | INFO | train | epoch 030 | loss 8.088 | nll_loss 7.293 | ppl 156.77 | wps 3437.9 | ups 0.7 | wpb 4899.7 | bsz 180.2 | num_updates 990 | lr 4.95753e-05 | gnorm 1.475 | clip 0 | train_wall 38 | wall 966
2020-10-11 01:39:30 | INFO | fairseq.trainer | begin training epoch 31
2020-10-11 01:39:42 | INFO | train_inner | epoch 031:     10 / 33 loss=8.113, nll_loss=7.321, ppl=159.92, wps=3432.1, ups=0.71, wpb=4866.8, bsz=183.6, num_updates=1000, lr=5.0075e-05, gnorm=1.469, clip=0, train_wall=116, wall=978
2020-10-11 01:40:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:40:10 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.876 | nll_loss 6.998 | ppl 127.82 | wps 12051.4 | wpb 1984.1 | bsz 74.6 | num_updates 1023 | best_loss 7.876
2020-10-11 01:40:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:40:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 31 @ 1023 updates, score 7.876) (writing took 4.258190516382456 seconds)
2020-10-11 01:40:15 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-11 01:40:15 | INFO | train | epoch 031 | loss 8.026 | nll_loss 7.221 | ppl 149.16 | wps 3621.1 | ups 0.74 | wpb 4899.7 | bsz 180.2 | num_updates 1023 | lr 5.12244e-05 | gnorm 1.353 | clip 0 | train_wall 39 | wall 1011
2020-10-11 01:40:15 | INFO | fairseq.trainer | begin training epoch 32
2020-10-11 01:40:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:40:55 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.845 | nll_loss 6.964 | ppl 124.86 | wps 11964 | wpb 1984.1 | bsz 74.6 | num_updates 1056 | best_loss 7.845
2020-10-11 01:40:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:41:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 32 @ 1056 updates, score 7.845) (writing took 11.417897660285234 seconds)
2020-10-11 01:41:06 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-11 01:41:06 | INFO | train | epoch 032 | loss 7.975 | nll_loss 7.162 | ppl 143.25 | wps 3133 | ups 0.64 | wpb 4899.7 | bsz 180.2 | num_updates 1056 | lr 5.28736e-05 | gnorm 1.531 | clip 0 | train_wall 38 | wall 1063
2020-10-11 01:41:06 | INFO | fairseq.trainer | begin training epoch 33
2020-10-11 01:41:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:41:46 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.793 | nll_loss 6.903 | ppl 119.7 | wps 12016.8 | wpb 1984.1 | bsz 74.6 | num_updates 1089 | best_loss 7.793
2020-10-11 01:41:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:41:56 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 33 @ 1089 updates, score 7.793) (writing took 9.488624647259712 seconds)
2020-10-11 01:41:56 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-11 01:41:56 | INFO | train | epoch 033 | loss 7.924 | nll_loss 7.103 | ppl 137.49 | wps 3267.6 | ups 0.67 | wpb 4899.7 | bsz 180.2 | num_updates 1089 | lr 5.45228e-05 | gnorm 1.494 | clip 0 | train_wall 38 | wall 1112
2020-10-11 01:41:56 | INFO | fairseq.trainer | begin training epoch 34
2020-10-11 01:42:09 | INFO | train_inner | epoch 034:     11 / 33 loss=7.952, nll_loss=7.135, ppl=140.57, wps=3358.6, ups=0.68, wpb=4931.2, bsz=180.1, num_updates=1100, lr=5.50725e-05, gnorm=1.464, clip=0, train_wall=116, wall=1125
2020-10-11 01:42:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:42:36 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.825 | nll_loss 6.922 | ppl 121.27 | wps 12179.1 | wpb 1984.1 | bsz 74.6 | num_updates 1122 | best_loss 7.793
2020-10-11 01:42:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:42:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt (epoch 34 @ 1122 updates, score 7.825) (writing took 2.530372202396393 seconds)
2020-10-11 01:42:38 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-11 01:42:38 | INFO | train | epoch 034 | loss 7.865 | nll_loss 7.036 | ppl 131.22 | wps 3791.6 | ups 0.77 | wpb 4899.7 | bsz 180.2 | num_updates 1122 | lr 5.6172e-05 | gnorm 1.431 | clip 0 | train_wall 38 | wall 1155
2020-10-11 01:42:38 | INFO | fairseq.trainer | begin training epoch 35
2020-10-11 01:43:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:43:18 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.804 | nll_loss 6.886 | ppl 118.25 | wps 12027 | wpb 1984.1 | bsz 74.6 | num_updates 1155 | best_loss 7.793
2020-10-11 01:43:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:43:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt (epoch 35 @ 1155 updates, score 7.804) (writing took 10.01154275611043 seconds)
2020-10-11 01:43:28 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-11 01:43:28 | INFO | train | epoch 035 | loss 7.833 | nll_loss 6.998 | ppl 127.79 | wps 3230.4 | ups 0.66 | wpb 4899.7 | bsz 180.2 | num_updates 1155 | lr 5.78211e-05 | gnorm 1.84 | clip 0 | train_wall 38 | wall 1205
2020-10-11 01:43:28 | INFO | fairseq.trainer | begin training epoch 36
2020-10-11 01:44:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:44:08 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.713 | nll_loss 6.806 | ppl 111.86 | wps 12073.7 | wpb 1984.1 | bsz 74.6 | num_updates 1188 | best_loss 7.713
2020-10-11 01:44:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:44:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 36 @ 1188 updates, score 7.713) (writing took 10.479173261672258 seconds)
2020-10-11 01:44:19 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-11 01:44:19 | INFO | train | epoch 036 | loss 7.768 | nll_loss 6.923 | ppl 121.34 | wps 3207.4 | ups 0.65 | wpb 4899.7 | bsz 180.2 | num_updates 1188 | lr 5.94703e-05 | gnorm 1.562 | clip 0 | train_wall 38 | wall 1255
2020-10-11 01:44:19 | INFO | fairseq.trainer | begin training epoch 37
2020-10-11 01:44:33 | INFO | train_inner | epoch 037:     12 / 33 loss=7.818, nll_loss=6.981, ppl=126.33, wps=3367.9, ups=0.69, wpb=4848.7, bsz=177.6, num_updates=1200, lr=6.007e-05, gnorm=1.632, clip=0, train_wall=116, wall=1269
2020-10-11 01:44:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:44:59 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.674 | nll_loss 6.762 | ppl 108.52 | wps 11932 | wpb 1984.1 | bsz 74.6 | num_updates 1221 | best_loss 7.674
2020-10-11 01:44:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:45:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 37 @ 1221 updates, score 7.674) (writing took 4.346076250076294 seconds)
2020-10-11 01:45:03 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-11 01:45:03 | INFO | train | epoch 037 | loss 7.712 | nll_loss 6.859 | ppl 116.09 | wps 3644.3 | ups 0.74 | wpb 4899.7 | bsz 180.2 | num_updates 1221 | lr 6.11195e-05 | gnorm 1.586 | clip 0 | train_wall 38 | wall 1299
2020-10-11 01:45:03 | INFO | fairseq.trainer | begin training epoch 38
2020-10-11 01:45:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:45:43 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.637 | nll_loss 6.713 | ppl 104.92 | wps 12388.8 | wpb 1984.1 | bsz 74.6 | num_updates 1254 | best_loss 7.637
2020-10-11 01:45:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:45:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 38 @ 1254 updates, score 7.637) (writing took 11.614242006093264 seconds)
2020-10-11 01:45:55 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-11 01:45:55 | INFO | train | epoch 038 | loss 7.659 | nll_loss 6.798 | ppl 111.24 | wps 3122.4 | ups 0.64 | wpb 4899.7 | bsz 180.2 | num_updates 1254 | lr 6.27687e-05 | gnorm 1.51 | clip 0 | train_wall 38 | wall 1351
2020-10-11 01:45:55 | INFO | fairseq.trainer | begin training epoch 39
2020-10-11 01:46:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:46:35 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.63 | nll_loss 6.679 | ppl 102.47 | wps 12098.1 | wpb 1984.1 | bsz 74.6 | num_updates 1287 | best_loss 7.63
2020-10-11 01:46:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:46:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 39 @ 1287 updates, score 7.63) (writing took 9.33350906893611 seconds)
2020-10-11 01:46:44 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-11 01:46:44 | INFO | train | epoch 039 | loss 7.611 | nll_loss 6.742 | ppl 107.06 | wps 3280.7 | ups 0.67 | wpb 4899.7 | bsz 180.2 | num_updates 1287 | lr 6.44178e-05 | gnorm 1.631 | clip 0 | train_wall 38 | wall 1401
2020-10-11 01:46:44 | INFO | fairseq.trainer | begin training epoch 40
2020-10-11 01:47:00 | INFO | train_inner | epoch 040:     13 / 33 loss=7.612, nll_loss=6.744, ppl=107.17, wps=3364.3, ups=0.68, wpb=4935.2, bsz=185, num_updates=1300, lr=6.50675e-05, gnorm=1.568, clip=0, train_wall=116, wall=1416
2020-10-11 01:47:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:47:24 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.582 | nll_loss 6.646 | ppl 100.17 | wps 11315.4 | wpb 1984.1 | bsz 74.6 | num_updates 1320 | best_loss 7.582
2020-10-11 01:47:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:47:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 40 @ 1320 updates, score 7.582) (writing took 4.28270497918129 seconds)
2020-10-11 01:47:29 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-11 01:47:29 | INFO | train | epoch 040 | loss 7.558 | nll_loss 6.68 | ppl 102.56 | wps 3651.7 | ups 0.75 | wpb 4899.7 | bsz 180.2 | num_updates 1320 | lr 6.6067e-05 | gnorm 1.563 | clip 0 | train_wall 38 | wall 1445
2020-10-11 01:47:29 | INFO | fairseq.trainer | begin training epoch 41
2020-10-11 01:48:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:48:08 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.58 | nll_loss 6.648 | ppl 100.29 | wps 11703.7 | wpb 1984.1 | bsz 74.6 | num_updates 1353 | best_loss 7.58
2020-10-11 01:48:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:48:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 41 @ 1353 updates, score 7.58) (writing took 11.09104960411787 seconds)
2020-10-11 01:48:20 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2020-10-11 01:48:20 | INFO | train | epoch 041 | loss 7.503 | nll_loss 6.619 | ppl 98.26 | wps 3172.7 | ups 0.65 | wpb 4899.7 | bsz 180.2 | num_updates 1353 | lr 6.77162e-05 | gnorm 1.493 | clip 0 | train_wall 38 | wall 1496
2020-10-11 01:48:20 | INFO | fairseq.trainer | begin training epoch 42
2020-10-11 01:48:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:49:00 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.567 | nll_loss 6.621 | ppl 98.4 | wps 12324.2 | wpb 1984.1 | bsz 74.6 | num_updates 1386 | best_loss 7.567
2020-10-11 01:49:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:49:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 42 @ 1386 updates, score 7.567) (writing took 10.622913908213377 seconds)
2020-10-11 01:49:10 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2020-10-11 01:49:10 | INFO | train | epoch 042 | loss 7.468 | nll_loss 6.576 | ppl 95.41 | wps 3196.9 | ups 0.65 | wpb 4899.7 | bsz 180.2 | num_updates 1386 | lr 6.93654e-05 | gnorm 1.718 | clip 0 | train_wall 38 | wall 1546
2020-10-11 01:49:10 | INFO | fairseq.trainer | begin training epoch 43
2020-10-11 01:49:27 | INFO | train_inner | epoch 043:     14 / 33 loss=7.5, nll_loss=6.613, ppl=97.9, wps=3308.5, ups=0.68, wpb=4866.1, bsz=176.6, num_updates=1400, lr=7.0065e-05, gnorm=1.636, clip=0, train_wall=116, wall=1563
2020-10-11 01:49:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:49:50 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.518 | nll_loss 6.567 | ppl 94.78 | wps 12397.4 | wpb 1984.1 | bsz 74.6 | num_updates 1419 | best_loss 7.518
2020-10-11 01:49:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:49:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 43 @ 1419 updates, score 7.518) (writing took 4.336011096835136 seconds)
2020-10-11 01:49:55 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2020-10-11 01:49:55 | INFO | train | epoch 043 | loss 7.42 | nll_loss 6.52 | ppl 91.79 | wps 3642 | ups 0.74 | wpb 4899.7 | bsz 180.2 | num_updates 1419 | lr 7.10145e-05 | gnorm 1.761 | clip 0 | train_wall 38 | wall 1591
2020-10-11 01:49:55 | INFO | fairseq.trainer | begin training epoch 44
2020-10-11 01:50:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:50:35 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.488 | nll_loss 6.536 | ppl 92.81 | wps 12281.1 | wpb 1984.1 | bsz 74.6 | num_updates 1452 | best_loss 7.488
2020-10-11 01:50:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:50:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 44 @ 1452 updates, score 7.488) (writing took 11.584675211459398 seconds)
2020-10-11 01:50:46 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2020-10-11 01:50:46 | INFO | train | epoch 044 | loss 7.358 | nll_loss 6.45 | ppl 87.42 | wps 3125.6 | ups 0.64 | wpb 4899.7 | bsz 180.2 | num_updates 1452 | lr 7.26637e-05 | gnorm 1.49 | clip 0 | train_wall 38 | wall 1643
2020-10-11 01:50:46 | INFO | fairseq.trainer | begin training epoch 45
2020-10-11 01:51:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:51:26 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.484 | nll_loss 6.509 | ppl 91.1 | wps 12310.2 | wpb 1984.1 | bsz 74.6 | num_updates 1485 | best_loss 7.484
2020-10-11 01:51:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:51:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 45 @ 1485 updates, score 7.484) (writing took 11.627171233296394 seconds)
2020-10-11 01:51:38 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2020-10-11 01:51:38 | INFO | train | epoch 045 | loss 7.322 | nll_loss 6.408 | ppl 84.89 | wps 3132.9 | ups 0.64 | wpb 4899.7 | bsz 180.2 | num_updates 1485 | lr 7.43129e-05 | gnorm 1.836 | clip 0 | train_wall 38 | wall 1694
2020-10-11 01:51:38 | INFO | fairseq.trainer | begin training epoch 46
2020-10-11 01:51:56 | INFO | train_inner | epoch 046:     15 / 33 loss=7.348, nll_loss=6.438, ppl=86.71, wps=3319.9, ups=0.67, wpb=4949.6, bsz=184.1, num_updates=1500, lr=7.50625e-05, gnorm=1.672, clip=0, train_wall=116, wall=1712
2020-10-11 01:52:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:52:18 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.457 | nll_loss 6.481 | ppl 89.31 | wps 12083 | wpb 1984.1 | bsz 74.6 | num_updates 1518 | best_loss 7.457
2020-10-11 01:52:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:52:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 46 @ 1518 updates, score 7.457) (writing took 9.912640824913979 seconds)
2020-10-11 01:52:28 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2020-10-11 01:52:28 | INFO | train | epoch 046 | loss 7.27 | nll_loss 6.348 | ppl 81.46 | wps 3249.4 | ups 0.66 | wpb 4899.7 | bsz 180.2 | num_updates 1518 | lr 7.59621e-05 | gnorm 1.669 | clip 0 | train_wall 38 | wall 1744
2020-10-11 01:52:28 | INFO | fairseq.trainer | begin training epoch 47
2020-10-11 01:53:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:53:08 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.458 | nll_loss 6.482 | ppl 89.4 | wps 12073.4 | wpb 1984.1 | bsz 74.6 | num_updates 1551 | best_loss 7.457
2020-10-11 01:53:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:53:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt (epoch 47 @ 1551 updates, score 7.458) (writing took 10.268003337085247 seconds)
2020-10-11 01:53:18 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2020-10-11 01:53:18 | INFO | train | epoch 047 | loss 7.231 | nll_loss 6.302 | ppl 78.88 | wps 3217.1 | ups 0.66 | wpb 4899.7 | bsz 180.2 | num_updates 1551 | lr 7.76112e-05 | gnorm 1.748 | clip 0 | train_wall 38 | wall 1794
2020-10-11 01:53:18 | INFO | fairseq.trainer | begin training epoch 48
2020-10-11 01:53:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:53:58 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.405 | nll_loss 6.431 | ppl 86.28 | wps 12378 | wpb 1984.1 | bsz 74.6 | num_updates 1584 | best_loss 7.405
2020-10-11 01:53:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:54:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 48 @ 1584 updates, score 7.405) (writing took 4.31617745757103 seconds)
2020-10-11 01:54:02 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2020-10-11 01:54:02 | INFO | train | epoch 048 | loss 7.177 | nll_loss 6.24 | ppl 75.57 | wps 3668.4 | ups 0.75 | wpb 4899.7 | bsz 180.2 | num_updates 1584 | lr 7.92604e-05 | gnorm 1.604 | clip 0 | train_wall 38 | wall 1838
2020-10-11 01:54:02 | INFO | fairseq.trainer | begin training epoch 49
2020-10-11 01:54:21 | INFO | train_inner | epoch 049:     16 / 33 loss=7.203, nll_loss=6.27, ppl=77.16, wps=3365.7, ups=0.69, wpb=4892.1, bsz=179.1, num_updates=1600, lr=8.006e-05, gnorm=1.661, clip=0, train_wall=116, wall=1857
2020-10-11 01:54:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:54:42 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.395 | nll_loss 6.405 | ppl 84.73 | wps 13246.4 | wpb 1984.1 | bsz 74.6 | num_updates 1617 | best_loss 7.395
2020-10-11 01:54:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:54:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 49 @ 1617 updates, score 7.395) (writing took 4.852572854608297 seconds)
2020-10-11 01:54:46 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2020-10-11 01:54:46 | INFO | train | epoch 049 | loss 7.12 | nll_loss 6.176 | ppl 72.31 | wps 3637.5 | ups 0.74 | wpb 4899.7 | bsz 180.2 | num_updates 1617 | lr 8.09096e-05 | gnorm 1.663 | clip 0 | train_wall 38 | wall 1883
2020-10-11 01:54:46 | INFO | fairseq.trainer | begin training epoch 50
2020-10-11 01:55:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:55:26 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.383 | nll_loss 6.408 | ppl 84.93 | wps 12407.8 | wpb 1984.1 | bsz 74.6 | num_updates 1650 | best_loss 7.383
2020-10-11 01:55:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:55:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 50 @ 1650 updates, score 7.383) (writing took 4.77413147687912 seconds)
2020-10-11 01:55:31 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2020-10-11 01:55:31 | INFO | train | epoch 050 | loss 7.083 | nll_loss 6.132 | ppl 70.13 | wps 3621.5 | ups 0.74 | wpb 4899.7 | bsz 180.2 | num_updates 1650 | lr 8.25588e-05 | gnorm 1.572 | clip 0 | train_wall 38 | wall 1927
2020-10-11 01:55:31 | INFO | fairseq.trainer | begin training epoch 51
2020-10-11 01:56:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:56:11 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.403 | nll_loss 6.408 | ppl 84.93 | wps 12083.9 | wpb 1984.1 | bsz 74.6 | num_updates 1683 | best_loss 7.383
2020-10-11 01:56:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:56:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt (epoch 51 @ 1683 updates, score 7.403) (writing took 2.3805591017007828 seconds)
2020-10-11 01:56:13 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2020-10-11 01:56:13 | INFO | train | epoch 051 | loss 7.035 | nll_loss 6.077 | ppl 67.49 | wps 3819.8 | ups 0.78 | wpb 4899.7 | bsz 180.2 | num_updates 1683 | lr 8.42079e-05 | gnorm 1.629 | clip 0 | train_wall 38 | wall 1970
2020-10-11 01:56:13 | INFO | fairseq.trainer | begin training epoch 52
2020-10-11 01:56:34 | INFO | train_inner | epoch 052:     17 / 33 loss=7.086, nll_loss=6.135, ppl=70.27, wps=3718.3, ups=0.75, wpb=4926.6, bsz=171.4, num_updates=1700, lr=8.50575e-05, gnorm=1.645, clip=0, train_wall=115, wall=1990
2020-10-11 01:56:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:56:53 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.361 | nll_loss 6.375 | ppl 83 | wps 12033.9 | wpb 1984.1 | bsz 74.6 | num_updates 1716 | best_loss 7.361
2020-10-11 01:56:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:56:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 52 @ 1716 updates, score 7.361) (writing took 4.462854910641909 seconds)
2020-10-11 01:56:58 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2020-10-11 01:56:58 | INFO | train | epoch 052 | loss 6.989 | nll_loss 6.024 | ppl 65.06 | wps 3659.1 | ups 0.75 | wpb 4899.7 | bsz 180.2 | num_updates 1716 | lr 8.58571e-05 | gnorm 1.708 | clip 0 | train_wall 38 | wall 2014
2020-10-11 01:56:58 | INFO | fairseq.trainer | begin training epoch 53
2020-10-11 01:57:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:57:37 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.327 | nll_loss 6.327 | ppl 80.27 | wps 12048.1 | wpb 1984.1 | bsz 74.6 | num_updates 1749 | best_loss 7.327
2020-10-11 01:57:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:57:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 53 @ 1749 updates, score 7.327) (writing took 4.507736701518297 seconds)
2020-10-11 01:57:42 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2020-10-11 01:57:42 | INFO | train | epoch 053 | loss 6.942 | nll_loss 5.97 | ppl 62.67 | wps 3647.4 | ups 0.74 | wpb 4899.7 | bsz 180.2 | num_updates 1749 | lr 8.75063e-05 | gnorm 1.658 | clip 0 | train_wall 38 | wall 2058
2020-10-11 01:57:42 | INFO | fairseq.trainer | begin training epoch 54
2020-10-11 01:58:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:58:22 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 7.355 | nll_loss 6.336 | ppl 80.78 | wps 11769.8 | wpb 1984.1 | bsz 74.6 | num_updates 1782 | best_loss 7.327
2020-10-11 01:58:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:58:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt (epoch 54 @ 1782 updates, score 7.355) (writing took 2.431212794035673 seconds)
2020-10-11 01:58:24 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2020-10-11 01:58:24 | INFO | train | epoch 054 | loss 6.909 | nll_loss 5.932 | ppl 61.04 | wps 3837.2 | ups 0.78 | wpb 4899.7 | bsz 180.2 | num_updates 1782 | lr 8.91555e-05 | gnorm 1.799 | clip 0 | train_wall 38 | wall 2100
2020-10-11 01:58:24 | INFO | fairseq.trainer | begin training epoch 55
2020-10-11 01:58:45 | INFO | train_inner | epoch 055:     18 / 33 loss=6.915, nll_loss=5.939, ppl=61.35, wps=3704.8, ups=0.76, wpb=4876.7, bsz=182.7, num_updates=1800, lr=9.0055e-05, gnorm=1.714, clip=0, train_wall=115, wall=2121
2020-10-11 01:59:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:59:04 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 7.341 | nll_loss 6.321 | ppl 79.97 | wps 12321.1 | wpb 1984.1 | bsz 74.6 | num_updates 1815 | best_loss 7.327
2020-10-11 01:59:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:59:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt (epoch 55 @ 1815 updates, score 7.341) (writing took 2.3294285871088505 seconds)
2020-10-11 01:59:06 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2020-10-11 01:59:06 | INFO | train | epoch 055 | loss 6.857 | nll_loss 5.872 | ppl 58.55 | wps 3829 | ups 0.78 | wpb 4899.7 | bsz 180.2 | num_updates 1815 | lr 9.08046e-05 | gnorm 1.632 | clip 0 | train_wall 38 | wall 2143
2020-10-11 01:59:06 | INFO | fairseq.trainer | begin training epoch 56
2020-10-11 01:59:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 01:59:46 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 7.313 | nll_loss 6.303 | ppl 78.97 | wps 12029.2 | wpb 1984.1 | bsz 74.6 | num_updates 1848 | best_loss 7.313
2020-10-11 01:59:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 01:59:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 56 @ 1848 updates, score 7.313) (writing took 4.642874825745821 seconds)
2020-10-11 01:59:51 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2020-10-11 01:59:51 | INFO | train | epoch 056 | loss 6.814 | nll_loss 5.821 | ppl 56.53 | wps 3625.4 | ups 0.74 | wpb 4899.7 | bsz 180.2 | num_updates 1848 | lr 9.24538e-05 | gnorm 1.698 | clip 0 | train_wall 38 | wall 2187
2020-10-11 01:59:51 | INFO | fairseq.trainer | begin training epoch 57
2020-10-11 02:00:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:00:31 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 7.302 | nll_loss 6.278 | ppl 77.6 | wps 12217.1 | wpb 1984.1 | bsz 74.6 | num_updates 1881 | best_loss 7.302
2020-10-11 02:00:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:00:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 57 @ 1881 updates, score 7.302) (writing took 4.383814070373774 seconds)
2020-10-11 02:00:35 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2020-10-11 02:00:35 | INFO | train | epoch 057 | loss 6.759 | nll_loss 5.759 | ppl 54.14 | wps 3661.8 | ups 0.75 | wpb 4899.7 | bsz 180.2 | num_updates 1881 | lr 9.4103e-05 | gnorm 1.628 | clip 0 | train_wall 38 | wall 2231
2020-10-11 02:00:35 | INFO | fairseq.trainer | begin training epoch 58
2020-10-11 02:00:57 | INFO | train_inner | epoch 058:     19 / 33 loss=6.754, nll_loss=5.753, ppl=53.92, wps=3698.1, ups=0.76, wpb=4890.5, bsz=187.1, num_updates=1900, lr=9.50525e-05, gnorm=1.638, clip=0, train_wall=116, wall=2254
2020-10-11 02:01:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:01:15 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 7.293 | nll_loss 6.266 | ppl 76.95 | wps 12354.8 | wpb 1984.1 | bsz 74.6 | num_updates 1914 | best_loss 7.293
2020-10-11 02:01:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:01:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 58 @ 1914 updates, score 7.293) (writing took 4.5530765280127525 seconds)
2020-10-11 02:01:20 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2020-10-11 02:01:20 | INFO | train | epoch 058 | loss 6.708 | nll_loss 5.7 | ppl 51.98 | wps 3629.7 | ups 0.74 | wpb 4899.7 | bsz 180.2 | num_updates 1914 | lr 9.57522e-05 | gnorm 1.57 | clip 0 | train_wall 38 | wall 2276
2020-10-11 02:01:20 | INFO | fairseq.trainer | begin training epoch 59
2020-10-11 02:01:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:02:00 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 7.329 | nll_loss 6.291 | ppl 78.31 | wps 11906.7 | wpb 1984.1 | bsz 74.6 | num_updates 1947 | best_loss 7.293
2020-10-11 02:02:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:02:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt (epoch 59 @ 1947 updates, score 7.329) (writing took 2.560019690543413 seconds)
2020-10-11 02:02:02 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2020-10-11 02:02:02 | INFO | train | epoch 059 | loss 6.664 | nll_loss 5.648 | ppl 50.15 | wps 3795.8 | ups 0.77 | wpb 4899.7 | bsz 180.2 | num_updates 1947 | lr 9.74013e-05 | gnorm 1.72 | clip 0 | train_wall 38 | wall 2318
2020-10-11 02:02:02 | INFO | fairseq.trainer | begin training epoch 60
2020-10-11 02:02:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:02:42 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 7.29 | nll_loss 6.265 | ppl 76.92 | wps 12318.4 | wpb 1984.1 | bsz 74.6 | num_updates 1980 | best_loss 7.29
2020-10-11 02:02:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:02:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 60 @ 1980 updates, score 7.29) (writing took 7.9532997608184814 seconds)
2020-10-11 02:02:50 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2020-10-11 02:02:50 | INFO | train | epoch 060 | loss 6.677 | nll_loss 5.661 | ppl 50.59 | wps 3396.8 | ups 0.69 | wpb 4899.7 | bsz 180.2 | num_updates 1980 | lr 9.90505e-05 | gnorm 2.005 | clip 0 | train_wall 38 | wall 2366
2020-10-11 02:02:50 | INFO | fairseq.trainer | begin training epoch 61
2020-10-11 02:03:14 | INFO | train_inner | epoch 061:     20 / 33 loss=6.67, nll_loss=5.654, ppl=50.36, wps=3613.1, ups=0.73, wpb=4916.4, bsz=176, num_updates=2000, lr=0.00010005, gnorm=1.738, clip=0, train_wall=116, wall=2390
2020-10-11 02:03:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:03:30 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.282 | nll_loss 6.256 | ppl 76.45 | wps 12191.4 | wpb 1984.1 | bsz 74.6 | num_updates 2013 | best_loss 7.282
2020-10-11 02:03:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:03:36 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 61 @ 2013 updates, score 7.282) (writing took 6.112955313175917 seconds)
2020-10-11 02:03:36 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2020-10-11 02:03:36 | INFO | train | epoch 061 | loss 6.579 | nll_loss 5.55 | ppl 46.85 | wps 3523.3 | ups 0.72 | wpb 4899.7 | bsz 180.2 | num_updates 2013 | lr 0.0001007 | gnorm 1.522 | clip 0 | train_wall 38 | wall 2412
2020-10-11 02:03:36 | INFO | fairseq.trainer | begin training epoch 62
2020-10-11 02:04:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:04:15 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 7.295 | nll_loss 6.272 | ppl 77.26 | wps 12149.4 | wpb 1984.1 | bsz 74.6 | num_updates 2046 | best_loss 7.282
2020-10-11 02:04:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:04:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt (epoch 62 @ 2046 updates, score 7.295) (writing took 5.090347435325384 seconds)
2020-10-11 02:04:20 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2020-10-11 02:04:20 | INFO | train | epoch 062 | loss 6.531 | nll_loss 5.494 | ppl 45.06 | wps 3623.5 | ups 0.74 | wpb 4899.7 | bsz 180.2 | num_updates 2046 | lr 0.000102349 | gnorm 1.592 | clip 0 | train_wall 38 | wall 2457
2020-10-11 02:04:20 | INFO | fairseq.trainer | begin training epoch 63
2020-10-11 02:04:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:04:59 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 7.278 | nll_loss 6.244 | ppl 75.82 | wps 12409.9 | wpb 1984.1 | bsz 74.6 | num_updates 2079 | best_loss 7.278
2020-10-11 02:04:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:05:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 63 @ 2079 updates, score 7.278) (writing took 6.417782675474882 seconds)
2020-10-11 02:05:06 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2020-10-11 02:05:06 | INFO | train | epoch 063 | loss 6.507 | nll_loss 5.466 | ppl 44.21 | wps 3556 | ups 0.73 | wpb 4899.7 | bsz 180.2 | num_updates 2079 | lr 0.000103998 | gnorm 1.929 | clip 0 | train_wall 37 | wall 2502
2020-10-11 02:05:06 | INFO | fairseq.trainer | begin training epoch 64
2020-10-11 02:05:31 | INFO | train_inner | epoch 064:     21 / 33 loss=6.506, nll_loss=5.466, ppl=44.19, wps=3549.2, ups=0.73, wpb=4861.3, bsz=184.6, num_updates=2100, lr=0.000105048, gnorm=1.722, clip=0, train_wall=114, wall=2527
2020-10-11 02:05:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:05:46 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 7.297 | nll_loss 6.244 | ppl 75.78 | wps 12485.3 | wpb 1984.1 | bsz 74.6 | num_updates 2112 | best_loss 7.278
2020-10-11 02:05:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:05:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt (epoch 64 @ 2112 updates, score 7.297) (writing took 3.8481718599796295 seconds)
2020-10-11 02:05:50 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2020-10-11 02:05:50 | INFO | train | epoch 064 | loss 6.455 | nll_loss 5.406 | ppl 42.39 | wps 3695.5 | ups 0.75 | wpb 4899.7 | bsz 180.2 | num_updates 2112 | lr 0.000105647 | gnorm 1.724 | clip 0 | train_wall 38 | wall 2546
2020-10-11 02:05:50 | INFO | fairseq.trainer | begin training epoch 65
2020-10-11 02:06:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:06:29 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 7.262 | nll_loss 6.226 | ppl 74.84 | wps 12244.7 | wpb 1984.1 | bsz 74.6 | num_updates 2145 | best_loss 7.262
2020-10-11 02:06:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:06:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 65 @ 2145 updates, score 7.262) (writing took 6.413747314363718 seconds)
2020-10-11 02:06:35 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2020-10-11 02:06:35 | INFO | train | epoch 065 | loss 6.397 | nll_loss 5.339 | ppl 40.46 | wps 3530.1 | ups 0.72 | wpb 4899.7 | bsz 180.2 | num_updates 2145 | lr 0.000107296 | gnorm 1.659 | clip 0 | train_wall 38 | wall 2592
2020-10-11 02:06:35 | INFO | fairseq.trainer | begin training epoch 66
2020-10-11 02:07:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:07:15 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 7.299 | nll_loss 6.26 | ppl 76.64 | wps 11823.2 | wpb 1984.1 | bsz 74.6 | num_updates 2178 | best_loss 7.262
2020-10-11 02:07:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:07:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt (epoch 66 @ 2178 updates, score 7.299) (writing took 3.0336674712598324 seconds)
2020-10-11 02:07:18 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2020-10-11 02:07:18 | INFO | train | epoch 066 | loss 6.349 | nll_loss 5.284 | ppl 38.96 | wps 3775.3 | ups 0.77 | wpb 4899.7 | bsz 180.2 | num_updates 2178 | lr 0.000108946 | gnorm 1.725 | clip 0 | train_wall 38 | wall 2634
2020-10-11 02:07:18 | INFO | fairseq.trainer | begin training epoch 67
2020-10-11 02:07:44 | INFO | train_inner | epoch 067:     22 / 33 loss=6.372, nll_loss=5.31, ppl=39.67, wps=3703.8, ups=0.75, wpb=4938.5, bsz=178.3, num_updates=2200, lr=0.000110045, gnorm=1.78, clip=0, train_wall=115, wall=2660
2020-10-11 02:07:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:07:58 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 7.255 | nll_loss 6.191 | ppl 73.08 | wps 12263.1 | wpb 1984.1 | bsz 74.6 | num_updates 2211 | best_loss 7.255
2020-10-11 02:07:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:08:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 67 @ 2211 updates, score 7.255) (writing took 8.003836754709482 seconds)
2020-10-11 02:08:06 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2020-10-11 02:08:06 | INFO | train | epoch 067 | loss 6.329 | nll_loss 5.261 | ppl 38.34 | wps 3383.4 | ups 0.69 | wpb 4899.7 | bsz 180.2 | num_updates 2211 | lr 0.000110595 | gnorm 2.003 | clip 0 | train_wall 38 | wall 2682
2020-10-11 02:08:06 | INFO | fairseq.trainer | begin training epoch 68
2020-10-11 02:08:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:08:46 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 7.243 | nll_loss 6.206 | ppl 73.85 | wps 12038.7 | wpb 1984.1 | bsz 74.6 | num_updates 2244 | best_loss 7.243
2020-10-11 02:08:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:08:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 68 @ 2244 updates, score 7.243) (writing took 4.492541775107384 seconds)
2020-10-11 02:08:50 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2020-10-11 02:08:50 | INFO | train | epoch 068 | loss 6.255 | nll_loss 5.174 | ppl 36.11 | wps 3649.6 | ups 0.74 | wpb 4899.7 | bsz 180.2 | num_updates 2244 | lr 0.000112244 | gnorm 1.643 | clip 0 | train_wall 38 | wall 2727
2020-10-11 02:08:50 | INFO | fairseq.trainer | begin training epoch 69
2020-10-11 02:09:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:09:30 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 7.258 | nll_loss 6.203 | ppl 73.69 | wps 11843.9 | wpb 1984.1 | bsz 74.6 | num_updates 2277 | best_loss 7.243
2020-10-11 02:09:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:09:36 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt (epoch 69 @ 2277 updates, score 7.258) (writing took 6.036374177783728 seconds)
2020-10-11 02:09:36 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2020-10-11 02:09:36 | INFO | train | epoch 069 | loss 6.21 | nll_loss 5.123 | ppl 34.85 | wps 3539.5 | ups 0.72 | wpb 4899.7 | bsz 180.2 | num_updates 2277 | lr 0.000113893 | gnorm 1.712 | clip 0 | train_wall 38 | wall 2772
2020-10-11 02:09:36 | INFO | fairseq.trainer | begin training epoch 70
2020-10-11 02:10:03 | INFO | train_inner | epoch 070:     23 / 33 loss=6.24, nll_loss=5.156, ppl=35.66, wps=3523.8, ups=0.72, wpb=4906.5, bsz=175.3, num_updates=2300, lr=0.000115043, gnorm=1.699, clip=0, train_wall=115, wall=2799
2020-10-11 02:10:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:10:16 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 7.249 | nll_loss 6.18 | ppl 72.52 | wps 12011.2 | wpb 1984.1 | bsz 74.6 | num_updates 2310 | best_loss 7.243
2020-10-11 02:10:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:10:20 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt (epoch 70 @ 2310 updates, score 7.249) (writing took 3.6595334373414516 seconds)
2020-10-11 02:10:20 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2020-10-11 02:10:20 | INFO | train | epoch 070 | loss 6.156 | nll_loss 5.06 | ppl 33.35 | wps 3715.7 | ups 0.76 | wpb 4899.7 | bsz 180.2 | num_updates 2310 | lr 0.000115542 | gnorm 1.643 | clip 0 | train_wall 38 | wall 2816
2020-10-11 02:10:20 | INFO | fairseq.trainer | begin training epoch 71
2020-10-11 02:10:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:11:00 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 7.274 | nll_loss 6.225 | ppl 74.81 | wps 12490.4 | wpb 1984.1 | bsz 74.6 | num_updates 2343 | best_loss 7.243
2020-10-11 02:11:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:11:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt (epoch 71 @ 2343 updates, score 7.274) (writing took 2.4829193092882633 seconds)
2020-10-11 02:11:02 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2020-10-11 02:11:02 | INFO | train | epoch 071 | loss 6.118 | nll_loss 5.015 | ppl 32.34 | wps 3801.3 | ups 0.78 | wpb 4899.7 | bsz 180.2 | num_updates 2343 | lr 0.000117191 | gnorm 1.765 | clip 0 | train_wall 38 | wall 2858
2020-10-11 02:11:02 | INFO | fairseq.trainer | begin training epoch 72
2020-10-11 02:11:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:11:42 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 7.237 | nll_loss 6.165 | ppl 71.78 | wps 12126.9 | wpb 1984.1 | bsz 74.6 | num_updates 2376 | best_loss 7.237
2020-10-11 02:11:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:11:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 72 @ 2376 updates, score 7.237) (writing took 7.904061537235975 seconds)
2020-10-11 02:11:50 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2020-10-11 02:11:50 | INFO | train | epoch 072 | loss 6.071 | nll_loss 4.96 | ppl 31.13 | wps 3384.7 | ups 0.69 | wpb 4899.7 | bsz 180.2 | num_updates 2376 | lr 0.000118841 | gnorm 1.771 | clip 0 | train_wall 38 | wall 2906
2020-10-11 02:11:50 | INFO | fairseq.trainer | begin training epoch 73
2020-10-11 02:12:18 | INFO | train_inner | epoch 073:     24 / 33 loss=6.082, nll_loss=4.974, ppl=31.42, wps=3624.1, ups=0.74, wpb=4889.9, bsz=182.5, num_updates=2400, lr=0.00012004, gnorm=1.749, clip=0, train_wall=116, wall=2934
2020-10-11 02:12:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:12:30 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 7.274 | nll_loss 6.23 | ppl 75.09 | wps 12307.7 | wpb 1984.1 | bsz 74.6 | num_updates 2409 | best_loss 7.237
2020-10-11 02:12:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:12:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt (epoch 73 @ 2409 updates, score 7.274) (writing took 3.629307236522436 seconds)
2020-10-11 02:12:33 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2020-10-11 02:12:33 | INFO | train | epoch 073 | loss 6.02 | nll_loss 4.901 | ppl 29.87 | wps 3717.1 | ups 0.76 | wpb 4899.7 | bsz 180.2 | num_updates 2409 | lr 0.00012049 | gnorm 1.794 | clip 0 | train_wall 38 | wall 2950
2020-10-11 02:12:33 | INFO | fairseq.trainer | begin training epoch 74
2020-10-11 02:13:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:13:13 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 7.25 | nll_loss 6.167 | ppl 71.87 | wps 12280.7 | wpb 1984.1 | bsz 74.6 | num_updates 2442 | best_loss 7.237
2020-10-11 02:13:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:13:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt (epoch 74 @ 2442 updates, score 7.25) (writing took 2.426472205668688 seconds)
2020-10-11 02:13:15 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2020-10-11 02:13:15 | INFO | train | epoch 074 | loss 5.974 | nll_loss 4.848 | ppl 28.8 | wps 3844 | ups 0.78 | wpb 4899.7 | bsz 180.2 | num_updates 2442 | lr 0.000122139 | gnorm 1.849 | clip 0 | train_wall 38 | wall 2992
2020-10-11 02:13:15 | INFO | fairseq.trainer | begin training epoch 75
2020-10-11 02:13:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:13:55 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 7.23 | nll_loss 6.149 | ppl 70.94 | wps 12379.1 | wpb 1984.1 | bsz 74.6 | num_updates 2475 | best_loss 7.23
2020-10-11 02:13:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:14:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 75 @ 2475 updates, score 7.23) (writing took 8.005134027451277 seconds)
2020-10-11 02:14:03 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2020-10-11 02:14:03 | INFO | train | epoch 075 | loss 5.927 | nll_loss 4.793 | ppl 27.72 | wps 3386 | ups 0.69 | wpb 4899.7 | bsz 180.2 | num_updates 2475 | lr 0.000123788 | gnorm 1.893 | clip 0 | train_wall 38 | wall 3039
2020-10-11 02:14:03 | INFO | fairseq.trainer | begin training epoch 76
2020-10-11 02:14:32 | INFO | train_inner | epoch 076:     25 / 33 loss=5.921, nll_loss=4.786, ppl=27.59, wps=3631.5, ups=0.74, wpb=4874.6, bsz=181.7, num_updates=2500, lr=0.000125037, gnorm=1.848, clip=0, train_wall=115, wall=3069
2020-10-11 02:14:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:14:43 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 7.238 | nll_loss 6.165 | ppl 71.73 | wps 12105.6 | wpb 1984.1 | bsz 74.6 | num_updates 2508 | best_loss 7.23
2020-10-11 02:14:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:14:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt (epoch 76 @ 2508 updates, score 7.238) (writing took 3.450515303760767 seconds)
2020-10-11 02:14:46 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2020-10-11 02:14:46 | INFO | train | epoch 076 | loss 5.873 | nll_loss 4.73 | ppl 26.55 | wps 3739.2 | ups 0.76 | wpb 4899.7 | bsz 180.2 | num_updates 2508 | lr 0.000125437 | gnorm 1.805 | clip 0 | train_wall 38 | wall 3083
2020-10-11 02:14:46 | INFO | fairseq.trainer | begin training epoch 77
2020-10-11 02:15:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:15:26 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 7.252 | nll_loss 6.189 | ppl 72.97 | wps 12113.6 | wpb 1984.1 | bsz 74.6 | num_updates 2541 | best_loss 7.23
2020-10-11 02:15:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:15:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt (epoch 77 @ 2541 updates, score 7.252) (writing took 5.738367095589638 seconds)
2020-10-11 02:15:32 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2020-10-11 02:15:32 | INFO | train | epoch 077 | loss 5.83 | nll_loss 4.682 | ppl 25.66 | wps 3569.7 | ups 0.73 | wpb 4899.7 | bsz 180.2 | num_updates 2541 | lr 0.000127086 | gnorm 1.862 | clip 0 | train_wall 38 | wall 3128
2020-10-11 02:15:32 | INFO | fairseq.trainer | begin training epoch 78
2020-10-11 02:16:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:16:12 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 7.241 | nll_loss 6.151 | ppl 71.04 | wps 12021.8 | wpb 1984.1 | bsz 74.6 | num_updates 2574 | best_loss 7.23
2020-10-11 02:16:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:16:17 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt (epoch 78 @ 2574 updates, score 7.241) (writing took 4.9272132478654385 seconds)
2020-10-11 02:16:17 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2020-10-11 02:16:17 | INFO | train | epoch 078 | loss 5.808 | nll_loss 4.652 | ppl 25.15 | wps 3607.5 | ups 0.74 | wpb 4899.7 | bsz 180.2 | num_updates 2574 | lr 0.000128736 | gnorm 2.105 | clip 0 | train_wall 38 | wall 3173
2020-10-11 02:16:17 | INFO | fairseq.trainer | begin training epoch 79
2020-10-11 02:16:47 | INFO | train_inner | epoch 079:     26 / 33 loss=5.798, nll_loss=4.643, ppl=24.98, wps=3641.3, ups=0.74, wpb=4892.5, bsz=182.2, num_updates=2600, lr=0.000130035, gnorm=1.914, clip=0, train_wall=115, wall=3203
2020-10-11 02:16:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:16:56 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 7.227 | nll_loss 6.15 | ppl 70.99 | wps 11999.2 | wpb 1984.1 | bsz 74.6 | num_updates 2607 | best_loss 7.227
2020-10-11 02:16:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:17:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_best.pt (epoch 79 @ 2607 updates, score 7.227) (writing took 4.237317658960819 seconds)
2020-10-11 02:17:00 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2020-10-11 02:17:00 | INFO | train | epoch 079 | loss 5.724 | nll_loss 4.557 | ppl 23.54 | wps 3679.2 | ups 0.75 | wpb 4899.7 | bsz 180.2 | num_updates 2607 | lr 0.000130385 | gnorm 1.764 | clip 0 | train_wall 38 | wall 3217
2020-10-11 02:17:01 | INFO | fairseq.trainer | begin training epoch 80
2020-10-11 02:17:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 02:17:40 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 7.257 | nll_loss 6.182 | ppl 72.61 | wps 12367.7 | wpb 1984.1 | bsz 74.6 | num_updates 2640 | best_loss 7.227
2020-10-11 02:17:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 02:17:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_aze_spm8000/aze_eng/checkpoint_last.pt (epoch 80 @ 2640 updates, score 7.257) (writing took 6.390612568706274 seconds)
2020-10-11 02:17:47 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2020-10-11 02:17:47 | INFO | train | epoch 080 | loss 5.672 | nll_loss 4.497 | ppl 22.58 | wps 3503 | ups 0.71 | wpb 4899.7 | bsz 180.2 | num_updates 2640 | lr 0.000132034 | gnorm 1.709 | clip 0 | train_wall 38 | wall 3263
2020-10-11 02:17:47 | INFO | fairseq_cli.train | done training in 3261.7 seconds
2020-10-13 10:59:07 | INFO | fairseq_cli.train | Namespace(activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='mbart_large', attention_dropout=0.1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_aze_spm8000/aze_eng/', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=12, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=0, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_interval_updates=10, keep_last_epochs=-1, label_smoothing=0.2, langs='ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN', layernorm_embedding=True, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=1024, max_tokens_valid=1024, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, prepend_bos=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='models/mbart.cc25', save_dir='fairseq/checkpoints/ted_aze_spm8000/aze_eng/', save_interval=1, save_interval_updates=5000, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_from_pretrained_bart', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update=40000, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=2500, weight_decay=0.0, zero_sharding='none')
2020-10-13 10:59:07 | INFO | fairseq.tasks.translation | [aze] dictionary: 7728 types
2020-10-13 10:59:07 | INFO | fairseq.tasks.translation | [eng] dictionary: 7728 types
2020-10-13 10:59:07 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_aze_spm8000/aze_eng/valid.aze-eng.aze
2020-10-13 10:59:07 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_aze_spm8000/aze_eng/valid.aze-eng.eng
2020-10-13 10:59:07 | INFO | fairseq.tasks.translation | fairseq/data-bin/ted_aze_spm8000/aze_eng/ valid aze-eng 671 examples
2020-10-13 10:59:17 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(7754, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(7754, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=7754, bias=False)
  )
  (classification_heads): ModuleDict()
)
2020-10-13 10:59:17 | INFO | fairseq_cli.train | task: translation_from_pretrained_bart (TranslationFromPretrainedBARTTask)
2020-10-13 10:59:17 | INFO | fairseq_cli.train | model: mbart_large (BARTModel)
2020-10-13 10:59:17 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-13 10:59:17 | INFO | fairseq_cli.train | num. model params: 362764288 (num. trained: 362764288)
2020-10-13 10:59:17 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-13 10:59:17 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-13 10:59:17 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-13 10:59:17 | INFO | fairseq_cli.train | max tokens per GPU = 1024 and max sentences per GPU = None
2020-10-13 10:59:17 | INFO | fairseq.trainer | no existing checkpoint found models/mbart.cc25
2020-10-13 10:59:17 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-13 10:59:17 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_aze_spm8000/aze_eng/train.aze-eng.aze
2020-10-13 10:59:17 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_aze_spm8000/aze_eng/train.aze-eng.eng
2020-10-13 10:59:17 | INFO | fairseq.tasks.translation | fairseq/data-bin/ted_aze_spm8000/aze_eng/ train aze-eng 5946 examples
2020-10-13 10:59:17 | INFO | fairseq.trainer | begin training epoch 1
Traceback (most recent call last):
  File "/home/ubuntu/anaconda3/envs/torch16/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/home/ubuntu/courses/fairseq/fairseq_cli/train.py", line 352, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/ubuntu/courses/fairseq/fairseq/distributed_utils.py", line 258, in call_main
    main(args, **kwargs)
  File "/home/ubuntu/courses/fairseq/fairseq_cli/train.py", line 125, in main
    valid_losses, should_stop = train(args, trainer, task, epoch_itr)
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/ubuntu/courses/fairseq/fairseq_cli/train.py", line 208, in train
    log_output = trainer.train_step(samples)
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/ubuntu/courses/fairseq/fairseq/trainer.py", line 471, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/home/ubuntu/courses/fairseq/fairseq/tasks/fairseq_task.py", line 411, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/courses/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 56, in forward
    net_output = model(**sample['net_input'])
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/courses/fairseq/fairseq/models/bart/model.py", line 74, in forward
    encoder_out = self.encoder(
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/courses/fairseq/fairseq/models/transformer.py", line 410, in forward
    x = layer(x, encoder_padding_mask)
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/courses/fairseq/fairseq/modules/transformer_layer.py", line 134, in forward
    x = self.fc2(x)
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 91, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/site-packages/torch/nn/functional.py", line 1676, in linear
    output = input.matmul(weight.t())
KeyboardInterrupt
