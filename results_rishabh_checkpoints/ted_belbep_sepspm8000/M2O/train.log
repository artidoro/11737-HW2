Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.
	Try to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.
Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.
	Try to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.
Traceback (most recent call last):
  File "/home/ubuntu/anaconda3/envs/torch16/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/home/ubuntu/courses/fairseq/fairseq_cli/train.py", line 352, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/ubuntu/courses/fairseq/fairseq/distributed_utils.py", line 241, in call_main
    torch.multiprocessing.spawn(
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/ubuntu/anaconda3/envs/torch16/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 111, in join
    raise Exception(
Exception: process 1 terminated with exit code 1
2020-10-13 06:47:13 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_belbep_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='bel-eng,bep-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_belbep_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-13 06:47:13 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-13 06:47:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['bel', 'bep', 'eng']
2020-10-13 06:47:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bel] dictionary: 21171 types
2020-10-13 06:47:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bep] dictionary: 21171 types
2020-10-13 06:47:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 21171 types
2020-10-13 06:47:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-13 06:47:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7686.44140625Mb; avail=236867.8515625Mb
2020-10-13 06:47:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-13 06:47:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:bel-eng': 1, 'main:bep-eng': 1}
2020-10-13 06:47:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:47:13 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belbep_sepspm8000/M2O/valid.bel-eng.bel
2020-10-13 06:47:13 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belbep_sepspm8000/M2O/valid.bel-eng.eng
2020-10-13 06:47:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belbep_sepspm8000/M2O/ valid bel-eng 248 examples
2020-10-13 06:47:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bep-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:47:13 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belbep_sepspm8000/M2O/valid.bep-eng.bep
2020-10-13 06:47:13 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belbep_sepspm8000/M2O/valid.bep-eng.eng
2020-10-13 06:47:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belbep_sepspm8000/M2O/ valid bep-eng 248 examples
2020-10-13 06:47:14 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21171, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21171, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=21171, bias=False)
  )
)
2020-10-13 06:47:14 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-13 06:47:14 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-13 06:47:14 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-13 06:47:14 | INFO | fairseq_cli.train | num. model params: 42382848 (num. trained: 42382848)
2020-10-13 06:47:18 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-13 06:47:18 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-13 06:47:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-13 06:47:18 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-13 06:47:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-13 06:47:18 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-13 06:47:18 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-13 06:47:18 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt
2020-10-13 06:47:18 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-13 06:47:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-13 06:47:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9977.421875Mb; avail=234566.95703125Mb
2020-10-13 06:47:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-13 06:47:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:bel-eng': 1, 'main:bep-eng': 1}
2020-10-13 06:47:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:47:18 | INFO | fairseq.data.data_utils | loaded 4508 examples from: fairseq/data-bin/ted_belbep_sepspm8000/M2O/train.bel-eng.bel
2020-10-13 06:47:18 | INFO | fairseq.data.data_utils | loaded 4508 examples from: fairseq/data-bin/ted_belbep_sepspm8000/M2O/train.bel-eng.eng
2020-10-13 06:47:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belbep_sepspm8000/M2O/ train bel-eng 4508 examples
2020-10-13 06:47:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bep-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:47:18 | INFO | fairseq.data.data_utils | loaded 4506 examples from: fairseq/data-bin/ted_belbep_sepspm8000/M2O/train.bep-eng.bep
2020-10-13 06:47:18 | INFO | fairseq.data.data_utils | loaded 4506 examples from: fairseq/data-bin/ted_belbep_sepspm8000/M2O/train.bep-eng.eng
2020-10-13 06:47:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belbep_sepspm8000/M2O/ train bep-eng 4506 examples
2020-10-13 06:47:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:bel-eng', 4508), ('main:bep-eng', 4506)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-13 06:47:18 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-13 06:47:18 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 9014
2020-10-13 06:47:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 9014; virtual dataset size 9014
2020-10-13 06:47:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-13 06:47:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:bel-eng': 4508, 'main:bep-eng': 4506}; raw total size: 9014
2020-10-13 06:47:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:bel-eng': 4508, 'main:bep-eng': 4506}; resampled total size: 9014
2020-10-13 06:47:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-13 06:47:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.001616
2020-10-13 06:47:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9977.421875Mb; avail=234566.95703125Mb
2020-10-13 06:47:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000202
2020-10-13 06:47:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002031
2020-10-13 06:47:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9977.421875Mb; avail=234566.95703125Mb
2020-10-13 06:47:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 06:47:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9977.421875Mb; avail=234566.95703125Mb
2020-10-13 06:47:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.036790
2020-10-13 06:47:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.039647
2020-10-13 06:47:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9977.421875Mb; avail=234566.95703125Mb
2020-10-13 06:47:18 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-13 06:47:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9977.421875Mb; avail=234566.95703125Mb
2020-10-13 06:47:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001567
2020-10-13 06:47:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9977.421875Mb; avail=234566.95703125Mb
2020-10-13 06:47:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000072
2020-10-13 06:47:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9977.421875Mb; avail=234566.95703125Mb
2020-10-13 06:47:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.035142
2020-10-13 06:47:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.037508
2020-10-13 06:47:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9977.421875Mb; avail=234566.95703125Mb
2020-10-13 06:47:18 | INFO | fairseq.trainer | begin training epoch 1
2020-10-13 06:47:58 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_belbep_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='bel-eng,bep-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=80, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_belbep_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-13 06:47:58 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-13 06:47:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['bel', 'bep', 'eng']
2020-10-13 06:47:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bel] dictionary: 21171 types
2020-10-13 06:47:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bep] dictionary: 21171 types
2020-10-13 06:47:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 21171 types
2020-10-13 06:47:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-13 06:47:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7685.42578125Mb; avail=236868.85546875Mb
2020-10-13 06:47:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-13 06:47:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:bel-eng': 1, 'main:bep-eng': 1}
2020-10-13 06:47:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:47:58 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belbep_sepspm8000/M2O/valid.bel-eng.bel
2020-10-13 06:47:58 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belbep_sepspm8000/M2O/valid.bel-eng.eng
2020-10-13 06:47:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belbep_sepspm8000/M2O/ valid bel-eng 248 examples
2020-10-13 06:47:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bep-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:47:58 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belbep_sepspm8000/M2O/valid.bep-eng.bep
2020-10-13 06:47:58 | INFO | fairseq.data.data_utils | loaded 248 examples from: fairseq/data-bin/ted_belbep_sepspm8000/M2O/valid.bep-eng.eng
2020-10-13 06:47:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belbep_sepspm8000/M2O/ valid bep-eng 248 examples
2020-10-13 06:47:59 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21171, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(21171, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=21171, bias=False)
  )
)
2020-10-13 06:47:59 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-13 06:47:59 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-13 06:47:59 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-13 06:47:59 | INFO | fairseq_cli.train | num. model params: 42382848 (num. trained: 42382848)
2020-10-13 06:48:02 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-13 06:48:02 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-13 06:48:02 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-13 06:48:02 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.752 GB ; name = Tesla V100-SXM2-16GB                    
2020-10-13 06:48:02 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-13 06:48:02 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-13 06:48:02 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-13 06:48:02 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt
2020-10-13 06:48:02 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-13 06:48:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-13 06:48:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10185.265625Mb; avail=234359.76953125Mb
2020-10-13 06:48:02 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-13 06:48:02 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:bel-eng': 1, 'main:bep-eng': 1}
2020-10-13 06:48:02 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bel-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:48:02 | INFO | fairseq.data.data_utils | loaded 4508 examples from: fairseq/data-bin/ted_belbep_sepspm8000/M2O/train.bel-eng.bel
2020-10-13 06:48:02 | INFO | fairseq.data.data_utils | loaded 4508 examples from: fairseq/data-bin/ted_belbep_sepspm8000/M2O/train.bel-eng.eng
2020-10-13 06:48:02 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belbep_sepspm8000/M2O/ train bel-eng 4508 examples
2020-10-13 06:48:02 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bep-eng src_langtok: None; tgt_langtok: None
2020-10-13 06:48:02 | INFO | fairseq.data.data_utils | loaded 4506 examples from: fairseq/data-bin/ted_belbep_sepspm8000/M2O/train.bep-eng.bep
2020-10-13 06:48:02 | INFO | fairseq.data.data_utils | loaded 4506 examples from: fairseq/data-bin/ted_belbep_sepspm8000/M2O/train.bep-eng.eng
2020-10-13 06:48:02 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_belbep_sepspm8000/M2O/ train bep-eng 4506 examples
2020-10-13 06:48:02 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:bel-eng', 4508), ('main:bep-eng', 4506)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-13 06:48:02 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-13 06:48:02 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 9014
2020-10-13 06:48:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 9014; virtual dataset size 9014
2020-10-13 06:48:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-13 06:48:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:bel-eng': 4508, 'main:bep-eng': 4506}; raw total size: 9014
2020-10-13 06:48:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:bel-eng': 4508, 'main:bep-eng': 4506}; resampled total size: 9014
2020-10-13 06:48:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-13 06:48:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.001609
2020-10-13 06:48:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10185.31640625Mb; avail=234360.0078125Mb
2020-10-13 06:48:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000228
2020-10-13 06:48:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002051
2020-10-13 06:48:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10184.9140625Mb; avail=234360.42578125Mb
2020-10-13 06:48:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 06:48:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10184.78515625Mb; avail=234360.3125Mb
2020-10-13 06:48:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032882
2020-10-13 06:48:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035818
2020-10-13 06:48:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10143.9453125Mb; avail=234400.609375Mb
2020-10-13 06:48:02 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-10-13 06:48:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10144.51171875Mb; avail=234400.55859375Mb
2020-10-13 06:48:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001651
2020-10-13 06:48:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10144.125Mb; avail=234400.21875Mb
2020-10-13 06:48:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 06:48:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10144.125Mb; avail=234400.21875Mb
2020-10-13 06:48:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032207
2020-10-13 06:48:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034755
2020-10-13 06:48:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10144.22265625Mb; avail=234399.9375Mb
2020-10-13 06:48:02 | INFO | fairseq.trainer | begin training epoch 1
2020-10-13 06:48:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10227.57421875Mb; avail=234313.96875Mb
2020-10-13 06:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000871
2020-10-13 06:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10227.57421875Mb; avail=234313.96875Mb
2020-10-13 06:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006972
2020-10-13 06:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10227.57421875Mb; avail=234313.96875Mb
2020-10-13 06:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005336
2020-10-13 06:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013962
2020-10-13 06:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10228.1796875Mb; avail=234313.36328125Mb
2020-10-13 06:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10228.1796875Mb; avail=234313.36328125Mb
2020-10-13 06:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000598
2020-10-13 06:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10228.1796875Mb; avail=234313.36328125Mb
2020-10-13 06:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006872
2020-10-13 06:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10228.1796875Mb; avail=234313.36328125Mb
2020-10-13 06:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005177
2020-10-13 06:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013415
2020-10-13 06:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10228.1796875Mb; avail=234313.36328125Mb
/home/ubuntu/courses/fairseq/fairseq/utils.py:340: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2020-10-13 06:48:17 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.767 | nll_loss 13.619 | ppl 12583.6 | wps 38479.1 | wpb 1971 | bsz 82.7 | num_updates 50
2020-10-13 06:48:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:48:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 1 @ 50 updates, score 13.767) (writing took 1.642355795018375 seconds)
2020-10-13 06:48:19 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-13 06:48:19 | INFO | train | epoch 001 | loss 14.755 | nll_loss 14.718 | ppl 26946.7 | wps 13979.4 | ups 3.12 | wpb 4470.2 | bsz 180.3 | num_updates 50 | lr 2.59875e-06 | gnorm 5.845 | clip 0 | train_wall 14 | wall 17
2020-10-13 06:48:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-13 06:48:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-13 06:48:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10131.8671875Mb; avail=234410.94921875Mb
2020-10-13 06:48:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000222
2020-10-13 06:48:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002193
2020-10-13 06:48:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10131.8671875Mb; avail=234410.94921875Mb
2020-10-13 06:48:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000078
2020-10-13 06:48:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10131.8671875Mb; avail=234410.94921875Mb
2020-10-13 06:48:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032850
2020-10-13 06:48:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035842
2020-10-13 06:48:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10131.75Mb; avail=234411.06640625Mb
2020-10-13 06:48:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10131.734375Mb; avail=234411.08203125Mb
2020-10-13 06:48:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001621
2020-10-13 06:48:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10131.734375Mb; avail=234411.08203125Mb
2020-10-13 06:48:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000068
2020-10-13 06:48:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10131.734375Mb; avail=234411.08203125Mb
2020-10-13 06:48:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.031956
2020-10-13 06:48:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034369
2020-10-13 06:48:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10131.734375Mb; avail=234411.08203125Mb
2020-10-13 06:48:19 | INFO | fairseq.trainer | begin training epoch 2
2020-10-13 06:48:33 | INFO | train_inner | epoch 002:     50 / 50 loss=14.074, nll_loss=13.959, ppl=15922.5, wps=15081.4, ups=3.37, wpb=4470.2, bsz=180.3, num_updates=100, lr=5.0975e-06, gnorm=4.67, clip=0, train_wall=27, wall=30
2020-10-13 06:48:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10151.69140625Mb; avail=234390.125Mb
2020-10-13 06:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000874
2020-10-13 06:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10151.69140625Mb; avail=234390.125Mb
2020-10-13 06:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006765
2020-10-13 06:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10151.69140625Mb; avail=234390.125Mb
2020-10-13 06:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005068
2020-10-13 06:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013467
2020-10-13 06:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10151.69140625Mb; avail=234390.125Mb
2020-10-13 06:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10151.69140625Mb; avail=234390.125Mb
2020-10-13 06:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000587
2020-10-13 06:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10151.69140625Mb; avail=234390.125Mb
2020-10-13 06:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006535
2020-10-13 06:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10151.69140625Mb; avail=234390.125Mb
2020-10-13 06:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005029
2020-10-13 06:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012875
2020-10-13 06:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10151.69140625Mb; avail=234390.125Mb
2020-10-13 06:48:33 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 12.487 | nll_loss 12.179 | ppl 4635.91 | wps 37047.5 | wpb 1971 | bsz 82.7 | num_updates 100 | best_loss 12.487
2020-10-13 06:48:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:48:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 2 @ 100 updates, score 12.487) (writing took 4.725367197999731 seconds)
2020-10-13 06:48:38 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-13 06:48:38 | INFO | train | epoch 002 | loss 13.392 | nll_loss 13.2 | ppl 9408.43 | wps 11848.2 | ups 2.65 | wpb 4470.2 | bsz 180.3 | num_updates 100 | lr 5.0975e-06 | gnorm 3.496 | clip 0 | train_wall 13 | wall 36
2020-10-13 06:48:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-13 06:48:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-13 06:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10131.01171875Mb; avail=234411.06640625Mb
2020-10-13 06:48:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000244
2020-10-13 06:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002213
2020-10-13 06:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10131.01171875Mb; avail=234411.06640625Mb
2020-10-13 06:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 06:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10131.01171875Mb; avail=234411.06640625Mb
2020-10-13 06:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032030
2020-10-13 06:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035060
2020-10-13 06:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10131.01171875Mb; avail=234411.06640625Mb
2020-10-13 06:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10131.01171875Mb; avail=234411.06640625Mb
2020-10-13 06:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001687
2020-10-13 06:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10131.01171875Mb; avail=234411.06640625Mb
2020-10-13 06:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000071
2020-10-13 06:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10131.01171875Mb; avail=234411.06640625Mb
2020-10-13 06:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.031933
2020-10-13 06:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034457
2020-10-13 06:48:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10131.01171875Mb; avail=234411.06640625Mb
2020-10-13 06:48:38 | INFO | fairseq.trainer | begin training epoch 3
2020-10-13 06:48:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10152.51953125Mb; avail=234388.78515625Mb
2020-10-13 06:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000888
2020-10-13 06:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10152.51953125Mb; avail=234388.78515625Mb
2020-10-13 06:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006912
2020-10-13 06:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10152.51953125Mb; avail=234388.78515625Mb
2020-10-13 06:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005095
2020-10-13 06:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013660
2020-10-13 06:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10152.51953125Mb; avail=234388.78515625Mb
2020-10-13 06:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10152.51953125Mb; avail=234388.78515625Mb
2020-10-13 06:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000591
2020-10-13 06:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10152.51953125Mb; avail=234388.78515625Mb
2020-10-13 06:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006683
2020-10-13 06:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10152.51953125Mb; avail=234388.78515625Mb
2020-10-13 06:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005178
2020-10-13 06:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013223
2020-10-13 06:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10152.51953125Mb; avail=234388.78515625Mb
2020-10-13 06:48:52 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 11.921 | nll_loss 11.545 | ppl 2988.58 | wps 38911.2 | wpb 1971 | bsz 82.7 | num_updates 150 | best_loss 11.921
2020-10-13 06:48:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:48:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 3 @ 150 updates, score 11.921) (writing took 4.706738789798692 seconds)
2020-10-13 06:48:57 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-13 06:48:57 | INFO | train | epoch 003 | loss 12.548 | nll_loss 12.257 | ppl 4894.34 | wps 11791.6 | ups 2.64 | wpb 4470.2 | bsz 180.3 | num_updates 150 | lr 7.59625e-06 | gnorm 2.307 | clip 0 | train_wall 13 | wall 55
2020-10-13 06:48:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-13 06:48:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10142.62890625Mb; avail=234398.90234375Mb
2020-10-13 06:48:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000374
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003021
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10142.62890625Mb; avail=234398.90234375Mb
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000109
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10142.62890625Mb; avail=234398.90234375Mb
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.044642
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.048891
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10142.3984375Mb; avail=234399.875Mb
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10132.58203125Mb; avail=234409.69140625Mb
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001636
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10132.58203125Mb; avail=234409.69140625Mb
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10132.58203125Mb; avail=234409.69140625Mb
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032506
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034925
2020-10-13 06:48:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10132.58203125Mb; avail=234409.69140625Mb
2020-10-13 06:48:57 | INFO | fairseq.trainer | begin training epoch 4
2020-10-13 06:49:11 | INFO | train_inner | epoch 004:     50 / 50 loss=12.302, nll_loss=11.983, ppl=4046.83, wps=11794.4, ups=2.64, wpb=4470.2, bsz=180.3, num_updates=200, lr=1.0095e-05, gnorm=2.075, clip=0, train_wall=26, wall=68
2020-10-13 06:49:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10157.95703125Mb; avail=234384.09765625Mb
2020-10-13 06:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000890
2020-10-13 06:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10157.95703125Mb; avail=234384.09765625Mb
2020-10-13 06:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006829
2020-10-13 06:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10158.5625Mb; avail=234383.4921875Mb
2020-10-13 06:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005170
2020-10-13 06:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013663
2020-10-13 06:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10161.58984375Mb; avail=234380.46484375Mb
2020-10-13 06:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10162.08203125Mb; avail=234379.97265625Mb
2020-10-13 06:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000573
2020-10-13 06:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10162.08203125Mb; avail=234379.97265625Mb
2020-10-13 06:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006758
2020-10-13 06:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10162.08203125Mb; avail=234379.97265625Mb
2020-10-13 06:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005039
2020-10-13 06:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013112
2020-10-13 06:49:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10162.08203125Mb; avail=234379.97265625Mb
2020-10-13 06:49:11 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 11.531 | nll_loss 11.108 | ppl 2207.12 | wps 38810.3 | wpb 1971 | bsz 82.7 | num_updates 200 | best_loss 11.531
2020-10-13 06:49:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:49:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 4 @ 200 updates, score 11.531) (writing took 4.715240642894059 seconds)
2020-10-13 06:49:16 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-13 06:49:16 | INFO | train | epoch 004 | loss 12.056 | nll_loss 11.708 | ppl 3346.08 | wps 11812.6 | ups 2.64 | wpb 4470.2 | bsz 180.3 | num_updates 200 | lr 1.0095e-05 | gnorm 1.844 | clip 0 | train_wall 13 | wall 73
2020-10-13 06:49:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-13 06:49:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10132.68359375Mb; avail=234409.3828125Mb
2020-10-13 06:49:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000251
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002220
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10132.68359375Mb; avail=234409.3828125Mb
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10132.68359375Mb; avail=234409.3828125Mb
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032725
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035774
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10132.68359375Mb; avail=234409.3828125Mb
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10135.66015625Mb; avail=234406.40625Mb
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001672
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10135.66015625Mb; avail=234406.40625Mb
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000072
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10135.66015625Mb; avail=234406.40625Mb
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032369
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034850
2020-10-13 06:49:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10140.30859375Mb; avail=234401.7578125Mb
2020-10-13 06:49:16 | INFO | fairseq.trainer | begin training epoch 5
2020-10-13 06:49:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9917.81640625Mb; avail=234624.53125Mb
2020-10-13 06:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000889
2020-10-13 06:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9917.81640625Mb; avail=234624.53125Mb
2020-10-13 06:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006808
2020-10-13 06:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9917.81640625Mb; avail=234624.53125Mb
2020-10-13 06:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005183
2020-10-13 06:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013646
2020-10-13 06:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9917.81640625Mb; avail=234624.53125Mb
2020-10-13 06:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9918.29296875Mb; avail=234624.0546875Mb
2020-10-13 06:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000573
2020-10-13 06:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9918.29296875Mb; avail=234624.0546875Mb
2020-10-13 06:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006882
2020-10-13 06:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9918.29296875Mb; avail=234624.0546875Mb
2020-10-13 06:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005063
2020-10-13 06:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013269
2020-10-13 06:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9918.29296875Mb; avail=234624.0546875Mb
2020-10-13 06:49:30 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 11.16 | nll_loss 10.692 | ppl 1653.99 | wps 38652.5 | wpb 1971 | bsz 82.7 | num_updates 250 | best_loss 11.16
2020-10-13 06:49:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:49:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 5 @ 250 updates, score 11.16) (writing took 4.71301147993654 seconds)
2020-10-13 06:49:35 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-13 06:49:35 | INFO | train | epoch 005 | loss 11.63 | nll_loss 11.233 | ppl 2406.19 | wps 11748.7 | ups 2.63 | wpb 4470.2 | bsz 180.3 | num_updates 250 | lr 1.25938e-05 | gnorm 1.638 | clip 0 | train_wall 13 | wall 92
2020-10-13 06:49:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-13 06:49:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-13 06:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9902.97265625Mb; avail=234639.34375Mb
2020-10-13 06:49:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000248
2020-10-13 06:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002317
2020-10-13 06:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9902.97265625Mb; avail=234639.34375Mb
2020-10-13 06:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000103
2020-10-13 06:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9902.97265625Mb; avail=234639.34375Mb
2020-10-13 06:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032276
2020-10-13 06:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035590
2020-10-13 06:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9902.97265625Mb; avail=234639.34375Mb
2020-10-13 06:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9902.97265625Mb; avail=234639.34375Mb
2020-10-13 06:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001695
2020-10-13 06:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9902.97265625Mb; avail=234639.34375Mb
2020-10-13 06:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000085
2020-10-13 06:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9902.97265625Mb; avail=234639.34375Mb
2020-10-13 06:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.031688
2020-10-13 06:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034324
2020-10-13 06:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9902.97265625Mb; avail=234639.34375Mb
2020-10-13 06:49:35 | INFO | fairseq.trainer | begin training epoch 6
2020-10-13 06:49:48 | INFO | train_inner | epoch 006:     50 / 50 loss=11.402, nll_loss=10.977, ppl=2015.7, wps=11815.1, ups=2.64, wpb=4470.2, bsz=180.3, num_updates=300, lr=1.50925e-05, gnorm=1.594, clip=0, train_wall=26, wall=106
2020-10-13 06:49:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:49:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9924.50390625Mb; avail=234617.77734375Mb
2020-10-13 06:49:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000913
2020-10-13 06:49:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9924.50390625Mb; avail=234617.77734375Mb
2020-10-13 06:49:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006773
2020-10-13 06:49:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9924.50390625Mb; avail=234617.77734375Mb
2020-10-13 06:49:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005152
2020-10-13 06:49:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013617
2020-10-13 06:49:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9924.50390625Mb; avail=234617.77734375Mb
2020-10-13 06:49:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9924.50390625Mb; avail=234617.77734375Mb
2020-10-13 06:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000568
2020-10-13 06:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9924.50390625Mb; avail=234617.77734375Mb
2020-10-13 06:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006766
2020-10-13 06:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9924.50390625Mb; avail=234617.77734375Mb
2020-10-13 06:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005037
2020-10-13 06:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013114
2020-10-13 06:49:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9924.50390625Mb; avail=234617.77734375Mb
2020-10-13 06:49:49 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 10.673 | nll_loss 10.135 | ppl 1124.15 | wps 38886.5 | wpb 1971 | bsz 82.7 | num_updates 300 | best_loss 10.673
2020-10-13 06:49:49 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:49:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 6 @ 300 updates, score 10.673) (writing took 4.690472020069137 seconds)
2020-10-13 06:49:54 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-13 06:49:54 | INFO | train | epoch 006 | loss 11.174 | nll_loss 10.722 | ppl 1688.57 | wps 11895 | ups 2.66 | wpb 4470.2 | bsz 180.3 | num_updates 300 | lr 1.50925e-05 | gnorm 1.551 | clip 0 | train_wall 13 | wall 111
2020-10-13 06:49:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-13 06:49:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-13 06:49:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9906.9375Mb; avail=234635.3125Mb
2020-10-13 06:49:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000225
2020-10-13 06:49:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002206
2020-10-13 06:49:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9906.9375Mb; avail=234635.3125Mb
2020-10-13 06:49:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 06:49:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9906.9375Mb; avail=234635.3125Mb
2020-10-13 06:49:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032901
2020-10-13 06:49:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035956
2020-10-13 06:49:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9906.9375Mb; avail=234635.3125Mb
2020-10-13 06:49:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9906.9375Mb; avail=234635.3125Mb
2020-10-13 06:49:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001688
2020-10-13 06:49:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9906.9375Mb; avail=234635.3125Mb
2020-10-13 06:49:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 06:49:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9906.9375Mb; avail=234635.3125Mb
2020-10-13 06:49:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032368
2020-10-13 06:49:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034856
2020-10-13 06:49:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9906.9375Mb; avail=234635.3125Mb
2020-10-13 06:49:54 | INFO | fairseq.trainer | begin training epoch 7
2020-10-13 06:50:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9929.58984375Mb; avail=234612.3671875Mb
2020-10-13 06:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000891
2020-10-13 06:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9929.58984375Mb; avail=234612.3671875Mb
2020-10-13 06:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006815
2020-10-13 06:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9929.58984375Mb; avail=234612.3671875Mb
2020-10-13 06:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005111
2020-10-13 06:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013594
2020-10-13 06:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9929.58984375Mb; avail=234612.3671875Mb
2020-10-13 06:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9929.58984375Mb; avail=234612.3671875Mb
2020-10-13 06:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000633
2020-10-13 06:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9929.58984375Mb; avail=234612.3671875Mb
2020-10-13 06:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006543
2020-10-13 06:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9929.58984375Mb; avail=234612.3671875Mb
2020-10-13 06:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005089
2020-10-13 06:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013004
2020-10-13 06:50:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9929.58984375Mb; avail=234612.3671875Mb
2020-10-13 06:50:08 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 10.024 | nll_loss 9.376 | ppl 664.33 | wps 38733 | wpb 1971 | bsz 82.7 | num_updates 350 | best_loss 10.024
2020-10-13 06:50:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:50:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 7 @ 350 updates, score 10.024) (writing took 4.695156814996153 seconds)
2020-10-13 06:50:12 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-13 06:50:12 | INFO | train | epoch 007 | loss 10.566 | nll_loss 10.033 | ppl 1047.79 | wps 11856.5 | ups 2.65 | wpb 4470.2 | bsz 180.3 | num_updates 350 | lr 1.75912e-05 | gnorm 1.872 | clip 0 | train_wall 13 | wall 130
2020-10-13 06:50:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-13 06:50:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-13 06:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7154.47265625Mb; avail=237400.01171875Mb
2020-10-13 06:50:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000491
2020-10-13 06:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003844
2020-10-13 06:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7154.47265625Mb; avail=237400.01171875Mb
2020-10-13 06:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000179
2020-10-13 06:50:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7154.47265625Mb; avail=237400.01171875Mb
2020-10-13 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.066073
2020-10-13 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.071442
2020-10-13 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7157.390625Mb; avail=237397.09375Mb
2020-10-13 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7158.5859375Mb; avail=237395.8984375Mb
2020-10-13 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002420
2020-10-13 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7158.5859375Mb; avail=237395.8984375Mb
2020-10-13 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000115
2020-10-13 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7158.5859375Mb; avail=237395.8984375Mb
2020-10-13 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.064912
2020-10-13 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.068621
2020-10-13 06:50:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7167.046875Mb; avail=237387.4375Mb
2020-10-13 06:50:13 | INFO | fairseq.trainer | begin training epoch 8
2020-10-13 06:50:27 | INFO | train_inner | epoch 008:     50 / 50 loss=10.277, nll_loss=9.697, ppl=829.96, wps=11694.7, ups=2.62, wpb=4470.2, bsz=180.3, num_updates=400, lr=2.009e-05, gnorm=1.785, clip=0, train_wall=26, wall=144
2020-10-13 06:50:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9915.01953125Mb; avail=234627.09375Mb
2020-10-13 06:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000872
2020-10-13 06:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9915.01953125Mb; avail=234627.09375Mb
2020-10-13 06:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006690
2020-10-13 06:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9915.01953125Mb; avail=234627.09375Mb
2020-10-13 06:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005048
2020-10-13 06:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013372
2020-10-13 06:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9915.01953125Mb; avail=234627.09375Mb
2020-10-13 06:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9915.01953125Mb; avail=234627.09375Mb
2020-10-13 06:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000595
2020-10-13 06:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9915.01953125Mb; avail=234627.09375Mb
2020-10-13 06:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006669
2020-10-13 06:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9916.1171875Mb; avail=234625.99609375Mb
2020-10-13 06:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005067
2020-10-13 06:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013058
2020-10-13 06:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9916.72265625Mb; avail=234625.390625Mb
2020-10-13 06:50:27 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.624 | nll_loss 8.893 | ppl 475.41 | wps 38570.1 | wpb 1971 | bsz 82.7 | num_updates 400 | best_loss 9.624
2020-10-13 06:50:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:50:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 8 @ 400 updates, score 9.624) (writing took 4.716538564069197 seconds)
2020-10-13 06:50:32 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-13 06:50:32 | INFO | train | epoch 008 | loss 9.988 | nll_loss 9.361 | ppl 657.42 | wps 11479.3 | ups 2.57 | wpb 4470.2 | bsz 180.3 | num_updates 400 | lr 2.009e-05 | gnorm 1.698 | clip 0 | train_wall 13 | wall 150
2020-10-13 06:50:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-13 06:50:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-13 06:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9898.0078125Mb; avail=234644.265625Mb
2020-10-13 06:50:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000224
2020-10-13 06:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002236
2020-10-13 06:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9898.0078125Mb; avail=234644.265625Mb
2020-10-13 06:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 06:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9898.0078125Mb; avail=234644.265625Mb
2020-10-13 06:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032429
2020-10-13 06:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035486
2020-10-13 06:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9898.0078125Mb; avail=234644.265625Mb
2020-10-13 06:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9898.0078125Mb; avail=234644.265625Mb
2020-10-13 06:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001607
2020-10-13 06:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9898.0078125Mb; avail=234644.265625Mb
2020-10-13 06:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 06:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9898.0078125Mb; avail=234644.265625Mb
2020-10-13 06:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032267
2020-10-13 06:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034679
2020-10-13 06:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9898.0078125Mb; avail=234644.265625Mb
2020-10-13 06:50:32 | INFO | fairseq.trainer | begin training epoch 9
2020-10-13 06:50:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:50:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9919.19140625Mb; avail=234622.8203125Mb
2020-10-13 06:50:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000887
2020-10-13 06:50:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9919.19140625Mb; avail=234622.8203125Mb
2020-10-13 06:50:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006830
2020-10-13 06:50:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9919.19140625Mb; avail=234622.8203125Mb
2020-10-13 06:50:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005065
2020-10-13 06:50:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013545
2020-10-13 06:50:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9919.19140625Mb; avail=234622.8203125Mb
2020-10-13 06:50:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9919.19140625Mb; avail=234622.8203125Mb
2020-10-13 06:50:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000566
2020-10-13 06:50:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9919.19140625Mb; avail=234622.8203125Mb
2020-10-13 06:50:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006690
2020-10-13 06:50:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9919.19140625Mb; avail=234622.8203125Mb
2020-10-13 06:50:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005106
2020-10-13 06:50:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013126
2020-10-13 06:50:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9919.19140625Mb; avail=234622.8203125Mb
2020-10-13 06:50:46 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 9.311 | nll_loss 8.518 | ppl 366.61 | wps 38101.7 | wpb 1971 | bsz 82.7 | num_updates 450 | best_loss 9.311
2020-10-13 06:50:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:50:51 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 9 @ 450 updates, score 9.311) (writing took 4.763551831012592 seconds)
2020-10-13 06:50:51 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-13 06:50:51 | INFO | train | epoch 009 | loss 9.59 | nll_loss 8.89 | ppl 474.35 | wps 11867.5 | ups 2.65 | wpb 4470.2 | bsz 180.3 | num_updates 450 | lr 2.25887e-05 | gnorm 1.325 | clip 0 | train_wall 13 | wall 168
2020-10-13 06:50:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-13 06:50:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-13 06:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7086.3984375Mb; avail=237468.07421875Mb
2020-10-13 06:50:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000248
2020-10-13 06:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002277
2020-10-13 06:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7086.3984375Mb; avail=237468.07421875Mb
2020-10-13 06:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000087
2020-10-13 06:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7086.3984375Mb; avail=237468.07421875Mb
2020-10-13 06:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032259
2020-10-13 06:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035443
2020-10-13 06:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.90625Mb; avail=237468.56640625Mb
2020-10-13 06:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7085.90625Mb; avail=237468.56640625Mb
2020-10-13 06:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001671
2020-10-13 06:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.90625Mb; avail=237468.56640625Mb
2020-10-13 06:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000074
2020-10-13 06:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.90625Mb; avail=237468.56640625Mb
2020-10-13 06:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032130
2020-10-13 06:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034667
2020-10-13 06:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.90625Mb; avail=237468.56640625Mb
2020-10-13 06:50:51 | INFO | fairseq.trainer | begin training epoch 10
2020-10-13 06:51:04 | INFO | train_inner | epoch 010:     50 / 50 loss=9.444, nll_loss=8.715, ppl=420.15, wps=11870.2, ups=2.66, wpb=4470.2, bsz=180.3, num_updates=500, lr=2.50875e-05, gnorm=1.354, clip=0, train_wall=26, wall=182
2020-10-13 06:51:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7107.12890625Mb; avail=237447.5Mb
2020-10-13 06:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000916
2020-10-13 06:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7107.12890625Mb; avail=237447.5Mb
2020-10-13 06:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006871
2020-10-13 06:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7108.33984375Mb; avail=237446.2890625Mb
2020-10-13 06:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005204
2020-10-13 06:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013814
2020-10-13 06:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7108.33984375Mb; avail=237446.2890625Mb
2020-10-13 06:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7110.125Mb; avail=237444.34375Mb
2020-10-13 06:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000651
2020-10-13 06:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7110.125Mb; avail=237444.34375Mb
2020-10-13 06:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006756
2020-10-13 06:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7110.73046875Mb; avail=237443.73828125Mb
2020-10-13 06:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005165
2020-10-13 06:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013383
2020-10-13 06:51:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7111.3359375Mb; avail=237443.1328125Mb
2020-10-13 06:51:05 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 9.154 | nll_loss 8.32 | ppl 319.67 | wps 38238.5 | wpb 1971 | bsz 82.7 | num_updates 500 | best_loss 9.154
2020-10-13 06:51:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:51:10 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 10 @ 500 updates, score 9.154) (writing took 4.77230018703267 seconds)
2020-10-13 06:51:10 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-13 06:51:10 | INFO | train | epoch 010 | loss 9.298 | nll_loss 8.54 | ppl 372.15 | wps 11882.9 | ups 2.66 | wpb 4470.2 | bsz 180.3 | num_updates 500 | lr 2.50875e-05 | gnorm 1.383 | clip 0 | train_wall 13 | wall 187
2020-10-13 06:51:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-13 06:51:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-13 06:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7084.7734375Mb; avail=237469.6953125Mb
2020-10-13 06:51:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000225
2020-10-13 06:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002224
2020-10-13 06:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.7734375Mb; avail=237469.6953125Mb
2020-10-13 06:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 06:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.7734375Mb; avail=237469.6953125Mb
2020-10-13 06:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032349
2020-10-13 06:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035380
2020-10-13 06:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.7734375Mb; avail=237469.6953125Mb
2020-10-13 06:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7084.7734375Mb; avail=237469.6953125Mb
2020-10-13 06:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001631
2020-10-13 06:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.7734375Mb; avail=237469.6953125Mb
2020-10-13 06:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000073
2020-10-13 06:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.7734375Mb; avail=237469.6953125Mb
2020-10-13 06:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032387
2020-10-13 06:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034807
2020-10-13 06:51:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.7734375Mb; avail=237469.6953125Mb
2020-10-13 06:51:10 | INFO | fairseq.trainer | begin training epoch 11
2020-10-13 06:51:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7106.1953125Mb; avail=237448.09765625Mb
2020-10-13 06:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000885
2020-10-13 06:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.1953125Mb; avail=237448.09765625Mb
2020-10-13 06:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006676
2020-10-13 06:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.1953125Mb; avail=237448.09765625Mb
2020-10-13 06:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005074
2020-10-13 06:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013393
2020-10-13 06:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.1953125Mb; avail=237448.09765625Mb
2020-10-13 06:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7106.1953125Mb; avail=237448.09765625Mb
2020-10-13 06:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000557
2020-10-13 06:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.1953125Mb; avail=237448.09765625Mb
2020-10-13 06:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006655
2020-10-13 06:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.1953125Mb; avail=237448.09765625Mb
2020-10-13 06:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005078
2020-10-13 06:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013011
2020-10-13 06:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.1953125Mb; avail=237448.09765625Mb
2020-10-13 06:51:24 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.955 | nll_loss 8.068 | ppl 268.29 | wps 38545 | wpb 1971 | bsz 82.7 | num_updates 550 | best_loss 8.955
2020-10-13 06:51:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:51:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 11 @ 550 updates, score 8.955) (writing took 4.728929598815739 seconds)
2020-10-13 06:51:28 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-13 06:51:28 | INFO | train | epoch 011 | loss 9.112 | nll_loss 8.305 | ppl 316.24 | wps 11915.8 | ups 2.67 | wpb 4470.2 | bsz 180.3 | num_updates 550 | lr 2.75863e-05 | gnorm 1.517 | clip 0 | train_wall 13 | wall 206
2020-10-13 06:51:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-13 06:51:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-13 06:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7089.19140625Mb; avail=237465.26953125Mb
2020-10-13 06:51:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000409
2020-10-13 06:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003481
2020-10-13 06:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7089.63671875Mb; avail=237464.82421875Mb
2020-10-13 06:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000156
2020-10-13 06:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.2421875Mb; avail=237464.21875Mb
2020-10-13 06:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.063372
2020-10-13 06:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.068292
2020-10-13 06:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.203125Mb; avail=237455.2578125Mb
2020-10-13 06:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7092.765625Mb; avail=237461.94140625Mb
2020-10-13 06:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001677
2020-10-13 06:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.765625Mb; avail=237461.94140625Mb
2020-10-13 06:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000093
2020-10-13 06:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.765625Mb; avail=237461.94140625Mb
2020-10-13 06:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032208
2020-10-13 06:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034764
2020-10-13 06:51:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7083.91796875Mb; avail=237470.7890625Mb
2020-10-13 06:51:28 | INFO | fairseq.trainer | begin training epoch 12
2020-10-13 06:51:42 | INFO | train_inner | epoch 012:     50 / 50 loss=9.043, nll_loss=8.217, ppl=297.62, wps=11853.5, ups=2.65, wpb=4470.2, bsz=180.3, num_updates=600, lr=3.0085e-05, gnorm=1.457, clip=0, train_wall=26, wall=220
2020-10-13 06:51:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7103.328125Mb; avail=237451.0Mb
2020-10-13 06:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000859
2020-10-13 06:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.93359375Mb; avail=237450.39453125Mb
2020-10-13 06:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006627
2020-10-13 06:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.93359375Mb; avail=237450.39453125Mb
2020-10-13 06:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005107
2020-10-13 06:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013371
2020-10-13 06:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.93359375Mb; avail=237450.39453125Mb
2020-10-13 06:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7103.93359375Mb; avail=237450.39453125Mb
2020-10-13 06:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000599
2020-10-13 06:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.93359375Mb; avail=237450.39453125Mb
2020-10-13 06:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006685
2020-10-13 06:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.93359375Mb; avail=237450.39453125Mb
2020-10-13 06:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004992
2020-10-13 06:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013023
2020-10-13 06:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.93359375Mb; avail=237450.39453125Mb
2020-10-13 06:51:42 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.913 | nll_loss 7.99 | ppl 254.21 | wps 38644.9 | wpb 1971 | bsz 82.7 | num_updates 600 | best_loss 8.913
2020-10-13 06:51:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:51:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 12 @ 600 updates, score 8.913) (writing took 4.732549651991576 seconds)
2020-10-13 06:51:47 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-13 06:51:47 | INFO | train | epoch 012 | loss 8.973 | nll_loss 8.13 | ppl 280.1 | wps 11812.4 | ups 2.64 | wpb 4470.2 | bsz 180.3 | num_updates 600 | lr 3.0085e-05 | gnorm 1.397 | clip 0 | train_wall 13 | wall 225
2020-10-13 06:51:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-13 06:51:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-13 06:51:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7084.8671875Mb; avail=237469.1484375Mb
2020-10-13 06:51:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000323
2020-10-13 06:51:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002388
2020-10-13 06:51:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.8671875Mb; avail=237469.1484375Mb
2020-10-13 06:51:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 06:51:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.8671875Mb; avail=237469.1484375Mb
2020-10-13 06:51:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032950
2020-10-13 06:51:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036158
2020-10-13 06:51:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.8671875Mb; avail=237469.1484375Mb
2020-10-13 06:51:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7084.8671875Mb; avail=237469.1484375Mb
2020-10-13 06:51:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001629
2020-10-13 06:51:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.8671875Mb; avail=237469.1484375Mb
2020-10-13 06:51:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069
2020-10-13 06:51:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.8671875Mb; avail=237469.1484375Mb
2020-10-13 06:51:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.031985
2020-10-13 06:51:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034411
2020-10-13 06:51:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.8671875Mb; avail=237469.1484375Mb
2020-10-13 06:51:47 | INFO | fairseq.trainer | begin training epoch 13
2020-10-13 06:52:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7106.8828125Mb; avail=237447.44921875Mb
2020-10-13 06:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000905
2020-10-13 06:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.8828125Mb; avail=237447.44921875Mb
2020-10-13 06:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006905
2020-10-13 06:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7107.48828125Mb; avail=237446.84375Mb
2020-10-13 06:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005208
2020-10-13 06:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013794
2020-10-13 06:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7108.09375Mb; avail=237446.23828125Mb
2020-10-13 06:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7109.3046875Mb; avail=237445.02734375Mb
2020-10-13 06:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000562
2020-10-13 06:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7109.91015625Mb; avail=237444.421875Mb
2020-10-13 06:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006644
2020-10-13 06:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7109.91015625Mb; avail=237444.421875Mb
2020-10-13 06:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005104
2020-10-13 06:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013056
2020-10-13 06:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7110.41015625Mb; avail=237443.921875Mb
2020-10-13 06:52:01 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.809 | nll_loss 7.857 | ppl 231.8 | wps 38733.1 | wpb 1971 | bsz 82.7 | num_updates 650 | best_loss 8.809
2020-10-13 06:52:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:52:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 13 @ 650 updates, score 8.809) (writing took 4.7091871129814535 seconds)
2020-10-13 06:52:06 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-13 06:52:06 | INFO | train | epoch 013 | loss 8.868 | nll_loss 7.996 | ppl 255.24 | wps 11932.9 | ups 2.67 | wpb 4470.2 | bsz 180.3 | num_updates 650 | lr 3.25838e-05 | gnorm 1.296 | clip 0 | train_wall 13 | wall 244
2020-10-13 06:52:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-13 06:52:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-13 06:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7083.79296875Mb; avail=237470.69921875Mb
2020-10-13 06:52:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000275
2020-10-13 06:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002543
2020-10-13 06:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7083.79296875Mb; avail=237470.69921875Mb
2020-10-13 06:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 06:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7083.79296875Mb; avail=237470.69921875Mb
2020-10-13 06:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.034058
2020-10-13 06:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.037474
2020-10-13 06:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7083.79296875Mb; avail=237470.69921875Mb
2020-10-13 06:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7083.79296875Mb; avail=237470.69921875Mb
2020-10-13 06:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001626
2020-10-13 06:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7083.79296875Mb; avail=237470.69921875Mb
2020-10-13 06:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000071
2020-10-13 06:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7083.79296875Mb; avail=237470.69921875Mb
2020-10-13 06:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032360
2020-10-13 06:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034782
2020-10-13 06:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.3984375Mb; avail=237470.09375Mb
2020-10-13 06:52:06 | INFO | fairseq.trainer | begin training epoch 14
2020-10-13 06:52:20 | INFO | train_inner | epoch 014:     50 / 50 loss=8.825, nll_loss=7.942, ppl=245.99, wps=11893.1, ups=2.66, wpb=4470.2, bsz=180.3, num_updates=700, lr=3.50825e-05, gnorm=1.41, clip=0, train_wall=26, wall=257
2020-10-13 06:52:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7113.65234375Mb; avail=237440.83203125Mb
2020-10-13 06:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000926
2020-10-13 06:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.16015625Mb; avail=237440.32421875Mb
2020-10-13 06:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006930
2020-10-13 06:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7116.58203125Mb; avail=237437.90234375Mb
2020-10-13 06:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005290
2020-10-13 06:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013934
2020-10-13 06:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7117.79296875Mb; avail=237436.69140625Mb
2020-10-13 06:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7117.79296875Mb; avail=237436.69140625Mb
2020-10-13 06:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000642
2020-10-13 06:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7117.79296875Mb; avail=237436.69140625Mb
2020-10-13 06:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006877
2020-10-13 06:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7118.29296875Mb; avail=237436.19140625Mb
2020-10-13 06:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005155
2020-10-13 06:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013435
2020-10-13 06:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7118.29296875Mb; avail=237436.19140625Mb
2020-10-13 06:52:20 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.677 | nll_loss 7.716 | ppl 210.32 | wps 38346.4 | wpb 1971 | bsz 82.7 | num_updates 700 | best_loss 8.677
2020-10-13 06:52:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:52:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 14 @ 700 updates, score 8.677) (writing took 4.717785402899608 seconds)
2020-10-13 06:52:25 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-13 06:52:25 | INFO | train | epoch 014 | loss 8.781 | nll_loss 7.889 | ppl 237.07 | wps 11866.1 | ups 2.65 | wpb 4470.2 | bsz 180.3 | num_updates 700 | lr 3.50825e-05 | gnorm 1.525 | clip 0 | train_wall 13 | wall 262
2020-10-13 06:52:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-13 06:52:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-13 06:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7086.5078125Mb; avail=237467.98046875Mb
2020-10-13 06:52:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000223
2020-10-13 06:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002216
2020-10-13 06:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7086.5078125Mb; avail=237467.98046875Mb
2020-10-13 06:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 06:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7086.5078125Mb; avail=237467.98046875Mb
2020-10-13 06:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032531
2020-10-13 06:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035558
2020-10-13 06:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7086.5078125Mb; avail=237467.98046875Mb
2020-10-13 06:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7086.5078125Mb; avail=237467.98046875Mb
2020-10-13 06:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001610
2020-10-13 06:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7086.5078125Mb; avail=237467.98046875Mb
2020-10-13 06:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000068
2020-10-13 06:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7086.5078125Mb; avail=237467.98046875Mb
2020-10-13 06:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032826
2020-10-13 06:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035223
2020-10-13 06:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7086.5078125Mb; avail=237467.98046875Mb
2020-10-13 06:52:25 | INFO | fairseq.trainer | begin training epoch 15
2020-10-13 06:52:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:52:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7105.09765625Mb; avail=237449.21875Mb
2020-10-13 06:52:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000884
2020-10-13 06:52:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7105.09765625Mb; avail=237449.21875Mb
2020-10-13 06:52:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006683
2020-10-13 06:52:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7105.09765625Mb; avail=237449.21875Mb
2020-10-13 06:52:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005054
2020-10-13 06:52:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013372
2020-10-13 06:52:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7105.09765625Mb; avail=237449.21875Mb
2020-10-13 06:52:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7105.09765625Mb; avail=237449.21875Mb
2020-10-13 06:52:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000587
2020-10-13 06:52:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7105.09765625Mb; avail=237449.21875Mb
2020-10-13 06:52:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006658
2020-10-13 06:52:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7105.09765625Mb; avail=237449.21875Mb
2020-10-13 06:52:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005016
2020-10-13 06:52:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012970
2020-10-13 06:52:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7105.09765625Mb; avail=237449.21875Mb
2020-10-13 06:52:39 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.621 | nll_loss 7.635 | ppl 198.81 | wps 38293.4 | wpb 1971 | bsz 82.7 | num_updates 750 | best_loss 8.621
2020-10-13 06:52:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:52:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 15 @ 750 updates, score 8.621) (writing took 4.733124052872881 seconds)
2020-10-13 06:52:44 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-13 06:52:44 | INFO | train | epoch 015 | loss 8.693 | nll_loss 7.785 | ppl 220.56 | wps 11881.9 | ups 2.66 | wpb 4470.2 | bsz 180.3 | num_updates 750 | lr 3.75813e-05 | gnorm 1.493 | clip 0 | train_wall 13 | wall 281
2020-10-13 06:52:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-13 06:52:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-13 06:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7089.59765625Mb; avail=237464.91015625Mb
2020-10-13 06:52:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000239
2020-10-13 06:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002189
2020-10-13 06:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7089.59765625Mb; avail=237464.91015625Mb
2020-10-13 06:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000082
2020-10-13 06:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7089.59765625Mb; avail=237464.91015625Mb
2020-10-13 06:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032014
2020-10-13 06:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035065
2020-10-13 06:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.23046875Mb; avail=237461.27734375Mb
2020-10-13 06:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7095.56640625Mb; avail=237458.94140625Mb
2020-10-13 06:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001678
2020-10-13 06:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.56640625Mb; avail=237458.94140625Mb
2020-10-13 06:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000073
2020-10-13 06:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.56640625Mb; avail=237458.94140625Mb
2020-10-13 06:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.031859
2020-10-13 06:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034359
2020-10-13 06:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.79296875Mb; avail=237468.71484375Mb
2020-10-13 06:52:44 | INFO | fairseq.trainer | begin training epoch 16
2020-10-13 06:52:57 | INFO | train_inner | epoch 016:     50 / 50 loss=8.651, nll_loss=7.735, ppl=213.08, wps=11874.5, ups=2.66, wpb=4470.2, bsz=180.3, num_updates=800, lr=4.008e-05, gnorm=1.534, clip=0, train_wall=26, wall=295
2020-10-13 06:52:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7109.6484375Mb; avail=237445.3828125Mb
2020-10-13 06:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000874
2020-10-13 06:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7109.6484375Mb; avail=237445.3828125Mb
2020-10-13 06:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006848
2020-10-13 06:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7109.6484375Mb; avail=237445.3828125Mb
2020-10-13 06:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005092
2020-10-13 06:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013570
2020-10-13 06:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7109.6484375Mb; avail=237445.3828125Mb
2020-10-13 06:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7109.77734375Mb; avail=237445.25390625Mb
2020-10-13 06:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000564
2020-10-13 06:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7109.77734375Mb; avail=237445.25390625Mb
2020-10-13 06:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006748
2020-10-13 06:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7109.76171875Mb; avail=237445.125Mb
2020-10-13 06:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005067
2020-10-13 06:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013139
2020-10-13 06:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7109.76171875Mb; avail=237445.125Mb
2020-10-13 06:52:58 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.548 | nll_loss 7.56 | ppl 188.65 | wps 38259.3 | wpb 1971 | bsz 82.7 | num_updates 800 | best_loss 8.548
2020-10-13 06:52:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:53:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 16 @ 800 updates, score 8.548) (writing took 4.722897624131292 seconds)
2020-10-13 06:53:03 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-13 06:53:03 | INFO | train | epoch 016 | loss 8.608 | nll_loss 7.686 | ppl 205.86 | wps 11810.2 | ups 2.64 | wpb 4470.2 | bsz 180.3 | num_updates 800 | lr 4.008e-05 | gnorm 1.576 | clip 0 | train_wall 13 | wall 300
2020-10-13 06:53:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-13 06:53:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-13 06:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7090.5625Mb; avail=237463.9375Mb
2020-10-13 06:53:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000231
2020-10-13 06:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002241
2020-10-13 06:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.5625Mb; avail=237463.9375Mb
2020-10-13 06:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 06:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.5625Mb; avail=237463.9375Mb
2020-10-13 06:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032894
2020-10-13 06:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035947
2020-10-13 06:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.5625Mb; avail=237463.9375Mb
2020-10-13 06:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7090.0703125Mb; avail=237464.4296875Mb
2020-10-13 06:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001649
2020-10-13 06:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.0703125Mb; avail=237464.4296875Mb
2020-10-13 06:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 06:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.0703125Mb; avail=237464.4296875Mb
2020-10-13 06:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032402
2020-10-13 06:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034854
2020-10-13 06:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.0703125Mb; avail=237464.4296875Mb
2020-10-13 06:53:03 | INFO | fairseq.trainer | begin training epoch 17
2020-10-13 06:53:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7110.16796875Mb; avail=237444.22265625Mb
2020-10-13 06:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000888
2020-10-13 06:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7110.16796875Mb; avail=237444.22265625Mb
2020-10-13 06:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006751
2020-10-13 06:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7111.87109375Mb; avail=237442.51953125Mb
2020-10-13 06:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005101
2020-10-13 06:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013509
2020-10-13 06:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.4765625Mb; avail=237441.9140625Mb
2020-10-13 06:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7113.6875Mb; avail=237440.703125Mb
2020-10-13 06:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000595
2020-10-13 06:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.29296875Mb; avail=237440.09765625Mb
2020-10-13 06:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006550
2020-10-13 06:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.8984375Mb; avail=237439.4921875Mb
2020-10-13 06:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005067
2020-10-13 06:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012947
2020-10-13 06:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7115.50390625Mb; avail=237438.88671875Mb
2020-10-13 06:53:17 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 8.48 | nll_loss 7.465 | ppl 176.74 | wps 38432.3 | wpb 1971 | bsz 82.7 | num_updates 850 | best_loss 8.48
2020-10-13 06:53:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:53:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 17 @ 850 updates, score 8.48) (writing took 4.718302668072283 seconds)
2020-10-13 06:53:21 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-13 06:53:21 | INFO | train | epoch 017 | loss 8.515 | nll_loss 7.58 | ppl 191.39 | wps 11867.5 | ups 2.65 | wpb 4470.2 | bsz 180.3 | num_updates 850 | lr 4.25788e-05 | gnorm 1.443 | clip 0 | train_wall 13 | wall 319
2020-10-13 06:53:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-13 06:53:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-13 06:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7092.59765625Mb; avail=237461.8828125Mb
2020-10-13 06:53:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000289
2020-10-13 06:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002374
2020-10-13 06:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.59765625Mb; avail=237461.8828125Mb
2020-10-13 06:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000107
2020-10-13 06:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.59765625Mb; avail=237461.8828125Mb
2020-10-13 06:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033424
2020-10-13 06:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036736
2020-10-13 06:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.3515625Mb; avail=237458.36328125Mb
2020-10-13 06:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7098.16796875Mb; avail=237456.546875Mb
2020-10-13 06:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001623
2020-10-13 06:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.66796875Mb; avail=237456.046875Mb
2020-10-13 06:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000072
2020-10-13 06:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.66796875Mb; avail=237456.046875Mb
2020-10-13 06:53:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032703
2020-10-13 06:53:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035136
2020-10-13 06:53:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.82421875Mb; avail=237447.890625Mb
2020-10-13 06:53:22 | INFO | fairseq.trainer | begin training epoch 18
2020-10-13 06:53:35 | INFO | train_inner | epoch 018:     50 / 50 loss=8.472, nll_loss=7.53, ppl=184.82, wps=11831.5, ups=2.65, wpb=4470.2, bsz=180.3, num_updates=900, lr=4.50775e-05, gnorm=1.505, clip=0, train_wall=26, wall=333
2020-10-13 06:53:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:53:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7117.734375Mb; avail=237436.32421875Mb
2020-10-13 06:53:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000877
2020-10-13 06:53:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7117.734375Mb; avail=237436.32421875Mb
2020-10-13 06:53:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006797
2020-10-13 06:53:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7118.33984375Mb; avail=237435.71875Mb
2020-10-13 06:53:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005054
2020-10-13 06:53:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013493
2020-10-13 06:53:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7118.9453125Mb; avail=237435.11328125Mb
2020-10-13 06:53:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7122.46484375Mb; avail=237431.59375Mb
2020-10-13 06:53:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000562
2020-10-13 06:53:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7122.46484375Mb; avail=237431.59375Mb
2020-10-13 06:53:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006645
2020-10-13 06:53:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7122.46484375Mb; avail=237431.59375Mb
2020-10-13 06:53:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005020
2020-10-13 06:53:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013019
2020-10-13 06:53:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7122.46484375Mb; avail=237431.59375Mb
2020-10-13 06:53:35 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 8.394 | nll_loss 7.377 | ppl 166.2 | wps 38529.6 | wpb 1971 | bsz 82.7 | num_updates 900 | best_loss 8.394
2020-10-13 06:53:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:53:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 18 @ 900 updates, score 8.394) (writing took 4.713577212067321 seconds)
2020-10-13 06:53:40 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-13 06:53:40 | INFO | train | epoch 018 | loss 8.429 | nll_loss 7.48 | ppl 178.47 | wps 11860.4 | ups 2.65 | wpb 4470.2 | bsz 180.3 | num_updates 900 | lr 4.50775e-05 | gnorm 1.567 | clip 0 | train_wall 13 | wall 338
2020-10-13 06:53:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-13 06:53:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-13 06:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7092.13671875Mb; avail=237462.3671875Mb
2020-10-13 06:53:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000268
2020-10-13 06:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002492
2020-10-13 06:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.13671875Mb; avail=237462.3671875Mb
2020-10-13 06:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 06:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.13671875Mb; avail=237462.3671875Mb
2020-10-13 06:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033095
2020-10-13 06:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036519
2020-10-13 06:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.13671875Mb; avail=237462.3671875Mb
2020-10-13 06:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7092.12109375Mb; avail=237462.3828125Mb
2020-10-13 06:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001626
2020-10-13 06:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.12109375Mb; avail=237462.3828125Mb
2020-10-13 06:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000071
2020-10-13 06:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.12109375Mb; avail=237462.3828125Mb
2020-10-13 06:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032101
2020-10-13 06:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034538
2020-10-13 06:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.12109375Mb; avail=237462.3828125Mb
2020-10-13 06:53:40 | INFO | fairseq.trainer | begin training epoch 19
2020-10-13 06:53:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7112.65625Mb; avail=237442.37109375Mb
2020-10-13 06:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000886
2020-10-13 06:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.65625Mb; avail=237442.37109375Mb
2020-10-13 06:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006838
2020-10-13 06:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.65625Mb; avail=237442.37109375Mb
2020-10-13 06:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005180
2020-10-13 06:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013673
2020-10-13 06:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.65625Mb; avail=237442.37109375Mb
2020-10-13 06:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7112.65625Mb; avail=237442.37109375Mb
2020-10-13 06:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000652
2020-10-13 06:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.65625Mb; avail=237442.37109375Mb
2020-10-13 06:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006586
2020-10-13 06:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.65625Mb; avail=237442.37109375Mb
2020-10-13 06:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005115
2020-10-13 06:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013096
2020-10-13 06:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.65625Mb; avail=237442.37109375Mb
2020-10-13 06:53:54 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 8.294 | nll_loss 7.265 | ppl 153.8 | wps 38224 | wpb 1971 | bsz 82.7 | num_updates 950 | best_loss 8.294
2020-10-13 06:53:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:53:59 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 19 @ 950 updates, score 8.294) (writing took 4.784712685039267 seconds)
2020-10-13 06:53:59 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-13 06:53:59 | INFO | train | epoch 019 | loss 8.34 | nll_loss 7.378 | ppl 166.36 | wps 11795.1 | ups 2.64 | wpb 4470.2 | bsz 180.3 | num_updates 950 | lr 4.75763e-05 | gnorm 1.526 | clip 0 | train_wall 13 | wall 357
2020-10-13 06:53:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-13 06:53:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-13 06:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7089.29296875Mb; avail=237465.1796875Mb
2020-10-13 06:53:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000225
2020-10-13 06:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002480
2020-10-13 06:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7089.29296875Mb; avail=237465.1796875Mb
2020-10-13 06:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 06:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7089.29296875Mb; avail=237465.1796875Mb
2020-10-13 06:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033995
2020-10-13 06:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.037342
2020-10-13 06:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7089.28515625Mb; avail=237465.1875Mb
2020-10-13 06:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7088.9453125Mb; avail=237465.52734375Mb
2020-10-13 06:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001633
2020-10-13 06:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7088.9453125Mb; avail=237465.52734375Mb
2020-10-13 06:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069
2020-10-13 06:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7088.9453125Mb; avail=237465.52734375Mb
2020-10-13 06:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032276
2020-10-13 06:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034699
2020-10-13 06:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7088.82421875Mb; avail=237465.6484375Mb
2020-10-13 06:53:59 | INFO | fairseq.trainer | begin training epoch 20
2020-10-13 06:54:13 | INFO | train_inner | epoch 020:     50 / 50 loss=8.296, nll_loss=7.328, ppl=160.69, wps=11846.6, ups=2.65, wpb=4470.2, bsz=180.3, num_updates=1000, lr=5.0075e-05, gnorm=1.52, clip=0, train_wall=26, wall=370
2020-10-13 06:54:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7108.98046875Mb; avail=237445.52734375Mb
2020-10-13 06:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000933
2020-10-13 06:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7108.98046875Mb; avail=237445.52734375Mb
2020-10-13 06:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006709
2020-10-13 06:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7108.98046875Mb; avail=237445.52734375Mb
2020-10-13 06:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005148
2020-10-13 06:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013589
2020-10-13 06:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7108.98046875Mb; avail=237445.52734375Mb
2020-10-13 06:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7108.98046875Mb; avail=237445.52734375Mb
2020-10-13 06:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000572
2020-10-13 06:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7108.98046875Mb; avail=237445.52734375Mb
2020-10-13 06:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006620
2020-10-13 06:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7108.98046875Mb; avail=237445.52734375Mb
2020-10-13 06:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005089
2020-10-13 06:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013053
2020-10-13 06:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7108.98046875Mb; avail=237445.52734375Mb
2020-10-13 06:54:13 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 8.274 | nll_loss 7.249 | ppl 152.12 | wps 38401.7 | wpb 1971 | bsz 82.7 | num_updates 1000 | best_loss 8.274
2020-10-13 06:54:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:54:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 20 @ 1000 updates, score 8.274) (writing took 4.713588434970006 seconds)
2020-10-13 06:54:18 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-13 06:54:18 | INFO | train | epoch 020 | loss 8.251 | nll_loss 7.278 | ppl 155.21 | wps 11888.7 | ups 2.66 | wpb 4470.2 | bsz 180.3 | num_updates 1000 | lr 5.0075e-05 | gnorm 1.513 | clip 0 | train_wall 13 | wall 376
2020-10-13 06:54:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-13 06:54:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-13 06:54:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7091.96484375Mb; avail=237462.546875Mb
2020-10-13 06:54:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000249
2020-10-13 06:54:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002215
2020-10-13 06:54:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.96484375Mb; avail=237462.546875Mb
2020-10-13 06:54:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 06:54:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.96484375Mb; avail=237462.546875Mb
2020-10-13 06:54:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033475
2020-10-13 06:54:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036505
2020-10-13 06:54:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.96484375Mb; avail=237462.546875Mb
2020-10-13 06:54:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7091.96484375Mb; avail=237462.546875Mb
2020-10-13 06:54:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001682
2020-10-13 06:54:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.96484375Mb; avail=237462.546875Mb
2020-10-13 06:54:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000071
2020-10-13 06:54:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.96484375Mb; avail=237462.546875Mb
2020-10-13 06:54:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032052
2020-10-13 06:54:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034533
2020-10-13 06:54:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7094.9921875Mb; avail=237459.51953125Mb
2020-10-13 06:54:18 | INFO | fairseq.trainer | begin training epoch 21
2020-10-13 06:54:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7113.66015625Mb; avail=237440.62109375Mb
2020-10-13 06:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000929
2020-10-13 06:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.66015625Mb; avail=237440.62109375Mb
2020-10-13 06:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006755
2020-10-13 06:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.66015625Mb; avail=237440.62109375Mb
2020-10-13 06:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005042
2020-10-13 06:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013497
2020-10-13 06:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.66015625Mb; avail=237440.62109375Mb
2020-10-13 06:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7113.66015625Mb; avail=237440.62109375Mb
2020-10-13 06:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000574
2020-10-13 06:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.66015625Mb; avail=237440.62109375Mb
2020-10-13 06:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006647
2020-10-13 06:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.66015625Mb; avail=237440.62109375Mb
2020-10-13 06:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004997
2020-10-13 06:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012972
2020-10-13 06:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.66015625Mb; avail=237440.62109375Mb
2020-10-13 06:54:32 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 8.178 | nll_loss 7.134 | ppl 140.43 | wps 38659.9 | wpb 1971 | bsz 82.7 | num_updates 1050 | best_loss 8.178
2020-10-13 06:54:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:54:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 21 @ 1050 updates, score 8.178) (writing took 4.7236417690292 seconds)
2020-10-13 06:54:37 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-13 06:54:37 | INFO | train | epoch 021 | loss 8.183 | nll_loss 7.198 | ppl 146.83 | wps 11807.2 | ups 2.64 | wpb 4470.2 | bsz 180.3 | num_updates 1050 | lr 5.25738e-05 | gnorm 1.84 | clip 0 | train_wall 13 | wall 395
2020-10-13 06:54:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-13 06:54:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-13 06:54:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7097.04296875Mb; avail=237457.43359375Mb
2020-10-13 06:54:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000407
2020-10-13 06:54:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003505
2020-10-13 06:54:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.04296875Mb; avail=237457.43359375Mb
2020-10-13 06:54:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000129
2020-10-13 06:54:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.6484375Mb; avail=237456.828125Mb
2020-10-13 06:54:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.047776
2020-10-13 06:54:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.052622
2020-10-13 06:54:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.7734375Mb; avail=237452.703125Mb
2020-10-13 06:54:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7091.9296875Mb; avail=237462.546875Mb
2020-10-13 06:54:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001785
2020-10-13 06:54:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.9296875Mb; avail=237462.546875Mb
2020-10-13 06:54:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000083
2020-10-13 06:54:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.9296875Mb; avail=237462.546875Mb
2020-10-13 06:54:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032487
2020-10-13 06:54:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035116
2020-10-13 06:54:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.9296875Mb; avail=237462.546875Mb
2020-10-13 06:54:37 | INFO | fairseq.trainer | begin training epoch 22
2020-10-13 06:54:51 | INFO | train_inner | epoch 022:     50 / 50 loss=8.133, nll_loss=7.142, ppl=141.26, wps=11818.7, ups=2.64, wpb=4470.2, bsz=180.3, num_updates=1100, lr=5.50725e-05, gnorm=1.637, clip=0, train_wall=26, wall=408
2020-10-13 06:54:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:54:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7117.5Mb; avail=237436.984375Mb
2020-10-13 06:54:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000910
2020-10-13 06:54:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7117.5Mb; avail=237436.984375Mb
2020-10-13 06:54:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006758
2020-10-13 06:54:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7118.10546875Mb; avail=237436.37890625Mb
2020-10-13 06:54:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005100
2020-10-13 06:54:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013555
2020-10-13 06:54:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7118.7109375Mb; avail=237435.7734375Mb
2020-10-13 06:54:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7121.6484375Mb; avail=237432.8359375Mb
2020-10-13 06:54:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000564
2020-10-13 06:54:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7121.6484375Mb; avail=237432.8359375Mb
2020-10-13 06:54:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006690
2020-10-13 06:54:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7121.6484375Mb; avail=237432.8359375Mb
2020-10-13 06:54:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005092
2020-10-13 06:54:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013071
2020-10-13 06:54:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7121.6484375Mb; avail=237432.8359375Mb
2020-10-13 06:54:51 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 8.143 | nll_loss 7.072 | ppl 134.56 | wps 38650.7 | wpb 1971 | bsz 82.7 | num_updates 1100 | best_loss 8.143
2020-10-13 06:54:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:54:56 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 22 @ 1100 updates, score 8.143) (writing took 4.71415298897773 seconds)
2020-10-13 06:54:56 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-13 06:54:56 | INFO | train | epoch 022 | loss 8.083 | nll_loss 7.086 | ppl 135.9 | wps 11839.4 | ups 2.65 | wpb 4470.2 | bsz 180.3 | num_updates 1100 | lr 5.50725e-05 | gnorm 1.433 | clip 0 | train_wall 13 | wall 413
2020-10-13 06:54:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-13 06:54:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-13 06:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7090.60546875Mb; avail=237463.875Mb
2020-10-13 06:54:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000315
2020-10-13 06:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002438
2020-10-13 06:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.60546875Mb; avail=237463.875Mb
2020-10-13 06:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 06:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.60546875Mb; avail=237463.875Mb
2020-10-13 06:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033343
2020-10-13 06:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036722
2020-10-13 06:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.60546875Mb; avail=237463.875Mb
2020-10-13 06:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7090.60546875Mb; avail=237463.875Mb
2020-10-13 06:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001677
2020-10-13 06:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.60546875Mb; avail=237463.875Mb
2020-10-13 06:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069
2020-10-13 06:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.60546875Mb; avail=237463.875Mb
2020-10-13 06:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032379
2020-10-13 06:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034854
2020-10-13 06:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.60546875Mb; avail=237463.875Mb
2020-10-13 06:54:56 | INFO | fairseq.trainer | begin training epoch 23
2020-10-13 06:55:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:55:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7122.12109375Mb; avail=237432.36328125Mb
2020-10-13 06:55:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000896
2020-10-13 06:55:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7122.12109375Mb; avail=237432.36328125Mb
2020-10-13 06:55:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006749
2020-10-13 06:55:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7122.12109375Mb; avail=237432.36328125Mb
2020-10-13 06:55:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005180
2020-10-13 06:55:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013606
2020-10-13 06:55:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7122.12109375Mb; avail=237432.36328125Mb
2020-10-13 06:55:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7112.76953125Mb; avail=237441.71484375Mb
2020-10-13 06:55:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000577
2020-10-13 06:55:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.76953125Mb; avail=237441.71484375Mb
2020-10-13 06:55:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006726
2020-10-13 06:55:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.76953125Mb; avail=237441.71484375Mb
2020-10-13 06:55:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005148
2020-10-13 06:55:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013194
2020-10-13 06:55:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.76953125Mb; avail=237441.71484375Mb
2020-10-13 06:55:10 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 8.085 | nll_loss 7.017 | ppl 129.49 | wps 38523.9 | wpb 1971 | bsz 82.7 | num_updates 1150 | best_loss 8.085
2020-10-13 06:55:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:55:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 23 @ 1150 updates, score 8.085) (writing took 4.718501950148493 seconds)
2020-10-13 06:55:15 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-13 06:55:15 | INFO | train | epoch 023 | loss 8.023 | nll_loss 7.016 | ppl 129.46 | wps 11820 | ups 2.64 | wpb 4470.2 | bsz 180.3 | num_updates 1150 | lr 5.75713e-05 | gnorm 1.819 | clip 0 | train_wall 13 | wall 432
2020-10-13 06:55:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-13 06:55:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-13 06:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7103.34375Mb; avail=237451.15234375Mb
2020-10-13 06:55:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000225
2020-10-13 06:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002511
2020-10-13 06:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.34375Mb; avail=237451.15234375Mb
2020-10-13 06:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 06:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.34375Mb; avail=237451.15234375Mb
2020-10-13 06:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033463
2020-10-13 06:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036882
2020-10-13 06:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.5Mb; avail=237460.99609375Mb
2020-10-13 06:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7093.5Mb; avail=237460.99609375Mb
2020-10-13 06:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001715
2020-10-13 06:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.5Mb; avail=237460.99609375Mb
2020-10-13 06:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000071
2020-10-13 06:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.5Mb; avail=237460.99609375Mb
2020-10-13 06:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032619
2020-10-13 06:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035128
2020-10-13 06:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.5Mb; avail=237460.99609375Mb
2020-10-13 06:55:15 | INFO | fairseq.trainer | begin training epoch 24
2020-10-13 06:55:28 | INFO | train_inner | epoch 024:     50 / 50 loss=7.98, nll_loss=6.968, ppl=125.2, wps=11831.2, ups=2.65, wpb=4470.2, bsz=180.3, num_updates=1200, lr=6.007e-05, gnorm=1.744, clip=0, train_wall=26, wall=446
2020-10-13 06:55:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7113.12109375Mb; avail=237441.88671875Mb
2020-10-13 06:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000927
2020-10-13 06:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.12109375Mb; avail=237441.88671875Mb
2020-10-13 06:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006845
2020-10-13 06:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.703125Mb; avail=237441.91015625Mb
2020-10-13 06:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005068
2020-10-13 06:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013651
2020-10-13 06:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.703125Mb; avail=237441.91015625Mb
2020-10-13 06:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7112.703125Mb; avail=237441.91015625Mb
2020-10-13 06:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000602
2020-10-13 06:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.703125Mb; avail=237441.91015625Mb
2020-10-13 06:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006702
2020-10-13 06:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.703125Mb; avail=237441.91015625Mb
2020-10-13 06:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005029
2020-10-13 06:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013089
2020-10-13 06:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.703125Mb; avail=237441.91015625Mb
2020-10-13 06:55:29 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.036 | nll_loss 6.971 | ppl 125.43 | wps 37896.9 | wpb 1971 | bsz 82.7 | num_updates 1200 | best_loss 8.036
2020-10-13 06:55:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:55:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 24 @ 1200 updates, score 8.036) (writing took 4.711721906205639 seconds)
2020-10-13 06:55:34 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-13 06:55:34 | INFO | train | epoch 024 | loss 7.937 | nll_loss 6.92 | ppl 121.08 | wps 11840.1 | ups 2.65 | wpb 4470.2 | bsz 180.3 | num_updates 1200 | lr 6.007e-05 | gnorm 1.669 | clip 0 | train_wall 13 | wall 451
2020-10-13 06:55:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-13 06:55:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-13 06:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7094.28515625Mb; avail=237460.19140625Mb
2020-10-13 06:55:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000273
2020-10-13 06:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002560
2020-10-13 06:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7094.28515625Mb; avail=237460.19140625Mb
2020-10-13 06:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 06:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7094.28515625Mb; avail=237460.19140625Mb
2020-10-13 06:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033260
2020-10-13 06:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036690
2020-10-13 06:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7094.28515625Mb; avail=237460.19140625Mb
2020-10-13 06:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7096.1015625Mb; avail=237458.375Mb
2020-10-13 06:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001681
2020-10-13 06:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.1015625Mb; avail=237458.375Mb
2020-10-13 06:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 06:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.1015625Mb; avail=237458.375Mb
2020-10-13 06:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.031898
2020-10-13 06:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034367
2020-10-13 06:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.12890625Mb; avail=237455.34765625Mb
2020-10-13 06:55:34 | INFO | fairseq.trainer | begin training epoch 25
2020-10-13 06:55:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:55:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7114.62109375Mb; avail=237439.98828125Mb
2020-10-13 06:55:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000867
2020-10-13 06:55:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.62109375Mb; avail=237439.98828125Mb
2020-10-13 06:55:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006753
2020-10-13 06:55:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.62109375Mb; avail=237439.98828125Mb
2020-10-13 06:55:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005088
2020-10-13 06:55:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013464
2020-10-13 06:55:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.62109375Mb; avail=237439.98828125Mb
2020-10-13 06:55:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7114.62109375Mb; avail=237439.98828125Mb
2020-10-13 06:55:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000603
2020-10-13 06:55:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.62109375Mb; avail=237439.98828125Mb
2020-10-13 06:55:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006645
2020-10-13 06:55:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.62109375Mb; avail=237439.98828125Mb
2020-10-13 06:55:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005049
2020-10-13 06:55:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013017
2020-10-13 06:55:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.62109375Mb; avail=237439.98828125Mb
2020-10-13 06:55:48 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.012 | nll_loss 6.926 | ppl 121.63 | wps 38470 | wpb 1971 | bsz 82.7 | num_updates 1250 | best_loss 8.012
2020-10-13 06:55:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:55:52 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 25 @ 1250 updates, score 8.012) (writing took 4.734287180006504 seconds)
2020-10-13 06:55:52 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-13 06:55:52 | INFO | train | epoch 025 | loss 7.854 | nll_loss 6.826 | ppl 113.46 | wps 11897.8 | ups 2.66 | wpb 4470.2 | bsz 180.3 | num_updates 1250 | lr 6.25687e-05 | gnorm 1.6 | clip 0 | train_wall 13 | wall 470
2020-10-13 06:55:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-13 06:55:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-13 06:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7093.734375Mb; avail=237460.8828125Mb
2020-10-13 06:55:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000242
2020-10-13 06:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002269
2020-10-13 06:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.734375Mb; avail=237460.8828125Mb
2020-10-13 06:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 06:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.734375Mb; avail=237460.8828125Mb
2020-10-13 06:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032974
2020-10-13 06:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036095
2020-10-13 06:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.734375Mb; avail=237460.8828125Mb
2020-10-13 06:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7097.28515625Mb; avail=237457.33203125Mb
2020-10-13 06:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001720
2020-10-13 06:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.28515625Mb; avail=237457.33203125Mb
2020-10-13 06:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 06:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.28515625Mb; avail=237457.33203125Mb
2020-10-13 06:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032246
2020-10-13 06:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034756
2020-10-13 06:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.3125Mb; avail=237454.3046875Mb
2020-10-13 06:55:52 | INFO | fairseq.trainer | begin training epoch 26
2020-10-13 06:56:06 | INFO | train_inner | epoch 026:     50 / 50 loss=7.815, nll_loss=6.782, ppl=110.06, wps=11876.8, ups=2.66, wpb=4470.2, bsz=180.3, num_updates=1300, lr=6.50675e-05, gnorm=1.636, clip=0, train_wall=26, wall=484
2020-10-13 06:56:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:56:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7112.59375Mb; avail=237441.8984375Mb
2020-10-13 06:56:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001280
2020-10-13 06:56:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.59375Mb; avail=237441.8984375Mb
2020-10-13 06:56:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012435
2020-10-13 06:56:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.59375Mb; avail=237441.8984375Mb
2020-10-13 06:56:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.008293
2020-10-13 06:56:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.023028
2020-10-13 06:56:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.5Mb; avail=237440.9921875Mb
2020-10-13 06:56:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7115.30078125Mb; avail=237439.19140625Mb
2020-10-13 06:56:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000606
2020-10-13 06:56:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7115.30078125Mb; avail=237439.19140625Mb
2020-10-13 06:56:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006654
2020-10-13 06:56:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7115.30078125Mb; avail=237439.19140625Mb
2020-10-13 06:56:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004984
2020-10-13 06:56:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012976
2020-10-13 06:56:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7116.51171875Mb; avail=237437.98046875Mb
2020-10-13 06:56:06 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.979 | nll_loss 6.901 | ppl 119.51 | wps 38266.4 | wpb 1971 | bsz 82.7 | num_updates 1300 | best_loss 7.979
2020-10-13 06:56:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:56:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 26 @ 1300 updates, score 7.979) (writing took 4.710626443149522 seconds)
2020-10-13 06:56:11 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-13 06:56:11 | INFO | train | epoch 026 | loss 7.777 | nll_loss 6.738 | ppl 106.76 | wps 11849.2 | ups 2.65 | wpb 4470.2 | bsz 180.3 | num_updates 1300 | lr 6.50675e-05 | gnorm 1.673 | clip 0 | train_wall 13 | wall 489
2020-10-13 06:56:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-13 06:56:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-13 06:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7093.76171875Mb; avail=237460.734375Mb
2020-10-13 06:56:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000230
2020-10-13 06:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002226
2020-10-13 06:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.76171875Mb; avail=237460.734375Mb
2020-10-13 06:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 06:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.76171875Mb; avail=237460.734375Mb
2020-10-13 06:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032201
2020-10-13 06:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035237
2020-10-13 06:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.28125Mb; avail=237457.21484375Mb
2020-10-13 06:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7099.09765625Mb; avail=237455.3984375Mb
2020-10-13 06:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001684
2020-10-13 06:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.59765625Mb; avail=237454.8984375Mb
2020-10-13 06:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 06:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.59765625Mb; avail=237454.8984375Mb
2020-10-13 06:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.031919
2020-10-13 06:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034391
2020-10-13 06:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.74609375Mb; avail=237450.75Mb
2020-10-13 06:56:11 | INFO | fairseq.trainer | begin training epoch 27
2020-10-13 06:56:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7113.48828125Mb; avail=237440.8359375Mb
2020-10-13 06:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000896
2020-10-13 06:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.48828125Mb; avail=237440.8359375Mb
2020-10-13 06:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006723
2020-10-13 06:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.09375Mb; avail=237440.23046875Mb
2020-10-13 06:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005073
2020-10-13 06:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013470
2020-10-13 06:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.69921875Mb; avail=237439.625Mb
2020-10-13 06:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7115.91015625Mb; avail=237438.4140625Mb
2020-10-13 06:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000609
2020-10-13 06:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7115.91015625Mb; avail=237438.4140625Mb
2020-10-13 06:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006678
2020-10-13 06:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7116.515625Mb; avail=237437.80859375Mb
2020-10-13 06:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005062
2020-10-13 06:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013083
2020-10-13 06:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7117.12109375Mb; avail=237437.203125Mb
2020-10-13 06:56:25 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.937 | nll_loss 6.849 | ppl 115.24 | wps 38528.2 | wpb 1971 | bsz 82.7 | num_updates 1350 | best_loss 7.937
2020-10-13 06:56:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:56:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 27 @ 1350 updates, score 7.937) (writing took 4.723129550926387 seconds)
2020-10-13 06:56:30 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-13 06:56:30 | INFO | train | epoch 027 | loss 7.705 | nll_loss 6.655 | ppl 100.79 | wps 11775.3 | ups 2.63 | wpb 4470.2 | bsz 180.3 | num_updates 1350 | lr 6.75663e-05 | gnorm 1.763 | clip 0 | train_wall 13 | wall 508
2020-10-13 06:56:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-13 06:56:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-13 06:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7091.68359375Mb; avail=237462.80859375Mb
2020-10-13 06:56:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000227
2020-10-13 06:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002431
2020-10-13 06:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.68359375Mb; avail=237462.80859375Mb
2020-10-13 06:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 06:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.68359375Mb; avail=237462.80859375Mb
2020-10-13 06:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033258
2020-10-13 06:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036645
2020-10-13 06:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.68359375Mb; avail=237462.80859375Mb
2020-10-13 06:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7091.68359375Mb; avail=237462.80859375Mb
2020-10-13 06:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001688
2020-10-13 06:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.68359375Mb; avail=237462.80859375Mb
2020-10-13 06:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 06:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.68359375Mb; avail=237462.80859375Mb
2020-10-13 06:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032283
2020-10-13 06:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034786
2020-10-13 06:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.68359375Mb; avail=237462.80859375Mb
2020-10-13 06:56:30 | INFO | fairseq.trainer | begin training epoch 28
2020-10-13 06:56:44 | INFO | train_inner | epoch 028:     50 / 50 loss=7.667, nll_loss=6.613, ppl=97.9, wps=11777.6, ups=2.63, wpb=4470.2, bsz=180.3, num_updates=1400, lr=7.0065e-05, gnorm=1.762, clip=0, train_wall=26, wall=522
2020-10-13 06:56:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7124.04296875Mb; avail=237430.359375Mb
2020-10-13 06:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000882
2020-10-13 06:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7124.04296875Mb; avail=237430.359375Mb
2020-10-13 06:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006957
2020-10-13 06:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7123.55078125Mb; avail=237430.8515625Mb
2020-10-13 06:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005143
2020-10-13 06:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013734
2020-10-13 06:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7123.55078125Mb; avail=237430.8515625Mb
2020-10-13 06:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7114.19921875Mb; avail=237440.203125Mb
2020-10-13 06:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000630
2020-10-13 06:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.19921875Mb; avail=237440.203125Mb
2020-10-13 06:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006723
2020-10-13 06:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.19921875Mb; avail=237440.203125Mb
2020-10-13 06:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005089
2020-10-13 06:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013195
2020-10-13 06:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.19921875Mb; avail=237440.203125Mb
2020-10-13 06:56:44 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.906 | nll_loss 6.813 | ppl 112.44 | wps 38267 | wpb 1971 | bsz 82.7 | num_updates 1400 | best_loss 7.906
2020-10-13 06:56:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:56:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 28 @ 1400 updates, score 7.906) (writing took 4.733461955096573 seconds)
2020-10-13 06:56:49 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-13 06:56:49 | INFO | train | epoch 028 | loss 7.63 | nll_loss 6.571 | ppl 95.09 | wps 11773 | ups 2.63 | wpb 4470.2 | bsz 180.3 | num_updates 1400 | lr 7.0065e-05 | gnorm 1.761 | clip 0 | train_wall 13 | wall 527
2020-10-13 06:56:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-13 06:56:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-13 06:56:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7093.75Mb; avail=237460.75390625Mb
2020-10-13 06:56:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000250
2020-10-13 06:56:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002465
2020-10-13 06:56:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.75Mb; avail=237460.75390625Mb
2020-10-13 06:56:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000126
2020-10-13 06:56:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.75Mb; avail=237460.75390625Mb
2020-10-13 06:56:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032936
2020-10-13 06:56:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036376
2020-10-13 06:56:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.7578125Mb; avail=237460.74609375Mb
2020-10-13 06:56:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7093.44140625Mb; avail=237461.0625Mb
2020-10-13 06:56:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001621
2020-10-13 06:56:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.44140625Mb; avail=237461.0625Mb
2020-10-13 06:56:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000071
2020-10-13 06:56:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.44140625Mb; avail=237461.0625Mb
2020-10-13 06:56:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032224
2020-10-13 06:56:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034633
2020-10-13 06:56:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.328125Mb; avail=237461.296875Mb
2020-10-13 06:56:49 | INFO | fairseq.trainer | begin training epoch 29
2020-10-13 06:57:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7115.35546875Mb; avail=237439.6640625Mb
2020-10-13 06:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000919
2020-10-13 06:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7115.9609375Mb; avail=237439.05859375Mb
2020-10-13 06:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006760
2020-10-13 06:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7116.56640625Mb; avail=237438.453125Mb
2020-10-13 06:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005177
2020-10-13 06:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013630
2020-10-13 06:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7117.171875Mb; avail=237437.84765625Mb
2020-10-13 06:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7118.74609375Mb; avail=237436.2734375Mb
2020-10-13 06:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000635
2020-10-13 06:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7118.74609375Mb; avail=237436.2734375Mb
2020-10-13 06:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006853
2020-10-13 06:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7119.3515625Mb; avail=237435.66796875Mb
2020-10-13 06:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005053
2020-10-13 06:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013282
2020-10-13 06:57:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7119.94921875Mb; avail=237435.0703125Mb
2020-10-13 06:57:03 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.85 | nll_loss 6.759 | ppl 108.31 | wps 37929 | wpb 1971 | bsz 82.7 | num_updates 1450 | best_loss 7.85
2020-10-13 06:57:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:57:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 29 @ 1450 updates, score 7.85) (writing took 4.726340332068503 seconds)
2020-10-13 06:57:08 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-13 06:57:08 | INFO | train | epoch 029 | loss 7.553 | nll_loss 6.483 | ppl 89.43 | wps 11745.2 | ups 2.63 | wpb 4470.2 | bsz 180.3 | num_updates 1450 | lr 7.25638e-05 | gnorm 1.784 | clip 0 | train_wall 13 | wall 546
2020-10-13 06:57:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-13 06:57:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-13 06:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7091.83984375Mb; avail=237462.6328125Mb
2020-10-13 06:57:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000274
2020-10-13 06:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002431
2020-10-13 06:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.83984375Mb; avail=237462.6328125Mb
2020-10-13 06:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 06:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.83984375Mb; avail=237462.6328125Mb
2020-10-13 06:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033185
2020-10-13 06:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036470
2020-10-13 06:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.83984375Mb; avail=237462.6328125Mb
2020-10-13 06:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7091.83984375Mb; avail=237462.6328125Mb
2020-10-13 06:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001707
2020-10-13 06:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.83984375Mb; avail=237462.6328125Mb
2020-10-13 06:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000072
2020-10-13 06:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.83984375Mb; avail=237462.6328125Mb
2020-10-13 06:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032636
2020-10-13 06:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035150
2020-10-13 06:57:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.80859375Mb; avail=237462.6640625Mb
2020-10-13 06:57:08 | INFO | fairseq.trainer | begin training epoch 30
2020-10-13 06:57:22 | INFO | train_inner | epoch 030:     50 / 50 loss=7.512, nll_loss=6.436, ppl=86.57, wps=11739.6, ups=2.63, wpb=4470.2, bsz=180.3, num_updates=1500, lr=7.50625e-05, gnorm=1.725, clip=0, train_wall=26, wall=560
2020-10-13 06:57:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7113.171875Mb; avail=237441.5Mb
2020-10-13 06:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000911
2020-10-13 06:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.171875Mb; avail=237441.5Mb
2020-10-13 06:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006804
2020-10-13 06:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.7734375Mb; avail=237441.8984375Mb
2020-10-13 06:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005060
2020-10-13 06:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013554
2020-10-13 06:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.7734375Mb; avail=237441.8984375Mb
2020-10-13 06:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7112.7578125Mb; avail=237441.9140625Mb
2020-10-13 06:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000601
2020-10-13 06:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.7578125Mb; avail=237441.9140625Mb
2020-10-13 06:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006512
2020-10-13 06:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.7578125Mb; avail=237441.9140625Mb
2020-10-13 06:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005126
2020-10-13 06:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012983
2020-10-13 06:57:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.7578125Mb; avail=237441.9140625Mb
2020-10-13 06:57:23 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.836 | nll_loss 6.722 | ppl 105.54 | wps 38059.2 | wpb 1971 | bsz 82.7 | num_updates 1500 | best_loss 7.836
2020-10-13 06:57:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:57:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 30 @ 1500 updates, score 7.836) (writing took 4.764381462009624 seconds)
2020-10-13 06:57:27 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-13 06:57:27 | INFO | train | epoch 030 | loss 7.47 | nll_loss 6.389 | ppl 83.8 | wps 11714.1 | ups 2.62 | wpb 4470.2 | bsz 180.3 | num_updates 1500 | lr 7.50625e-05 | gnorm 1.665 | clip 0 | train_wall 13 | wall 565
2020-10-13 06:57:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-13 06:57:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-13 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7091.9453125Mb; avail=237462.53515625Mb
2020-10-13 06:57:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000244
2020-10-13 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002285
2020-10-13 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.9453125Mb; avail=237462.53515625Mb
2020-10-13 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000098
2020-10-13 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.9453125Mb; avail=237462.53515625Mb
2020-10-13 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033008
2020-10-13 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036228
2020-10-13 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.9453125Mb; avail=237462.53515625Mb
2020-10-13 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7092.04296875Mb; avail=237462.4375Mb
2020-10-13 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001696
2020-10-13 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.04296875Mb; avail=237462.4375Mb
2020-10-13 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000073
2020-10-13 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.04296875Mb; avail=237462.4375Mb
2020-10-13 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032038
2020-10-13 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034530
2020-10-13 06:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.2578125Mb; avail=237462.453125Mb
2020-10-13 06:57:27 | INFO | fairseq.trainer | begin training epoch 31
2020-10-13 06:57:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7113.8671875Mb; avail=237440.42578125Mb
2020-10-13 06:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000922
2020-10-13 06:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.8671875Mb; avail=237440.42578125Mb
2020-10-13 06:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006785
2020-10-13 06:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.8671875Mb; avail=237440.42578125Mb
2020-10-13 06:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005119
2020-10-13 06:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013606
2020-10-13 06:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.8671875Mb; avail=237440.42578125Mb
2020-10-13 06:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7113.8671875Mb; avail=237440.42578125Mb
2020-10-13 06:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000600
2020-10-13 06:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.8671875Mb; avail=237440.42578125Mb
2020-10-13 06:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006757
2020-10-13 06:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.859375Mb; avail=237440.43359375Mb
2020-10-13 06:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005051
2020-10-13 06:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013140
2020-10-13 06:57:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.859375Mb; avail=237440.43359375Mb
2020-10-13 06:57:41 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.811 | nll_loss 6.72 | ppl 105.4 | wps 38293.2 | wpb 1971 | bsz 82.7 | num_updates 1550 | best_loss 7.811
2020-10-13 06:57:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:57:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 31 @ 1550 updates, score 7.811) (writing took 4.749857274116948 seconds)
2020-10-13 06:57:46 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-13 06:57:46 | INFO | train | epoch 031 | loss 7.43 | nll_loss 6.34 | ppl 81.03 | wps 11805.2 | ups 2.64 | wpb 4470.2 | bsz 180.3 | num_updates 1550 | lr 7.75613e-05 | gnorm 2.196 | clip 0 | train_wall 13 | wall 584
2020-10-13 06:57:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-13 06:57:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-13 06:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7093.4921875Mb; avail=237460.98046875Mb
2020-10-13 06:57:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000277
2020-10-13 06:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002431
2020-10-13 06:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.4921875Mb; avail=237460.98046875Mb
2020-10-13 06:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 06:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.4921875Mb; avail=237460.98046875Mb
2020-10-13 06:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033600
2020-10-13 06:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036904
2020-10-13 06:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.4921875Mb; avail=237460.98046875Mb
2020-10-13 06:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7093.4921875Mb; avail=237460.98046875Mb
2020-10-13 06:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001704
2020-10-13 06:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.4921875Mb; avail=237460.98046875Mb
2020-10-13 06:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000072
2020-10-13 06:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.4921875Mb; avail=237460.98046875Mb
2020-10-13 06:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032534
2020-10-13 06:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035035
2020-10-13 06:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7093.4921875Mb; avail=237460.98046875Mb
2020-10-13 06:57:46 | INFO | fairseq.trainer | begin training epoch 32
2020-10-13 06:58:00 | INFO | train_inner | epoch 032:     50 / 50 loss=7.381, nll_loss=6.285, ppl=77.98, wps=11805.7, ups=2.64, wpb=4470.2, bsz=180.3, num_updates=1600, lr=8.006e-05, gnorm=1.957, clip=0, train_wall=26, wall=598
2020-10-13 06:58:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:58:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7114.0546875Mb; avail=237440.0703125Mb
2020-10-13 06:58:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000880
2020-10-13 06:58:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.0546875Mb; avail=237440.0703125Mb
2020-10-13 06:58:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006935
2020-10-13 06:58:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.0546875Mb; avail=237440.0703125Mb
2020-10-13 06:58:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005180
2020-10-13 06:58:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013759
2020-10-13 06:58:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.0546875Mb; avail=237440.0703125Mb
2020-10-13 06:58:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7114.0546875Mb; avail=237440.0703125Mb
2020-10-13 06:58:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000622
2020-10-13 06:58:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.0546875Mb; avail=237440.0703125Mb
2020-10-13 06:58:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006647
2020-10-13 06:58:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.0546875Mb; avail=237440.0703125Mb
2020-10-13 06:58:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004991
2020-10-13 06:58:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013012
2020-10-13 06:58:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.0546875Mb; avail=237440.0703125Mb
2020-10-13 06:58:00 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.794 | nll_loss 6.672 | ppl 101.98 | wps 38360.1 | wpb 1971 | bsz 82.7 | num_updates 1600 | best_loss 7.794
2020-10-13 06:58:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:58:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 32 @ 1600 updates, score 7.794) (writing took 4.724445074098185 seconds)
2020-10-13 06:58:05 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-13 06:58:05 | INFO | train | epoch 032 | loss 7.332 | nll_loss 6.23 | ppl 75.04 | wps 11833 | ups 2.65 | wpb 4470.2 | bsz 180.3 | num_updates 1600 | lr 8.006e-05 | gnorm 1.719 | clip 0 | train_wall 13 | wall 603
2020-10-13 06:58:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-13 06:58:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-13 06:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7092.37890625Mb; avail=237462.10546875Mb
2020-10-13 06:58:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000227
2020-10-13 06:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002516
2020-10-13 06:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.37890625Mb; avail=237462.10546875Mb
2020-10-13 06:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000132
2020-10-13 06:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.37890625Mb; avail=237462.10546875Mb
2020-10-13 06:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033816
2020-10-13 06:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.037257
2020-10-13 06:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.40625Mb; avail=237459.078125Mb
2020-10-13 06:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7097.828125Mb; avail=237456.65625Mb
2020-10-13 06:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001703
2020-10-13 06:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.828125Mb; avail=237456.65625Mb
2020-10-13 06:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000074
2020-10-13 06:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.828125Mb; avail=237456.65625Mb
2020-10-13 06:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032482
2020-10-13 06:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035023
2020-10-13 06:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.9765625Mb; avail=237452.5078125Mb
2020-10-13 06:58:05 | INFO | fairseq.trainer | begin training epoch 33
2020-10-13 06:58:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:58:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7112.77734375Mb; avail=237442.2421875Mb
2020-10-13 06:58:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000942
2020-10-13 06:58:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.77734375Mb; avail=237442.2421875Mb
2020-10-13 06:58:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006845
2020-10-13 06:58:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.77734375Mb; avail=237442.2421875Mb
2020-10-13 06:58:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005142
2020-10-13 06:58:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013691
2020-10-13 06:58:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.77734375Mb; avail=237442.2421875Mb
2020-10-13 06:58:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7112.77734375Mb; avail=237442.2421875Mb
2020-10-13 06:58:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000596
2020-10-13 06:58:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.77734375Mb; avail=237442.2421875Mb
2020-10-13 06:58:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006842
2020-10-13 06:58:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.77734375Mb; avail=237442.2421875Mb
2020-10-13 06:58:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005097
2020-10-13 06:58:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013305
2020-10-13 06:58:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7112.77734375Mb; avail=237442.2421875Mb
2020-10-13 06:58:19 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.775 | nll_loss 6.661 | ppl 101.17 | wps 38338.1 | wpb 1971 | bsz 82.7 | num_updates 1650 | best_loss 7.775
2020-10-13 06:58:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:58:24 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 33 @ 1650 updates, score 7.775) (writing took 4.724204896017909 seconds)
2020-10-13 06:58:24 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-13 06:58:24 | INFO | train | epoch 033 | loss 7.242 | nll_loss 6.129 | ppl 69.97 | wps 11800 | ups 2.64 | wpb 4470.2 | bsz 180.3 | num_updates 1650 | lr 8.25588e-05 | gnorm 1.654 | clip 0 | train_wall 13 | wall 622
2020-10-13 06:58:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-13 06:58:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-13 06:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7107.61328125Mb; avail=237446.87109375Mb
2020-10-13 06:58:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000251
2020-10-13 06:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002414
2020-10-13 06:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7107.61328125Mb; avail=237446.87109375Mb
2020-10-13 06:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 06:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7107.61328125Mb; avail=237446.87109375Mb
2020-10-13 06:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032976
2020-10-13 06:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036216
2020-10-13 06:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.79296875Mb; avail=237452.69140625Mb
2020-10-13 06:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7092.2421875Mb; avail=237462.2421875Mb
2020-10-13 06:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001627
2020-10-13 06:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.2421875Mb; avail=237462.2421875Mb
2020-10-13 06:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000075
2020-10-13 06:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.2421875Mb; avail=237462.2421875Mb
2020-10-13 06:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032587
2020-10-13 06:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035010
2020-10-13 06:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.21875Mb; avail=237462.265625Mb
2020-10-13 06:58:24 | INFO | fairseq.trainer | begin training epoch 34
2020-10-13 06:58:38 | INFO | train_inner | epoch 034:     50 / 50 loss=7.208, nll_loss=6.089, ppl=68.06, wps=11806.4, ups=2.64, wpb=4470.2, bsz=180.3, num_updates=1700, lr=8.50575e-05, gnorm=1.771, clip=0, train_wall=26, wall=635
2020-10-13 06:58:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7122.8984375Mb; avail=237431.41015625Mb
2020-10-13 06:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000876
2020-10-13 06:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7122.8984375Mb; avail=237431.41015625Mb
2020-10-13 06:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006834
2020-10-13 06:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7123.41796875Mb; avail=237430.890625Mb
2020-10-13 06:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005146
2020-10-13 06:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013630
2020-10-13 06:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7123.41796875Mb; avail=237430.890625Mb
2020-10-13 06:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7123.41796875Mb; avail=237430.890625Mb
2020-10-13 06:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000613
2020-10-13 06:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7123.41796875Mb; avail=237430.890625Mb
2020-10-13 06:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006563
2020-10-13 06:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7122.92578125Mb; avail=237431.3828125Mb
2020-10-13 06:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005043
2020-10-13 06:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012961
2020-10-13 06:58:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7123.4453125Mb; avail=237430.86328125Mb
2020-10-13 06:58:38 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.796 | nll_loss 6.68 | ppl 102.52 | wps 38254 | wpb 1971 | bsz 82.7 | num_updates 1700 | best_loss 7.775
2020-10-13 06:58:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:58:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 34 @ 1700 updates, score 7.796) (writing took 2.529268875019625 seconds)
2020-10-13 06:58:41 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-13 06:58:41 | INFO | train | epoch 034 | loss 7.173 | nll_loss 6.049 | ppl 66.21 | wps 13362.4 | ups 2.99 | wpb 4470.2 | bsz 180.3 | num_updates 1700 | lr 8.50575e-05 | gnorm 1.887 | clip 0 | train_wall 13 | wall 638
2020-10-13 06:58:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-13 06:58:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-13 06:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7091.5390625Mb; avail=237462.94140625Mb
2020-10-13 06:58:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000224
2020-10-13 06:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002188
2020-10-13 06:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.5390625Mb; avail=237462.94140625Mb
2020-10-13 06:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 06:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.5390625Mb; avail=237462.94140625Mb
2020-10-13 06:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032687
2020-10-13 06:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035678
2020-10-13 06:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.5390625Mb; avail=237462.94140625Mb
2020-10-13 06:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7091.5390625Mb; avail=237462.94140625Mb
2020-10-13 06:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001639
2020-10-13 06:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.5390625Mb; avail=237462.94140625Mb
2020-10-13 06:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 06:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.5390625Mb; avail=237462.94140625Mb
2020-10-13 06:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032021
2020-10-13 06:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034463
2020-10-13 06:58:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.5390625Mb; avail=237462.94140625Mb
2020-10-13 06:58:41 | INFO | fairseq.trainer | begin training epoch 35
2020-10-13 06:58:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7124.05078125Mb; avail=237430.44140625Mb
2020-10-13 06:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000900
2020-10-13 06:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7124.05078125Mb; avail=237430.44140625Mb
2020-10-13 06:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006786
2020-10-13 06:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7124.05078125Mb; avail=237430.44140625Mb
2020-10-13 06:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005114
2020-10-13 06:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013562
2020-10-13 06:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.69921875Mb; avail=237439.79296875Mb
2020-10-13 06:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7114.69921875Mb; avail=237439.79296875Mb
2020-10-13 06:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000639
2020-10-13 06:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.69921875Mb; avail=237439.79296875Mb
2020-10-13 06:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006675
2020-10-13 06:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.69921875Mb; avail=237439.79296875Mb
2020-10-13 06:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005068
2020-10-13 06:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013107
2020-10-13 06:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.69921875Mb; avail=237439.79296875Mb
2020-10-13 06:58:55 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.783 | nll_loss 6.665 | ppl 101.45 | wps 38402.4 | wpb 1971 | bsz 82.7 | num_updates 1750 | best_loss 7.775
2020-10-13 06:58:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:58:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 35 @ 1750 updates, score 7.783) (writing took 2.5304272149223834 seconds)
2020-10-13 06:58:58 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-13 06:58:58 | INFO | train | epoch 035 | loss 7.105 | nll_loss 5.972 | ppl 62.76 | wps 13324 | ups 2.98 | wpb 4470.2 | bsz 180.3 | num_updates 1750 | lr 8.75563e-05 | gnorm 1.862 | clip 0 | train_wall 13 | wall 655
2020-10-13 06:58:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-13 06:58:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-13 06:58:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7091.65234375Mb; avail=237462.83203125Mb
2020-10-13 06:58:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000226
2020-10-13 06:58:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002197
2020-10-13 06:58:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.65234375Mb; avail=237462.83203125Mb
2020-10-13 06:58:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000078
2020-10-13 06:58:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.65234375Mb; avail=237462.83203125Mb
2020-10-13 06:58:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032367
2020-10-13 06:58:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035373
2020-10-13 06:58:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.65234375Mb; avail=237462.83203125Mb
2020-10-13 06:58:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7091.65234375Mb; avail=237462.83203125Mb
2020-10-13 06:58:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001689
2020-10-13 06:58:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.65234375Mb; avail=237462.83203125Mb
2020-10-13 06:58:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 06:58:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.65234375Mb; avail=237462.83203125Mb
2020-10-13 06:58:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032300
2020-10-13 06:58:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034795
2020-10-13 06:58:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.65234375Mb; avail=237462.83203125Mb
2020-10-13 06:58:58 | INFO | fairseq.trainer | begin training epoch 36
2020-10-13 06:59:11 | INFO | train_inner | epoch 036:     50 / 50 loss=7.063, nll_loss=5.923, ppl=60.67, wps=13311, ups=2.98, wpb=4470.2, bsz=180.3, num_updates=1800, lr=9.0055e-05, gnorm=1.82, clip=0, train_wall=26, wall=669
2020-10-13 06:59:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:59:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7114.66796875Mb; avail=237439.80859375Mb
2020-10-13 06:59:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000885
2020-10-13 06:59:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.66796875Mb; avail=237439.80859375Mb
2020-10-13 06:59:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006866
2020-10-13 06:59:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.66796875Mb; avail=237439.80859375Mb
2020-10-13 06:59:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005122
2020-10-13 06:59:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013641
2020-10-13 06:59:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.66796875Mb; avail=237439.80859375Mb
2020-10-13 06:59:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7114.21875Mb; avail=237440.2578125Mb
2020-10-13 06:59:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000605
2020-10-13 06:59:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.21875Mb; avail=237440.2578125Mb
2020-10-13 06:59:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006599
2020-10-13 06:59:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.21875Mb; avail=237440.2578125Mb
2020-10-13 06:59:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005010
2020-10-13 06:59:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012940
2020-10-13 06:59:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.21875Mb; avail=237440.2578125Mb
2020-10-13 06:59:12 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.741 | nll_loss 6.611 | ppl 97.72 | wps 37823.4 | wpb 1971 | bsz 82.7 | num_updates 1800 | best_loss 7.741
2020-10-13 06:59:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:59:17 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 36 @ 1800 updates, score 7.741) (writing took 4.720457155024633 seconds)
2020-10-13 06:59:17 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-13 06:59:17 | INFO | train | epoch 036 | loss 7.021 | nll_loss 5.874 | ppl 58.65 | wps 11759.4 | ups 2.63 | wpb 4470.2 | bsz 180.3 | num_updates 1800 | lr 9.0055e-05 | gnorm 1.778 | clip 0 | train_wall 13 | wall 674
2020-10-13 06:59:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-13 06:59:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-13 06:59:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7090.36328125Mb; avail=237464.11328125Mb
2020-10-13 06:59:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000273
2020-10-13 06:59:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002542
2020-10-13 06:59:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.36328125Mb; avail=237464.11328125Mb
2020-10-13 06:59:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 06:59:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.36328125Mb; avail=237464.11328125Mb
2020-10-13 06:59:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.034383
2020-10-13 06:59:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.037794
2020-10-13 06:59:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.36328125Mb; avail=237464.11328125Mb
2020-10-13 06:59:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7090.36328125Mb; avail=237464.11328125Mb
2020-10-13 06:59:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001707
2020-10-13 06:59:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.36328125Mb; avail=237464.11328125Mb
2020-10-13 06:59:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 06:59:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.36328125Mb; avail=237464.11328125Mb
2020-10-13 06:59:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032760
2020-10-13 06:59:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035261
2020-10-13 06:59:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7090.36328125Mb; avail=237464.11328125Mb
2020-10-13 06:59:17 | INFO | fairseq.trainer | begin training epoch 37
2020-10-13 06:59:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:59:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7096.60546875Mb; avail=237458.265625Mb
2020-10-13 06:59:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000925
2020-10-13 06:59:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.60546875Mb; avail=237458.265625Mb
2020-10-13 06:59:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006760
2020-10-13 06:59:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.2109375Mb; avail=237457.66015625Mb
2020-10-13 06:59:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005166
2020-10-13 06:59:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013642
2020-10-13 06:59:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.81640625Mb; avail=237457.0546875Mb
2020-10-13 06:59:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7099.02734375Mb; avail=237455.84375Mb
2020-10-13 06:59:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000566
2020-10-13 06:59:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.02734375Mb; avail=237455.84375Mb
2020-10-13 06:59:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006568
2020-10-13 06:59:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.6328125Mb; avail=237455.23828125Mb
2020-10-13 06:59:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005054
2020-10-13 06:59:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012964
2020-10-13 06:59:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.23828125Mb; avail=237454.6328125Mb
2020-10-13 06:59:31 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.723 | nll_loss 6.589 | ppl 96.28 | wps 38366.7 | wpb 1971 | bsz 82.7 | num_updates 1850 | best_loss 7.723
2020-10-13 06:59:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:59:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 37 @ 1850 updates, score 7.723) (writing took 4.712756654014811 seconds)
2020-10-13 06:59:35 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-13 06:59:35 | INFO | train | epoch 037 | loss 6.96 | nll_loss 5.804 | ppl 55.88 | wps 11858.8 | ups 2.65 | wpb 4470.2 | bsz 180.3 | num_updates 1850 | lr 9.25538e-05 | gnorm 2.085 | clip 0 | train_wall 13 | wall 693
2020-10-13 06:59:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-13 06:59:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-13 06:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7072.69140625Mb; avail=237481.80078125Mb
2020-10-13 06:59:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000333
2020-10-13 06:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002469
2020-10-13 06:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7072.69140625Mb; avail=237481.80078125Mb
2020-10-13 06:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 06:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7072.69140625Mb; avail=237481.80078125Mb
2020-10-13 06:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033456
2020-10-13 06:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036840
2020-10-13 06:59:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7072.69140625Mb; avail=237481.80078125Mb
2020-10-13 06:59:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7072.69140625Mb; avail=237481.80078125Mb
2020-10-13 06:59:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001714
2020-10-13 06:59:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7072.69140625Mb; avail=237481.80078125Mb
2020-10-13 06:59:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069
2020-10-13 06:59:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7072.69140625Mb; avail=237481.80078125Mb
2020-10-13 06:59:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032081
2020-10-13 06:59:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034580
2020-10-13 06:59:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7072.69140625Mb; avail=237481.80078125Mb
2020-10-13 06:59:36 | INFO | fairseq.trainer | begin training epoch 38
2020-10-13 06:59:49 | INFO | train_inner | epoch 038:     50 / 50 loss=6.914, nll_loss=5.752, ppl=53.9, wps=11861.9, ups=2.65, wpb=4470.2, bsz=180.3, num_updates=1900, lr=9.50525e-05, gnorm=1.953, clip=0, train_wall=26, wall=707
2020-10-13 06:59:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 06:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7095.54296875Mb; avail=237459.49609375Mb
2020-10-13 06:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000900
2020-10-13 06:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.54296875Mb; avail=237459.49609375Mb
2020-10-13 06:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006811
2020-10-13 06:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.54296875Mb; avail=237459.49609375Mb
2020-10-13 06:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005069
2020-10-13 06:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013529
2020-10-13 06:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.3203125Mb; avail=237459.71875Mb
2020-10-13 06:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7095.328125Mb; avail=237459.7109375Mb
2020-10-13 06:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000597
2020-10-13 06:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.328125Mb; avail=237459.7109375Mb
2020-10-13 06:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006743
2020-10-13 06:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.3203125Mb; avail=237459.71875Mb
2020-10-13 06:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005074
2020-10-13 06:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013207
2020-10-13 06:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.3203125Mb; avail=237459.71875Mb
2020-10-13 06:59:50 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.707 | nll_loss 6.565 | ppl 94.68 | wps 38419.3 | wpb 1971 | bsz 82.7 | num_updates 1900 | best_loss 7.707
2020-10-13 06:59:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 06:59:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 38 @ 1900 updates, score 7.707) (writing took 4.766897422960028 seconds)
2020-10-13 06:59:54 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-13 06:59:54 | INFO | train | epoch 038 | loss 6.868 | nll_loss 5.7 | ppl 51.99 | wps 11842.8 | ups 2.65 | wpb 4470.2 | bsz 180.3 | num_updates 1900 | lr 9.50525e-05 | gnorm 1.821 | clip 0 | train_wall 13 | wall 712
2020-10-13 06:59:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-13 06:59:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-13 06:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7072.55859375Mb; avail=237481.90625Mb
2020-10-13 06:59:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000279
2020-10-13 06:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002489
2020-10-13 06:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7072.55859375Mb; avail=237481.90625Mb
2020-10-13 06:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 06:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7072.55859375Mb; avail=237482.02734375Mb
2020-10-13 06:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.034248
2020-10-13 06:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.037668
2020-10-13 06:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7072.4453125Mb; avail=237482.01953125Mb
2020-10-13 06:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7078.76953125Mb; avail=237475.6953125Mb
2020-10-13 06:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001641
2020-10-13 06:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.76953125Mb; avail=237475.6953125Mb
2020-10-13 06:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000072
2020-10-13 06:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.76953125Mb; avail=237475.6953125Mb
2020-10-13 06:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032276
2020-10-13 06:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034703
2020-10-13 06:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.46484375Mb; avail=237470.24609375Mb
2020-10-13 06:59:54 | INFO | fairseq.trainer | begin training epoch 39
2020-10-13 07:00:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7095.8203125Mb; avail=237458.453125Mb
2020-10-13 07:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000917
2020-10-13 07:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.8203125Mb; avail=237458.453125Mb
2020-10-13 07:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006980
2020-10-13 07:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.8203125Mb; avail=237458.453125Mb
2020-10-13 07:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005114
2020-10-13 07:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013818
2020-10-13 07:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.8203125Mb; avail=237458.453125Mb
2020-10-13 07:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7096.3515625Mb; avail=237457.921875Mb
2020-10-13 07:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000623
2020-10-13 07:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.3515625Mb; avail=237457.921875Mb
2020-10-13 07:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006791
2020-10-13 07:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.3515625Mb; avail=237457.921875Mb
2020-10-13 07:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005129
2020-10-13 07:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013291
2020-10-13 07:00:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.3515625Mb; avail=237457.921875Mb
2020-10-13 07:00:09 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.699 | nll_loss 6.553 | ppl 93.9 | wps 38084 | wpb 1971 | bsz 82.7 | num_updates 1950 | best_loss 7.699
2020-10-13 07:00:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:00:13 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 39 @ 1950 updates, score 7.699) (writing took 4.726330895908177 seconds)
2020-10-13 07:00:13 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-13 07:00:13 | INFO | train | epoch 039 | loss 6.811 | nll_loss 5.633 | ppl 49.63 | wps 11778.4 | ups 2.63 | wpb 4470.2 | bsz 180.3 | num_updates 1950 | lr 9.75513e-05 | gnorm 2.175 | clip 0 | train_wall 13 | wall 731
2020-10-13 07:00:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-13 07:00:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-13 07:00:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7074.59765625Mb; avail=237479.8984375Mb
2020-10-13 07:00:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000229
2020-10-13 07:00:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002210
2020-10-13 07:00:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7074.59765625Mb; avail=237479.8984375Mb
2020-10-13 07:00:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 07:00:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7074.59765625Mb; avail=237479.8984375Mb
2020-10-13 07:00:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032528
2020-10-13 07:00:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035587
2020-10-13 07:00:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7074.59765625Mb; avail=237479.8984375Mb
2020-10-13 07:00:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7074.59765625Mb; avail=237479.8984375Mb
2020-10-13 07:00:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001666
2020-10-13 07:00:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7074.59765625Mb; avail=237479.8984375Mb
2020-10-13 07:00:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000071
2020-10-13 07:00:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7074.59765625Mb; avail=237479.8984375Mb
2020-10-13 07:00:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032273
2020-10-13 07:00:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034738
2020-10-13 07:00:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7074.59765625Mb; avail=237479.8984375Mb
2020-10-13 07:00:13 | INFO | fairseq.trainer | begin training epoch 40
2020-10-13 07:00:27 | INFO | train_inner | epoch 040:     50 / 50 loss=6.765, nll_loss=5.581, ppl=47.87, wps=11786.8, ups=2.64, wpb=4470.2, bsz=180.3, num_updates=2000, lr=0.00010005, gnorm=2.055, clip=0, train_wall=26, wall=745
2020-10-13 07:00:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7096.70703125Mb; avail=237457.4296875Mb
2020-10-13 07:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000894
2020-10-13 07:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.70703125Mb; avail=237457.4296875Mb
2020-10-13 07:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006749
2020-10-13 07:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.70703125Mb; avail=237457.4296875Mb
2020-10-13 07:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005192
2020-10-13 07:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013606
2020-10-13 07:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.70703125Mb; avail=237457.4296875Mb
2020-10-13 07:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7096.70703125Mb; avail=237457.4296875Mb
2020-10-13 07:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000567
2020-10-13 07:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.70703125Mb; avail=237457.4296875Mb
2020-10-13 07:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006540
2020-10-13 07:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.70703125Mb; avail=237457.4296875Mb
2020-10-13 07:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005092
2020-10-13 07:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012932
2020-10-13 07:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.70703125Mb; avail=237457.4296875Mb
2020-10-13 07:00:27 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.696 | nll_loss 6.549 | ppl 93.66 | wps 38339.8 | wpb 1971 | bsz 82.7 | num_updates 2000 | best_loss 7.696
2020-10-13 07:00:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:00:32 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 40 @ 2000 updates, score 7.696) (writing took 4.714787870878354 seconds)
2020-10-13 07:00:32 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-13 07:00:32 | INFO | train | epoch 040 | loss 6.719 | nll_loss 5.529 | ppl 46.17 | wps 11826.5 | ups 2.65 | wpb 4470.2 | bsz 180.3 | num_updates 2000 | lr 0.00010005 | gnorm 1.934 | clip 0 | train_wall 13 | wall 750
2020-10-13 07:00:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-13 07:00:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-13 07:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7074.31640625Mb; avail=237480.171875Mb
2020-10-13 07:00:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000350
2020-10-13 07:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002515
2020-10-13 07:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7074.31640625Mb; avail=237480.171875Mb
2020-10-13 07:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 07:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7074.31640625Mb; avail=237480.171875Mb
2020-10-13 07:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032981
2020-10-13 07:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036434
2020-10-13 07:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7074.31640625Mb; avail=237480.171875Mb
2020-10-13 07:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7074.31640625Mb; avail=237480.171875Mb
2020-10-13 07:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001632
2020-10-13 07:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7074.31640625Mb; avail=237480.171875Mb
2020-10-13 07:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000071
2020-10-13 07:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7074.31640625Mb; avail=237480.171875Mb
2020-10-13 07:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032019
2020-10-13 07:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034433
2020-10-13 07:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7074.31640625Mb; avail=237480.171875Mb
2020-10-13 07:00:32 | INFO | fairseq.trainer | begin training epoch 41
2020-10-13 07:00:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7096.234375Mb; avail=237458.078125Mb
2020-10-13 07:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000892
2020-10-13 07:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.234375Mb; avail=237458.078125Mb
2020-10-13 07:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006830
2020-10-13 07:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.234375Mb; avail=237458.078125Mb
2020-10-13 07:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005116
2020-10-13 07:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013602
2020-10-13 07:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.234375Mb; avail=237458.078125Mb
2020-10-13 07:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7096.234375Mb; avail=237458.078125Mb
2020-10-13 07:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000589
2020-10-13 07:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.234375Mb; avail=237458.078125Mb
2020-10-13 07:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006636
2020-10-13 07:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.234375Mb; avail=237458.078125Mb
2020-10-13 07:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005047
2020-10-13 07:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012984
2020-10-13 07:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.234375Mb; avail=237458.078125Mb
2020-10-13 07:00:46 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.705 | nll_loss 6.562 | ppl 94.52 | wps 38083.4 | wpb 1971 | bsz 82.7 | num_updates 2050 | best_loss 7.696
2020-10-13 07:00:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:00:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 41 @ 2050 updates, score 7.705) (writing took 2.5386773000936955 seconds)
2020-10-13 07:00:49 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2020-10-13 07:00:49 | INFO | train | epoch 041 | loss 6.653 | nll_loss 5.452 | ppl 43.78 | wps 13375.6 | ups 2.99 | wpb 4470.2 | bsz 180.3 | num_updates 2050 | lr 0.000102549 | gnorm 2.151 | clip 0 | train_wall 13 | wall 767
2020-10-13 07:00:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=42/shard_epoch=41
2020-10-13 07:00:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=42/shard_epoch=42
2020-10-13 07:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7084.08203125Mb; avail=237470.40234375Mb
2020-10-13 07:00:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000238
2020-10-13 07:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002241
2020-10-13 07:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.6875Mb; avail=237469.796875Mb
2020-10-13 07:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 07:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.6875Mb; avail=237469.796875Mb
2020-10-13 07:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032514
2020-10-13 07:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035561
2020-10-13 07:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.375Mb; avail=237459.109375Mb
2020-10-13 07:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7096.47265625Mb; avail=237458.01171875Mb
2020-10-13 07:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001629
2020-10-13 07:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.47265625Mb; avail=237458.01171875Mb
2020-10-13 07:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000072
2020-10-13 07:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7096.47265625Mb; avail=237458.01171875Mb
2020-10-13 07:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.031697
2020-10-13 07:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034155
2020-10-13 07:00:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.78515625Mb; avail=237477.69921875Mb
2020-10-13 07:00:49 | INFO | fairseq.trainer | begin training epoch 42
2020-10-13 07:01:03 | INFO | train_inner | epoch 042:     50 / 50 loss=6.602, nll_loss=5.394, ppl=42.05, wps=12536.5, ups=2.8, wpb=4470.2, bsz=180.3, num_updates=2100, lr=0.000105048, gnorm=1.994, clip=0, train_wall=26, wall=780
2020-10-13 07:01:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7097.0703125Mb; avail=237457.41796875Mb
2020-10-13 07:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000927
2020-10-13 07:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.0703125Mb; avail=237457.41796875Mb
2020-10-13 07:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006849
2020-10-13 07:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.0703125Mb; avail=237457.41796875Mb
2020-10-13 07:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005157
2020-10-13 07:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013715
2020-10-13 07:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.0703125Mb; avail=237457.41796875Mb
2020-10-13 07:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7097.0703125Mb; avail=237457.41796875Mb
2020-10-13 07:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000596
2020-10-13 07:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.0703125Mb; avail=237457.41796875Mb
2020-10-13 07:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006740
2020-10-13 07:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.0703125Mb; avail=237457.41796875Mb
2020-10-13 07:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005172
2020-10-13 07:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013228
2020-10-13 07:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.0703125Mb; avail=237457.41796875Mb
2020-10-13 07:01:03 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.665 | nll_loss 6.53 | ppl 92.39 | wps 38076.9 | wpb 1971 | bsz 82.7 | num_updates 2100 | best_loss 7.665
2020-10-13 07:01:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:01:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_best.pt (epoch 42 @ 2100 updates, score 7.665) (writing took 4.711102155037224 seconds)
2020-10-13 07:01:08 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2020-10-13 07:01:08 | INFO | train | epoch 042 | loss 6.55 | nll_loss 5.336 | ppl 40.39 | wps 11796.6 | ups 2.64 | wpb 4470.2 | bsz 180.3 | num_updates 2100 | lr 0.000105048 | gnorm 1.837 | clip 0 | train_wall 13 | wall 786
2020-10-13 07:01:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=43/shard_epoch=42
2020-10-13 07:01:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=43/shard_epoch=43
2020-10-13 07:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7082.328125Mb; avail=237472.16015625Mb
2020-10-13 07:01:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000314
2020-10-13 07:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002459
2020-10-13 07:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7082.328125Mb; avail=237472.16015625Mb
2020-10-13 07:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 07:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7082.328125Mb; avail=237472.16015625Mb
2020-10-13 07:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033007
2020-10-13 07:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036367
2020-10-13 07:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7082.8203125Mb; avail=237471.66796875Mb
2020-10-13 07:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7073.46875Mb; avail=237481.01953125Mb
2020-10-13 07:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001637
2020-10-13 07:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7073.46875Mb; avail=237481.01953125Mb
2020-10-13 07:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000071
2020-10-13 07:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7073.46875Mb; avail=237481.01953125Mb
2020-10-13 07:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032178
2020-10-13 07:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034646
2020-10-13 07:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7073.46875Mb; avail=237481.01953125Mb
2020-10-13 07:01:08 | INFO | fairseq.trainer | begin training epoch 43
2020-10-13 07:01:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:01:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7097.21484375Mb; avail=237457.27734375Mb
2020-10-13 07:01:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000942
2020-10-13 07:01:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.21484375Mb; avail=237457.27734375Mb
2020-10-13 07:01:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006771
2020-10-13 07:01:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.21484375Mb; avail=237457.27734375Mb
2020-10-13 07:01:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005232
2020-10-13 07:01:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013703
2020-10-13 07:01:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.21484375Mb; avail=237457.27734375Mb
2020-10-13 07:01:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7097.21484375Mb; avail=237457.27734375Mb
2020-10-13 07:01:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000570
2020-10-13 07:01:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.21484375Mb; avail=237457.27734375Mb
2020-10-13 07:01:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006786
2020-10-13 07:01:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.21484375Mb; avail=237457.27734375Mb
2020-10-13 07:01:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005000
2020-10-13 07:01:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013111
2020-10-13 07:01:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.21484375Mb; avail=237457.27734375Mb
2020-10-13 07:01:22 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.67 | nll_loss 6.512 | ppl 91.27 | wps 37730.9 | wpb 1971 | bsz 82.7 | num_updates 2150 | best_loss 7.665
2020-10-13 07:01:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:01:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 43 @ 2150 updates, score 7.67) (writing took 2.5450774778146297 seconds)
2020-10-13 07:01:25 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2020-10-13 07:01:25 | INFO | train | epoch 043 | loss 6.488 | nll_loss 5.264 | ppl 38.41 | wps 13278.3 | ups 2.97 | wpb 4470.2 | bsz 180.3 | num_updates 2150 | lr 0.000107546 | gnorm 2.047 | clip 0 | train_wall 13 | wall 802
2020-10-13 07:01:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=44/shard_epoch=43
2020-10-13 07:01:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=44/shard_epoch=44
2020-10-13 07:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7084.62890625Mb; avail=237469.859375Mb
2020-10-13 07:01:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000244
2020-10-13 07:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002193
2020-10-13 07:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7083.64453125Mb; avail=237472.8125Mb
2020-10-13 07:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 07:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.27734375Mb; avail=237479.2109375Mb
2020-10-13 07:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032219
2020-10-13 07:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035204
2020-10-13 07:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.27734375Mb; avail=237479.2109375Mb
2020-10-13 07:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7075.27734375Mb; avail=237479.2109375Mb
2020-10-13 07:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001613
2020-10-13 07:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.27734375Mb; avail=237479.2109375Mb
2020-10-13 07:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069
2020-10-13 07:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.27734375Mb; avail=237479.2109375Mb
2020-10-13 07:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032027
2020-10-13 07:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034443
2020-10-13 07:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.27734375Mb; avail=237479.2109375Mb
2020-10-13 07:01:25 | INFO | fairseq.trainer | begin training epoch 44
2020-10-13 07:01:38 | INFO | train_inner | epoch 044:     50 / 50 loss=6.44, nll_loss=5.209, ppl=37, wps=12536.2, ups=2.8, wpb=4470.2, bsz=180.3, num_updates=2200, lr=0.000110045, gnorm=1.974, clip=0, train_wall=26, wall=816
2020-10-13 07:01:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:01:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7098.265625Mb; avail=237456.04296875Mb
2020-10-13 07:01:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000886
2020-10-13 07:01:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.265625Mb; avail=237456.04296875Mb
2020-10-13 07:01:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006915
2020-10-13 07:01:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.265625Mb; avail=237456.04296875Mb
2020-10-13 07:01:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005079
2020-10-13 07:01:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013640
2020-10-13 07:01:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.265625Mb; avail=237456.04296875Mb
2020-10-13 07:01:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7098.265625Mb; avail=237456.04296875Mb
2020-10-13 07:01:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000610
2020-10-13 07:01:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.265625Mb; avail=237456.04296875Mb
2020-10-13 07:01:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006601
2020-10-13 07:01:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.265625Mb; avail=237456.04296875Mb
2020-10-13 07:01:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004934
2020-10-13 07:01:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012868
2020-10-13 07:01:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.265625Mb; avail=237456.04296875Mb
2020-10-13 07:01:39 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.702 | nll_loss 6.536 | ppl 92.78 | wps 38288.2 | wpb 1971 | bsz 82.7 | num_updates 2200 | best_loss 7.665
2020-10-13 07:01:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:01:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 44 @ 2200 updates, score 7.702) (writing took 2.5583500780630857 seconds)
2020-10-13 07:01:41 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2020-10-13 07:01:41 | INFO | train | epoch 044 | loss 6.392 | nll_loss 5.155 | ppl 35.63 | wps 13408.8 | ups 3 | wpb 4470.2 | bsz 180.3 | num_updates 2200 | lr 0.000110045 | gnorm 1.902 | clip 0 | train_wall 13 | wall 819
2020-10-13 07:01:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=45/shard_epoch=44
2020-10-13 07:01:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=45/shard_epoch=45
2020-10-13 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7076.7890625Mb; avail=237477.6875Mb
2020-10-13 07:01:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000231
2020-10-13 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002185
2020-10-13 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.39453125Mb; avail=237477.08203125Mb
2020-10-13 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.39453125Mb; avail=237476.9609375Mb
2020-10-13 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032324
2020-10-13 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035367
2020-10-13 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.54296875Mb; avail=237473.93359375Mb
2020-10-13 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7085.39453125Mb; avail=237469.08203125Mb
2020-10-13 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001663
2020-10-13 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7086.00390625Mb; avail=237468.47265625Mb
2020-10-13 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000071
2020-10-13 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7086.00390625Mb; avail=237468.47265625Mb
2020-10-13 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.031976
2020-10-13 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034425
2020-10-13 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7086.5Mb; avail=237467.9765625Mb
2020-10-13 07:01:41 | INFO | fairseq.trainer | begin training epoch 45
2020-10-13 07:01:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7099.79296875Mb; avail=237454.50390625Mb
2020-10-13 07:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000888
2020-10-13 07:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.79296875Mb; avail=237454.50390625Mb
2020-10-13 07:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006820
2020-10-13 07:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.79296875Mb; avail=237454.50390625Mb
2020-10-13 07:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005095
2020-10-13 07:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013554
2020-10-13 07:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.79296875Mb; avail=237454.50390625Mb
2020-10-13 07:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7099.79296875Mb; avail=237454.50390625Mb
2020-10-13 07:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000567
2020-10-13 07:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.79296875Mb; avail=237454.50390625Mb
2020-10-13 07:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006586
2020-10-13 07:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.79296875Mb; avail=237454.50390625Mb
2020-10-13 07:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004940
2020-10-13 07:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012810
2020-10-13 07:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.79296875Mb; avail=237454.50390625Mb
2020-10-13 07:01:55 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.811 | nll_loss 6.68 | ppl 102.51 | wps 37886.2 | wpb 1971 | bsz 82.7 | num_updates 2250 | best_loss 7.665
2020-10-13 07:01:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:01:58 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 45 @ 2250 updates, score 7.811) (writing took 2.5278844151180238 seconds)
2020-10-13 07:01:58 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2020-10-13 07:01:58 | INFO | train | epoch 045 | loss 6.316 | nll_loss 5.067 | ppl 33.52 | wps 13396.2 | ups 3 | wpb 4470.2 | bsz 180.3 | num_updates 2250 | lr 0.000112544 | gnorm 1.986 | clip 0 | train_wall 13 | wall 836
2020-10-13 07:01:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=46/shard_epoch=45
2020-10-13 07:01:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=46/shard_epoch=46
2020-10-13 07:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7083.625Mb; avail=237470.8671875Mb
2020-10-13 07:01:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000225
2020-10-13 07:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002199
2020-10-13 07:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7083.625Mb; avail=237470.8671875Mb
2020-10-13 07:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 07:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7083.625Mb; avail=237470.8671875Mb
2020-10-13 07:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033037
2020-10-13 07:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036037
2020-10-13 07:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.5859375Mb; avail=237461.90625Mb
2020-10-13 07:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7094.7734375Mb; avail=237459.71875Mb
2020-10-13 07:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001690
2020-10-13 07:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7094.7734375Mb; avail=237459.71875Mb
2020-10-13 07:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000072
2020-10-13 07:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7094.7734375Mb; avail=237459.71875Mb
2020-10-13 07:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032916
2020-10-13 07:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035395
2020-10-13 07:01:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7092.65625Mb; avail=237461.8359375Mb
2020-10-13 07:01:58 | INFO | fairseq.trainer | begin training epoch 46
2020-10-13 07:02:12 | INFO | train_inner | epoch 046:     50 / 50 loss=6.273, nll_loss=5.017, ppl=32.37, wps=13347.2, ups=2.99, wpb=4470.2, bsz=180.3, num_updates=2300, lr=0.000115043, gnorm=1.965, clip=0, train_wall=26, wall=849
2020-10-13 07:02:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7099.140625Mb; avail=237455.53515625Mb
2020-10-13 07:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000888
2020-10-13 07:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.140625Mb; avail=237455.53515625Mb
2020-10-13 07:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006693
2020-10-13 07:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.140625Mb; avail=237455.53515625Mb
2020-10-13 07:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005116
2020-10-13 07:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013504
2020-10-13 07:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.140625Mb; avail=237455.53515625Mb
2020-10-13 07:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7099.140625Mb; avail=237455.53515625Mb
2020-10-13 07:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000607
2020-10-13 07:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.140625Mb; avail=237455.53515625Mb
2020-10-13 07:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006571
2020-10-13 07:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.140625Mb; avail=237455.53515625Mb
2020-10-13 07:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005005
2020-10-13 07:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012912
2020-10-13 07:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.140625Mb; avail=237455.53515625Mb
2020-10-13 07:02:12 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.67 | nll_loss 6.515 | ppl 91.45 | wps 38388.4 | wpb 1971 | bsz 82.7 | num_updates 2300 | best_loss 7.665
2020-10-13 07:02:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:02:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 46 @ 2300 updates, score 7.67) (writing took 2.356689319945872 seconds)
2020-10-13 07:02:15 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2020-10-13 07:02:15 | INFO | train | epoch 046 | loss 6.229 | nll_loss 4.966 | ppl 31.26 | wps 13459.7 | ups 3.01 | wpb 4470.2 | bsz 180.3 | num_updates 2300 | lr 0.000115043 | gnorm 1.944 | clip 0 | train_wall 13 | wall 852
2020-10-13 07:02:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=47/shard_epoch=46
2020-10-13 07:02:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=47/shard_epoch=47
2020-10-13 07:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7077.6015625Mb; avail=237476.8828125Mb
2020-10-13 07:02:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000246
2020-10-13 07:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002183
2020-10-13 07:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.6015625Mb; avail=237476.8828125Mb
2020-10-13 07:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 07:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.6015625Mb; avail=237476.8828125Mb
2020-10-13 07:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032068
2020-10-13 07:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035061
2020-10-13 07:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.6015625Mb; avail=237476.8828125Mb
2020-10-13 07:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7077.6015625Mb; avail=237476.8828125Mb
2020-10-13 07:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001630
2020-10-13 07:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.6015625Mb; avail=237476.8828125Mb
2020-10-13 07:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069
2020-10-13 07:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.6015625Mb; avail=237476.8828125Mb
2020-10-13 07:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032128
2020-10-13 07:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034573
2020-10-13 07:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.6015625Mb; avail=237476.8828125Mb
2020-10-13 07:02:15 | INFO | fairseq.trainer | begin training epoch 47
2020-10-13 07:02:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7100.48828125Mb; avail=237453.98828125Mb
2020-10-13 07:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000891
2020-10-13 07:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.48828125Mb; avail=237453.98828125Mb
2020-10-13 07:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006705
2020-10-13 07:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.09375Mb; avail=237453.3828125Mb
2020-10-13 07:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005130
2020-10-13 07:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013486
2020-10-13 07:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.69921875Mb; avail=237452.77734375Mb
2020-10-13 07:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7103.15234375Mb; avail=237451.32421875Mb
2020-10-13 07:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000685
2020-10-13 07:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.15234375Mb; avail=237451.32421875Mb
2020-10-13 07:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006668
2020-10-13 07:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.7578125Mb; avail=237450.71875Mb
2020-10-13 07:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005038
2020-10-13 07:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013191
2020-10-13 07:02:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.7421875Mb; avail=237450.734375Mb
2020-10-13 07:02:29 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.714 | nll_loss 6.539 | ppl 92.99 | wps 38036.3 | wpb 1971 | bsz 82.7 | num_updates 2350 | best_loss 7.665
2020-10-13 07:02:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:02:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 47 @ 2350 updates, score 7.714) (writing took 2.531321967020631 seconds)
2020-10-13 07:02:31 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2020-10-13 07:02:31 | INFO | train | epoch 047 | loss 6.156 | nll_loss 4.882 | ppl 29.5 | wps 13408.9 | ups 3 | wpb 4470.2 | bsz 180.3 | num_updates 2350 | lr 0.000117541 | gnorm 2.128 | clip 0 | train_wall 13 | wall 869
2020-10-13 07:02:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=48/shard_epoch=47
2020-10-13 07:02:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=48/shard_epoch=48
2020-10-13 07:02:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7077.015625Mb; avail=237477.43359375Mb
2020-10-13 07:02:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000242
2020-10-13 07:02:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002186
2020-10-13 07:02:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.015625Mb; avail=237477.43359375Mb
2020-10-13 07:02:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000102
2020-10-13 07:02:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.015625Mb; avail=237477.43359375Mb
2020-10-13 07:02:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032660
2020-10-13 07:02:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035670
2020-10-13 07:02:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.015625Mb; avail=237477.43359375Mb
2020-10-13 07:02:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7077.015625Mb; avail=237477.43359375Mb
2020-10-13 07:02:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001644
2020-10-13 07:02:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.015625Mb; avail=237477.43359375Mb
2020-10-13 07:02:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069
2020-10-13 07:02:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.015625Mb; avail=237477.43359375Mb
2020-10-13 07:02:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032177
2020-10-13 07:02:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034597
2020-10-13 07:02:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.015625Mb; avail=237477.43359375Mb
2020-10-13 07:02:31 | INFO | fairseq.trainer | begin training epoch 48
2020-10-13 07:02:45 | INFO | train_inner | epoch 048:     50 / 50 loss=6.129, nll_loss=4.851, ppl=28.86, wps=13451.3, ups=3.01, wpb=4470.2, bsz=180.3, num_updates=2400, lr=0.00012004, gnorm=2.252, clip=0, train_wall=26, wall=883
2020-10-13 07:02:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7098.22265625Mb; avail=237456.29296875Mb
2020-10-13 07:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000880
2020-10-13 07:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.22265625Mb; avail=237456.29296875Mb
2020-10-13 07:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006733
2020-10-13 07:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.3515625Mb; avail=237456.1640625Mb
2020-10-13 07:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005103
2020-10-13 07:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013489
2020-10-13 07:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.3515625Mb; avail=237456.1640625Mb
2020-10-13 07:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7098.34375Mb; avail=237456.171875Mb
2020-10-13 07:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000606
2020-10-13 07:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.34375Mb; avail=237456.171875Mb
2020-10-13 07:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006652
2020-10-13 07:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.34375Mb; avail=237456.171875Mb
2020-10-13 07:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004976
2020-10-13 07:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012944
2020-10-13 07:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.34375Mb; avail=237456.171875Mb
2020-10-13 07:02:45 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.736 | nll_loss 6.543 | ppl 93.23 | wps 38250.9 | wpb 1971 | bsz 82.7 | num_updates 2400 | best_loss 7.665
2020-10-13 07:02:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:02:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 48 @ 2400 updates, score 7.736) (writing took 2.5364019968546927 seconds)
2020-10-13 07:02:48 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2020-10-13 07:02:48 | INFO | train | epoch 048 | loss 6.102 | nll_loss 4.82 | ppl 28.25 | wps 13349.5 | ups 2.99 | wpb 4470.2 | bsz 180.3 | num_updates 2400 | lr 0.00012004 | gnorm 2.375 | clip 0 | train_wall 13 | wall 886
2020-10-13 07:02:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=49/shard_epoch=48
2020-10-13 07:02:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=49/shard_epoch=49
2020-10-13 07:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7077.90234375Mb; avail=237475.9921875Mb
2020-10-13 07:02:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000235
2020-10-13 07:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002191
2020-10-13 07:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.90234375Mb; avail=237475.9921875Mb
2020-10-13 07:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 07:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.90234375Mb; avail=237475.9921875Mb
2020-10-13 07:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032291
2020-10-13 07:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035300
2020-10-13 07:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.90234375Mb; avail=237475.9921875Mb
2020-10-13 07:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7077.90234375Mb; avail=237475.9921875Mb
2020-10-13 07:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001647
2020-10-13 07:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.90234375Mb; avail=237475.9921875Mb
2020-10-13 07:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069
2020-10-13 07:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.90234375Mb; avail=237475.9921875Mb
2020-10-13 07:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032209
2020-10-13 07:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034645
2020-10-13 07:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.90234375Mb; avail=237475.9921875Mb
2020-10-13 07:02:48 | INFO | fairseq.trainer | begin training epoch 49
2020-10-13 07:03:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7101.7734375Mb; avail=237452.70703125Mb
2020-10-13 07:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000926
2020-10-13 07:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.7734375Mb; avail=237452.70703125Mb
2020-10-13 07:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006755
2020-10-13 07:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.37890625Mb; avail=237452.1015625Mb
2020-10-13 07:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005160
2020-10-13 07:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013613
2020-10-13 07:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.984375Mb; avail=237451.49609375Mb
2020-10-13 07:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7104.1953125Mb; avail=237450.28515625Mb
2020-10-13 07:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000610
2020-10-13 07:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7104.1953125Mb; avail=237450.28515625Mb
2020-10-13 07:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006657
2020-10-13 07:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7104.80078125Mb; avail=237449.6796875Mb
2020-10-13 07:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005117
2020-10-13 07:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013135
2020-10-13 07:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7105.40625Mb; avail=237449.07421875Mb
2020-10-13 07:03:02 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.732 | nll_loss 6.55 | ppl 93.72 | wps 38160.6 | wpb 1971 | bsz 82.7 | num_updates 2450 | best_loss 7.665
2020-10-13 07:03:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:03:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 49 @ 2450 updates, score 7.732) (writing took 2.5417993587907404 seconds)
2020-10-13 07:03:05 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2020-10-13 07:03:05 | INFO | train | epoch 049 | loss 5.993 | nll_loss 4.694 | ppl 25.89 | wps 13143.6 | ups 2.94 | wpb 4470.2 | bsz 180.3 | num_updates 2450 | lr 0.000122539 | gnorm 2.078 | clip 0 | train_wall 13 | wall 903
2020-10-13 07:03:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=50/shard_epoch=49
2020-10-13 07:03:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=50/shard_epoch=50
2020-10-13 07:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7078.5234375Mb; avail=237475.9765625Mb
2020-10-13 07:03:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000225
2020-10-13 07:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002232
2020-10-13 07:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.5234375Mb; avail=237475.9765625Mb
2020-10-13 07:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 07:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.5234375Mb; avail=237475.9765625Mb
2020-10-13 07:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032288
2020-10-13 07:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035325
2020-10-13 07:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.5Mb; avail=237476.0Mb
2020-10-13 07:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7078.5Mb; avail=237476.0Mb
2020-10-13 07:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001708
2020-10-13 07:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.5Mb; avail=237476.0Mb
2020-10-13 07:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 07:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.5Mb; avail=237476.0Mb
2020-10-13 07:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032296
2020-10-13 07:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034787
2020-10-13 07:03:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.5Mb; avail=237476.0Mb
2020-10-13 07:03:05 | INFO | fairseq.trainer | begin training epoch 50
2020-10-13 07:03:19 | INFO | train_inner | epoch 050:     50 / 50 loss=5.947, nll_loss=4.642, ppl=24.97, wps=13242.3, ups=2.96, wpb=4470.2, bsz=180.3, num_updates=2500, lr=0.000125037, gnorm=2.036, clip=0, train_wall=26, wall=916
2020-10-13 07:03:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7099.5Mb; avail=237454.79296875Mb
2020-10-13 07:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000884
2020-10-13 07:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.5Mb; avail=237454.79296875Mb
2020-10-13 07:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006701
2020-10-13 07:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.5Mb; avail=237454.79296875Mb
2020-10-13 07:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005091
2020-10-13 07:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013429
2020-10-13 07:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.5Mb; avail=237454.79296875Mb
2020-10-13 07:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7099.5Mb; avail=237454.79296875Mb
2020-10-13 07:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000601
2020-10-13 07:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.5Mb; avail=237454.79296875Mb
2020-10-13 07:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006570
2020-10-13 07:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.5Mb; avail=237454.79296875Mb
2020-10-13 07:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005030
2020-10-13 07:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012934
2020-10-13 07:03:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.5Mb; avail=237454.79296875Mb
2020-10-13 07:03:19 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.792 | nll_loss 6.595 | ppl 96.7 | wps 38144 | wpb 1971 | bsz 82.7 | num_updates 2500 | best_loss 7.665
2020-10-13 07:03:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:03:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 50 @ 2500 updates, score 7.792) (writing took 2.5376898390240967 seconds)
2020-10-13 07:03:22 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2020-10-13 07:03:22 | INFO | train | epoch 050 | loss 5.902 | nll_loss 4.59 | ppl 24.09 | wps 13342 | ups 2.98 | wpb 4470.2 | bsz 180.3 | num_updates 2500 | lr 0.000125037 | gnorm 1.995 | clip 0 | train_wall 13 | wall 919
2020-10-13 07:03:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=51/shard_epoch=50
2020-10-13 07:03:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=51/shard_epoch=51
2020-10-13 07:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7079.34765625Mb; avail=237474.80078125Mb
2020-10-13 07:03:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000243
2020-10-13 07:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002231
2020-10-13 07:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.34765625Mb; avail=237474.80078125Mb
2020-10-13 07:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000078
2020-10-13 07:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.34765625Mb; avail=237474.80078125Mb
2020-10-13 07:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032551
2020-10-13 07:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035586
2020-10-13 07:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.34765625Mb; avail=237474.80078125Mb
2020-10-13 07:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7079.34765625Mb; avail=237474.80078125Mb
2020-10-13 07:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001676
2020-10-13 07:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.34765625Mb; avail=237474.80078125Mb
2020-10-13 07:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 07:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.34765625Mb; avail=237474.80078125Mb
2020-10-13 07:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032102
2020-10-13 07:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034562
2020-10-13 07:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.34765625Mb; avail=237474.80078125Mb
2020-10-13 07:03:22 | INFO | fairseq.trainer | begin training epoch 51
2020-10-13 07:03:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7095.9921875Mb; avail=237458.49609375Mb
2020-10-13 07:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000897
2020-10-13 07:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.9921875Mb; avail=237458.49609375Mb
2020-10-13 07:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006794
2020-10-13 07:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.9921875Mb; avail=237458.49609375Mb
2020-10-13 07:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005178
2020-10-13 07:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013634
2020-10-13 07:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.9921875Mb; avail=237458.49609375Mb
2020-10-13 07:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7095.9921875Mb; avail=237458.49609375Mb
2020-10-13 07:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000566
2020-10-13 07:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.9921875Mb; avail=237458.49609375Mb
2020-10-13 07:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006542
2020-10-13 07:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.9921875Mb; avail=237458.49609375Mb
2020-10-13 07:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005152
2020-10-13 07:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012996
2020-10-13 07:03:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7095.9921875Mb; avail=237458.49609375Mb
2020-10-13 07:03:36 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.782 | nll_loss 6.6 | ppl 97.02 | wps 34773.6 | wpb 1971 | bsz 82.7 | num_updates 2550 | best_loss 7.665
2020-10-13 07:03:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:03:39 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 51 @ 2550 updates, score 7.782) (writing took 2.5445463738869876 seconds)
2020-10-13 07:03:39 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2020-10-13 07:03:39 | INFO | train | epoch 051 | loss 5.832 | nll_loss 4.51 | ppl 22.79 | wps 13180.6 | ups 2.95 | wpb 4470.2 | bsz 180.3 | num_updates 2550 | lr 0.000127536 | gnorm 2.2 | clip 0 | train_wall 13 | wall 936
2020-10-13 07:03:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=52/shard_epoch=51
2020-10-13 07:03:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=52/shard_epoch=52
2020-10-13 07:03:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7075.66015625Mb; avail=237478.83203125Mb
2020-10-13 07:03:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000225
2020-10-13 07:03:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002178
2020-10-13 07:03:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.66015625Mb; avail=237478.83203125Mb
2020-10-13 07:03:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 07:03:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.66015625Mb; avail=237478.83203125Mb
2020-10-13 07:03:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032417
2020-10-13 07:03:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035408
2020-10-13 07:03:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.66015625Mb; avail=237478.83203125Mb
2020-10-13 07:03:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7075.66015625Mb; avail=237478.83203125Mb
2020-10-13 07:03:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001735
2020-10-13 07:03:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.66015625Mb; avail=237478.83203125Mb
2020-10-13 07:03:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 07:03:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.66015625Mb; avail=237478.83203125Mb
2020-10-13 07:03:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032139
2020-10-13 07:03:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034657
2020-10-13 07:03:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.66015625Mb; avail=237478.83203125Mb
2020-10-13 07:03:39 | INFO | fairseq.trainer | begin training epoch 52
2020-10-13 07:03:52 | INFO | train_inner | epoch 052:     50 / 50 loss=5.786, nll_loss=4.456, ppl=21.96, wps=13274.9, ups=2.97, wpb=4470.2, bsz=180.3, num_updates=2600, lr=0.000130035, gnorm=2.173, clip=0, train_wall=26, wall=950
2020-10-13 07:03:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:03:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7097.796875Mb; avail=237457.24609375Mb
2020-10-13 07:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000892
2020-10-13 07:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.796875Mb; avail=237457.24609375Mb
2020-10-13 07:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006888
2020-10-13 07:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.796875Mb; avail=237457.24609375Mb
2020-10-13 07:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005126
2020-10-13 07:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013679
2020-10-13 07:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.796875Mb; avail=237457.24609375Mb
2020-10-13 07:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7097.796875Mb; avail=237457.24609375Mb
2020-10-13 07:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000597
2020-10-13 07:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.796875Mb; avail=237457.24609375Mb
2020-10-13 07:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006899
2020-10-13 07:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.796875Mb; avail=237457.24609375Mb
2020-10-13 07:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005089
2020-10-13 07:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013308
2020-10-13 07:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.796875Mb; avail=237457.24609375Mb
2020-10-13 07:03:53 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.811 | nll_loss 6.644 | ppl 100.04 | wps 38369.2 | wpb 1971 | bsz 82.7 | num_updates 2600 | best_loss 7.665
2020-10-13 07:03:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:03:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 52 @ 2600 updates, score 7.811) (writing took 2.5291040069423616 seconds)
2020-10-13 07:03:55 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2020-10-13 07:03:55 | INFO | train | epoch 052 | loss 5.74 | nll_loss 4.403 | ppl 21.15 | wps 13377.9 | ups 2.99 | wpb 4470.2 | bsz 180.3 | num_updates 2600 | lr 0.000130035 | gnorm 2.147 | clip 0 | train_wall 13 | wall 953
2020-10-13 07:03:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=53/shard_epoch=52
2020-10-13 07:03:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=53/shard_epoch=53
2020-10-13 07:03:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7075.48046875Mb; avail=237479.01171875Mb
2020-10-13 07:03:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000244
2020-10-13 07:03:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002199
2020-10-13 07:03:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.48046875Mb; avail=237479.01171875Mb
2020-10-13 07:03:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000088
2020-10-13 07:03:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.48046875Mb; avail=237479.01171875Mb
2020-10-13 07:03:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032771
2020-10-13 07:03:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035774
2020-10-13 07:03:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.48046875Mb; avail=237479.01171875Mb
2020-10-13 07:03:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7075.2421875Mb; avail=237479.25Mb
2020-10-13 07:03:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001647
2020-10-13 07:03:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.2421875Mb; avail=237479.25Mb
2020-10-13 07:03:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000073
2020-10-13 07:03:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.2421875Mb; avail=237479.25Mb
2020-10-13 07:03:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032200
2020-10-13 07:03:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034625
2020-10-13 07:03:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7075.2421875Mb; avail=237479.25Mb
2020-10-13 07:03:56 | INFO | fairseq.trainer | begin training epoch 53
2020-10-13 07:04:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7098.71484375Mb; avail=237455.57421875Mb
2020-10-13 07:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000892
2020-10-13 07:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.71484375Mb; avail=237455.57421875Mb
2020-10-13 07:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006692
2020-10-13 07:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.71484375Mb; avail=237455.57421875Mb
2020-10-13 07:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005254
2020-10-13 07:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013605
2020-10-13 07:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.71484375Mb; avail=237455.57421875Mb
2020-10-13 07:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7098.71484375Mb; avail=237455.57421875Mb
2020-10-13 07:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000602
2020-10-13 07:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.71484375Mb; avail=237455.57421875Mb
2020-10-13 07:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006494
2020-10-13 07:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.71484375Mb; avail=237455.57421875Mb
2020-10-13 07:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005145
2020-10-13 07:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012999
2020-10-13 07:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.98828125Mb; avail=237455.69921875Mb
2020-10-13 07:04:10 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.784 | nll_loss 6.586 | ppl 96.09 | wps 37890.5 | wpb 1971 | bsz 82.7 | num_updates 2650 | best_loss 7.665
2020-10-13 07:04:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:04:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 53 @ 2650 updates, score 7.784) (writing took 2.5923123378306627 seconds)
2020-10-13 07:04:12 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2020-10-13 07:04:12 | INFO | train | epoch 053 | loss 5.633 | nll_loss 4.28 | ppl 19.43 | wps 13340.3 | ups 2.98 | wpb 4470.2 | bsz 180.3 | num_updates 2650 | lr 0.000132534 | gnorm 1.975 | clip 0 | train_wall 13 | wall 970
2020-10-13 07:04:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=54/shard_epoch=53
2020-10-13 07:04:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=54/shard_epoch=54
2020-10-13 07:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7078.53515625Mb; avail=237475.64453125Mb
2020-10-13 07:04:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000235
2020-10-13 07:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002224
2020-10-13 07:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.53515625Mb; avail=237475.64453125Mb
2020-10-13 07:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 07:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.53515625Mb; avail=237475.64453125Mb
2020-10-13 07:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032413
2020-10-13 07:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035437
2020-10-13 07:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.53515625Mb; avail=237475.64453125Mb
2020-10-13 07:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7078.53515625Mb; avail=237475.64453125Mb
2020-10-13 07:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001636
2020-10-13 07:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.53515625Mb; avail=237475.64453125Mb
2020-10-13 07:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069
2020-10-13 07:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.53515625Mb; avail=237475.64453125Mb
2020-10-13 07:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.031909
2020-10-13 07:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034328
2020-10-13 07:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.53515625Mb; avail=237475.64453125Mb
2020-10-13 07:04:12 | INFO | fairseq.trainer | begin training epoch 54
2020-10-13 07:04:26 | INFO | train_inner | epoch 054:     50 / 50 loss=5.595, nll_loss=4.236, ppl=18.85, wps=13372.6, ups=2.99, wpb=4470.2, bsz=180.3, num_updates=2700, lr=0.000135032, gnorm=2.059, clip=0, train_wall=26, wall=984
2020-10-13 07:04:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7100.84375Mb; avail=237453.57421875Mb
2020-10-13 07:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000875
2020-10-13 07:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.84375Mb; avail=237453.57421875Mb
2020-10-13 07:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006841
2020-10-13 07:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.84375Mb; avail=237453.57421875Mb
2020-10-13 07:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005140
2020-10-13 07:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013608
2020-10-13 07:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.84375Mb; avail=237453.57421875Mb
2020-10-13 07:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7100.84375Mb; avail=237453.57421875Mb
2020-10-13 07:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000555
2020-10-13 07:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.84375Mb; avail=237453.57421875Mb
2020-10-13 07:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006746
2020-10-13 07:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.84375Mb; avail=237453.57421875Mb
2020-10-13 07:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005189
2020-10-13 07:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013275
2020-10-13 07:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.84375Mb; avail=237453.57421875Mb
2020-10-13 07:04:26 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 7.797 | nll_loss 6.586 | ppl 96.1 | wps 38158.2 | wpb 1971 | bsz 82.7 | num_updates 2700 | best_loss 7.665
2020-10-13 07:04:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:04:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 54 @ 2700 updates, score 7.797) (writing took 2.5138269539456815 seconds)
2020-10-13 07:04:29 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2020-10-13 07:04:29 | INFO | train | epoch 054 | loss 5.557 | nll_loss 4.192 | ppl 18.28 | wps 13407 | ups 3 | wpb 4470.2 | bsz 180.3 | num_updates 2700 | lr 0.000135032 | gnorm 2.143 | clip 0 | train_wall 13 | wall 987
2020-10-13 07:04:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=55/shard_epoch=54
2020-10-13 07:04:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=55/shard_epoch=55
2020-10-13 07:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7084.65234375Mb; avail=237469.84375Mb
2020-10-13 07:04:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000246
2020-10-13 07:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002261
2020-10-13 07:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.65234375Mb; avail=237469.84375Mb
2020-10-13 07:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 07:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.65234375Mb; avail=237469.84375Mb
2020-10-13 07:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032668
2020-10-13 07:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035726
2020-10-13 07:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7088.8828125Mb; avail=237465.61328125Mb
2020-10-13 07:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7088.890625Mb; avail=237465.60546875Mb
2020-10-13 07:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001636
2020-10-13 07:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7088.890625Mb; avail=237465.60546875Mb
2020-10-13 07:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 07:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7088.890625Mb; avail=237465.60546875Mb
2020-10-13 07:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032933
2020-10-13 07:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035351
2020-10-13 07:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.046875Mb; avail=237475.44921875Mb
2020-10-13 07:04:29 | INFO | fairseq.trainer | begin training epoch 55
2020-10-13 07:04:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7101.79296875Mb; avail=237452.71484375Mb
2020-10-13 07:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000973
2020-10-13 07:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.79296875Mb; avail=237452.71484375Mb
2020-10-13 07:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006721
2020-10-13 07:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.79296875Mb; avail=237452.71484375Mb
2020-10-13 07:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005249
2020-10-13 07:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013712
2020-10-13 07:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.79296875Mb; avail=237452.71484375Mb
2020-10-13 07:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7101.79296875Mb; avail=237452.71484375Mb
2020-10-13 07:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000610
2020-10-13 07:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.79296875Mb; avail=237452.71484375Mb
2020-10-13 07:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006503
2020-10-13 07:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.79296875Mb; avail=237452.71484375Mb
2020-10-13 07:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005103
2020-10-13 07:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012949
2020-10-13 07:04:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.79296875Mb; avail=237452.71484375Mb
2020-10-13 07:04:43 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 7.83 | nll_loss 6.652 | ppl 100.58 | wps 38089.1 | wpb 1971 | bsz 82.7 | num_updates 2750 | best_loss 7.665
2020-10-13 07:04:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:04:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 55 @ 2750 updates, score 7.83) (writing took 2.5431442509870976 seconds)
2020-10-13 07:04:46 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2020-10-13 07:04:46 | INFO | train | epoch 055 | loss 5.474 | nll_loss 4.095 | ppl 17.09 | wps 13383.1 | ups 2.99 | wpb 4470.2 | bsz 180.3 | num_updates 2750 | lr 0.000137531 | gnorm 2.173 | clip 0 | train_wall 13 | wall 1003
2020-10-13 07:04:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=56/shard_epoch=55
2020-10-13 07:04:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=56/shard_epoch=56
2020-10-13 07:04:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7087.80078125Mb; avail=237466.703125Mb
2020-10-13 07:04:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000245
2020-10-13 07:04:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002171
2020-10-13 07:04:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7087.80078125Mb; avail=237466.703125Mb
2020-10-13 07:04:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 07:04:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7087.80078125Mb; avail=237466.703125Mb
2020-10-13 07:04:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033010
2020-10-13 07:04:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035981
2020-10-13 07:04:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.95703125Mb; avail=237476.546875Mb
2020-10-13 07:04:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7081.40234375Mb; avail=237473.1015625Mb
2020-10-13 07:04:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001639
2020-10-13 07:04:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7081.40234375Mb; avail=237473.1015625Mb
2020-10-13 07:04:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000071
2020-10-13 07:04:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7081.40234375Mb; avail=237473.1015625Mb
2020-10-13 07:04:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032400
2020-10-13 07:04:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034832
2020-10-13 07:04:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.32421875Mb; avail=237470.1796875Mb
2020-10-13 07:04:46 | INFO | fairseq.trainer | begin training epoch 56
2020-10-13 07:04:59 | INFO | train_inner | epoch 056:     50 / 50 loss=5.447, nll_loss=4.065, ppl=16.73, wps=13378.2, ups=2.99, wpb=4470.2, bsz=180.3, num_updates=2800, lr=0.00014003, gnorm=2.304, clip=0, train_wall=26, wall=1017
2020-10-13 07:04:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7166.05078125Mb; avail=237388.2421875Mb
2020-10-13 07:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000841
2020-10-13 07:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7166.05078125Mb; avail=237388.2421875Mb
2020-10-13 07:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006770
2020-10-13 07:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7156.859375Mb; avail=237397.48046875Mb
2020-10-13 07:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005108
2020-10-13 07:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013496
2020-10-13 07:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7156.859375Mb; avail=237397.48046875Mb
2020-10-13 07:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7157.1015625Mb; avail=237397.23828125Mb
2020-10-13 07:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000562
2020-10-13 07:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7157.1015625Mb; avail=237397.23828125Mb
2020-10-13 07:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006655
2020-10-13 07:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7157.1015625Mb; avail=237397.23828125Mb
2020-10-13 07:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005013
2020-10-13 07:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012984
2020-10-13 07:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7157.1015625Mb; avail=237397.23828125Mb
2020-10-13 07:05:00 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 7.883 | nll_loss 6.682 | ppl 102.66 | wps 38212.1 | wpb 1971 | bsz 82.7 | num_updates 2800 | best_loss 7.665
2020-10-13 07:05:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:05:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 56 @ 2800 updates, score 7.883) (writing took 2.538959396071732 seconds)
2020-10-13 07:05:02 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2020-10-13 07:05:02 | INFO | train | epoch 056 | loss 5.421 | nll_loss 4.034 | ppl 16.38 | wps 13359 | ups 2.99 | wpb 4470.2 | bsz 180.3 | num_updates 2800 | lr 0.00014003 | gnorm 2.435 | clip 0 | train_wall 13 | wall 1020
2020-10-13 07:05:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=57/shard_epoch=56
2020-10-13 07:05:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=57/shard_epoch=57
2020-10-13 07:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7080.98828125Mb; avail=237473.515625Mb
2020-10-13 07:05:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000248
2020-10-13 07:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002227
2020-10-13 07:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.98828125Mb; avail=237473.515625Mb
2020-10-13 07:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 07:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.98828125Mb; avail=237473.515625Mb
2020-10-13 07:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033382
2020-10-13 07:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036407
2020-10-13 07:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7086.31640625Mb; avail=237468.1875Mb
2020-10-13 07:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7086.8203125Mb; avail=237467.68359375Mb
2020-10-13 07:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001660
2020-10-13 07:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7086.8203125Mb; avail=237467.68359375Mb
2020-10-13 07:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000072
2020-10-13 07:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7086.8203125Mb; avail=237467.68359375Mb
2020-10-13 07:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032462
2020-10-13 07:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034903
2020-10-13 07:05:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7077.5859375Mb; avail=237476.91796875Mb
2020-10-13 07:05:02 | INFO | fairseq.trainer | begin training epoch 57
2020-10-13 07:05:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:05:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7191.68359375Mb; avail=237362.609375Mb
2020-10-13 07:05:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000811
2020-10-13 07:05:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7192.2890625Mb; avail=237362.00390625Mb
2020-10-13 07:05:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006761
2020-10-13 07:05:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7195.72265625Mb; avail=237358.5703125Mb
2020-10-13 07:05:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005095
2020-10-13 07:05:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013422
2020-10-13 07:05:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7195.72265625Mb; avail=237358.5703125Mb
2020-10-13 07:05:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7195.72265625Mb; avail=237358.5703125Mb
2020-10-13 07:05:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000673
2020-10-13 07:05:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7195.72265625Mb; avail=237358.5703125Mb
2020-10-13 07:05:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006644
2020-10-13 07:05:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7185.87890625Mb; avail=237368.4140625Mb
2020-10-13 07:05:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005055
2020-10-13 07:05:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013109
2020-10-13 07:05:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7186.375Mb; avail=237367.91796875Mb
2020-10-13 07:05:17 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 7.874 | nll_loss 6.663 | ppl 101.32 | wps 38032 | wpb 1971 | bsz 82.7 | num_updates 2850 | best_loss 7.665
2020-10-13 07:05:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:05:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 57 @ 2850 updates, score 7.874) (writing took 2.6181407391559333 seconds)
2020-10-13 07:05:19 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2020-10-13 07:05:19 | INFO | train | epoch 057 | loss 5.342 | nll_loss 3.94 | ppl 15.35 | wps 13207.4 | ups 2.95 | wpb 4470.2 | bsz 180.3 | num_updates 2850 | lr 0.000142529 | gnorm 2.323 | clip 0 | train_wall 13 | wall 1037
2020-10-13 07:05:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=58/shard_epoch=57
2020-10-13 07:05:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=58/shard_epoch=58
2020-10-13 07:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7098.0546875Mb; avail=237456.42578125Mb
2020-10-13 07:05:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000218
2020-10-13 07:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002150
2020-10-13 07:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.0546875Mb; avail=237456.42578125Mb
2020-10-13 07:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000077
2020-10-13 07:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.0546875Mb; avail=237456.42578125Mb
2020-10-13 07:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032264
2020-10-13 07:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035207
2020-10-13 07:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.9921875Mb; avail=237456.48828125Mb
2020-10-13 07:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7097.75Mb; avail=237456.73046875Mb
2020-10-13 07:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001624
2020-10-13 07:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.75Mb; avail=237456.73046875Mb
2020-10-13 07:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000071
2020-10-13 07:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.75Mb; avail=237456.73046875Mb
2020-10-13 07:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.031875
2020-10-13 07:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034281
2020-10-13 07:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7097.75Mb; avail=237456.73046875Mb
2020-10-13 07:05:19 | INFO | fairseq.trainer | begin training epoch 58
2020-10-13 07:05:33 | INFO | train_inner | epoch 058:     50 / 50 loss=5.276, nll_loss=3.865, ppl=14.57, wps=13293.6, ups=2.97, wpb=4470.2, bsz=180.3, num_updates=2900, lr=0.000145028, gnorm=2.19, clip=0, train_wall=26, wall=1051
2020-10-13 07:05:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:05:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7099.38671875Mb; avail=237455.09765625Mb
2020-10-13 07:05:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000938
2020-10-13 07:05:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.38671875Mb; avail=237455.09765625Mb
2020-10-13 07:05:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006752
2020-10-13 07:05:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.38671875Mb; avail=237455.09765625Mb
2020-10-13 07:05:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005185
2020-10-13 07:05:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013623
2020-10-13 07:05:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.38671875Mb; avail=237455.09765625Mb
2020-10-13 07:05:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7099.38671875Mb; avail=237455.09765625Mb
2020-10-13 07:05:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000552
2020-10-13 07:05:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.38671875Mb; avail=237455.09765625Mb
2020-10-13 07:05:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006517
2020-10-13 07:05:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.38671875Mb; avail=237455.09765625Mb
2020-10-13 07:05:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004999
2020-10-13 07:05:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012812
2020-10-13 07:05:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.38671875Mb; avail=237455.09765625Mb
2020-10-13 07:05:33 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 7.913 | nll_loss 6.717 | ppl 105.23 | wps 38129.4 | wpb 1971 | bsz 82.7 | num_updates 2900 | best_loss 7.665
2020-10-13 07:05:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:05:36 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 58 @ 2900 updates, score 7.913) (writing took 2.5222639460116625 seconds)
2020-10-13 07:05:36 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2020-10-13 07:05:36 | INFO | train | epoch 058 | loss 5.21 | nll_loss 3.789 | ppl 13.83 | wps 13378.8 | ups 2.99 | wpb 4470.2 | bsz 180.3 | num_updates 2900 | lr 0.000145028 | gnorm 2.057 | clip 0 | train_wall 13 | wall 1054
2020-10-13 07:05:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=59/shard_epoch=58
2020-10-13 07:05:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=59/shard_epoch=59
2020-10-13 07:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7083.7421875Mb; avail=237470.74609375Mb
2020-10-13 07:05:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000247
2020-10-13 07:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002218
2020-10-13 07:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.2421875Mb; avail=237470.24609375Mb
2020-10-13 07:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 07:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7084.2421875Mb; avail=237470.24609375Mb
2020-10-13 07:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032547
2020-10-13 07:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035617
2020-10-13 07:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7088.46484375Mb; avail=237466.0234375Mb
2020-10-13 07:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7088.9609375Mb; avail=237465.52734375Mb
2020-10-13 07:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001704
2020-10-13 07:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7088.9609375Mb; avail=237465.52734375Mb
2020-10-13 07:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000071
2020-10-13 07:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7088.9609375Mb; avail=237465.52734375Mb
2020-10-13 07:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032016
2020-10-13 07:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034498
2020-10-13 07:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.609375Mb; avail=237474.87890625Mb
2020-10-13 07:05:36 | INFO | fairseq.trainer | begin training epoch 59
2020-10-13 07:05:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7099.71875Mb; avail=237454.8046875Mb
2020-10-13 07:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000883
2020-10-13 07:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.71875Mb; avail=237454.8046875Mb
2020-10-13 07:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006895
2020-10-13 07:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.71875Mb; avail=237454.8046875Mb
2020-10-13 07:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005039
2020-10-13 07:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013563
2020-10-13 07:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.71875Mb; avail=237454.8046875Mb
2020-10-13 07:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7099.71875Mb; avail=237454.8046875Mb
2020-10-13 07:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000567
2020-10-13 07:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.71875Mb; avail=237454.8046875Mb
2020-10-13 07:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006685
2020-10-13 07:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.71875Mb; avail=237454.8046875Mb
2020-10-13 07:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004984
2020-10-13 07:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012948
2020-10-13 07:05:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.71875Mb; avail=237454.8046875Mb
2020-10-13 07:05:50 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 7.924 | nll_loss 6.724 | ppl 105.71 | wps 38288.9 | wpb 1971 | bsz 82.7 | num_updates 2950 | best_loss 7.665
2020-10-13 07:05:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:05:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 59 @ 2950 updates, score 7.924) (writing took 2.5340777467936277 seconds)
2020-10-13 07:05:53 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2020-10-13 07:05:53 | INFO | train | epoch 059 | loss 5.132 | nll_loss 3.699 | ppl 12.98 | wps 13419.6 | ups 3 | wpb 4470.2 | bsz 180.3 | num_updates 2950 | lr 0.000147526 | gnorm 2.211 | clip 0 | train_wall 13 | wall 1070
2020-10-13 07:05:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=60/shard_epoch=59
2020-10-13 07:05:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=60/shard_epoch=60
2020-10-13 07:05:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7076.54296875Mb; avail=237477.9765625Mb
2020-10-13 07:05:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000248
2020-10-13 07:05:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002251
2020-10-13 07:05:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.54296875Mb; avail=237477.9765625Mb
2020-10-13 07:05:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 07:05:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.54296875Mb; avail=237477.9765625Mb
2020-10-13 07:05:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032071
2020-10-13 07:05:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035144
2020-10-13 07:05:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.54296875Mb; avail=237477.9765625Mb
2020-10-13 07:05:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7076.54296875Mb; avail=237477.9765625Mb
2020-10-13 07:05:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001714
2020-10-13 07:05:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.54296875Mb; avail=237477.9765625Mb
2020-10-13 07:05:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069
2020-10-13 07:05:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.54296875Mb; avail=237477.9765625Mb
2020-10-13 07:05:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032187
2020-10-13 07:05:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034683
2020-10-13 07:05:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.54296875Mb; avail=237477.9765625Mb
2020-10-13 07:05:53 | INFO | fairseq.trainer | begin training epoch 60
2020-10-13 07:06:06 | INFO | train_inner | epoch 060:     50 / 50 loss=5.079, nll_loss=3.638, ppl=12.45, wps=13386.8, ups=2.99, wpb=4470.2, bsz=180.3, num_updates=3000, lr=0.000150025, gnorm=2.117, clip=0, train_wall=26, wall=1084
2020-10-13 07:06:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7100.96484375Mb; avail=237454.07421875Mb
2020-10-13 07:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000892
2020-10-13 07:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.96484375Mb; avail=237454.07421875Mb
2020-10-13 07:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006720
2020-10-13 07:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.96484375Mb; avail=237454.07421875Mb
2020-10-13 07:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005107
2020-10-13 07:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013490
2020-10-13 07:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.96484375Mb; avail=237454.07421875Mb
2020-10-13 07:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7100.96484375Mb; avail=237454.07421875Mb
2020-10-13 07:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000595
2020-10-13 07:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.96484375Mb; avail=237454.07421875Mb
2020-10-13 07:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006888
2020-10-13 07:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.96484375Mb; avail=237454.07421875Mb
2020-10-13 07:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005111
2020-10-13 07:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013343
2020-10-13 07:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.96484375Mb; avail=237454.07421875Mb
2020-10-13 07:06:07 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 7.944 | nll_loss 6.755 | ppl 107.98 | wps 38332 | wpb 1971 | bsz 82.7 | num_updates 3000 | best_loss 7.665
2020-10-13 07:06:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:06:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 60 @ 3000 updates, score 7.944) (writing took 2.5253964711446315 seconds)
2020-10-13 07:06:09 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2020-10-13 07:06:09 | INFO | train | epoch 060 | loss 5.027 | nll_loss 3.577 | ppl 11.93 | wps 13369.3 | ups 2.99 | wpb 4470.2 | bsz 180.3 | num_updates 3000 | lr 0.000150025 | gnorm 2.024 | clip 0 | train_wall 13 | wall 1087
2020-10-13 07:06:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=61/shard_epoch=60
2020-10-13 07:06:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=61/shard_epoch=61
2020-10-13 07:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7076.95703125Mb; avail=237477.53515625Mb
2020-10-13 07:06:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000251
2020-10-13 07:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002245
2020-10-13 07:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.95703125Mb; avail=237477.53515625Mb
2020-10-13 07:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 07:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.95703125Mb; avail=237477.53515625Mb
2020-10-13 07:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033630
2020-10-13 07:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036673
2020-10-13 07:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7076.95703125Mb; avail=237477.53515625Mb
2020-10-13 07:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7078.0546875Mb; avail=237476.4375Mb
2020-10-13 07:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001691
2020-10-13 07:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.66015625Mb; avail=237475.83203125Mb
2020-10-13 07:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069
2020-10-13 07:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.66015625Mb; avail=237475.83203125Mb
2020-10-13 07:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032382
2020-10-13 07:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034855
2020-10-13 07:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7083.8828125Mb; avail=237470.83203125Mb
2020-10-13 07:06:09 | INFO | fairseq.trainer | begin training epoch 61
2020-10-13 07:06:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7114.96875Mb; avail=237442.76171875Mb
2020-10-13 07:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000915
2020-10-13 07:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7109.5546875Mb; avail=237444.73046875Mb
2020-10-13 07:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006850
2020-10-13 07:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7109.5546875Mb; avail=237444.73046875Mb
2020-10-13 07:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005192
2020-10-13 07:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013752
2020-10-13 07:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7110.1171875Mb; avail=237444.16796875Mb
2020-10-13 07:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7109.625Mb; avail=237444.66015625Mb
2020-10-13 07:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000628
2020-10-13 07:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7105.1953125Mb; avail=237451.55078125Mb
2020-10-13 07:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006801
2020-10-13 07:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.2734375Mb; avail=237454.01171875Mb
2020-10-13 07:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005138
2020-10-13 07:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013304
2020-10-13 07:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.2734375Mb; avail=237454.01171875Mb
2020-10-13 07:06:24 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 8.015 | nll_loss 6.821 | ppl 113.08 | wps 38299 | wpb 1971 | bsz 82.7 | num_updates 3050 | best_loss 7.665
2020-10-13 07:06:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:06:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 61 @ 3050 updates, score 8.015) (writing took 2.5353690031915903 seconds)
2020-10-13 07:06:26 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2020-10-13 07:06:26 | INFO | train | epoch 061 | loss 4.966 | nll_loss 3.504 | ppl 11.35 | wps 13296.2 | ups 2.97 | wpb 4470.2 | bsz 180.3 | num_updates 3050 | lr 0.000152524 | gnorm 2.349 | clip 0 | train_wall 13 | wall 1104
2020-10-13 07:06:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=62/shard_epoch=61
2020-10-13 07:06:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=62/shard_epoch=62
2020-10-13 07:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7078.1484375Mb; avail=237476.34765625Mb
2020-10-13 07:06:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000248
2020-10-13 07:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002268
2020-10-13 07:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.1484375Mb; avail=237476.34765625Mb
2020-10-13 07:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 07:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.1484375Mb; avail=237476.34765625Mb
2020-10-13 07:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032411
2020-10-13 07:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035475
2020-10-13 07:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.1484375Mb; avail=237476.34765625Mb
2020-10-13 07:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7078.125Mb; avail=237476.37890625Mb
2020-10-13 07:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001661
2020-10-13 07:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.125Mb; avail=237476.37890625Mb
2020-10-13 07:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 07:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.125Mb; avail=237476.37890625Mb
2020-10-13 07:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032354
2020-10-13 07:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034794
2020-10-13 07:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.125Mb; avail=237476.37890625Mb
2020-10-13 07:06:26 | INFO | fairseq.trainer | begin training epoch 62
2020-10-13 07:06:40 | INFO | train_inner | epoch 062:     50 / 50 loss=4.915, nll_loss=3.445, ppl=10.89, wps=13344.1, ups=2.99, wpb=4470.2, bsz=180.3, num_updates=3100, lr=0.000155023, gnorm=2.288, clip=0, train_wall=26, wall=1118
2020-10-13 07:06:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7100.359375Mb; avail=237454.12109375Mb
2020-10-13 07:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000893
2020-10-13 07:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.359375Mb; avail=237454.12109375Mb
2020-10-13 07:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006683
2020-10-13 07:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.5703125Mb; avail=237452.91015625Mb
2020-10-13 07:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005173
2020-10-13 07:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013512
2020-10-13 07:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.17578125Mb; avail=237452.3046875Mb
2020-10-13 07:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7103.38671875Mb; avail=237451.09375Mb
2020-10-13 07:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000603
2020-10-13 07:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.38671875Mb; avail=237451.09375Mb
2020-10-13 07:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006613
2020-10-13 07:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.9921875Mb; avail=237450.48828125Mb
2020-10-13 07:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005100
2020-10-13 07:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013048
2020-10-13 07:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7104.59765625Mb; avail=237449.8828125Mb
2020-10-13 07:06:40 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 8.066 | nll_loss 6.866 | ppl 116.62 | wps 36185.3 | wpb 1971 | bsz 82.7 | num_updates 3100 | best_loss 7.665
2020-10-13 07:06:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:06:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 62 @ 3100 updates, score 8.066) (writing took 2.5171114089898765 seconds)
2020-10-13 07:06:43 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2020-10-13 07:06:43 | INFO | train | epoch 062 | loss 4.865 | nll_loss 3.386 | ppl 10.46 | wps 13387.2 | ups 2.99 | wpb 4470.2 | bsz 180.3 | num_updates 3100 | lr 0.000155023 | gnorm 2.227 | clip 0 | train_wall 13 | wall 1121
2020-10-13 07:06:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=63/shard_epoch=62
2020-10-13 07:06:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=63/shard_epoch=63
2020-10-13 07:06:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7085.2578125Mb; avail=237469.21875Mb
2020-10-13 07:06:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000250
2020-10-13 07:06:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002275
2020-10-13 07:06:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.2578125Mb; avail=237469.21875Mb
2020-10-13 07:06:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000082
2020-10-13 07:06:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.2578125Mb; avail=237469.21875Mb
2020-10-13 07:06:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032654
2020-10-13 07:06:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035743
2020-10-13 07:06:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.2578125Mb; avail=237469.21875Mb
2020-10-13 07:06:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7085.2578125Mb; avail=237469.21875Mb
2020-10-13 07:06:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001663
2020-10-13 07:06:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.2578125Mb; avail=237469.21875Mb
2020-10-13 07:06:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000068
2020-10-13 07:06:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.2578125Mb; avail=237469.21875Mb
2020-10-13 07:06:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032161
2020-10-13 07:06:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034607
2020-10-13 07:06:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.2578125Mb; avail=237469.21875Mb
2020-10-13 07:06:43 | INFO | fairseq.trainer | begin training epoch 63
2020-10-13 07:06:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7102.671875Mb; avail=237451.63671875Mb
2020-10-13 07:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000894
2020-10-13 07:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.671875Mb; avail=237451.63671875Mb
2020-10-13 07:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006671
2020-10-13 07:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.671875Mb; avail=237451.63671875Mb
2020-10-13 07:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005103
2020-10-13 07:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013415
2020-10-13 07:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.671875Mb; avail=237451.63671875Mb
2020-10-13 07:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7102.671875Mb; avail=237451.63671875Mb
2020-10-13 07:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000597
2020-10-13 07:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.671875Mb; avail=237451.63671875Mb
2020-10-13 07:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006797
2020-10-13 07:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.671875Mb; avail=237451.63671875Mb
2020-10-13 07:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005015
2020-10-13 07:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013117
2020-10-13 07:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.671875Mb; avail=237451.63671875Mb
2020-10-13 07:06:57 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 8.092 | nll_loss 6.89 | ppl 118.62 | wps 38197.6 | wpb 1971 | bsz 82.7 | num_updates 3150 | best_loss 7.665
2020-10-13 07:06:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:07:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 63 @ 3150 updates, score 8.092) (writing took 2.5311078201048076 seconds)
2020-10-13 07:07:00 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2020-10-13 07:07:00 | INFO | train | epoch 063 | loss 4.772 | nll_loss 3.278 | ppl 9.7 | wps 13370.2 | ups 2.99 | wpb 4470.2 | bsz 180.3 | num_updates 3150 | lr 0.000157521 | gnorm 2.238 | clip 0 | train_wall 13 | wall 1137
2020-10-13 07:07:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=64/shard_epoch=63
2020-10-13 07:07:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=64/shard_epoch=64
2020-10-13 07:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7080.1640625Mb; avail=237474.3203125Mb
2020-10-13 07:07:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000254
2020-10-13 07:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002246
2020-10-13 07:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.1640625Mb; avail=237474.3203125Mb
2020-10-13 07:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000080
2020-10-13 07:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.1640625Mb; avail=237474.3203125Mb
2020-10-13 07:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032318
2020-10-13 07:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035367
2020-10-13 07:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.1640625Mb; avail=237474.3203125Mb
2020-10-13 07:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7080.1640625Mb; avail=237474.3203125Mb
2020-10-13 07:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001687
2020-10-13 07:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.1640625Mb; avail=237474.3203125Mb
2020-10-13 07:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 07:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.1640625Mb; avail=237474.3203125Mb
2020-10-13 07:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032404
2020-10-13 07:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034893
2020-10-13 07:07:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.1640625Mb; avail=237474.3203125Mb
2020-10-13 07:07:00 | INFO | fairseq.trainer | begin training epoch 64
2020-10-13 07:07:13 | INFO | train_inner | epoch 064:     50 / 50 loss=4.741, nll_loss=3.242, ppl=9.46, wps=13354.9, ups=2.99, wpb=4470.2, bsz=180.3, num_updates=3200, lr=0.00016002, gnorm=2.249, clip=0, train_wall=26, wall=1151
2020-10-13 07:07:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:07:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7100.0078125Mb; avail=237454.65625Mb
2020-10-13 07:07:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000911
2020-10-13 07:07:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.0078125Mb; avail=237454.65625Mb
2020-10-13 07:07:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006733
2020-10-13 07:07:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.0078125Mb; avail=237454.65625Mb
2020-10-13 07:07:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005188
2020-10-13 07:07:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013594
2020-10-13 07:07:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.0078125Mb; avail=237454.65625Mb
2020-10-13 07:07:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7100.0078125Mb; avail=237454.65625Mb
2020-10-13 07:07:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000615
2020-10-13 07:07:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.0078125Mb; avail=237454.65625Mb
2020-10-13 07:07:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006730
2020-10-13 07:07:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.0Mb; avail=237454.6640625Mb
2020-10-13 07:07:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005150
2020-10-13 07:07:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013278
2020-10-13 07:07:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.0Mb; avail=237454.6640625Mb
2020-10-13 07:07:14 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 8.088 | nll_loss 6.862 | ppl 116.33 | wps 38120.6 | wpb 1971 | bsz 82.7 | num_updates 3200 | best_loss 7.665
2020-10-13 07:07:14 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:07:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 64 @ 3200 updates, score 8.088) (writing took 2.5313173751346767 seconds)
2020-10-13 07:07:16 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2020-10-13 07:07:16 | INFO | train | epoch 064 | loss 4.71 | nll_loss 3.206 | ppl 9.23 | wps 13325.1 | ups 2.98 | wpb 4470.2 | bsz 180.3 | num_updates 3200 | lr 0.00016002 | gnorm 2.26 | clip 0 | train_wall 13 | wall 1154
2020-10-13 07:07:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=65/shard_epoch=64
2020-10-13 07:07:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=65/shard_epoch=65
2020-10-13 07:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7079.26171875Mb; avail=237475.23046875Mb
2020-10-13 07:07:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000225
2020-10-13 07:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002238
2020-10-13 07:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.26171875Mb; avail=237475.23046875Mb
2020-10-13 07:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 07:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.26171875Mb; avail=237475.23046875Mb
2020-10-13 07:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033080
2020-10-13 07:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036129
2020-10-13 07:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.26171875Mb; avail=237475.23046875Mb
2020-10-13 07:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7078.76953125Mb; avail=237475.72265625Mb
2020-10-13 07:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001613
2020-10-13 07:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.76953125Mb; avail=237475.72265625Mb
2020-10-13 07:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000071
2020-10-13 07:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.76953125Mb; avail=237475.72265625Mb
2020-10-13 07:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033002
2020-10-13 07:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035396
2020-10-13 07:07:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.76953125Mb; avail=237475.72265625Mb
2020-10-13 07:07:16 | INFO | fairseq.trainer | begin training epoch 65
2020-10-13 07:07:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7120.44921875Mb; avail=237433.82421875Mb
2020-10-13 07:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000905
2020-10-13 07:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7120.44921875Mb; avail=237433.82421875Mb
2020-10-13 07:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006903
2020-10-13 07:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7120.44921875Mb; avail=237433.82421875Mb
2020-10-13 07:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005197
2020-10-13 07:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013759
2020-10-13 07:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7120.44921875Mb; avail=237433.82421875Mb
2020-10-13 07:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7120.44921875Mb; avail=237433.82421875Mb
2020-10-13 07:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000634
2020-10-13 07:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7120.43359375Mb; avail=237433.83984375Mb
2020-10-13 07:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006843
2020-10-13 07:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7120.43359375Mb; avail=237433.83984375Mb
2020-10-13 07:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005143
2020-10-13 07:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013399
2020-10-13 07:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7119.94921875Mb; avail=237434.32421875Mb
2020-10-13 07:07:30 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 8.132 | nll_loss 6.919 | ppl 121.05 | wps 38433 | wpb 1971 | bsz 82.7 | num_updates 3250 | best_loss 7.665
2020-10-13 07:07:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:07:33 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 65 @ 3250 updates, score 8.132) (writing took 2.5265448628924787 seconds)
2020-10-13 07:07:33 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2020-10-13 07:07:33 | INFO | train | epoch 065 | loss 4.596 | nll_loss 3.072 | ppl 8.41 | wps 13401.9 | ups 3 | wpb 4470.2 | bsz 180.3 | num_updates 3250 | lr 0.000162519 | gnorm 2.031 | clip 0 | train_wall 13 | wall 1171
2020-10-13 07:07:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=66/shard_epoch=65
2020-10-13 07:07:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=66/shard_epoch=66
2020-10-13 07:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7099.83203125Mb; avail=237454.64453125Mb
2020-10-13 07:07:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000225
2020-10-13 07:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002232
2020-10-13 07:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.83203125Mb; avail=237454.64453125Mb
2020-10-13 07:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 07:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.83203125Mb; avail=237454.64453125Mb
2020-10-13 07:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032304
2020-10-13 07:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035343
2020-10-13 07:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.83203125Mb; avail=237454.64453125Mb
2020-10-13 07:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7099.79296875Mb; avail=237454.68359375Mb
2020-10-13 07:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001621
2020-10-13 07:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.79296875Mb; avail=237454.68359375Mb
2020-10-13 07:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069
2020-10-13 07:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.79296875Mb; avail=237454.68359375Mb
2020-10-13 07:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032085
2020-10-13 07:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034519
2020-10-13 07:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.44140625Mb; avail=237455.03515625Mb
2020-10-13 07:07:33 | INFO | fairseq.trainer | begin training epoch 66
2020-10-13 07:07:47 | INFO | train_inner | epoch 066:     50 / 50 loss=4.58, nll_loss=3.052, ppl=8.3, wps=13378.2, ups=2.99, wpb=4470.2, bsz=180.3, num_updates=3300, lr=0.000165018, gnorm=2.279, clip=0, train_wall=26, wall=1184
2020-10-13 07:07:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:07:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7107.14453125Mb; avail=237447.12890625Mb
2020-10-13 07:07:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000871
2020-10-13 07:07:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7107.14453125Mb; avail=237447.12890625Mb
2020-10-13 07:07:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006683
2020-10-13 07:07:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7111.1796875Mb; avail=237443.09375Mb
2020-10-13 07:07:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005110
2020-10-13 07:07:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013417
2020-10-13 07:07:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7111.78515625Mb; avail=237442.48828125Mb
2020-10-13 07:07:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7113.6015625Mb; avail=237440.671875Mb
2020-10-13 07:07:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000603
2020-10-13 07:07:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7113.6015625Mb; avail=237440.671875Mb
2020-10-13 07:07:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006607
2020-10-13 07:07:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.703125Mb; avail=237439.5703125Mb
2020-10-13 07:07:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005029
2020-10-13 07:07:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012949
2020-10-13 07:07:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7114.81640625Mb; avail=237439.45703125Mb
2020-10-13 07:07:47 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 8.163 | nll_loss 6.954 | ppl 123.97 | wps 38427 | wpb 1971 | bsz 82.7 | num_updates 3300 | best_loss 7.665
2020-10-13 07:07:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:07:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 66 @ 3300 updates, score 8.163) (writing took 2.5244739160407335 seconds)
2020-10-13 07:07:50 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2020-10-13 07:07:50 | INFO | train | epoch 066 | loss 4.563 | nll_loss 3.032 | ppl 8.18 | wps 13378.9 | ups 2.99 | wpb 4470.2 | bsz 180.3 | num_updates 3300 | lr 0.000165018 | gnorm 2.527 | clip 0 | train_wall 13 | wall 1187
2020-10-13 07:07:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=67/shard_epoch=66
2020-10-13 07:07:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=67/shard_epoch=67
2020-10-13 07:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7079.92578125Mb; avail=237474.56640625Mb
2020-10-13 07:07:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000253
2020-10-13 07:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002252
2020-10-13 07:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.92578125Mb; avail=237474.56640625Mb
2020-10-13 07:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 07:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.92578125Mb; avail=237474.56640625Mb
2020-10-13 07:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032603
2020-10-13 07:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035662
2020-10-13 07:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.92578125Mb; avail=237474.56640625Mb
2020-10-13 07:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7079.92578125Mb; avail=237474.56640625Mb
2020-10-13 07:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001695
2020-10-13 07:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.92578125Mb; avail=237474.56640625Mb
2020-10-13 07:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069
2020-10-13 07:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.92578125Mb; avail=237474.56640625Mb
2020-10-13 07:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032141
2020-10-13 07:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034623
2020-10-13 07:07:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.92578125Mb; avail=237474.56640625Mb
2020-10-13 07:07:50 | INFO | fairseq.trainer | begin training epoch 67
2020-10-13 07:08:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7102.63671875Mb; avail=237451.63671875Mb
2020-10-13 07:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000920
2020-10-13 07:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.63671875Mb; avail=237451.63671875Mb
2020-10-13 07:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006778
2020-10-13 07:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.2421875Mb; avail=237451.03125Mb
2020-10-13 07:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005157
2020-10-13 07:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013730
2020-10-13 07:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.84765625Mb; avail=237450.42578125Mb
2020-10-13 07:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7105.6640625Mb; avail=237448.609375Mb
2020-10-13 07:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000642
2020-10-13 07:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.875Mb; avail=237447.3984375Mb
2020-10-13 07:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006592
2020-10-13 07:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7108.0859375Mb; avail=237446.1875Mb
2020-10-13 07:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005047
2020-10-13 07:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013101
2020-10-13 07:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7108.0859375Mb; avail=237446.1875Mb
2020-10-13 07:08:04 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 8.229 | nll_loss 7.017 | ppl 129.54 | wps 38118.7 | wpb 1971 | bsz 82.7 | num_updates 3350 | best_loss 7.665
2020-10-13 07:08:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:08:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 67 @ 3350 updates, score 8.229) (writing took 2.5465119620785117 seconds)
2020-10-13 07:08:07 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2020-10-13 07:08:07 | INFO | train | epoch 067 | loss 4.436 | nll_loss 2.883 | ppl 7.38 | wps 13264.5 | ups 2.97 | wpb 4470.2 | bsz 180.3 | num_updates 3350 | lr 0.000167516 | gnorm 2.085 | clip 0 | train_wall 13 | wall 1204
2020-10-13 07:08:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=68/shard_epoch=67
2020-10-13 07:08:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=68/shard_epoch=68
2020-10-13 07:08:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7078.74609375Mb; avail=237475.734375Mb
2020-10-13 07:08:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000225
2020-10-13 07:08:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002201
2020-10-13 07:08:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.74609375Mb; avail=237475.734375Mb
2020-10-13 07:08:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 07:08:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.74609375Mb; avail=237475.734375Mb
2020-10-13 07:08:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032933
2020-10-13 07:08:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035935
2020-10-13 07:08:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.74609375Mb; avail=237475.734375Mb
2020-10-13 07:08:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7078.74609375Mb; avail=237475.734375Mb
2020-10-13 07:08:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001718
2020-10-13 07:08:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.74609375Mb; avail=237475.734375Mb
2020-10-13 07:08:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 07:08:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.74609375Mb; avail=237475.734375Mb
2020-10-13 07:08:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032107
2020-10-13 07:08:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034620
2020-10-13 07:08:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.74609375Mb; avail=237475.734375Mb
2020-10-13 07:08:07 | INFO | fairseq.trainer | begin training epoch 68
2020-10-13 07:08:20 | INFO | train_inner | epoch 068:     50 / 50 loss=4.389, nll_loss=2.829, ppl=7.11, wps=13304.2, ups=2.98, wpb=4470.2, bsz=180.3, num_updates=3400, lr=0.000170015, gnorm=2.093, clip=0, train_wall=26, wall=1218
2020-10-13 07:08:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7100.484375Mb; avail=237453.82421875Mb
2020-10-13 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000888
2020-10-13 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.484375Mb; avail=237453.82421875Mb
2020-10-13 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006789
2020-10-13 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.484375Mb; avail=237453.82421875Mb
2020-10-13 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005143
2020-10-13 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013576
2020-10-13 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.484375Mb; avail=237453.82421875Mb
2020-10-13 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7102.203125Mb; avail=237452.10546875Mb
2020-10-13 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000604
2020-10-13 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.203125Mb; avail=237452.10546875Mb
2020-10-13 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006614
2020-10-13 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.80859375Mb; avail=237451.5Mb
2020-10-13 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005026
2020-10-13 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012968
2020-10-13 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.4140625Mb; avail=237450.89453125Mb
2020-10-13 07:08:21 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 8.278 | nll_loss 7.077 | ppl 135 | wps 38369.2 | wpb 1971 | bsz 82.7 | num_updates 3400 | best_loss 7.665
2020-10-13 07:08:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:08:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 68 @ 3400 updates, score 8.278) (writing took 2.527983539039269 seconds)
2020-10-13 07:08:23 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2020-10-13 07:08:23 | INFO | train | epoch 068 | loss 4.343 | nll_loss 2.775 | ppl 6.84 | wps 13335 | ups 2.98 | wpb 4470.2 | bsz 180.3 | num_updates 3400 | lr 0.000170015 | gnorm 2.102 | clip 0 | train_wall 13 | wall 1221
2020-10-13 07:08:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=69/shard_epoch=68
2020-10-13 07:08:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=69/shard_epoch=69
2020-10-13 07:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7078.94140625Mb; avail=237475.5625Mb
2020-10-13 07:08:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000228
2020-10-13 07:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002228
2020-10-13 07:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.94140625Mb; avail=237475.5625Mb
2020-10-13 07:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 07:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.94140625Mb; avail=237475.5625Mb
2020-10-13 07:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032407
2020-10-13 07:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035438
2020-10-13 07:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.94140625Mb; avail=237475.5625Mb
2020-10-13 07:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7078.94140625Mb; avail=237475.5625Mb
2020-10-13 07:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001664
2020-10-13 07:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.94140625Mb; avail=237475.5625Mb
2020-10-13 07:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069
2020-10-13 07:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.94140625Mb; avail=237475.5625Mb
2020-10-13 07:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032329
2020-10-13 07:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034774
2020-10-13 07:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.94140625Mb; avail=237475.5625Mb
2020-10-13 07:08:23 | INFO | fairseq.trainer | begin training epoch 69
2020-10-13 07:08:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:08:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7100.83984375Mb; avail=237453.46875Mb
2020-10-13 07:08:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000932
2020-10-13 07:08:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.83984375Mb; avail=237453.46875Mb
2020-10-13 07:08:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006823
2020-10-13 07:08:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.83984375Mb; avail=237453.46875Mb
2020-10-13 07:08:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005187
2020-10-13 07:08:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013712
2020-10-13 07:08:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.83984375Mb; avail=237453.46875Mb
2020-10-13 07:08:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7102.05078125Mb; avail=237452.2578125Mb
2020-10-13 07:08:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000615
2020-10-13 07:08:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.05078125Mb; avail=237452.2578125Mb
2020-10-13 07:08:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006668
2020-10-13 07:08:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.65625Mb; avail=237451.65234375Mb
2020-10-13 07:08:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005150
2020-10-13 07:08:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013193
2020-10-13 07:08:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.26171875Mb; avail=237451.046875Mb
2020-10-13 07:08:38 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 8.32 | nll_loss 7.111 | ppl 138.24 | wps 38249.9 | wpb 1971 | bsz 82.7 | num_updates 3450 | best_loss 7.665
2020-10-13 07:08:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:08:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 69 @ 3450 updates, score 8.32) (writing took 2.5297690860461444 seconds)
2020-10-13 07:08:40 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2020-10-13 07:08:40 | INFO | train | epoch 069 | loss 4.263 | nll_loss 2.681 | ppl 6.41 | wps 13320.1 | ups 2.98 | wpb 4470.2 | bsz 180.3 | num_updates 3450 | lr 0.000172514 | gnorm 2.176 | clip 0 | train_wall 13 | wall 1238
2020-10-13 07:08:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=70/shard_epoch=69
2020-10-13 07:08:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=70/shard_epoch=70
2020-10-13 07:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7102.7734375Mb; avail=237451.703125Mb
2020-10-13 07:08:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000232
2020-10-13 07:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002221
2020-10-13 07:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.37890625Mb; avail=237451.09765625Mb
2020-10-13 07:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 07:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.37890625Mb; avail=237451.09765625Mb
2020-10-13 07:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032363
2020-10-13 07:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035382
2020-10-13 07:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7107.71875Mb; avail=237446.7578125Mb
2020-10-13 07:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7107.71875Mb; avail=237446.7578125Mb
2020-10-13 07:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001708
2020-10-13 07:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7107.71875Mb; avail=237446.7578125Mb
2020-10-13 07:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000074
2020-10-13 07:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7107.71875Mb; avail=237446.7578125Mb
2020-10-13 07:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032545
2020-10-13 07:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035053
2020-10-13 07:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7098.3671875Mb; avail=237456.109375Mb
2020-10-13 07:08:40 | INFO | fairseq.trainer | begin training epoch 70
2020-10-13 07:08:54 | INFO | train_inner | epoch 070:     50 / 50 loss=4.225, nll_loss=2.636, ppl=6.21, wps=13304.8, ups=2.98, wpb=4470.2, bsz=180.3, num_updates=3500, lr=0.000175013, gnorm=2.201, clip=0, train_wall=26, wall=1252
2020-10-13 07:08:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7104.51171875Mb; avail=237449.7890625Mb
2020-10-13 07:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000860
2020-10-13 07:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7104.51171875Mb; avail=237449.7890625Mb
2020-10-13 07:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006775
2020-10-13 07:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7105.1171875Mb; avail=237449.18359375Mb
2020-10-13 07:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005111
2020-10-13 07:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013485
2020-10-13 07:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7105.1171875Mb; avail=237449.18359375Mb
2020-10-13 07:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7109.51171875Mb; avail=237444.7890625Mb
2020-10-13 07:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000596
2020-10-13 07:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7109.51171875Mb; avail=237444.7890625Mb
2020-10-13 07:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006708
2020-10-13 07:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7109.51171875Mb; avail=237444.7890625Mb
2020-10-13 07:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005032
2020-10-13 07:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013040
2020-10-13 07:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7109.51171875Mb; avail=237444.7890625Mb
2020-10-13 07:08:54 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 8.311 | nll_loss 7.105 | ppl 137.71 | wps 38217.8 | wpb 1971 | bsz 82.7 | num_updates 3500 | best_loss 7.665
2020-10-13 07:08:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:08:57 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 70 @ 3500 updates, score 8.311) (writing took 2.535683484049514 seconds)
2020-10-13 07:08:57 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2020-10-13 07:08:57 | INFO | train | epoch 070 | loss 4.187 | nll_loss 2.59 | ppl 6.02 | wps 13281.3 | ups 2.97 | wpb 4470.2 | bsz 180.3 | num_updates 3500 | lr 0.000175013 | gnorm 2.227 | clip 0 | train_wall 13 | wall 1255
2020-10-13 07:08:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=71/shard_epoch=70
2020-10-13 07:08:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=71/shard_epoch=71
2020-10-13 07:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7083.5703125Mb; avail=237470.94921875Mb
2020-10-13 07:08:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000224
2020-10-13 07:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002212
2020-10-13 07:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7083.5703125Mb; avail=237470.94921875Mb
2020-10-13 07:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 07:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7083.5703125Mb; avail=237470.94921875Mb
2020-10-13 07:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.033006
2020-10-13 07:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.036018
2020-10-13 07:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7089.01953125Mb; avail=237465.5Mb
2020-10-13 07:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7089.5234375Mb; avail=237464.99609375Mb
2020-10-13 07:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001708
2020-10-13 07:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7089.5234375Mb; avail=237464.99609375Mb
2020-10-13 07:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069
2020-10-13 07:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7089.5234375Mb; avail=237464.99609375Mb
2020-10-13 07:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032495
2020-10-13 07:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034987
2020-10-13 07:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.6796875Mb; avail=237474.83984375Mb
2020-10-13 07:08:57 | INFO | fairseq.trainer | begin training epoch 71
2020-10-13 07:09:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7100.1328125Mb; avail=237454.0Mb
2020-10-13 07:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000924
2020-10-13 07:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.1328125Mb; avail=237454.0Mb
2020-10-13 07:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006718
2020-10-13 07:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.1328125Mb; avail=237454.0Mb
2020-10-13 07:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005114
2020-10-13 07:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013523
2020-10-13 07:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.1328125Mb; avail=237454.0Mb
2020-10-13 07:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7100.1328125Mb; avail=237454.0Mb
2020-10-13 07:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000600
2020-10-13 07:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.1328125Mb; avail=237454.0Mb
2020-10-13 07:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006857
2020-10-13 07:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7100.1328125Mb; avail=237454.0Mb
2020-10-13 07:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005127
2020-10-13 07:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013367
2020-10-13 07:09:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.34765625Mb; avail=237452.78515625Mb
2020-10-13 07:09:11 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 8.43 | nll_loss 7.235 | ppl 150.67 | wps 38381.6 | wpb 1971 | bsz 82.7 | num_updates 3550 | best_loss 7.665
2020-10-13 07:09:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:09:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 71 @ 3550 updates, score 8.43) (writing took 2.525425527943298 seconds)
2020-10-13 07:09:14 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2020-10-13 07:09:14 | INFO | train | epoch 071 | loss 4.09 | nll_loss 2.476 | ppl 5.56 | wps 13350.2 | ups 2.99 | wpb 4470.2 | bsz 180.3 | num_updates 3550 | lr 0.000177511 | gnorm 2.102 | clip 0 | train_wall 13 | wall 1271
2020-10-13 07:09:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=72/shard_epoch=71
2020-10-13 07:09:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=72/shard_epoch=72
2020-10-13 07:09:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7078.46875Mb; avail=237476.03125Mb
2020-10-13 07:09:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000224
2020-10-13 07:09:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002238
2020-10-13 07:09:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.46875Mb; avail=237476.03125Mb
2020-10-13 07:09:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 07:09:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.46875Mb; avail=237476.03125Mb
2020-10-13 07:09:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032625
2020-10-13 07:09:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035677
2020-10-13 07:09:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.46875Mb; avail=237476.03125Mb
2020-10-13 07:09:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7078.4609375Mb; avail=237476.0390625Mb
2020-10-13 07:09:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001701
2020-10-13 07:09:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.4609375Mb; avail=237476.0390625Mb
2020-10-13 07:09:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 07:09:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.4609375Mb; avail=237476.0390625Mb
2020-10-13 07:09:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032136
2020-10-13 07:09:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034616
2020-10-13 07:09:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.4609375Mb; avail=237476.0390625Mb
2020-10-13 07:09:14 | INFO | fairseq.trainer | begin training epoch 72
2020-10-13 07:09:27 | INFO | train_inner | epoch 072:     50 / 50 loss=4.072, nll_loss=2.454, ppl=5.48, wps=13369.3, ups=2.99, wpb=4470.2, bsz=180.3, num_updates=3600, lr=0.00018001, gnorm=2.195, clip=0, train_wall=26, wall=1285
2020-10-13 07:09:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:09:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7110.375Mb; avail=237444.0234375Mb
2020-10-13 07:09:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000899
2020-10-13 07:09:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7110.375Mb; avail=237444.0234375Mb
2020-10-13 07:09:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006738
2020-10-13 07:09:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7110.359375Mb; avail=237444.0390625Mb
2020-10-13 07:09:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005215
2020-10-13 07:09:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013615
2020-10-13 07:09:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7110.359375Mb; avail=237444.0390625Mb
2020-10-13 07:09:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7110.3671875Mb; avail=237444.03125Mb
2020-10-13 07:09:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000588
2020-10-13 07:09:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7110.3671875Mb; avail=237444.03125Mb
2020-10-13 07:09:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006551
2020-10-13 07:09:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7110.03515625Mb; avail=237444.36328125Mb
2020-10-13 07:09:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005222
2020-10-13 07:09:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013144
2020-10-13 07:09:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7099.86328125Mb; avail=237454.53515625Mb
2020-10-13 07:09:28 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 8.434 | nll_loss 7.211 | ppl 148.14 | wps 38314.3 | wpb 1971 | bsz 82.7 | num_updates 3600 | best_loss 7.665
2020-10-13 07:09:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:09:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 72 @ 3600 updates, score 8.434) (writing took 2.5249921488575637 seconds)
2020-10-13 07:09:30 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2020-10-13 07:09:30 | INFO | train | epoch 072 | loss 4.054 | nll_loss 2.432 | ppl 5.4 | wps 13402.6 | ups 3 | wpb 4470.2 | bsz 180.3 | num_updates 3600 | lr 0.00018001 | gnorm 2.289 | clip 0 | train_wall 13 | wall 1288
2020-10-13 07:09:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=73/shard_epoch=72
2020-10-13 07:09:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=73/shard_epoch=73
2020-10-13 07:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7079.53515625Mb; avail=237474.96484375Mb
2020-10-13 07:09:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000259
2020-10-13 07:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002226
2020-10-13 07:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.53515625Mb; avail=237474.96484375Mb
2020-10-13 07:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000078
2020-10-13 07:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.53515625Mb; avail=237474.96484375Mb
2020-10-13 07:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032377
2020-10-13 07:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035409
2020-10-13 07:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7082.67578125Mb; avail=237471.82421875Mb
2020-10-13 07:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7085.00390625Mb; avail=237469.71484375Mb
2020-10-13 07:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001689
2020-10-13 07:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.00390625Mb; avail=237469.71484375Mb
2020-10-13 07:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000071
2020-10-13 07:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7085.59375Mb; avail=237469.125Mb
2020-10-13 07:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032000
2020-10-13 07:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034470
2020-10-13 07:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7089.72265625Mb; avail=237464.99609375Mb
2020-10-13 07:09:30 | INFO | fairseq.trainer | begin training epoch 73
2020-10-13 07:09:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:09:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7130.02734375Mb; avail=237424.453125Mb
2020-10-13 07:09:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000922
2020-10-13 07:09:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7130.02734375Mb; avail=237424.453125Mb
2020-10-13 07:09:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006683
2020-10-13 07:09:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7133.0546875Mb; avail=237421.42578125Mb
2020-10-13 07:09:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005186
2020-10-13 07:09:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013560
2020-10-13 07:09:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7134.87109375Mb; avail=237419.00390625Mb
2020-10-13 07:09:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7141.0390625Mb; avail=237413.44140625Mb
2020-10-13 07:09:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000626
2020-10-13 07:09:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7141.0390625Mb; avail=237413.44140625Mb
2020-10-13 07:09:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006755
2020-10-13 07:09:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7143.453125Mb; avail=237411.02734375Mb
2020-10-13 07:09:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005199
2020-10-13 07:09:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013328
2020-10-13 07:09:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7136.5234375Mb; avail=237417.95703125Mb
2020-10-13 07:09:45 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 8.473 | nll_loss 7.26 | ppl 153.3 | wps 38210.7 | wpb 1971 | bsz 82.7 | num_updates 3650 | best_loss 7.665
2020-10-13 07:09:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:09:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 73 @ 3650 updates, score 8.473) (writing took 2.5168962150346488 seconds)
2020-10-13 07:09:47 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2020-10-13 07:09:47 | INFO | train | epoch 073 | loss 3.94 | nll_loss 2.298 | ppl 4.92 | wps 13277.8 | ups 2.97 | wpb 4470.2 | bsz 180.3 | num_updates 3650 | lr 0.000182509 | gnorm 2.1 | clip 0 | train_wall 13 | wall 1305
2020-10-13 07:09:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=74/shard_epoch=73
2020-10-13 07:09:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=74/shard_epoch=74
2020-10-13 07:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7080.2109375Mb; avail=237474.296875Mb
2020-10-13 07:09:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000225
2020-10-13 07:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002194
2020-10-13 07:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.2109375Mb; avail=237474.296875Mb
2020-10-13 07:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000082
2020-10-13 07:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.2109375Mb; avail=237474.296875Mb
2020-10-13 07:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032762
2020-10-13 07:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035754
2020-10-13 07:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.7109375Mb; avail=237474.796875Mb
2020-10-13 07:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7079.7109375Mb; avail=237474.796875Mb
2020-10-13 07:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001708
2020-10-13 07:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.7109375Mb; avail=237474.796875Mb
2020-10-13 07:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069
2020-10-13 07:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.7109375Mb; avail=237474.796875Mb
2020-10-13 07:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032152
2020-10-13 07:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034640
2020-10-13 07:09:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7079.7109375Mb; avail=237474.796875Mb
2020-10-13 07:09:47 | INFO | fairseq.trainer | begin training epoch 74
2020-10-13 07:10:01 | INFO | train_inner | epoch 074:     50 / 50 loss=3.898, nll_loss=2.248, ppl=4.75, wps=13326.4, ups=2.98, wpb=4470.2, bsz=180.3, num_updates=3700, lr=0.000185008, gnorm=2.061, clip=0, train_wall=26, wall=1319
2020-10-13 07:10:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7110.4140625Mb; avail=237444.01171875Mb
2020-10-13 07:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000907
2020-10-13 07:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7110.4140625Mb; avail=237444.01171875Mb
2020-10-13 07:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006674
2020-10-13 07:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7110.4140625Mb; avail=237444.01171875Mb
2020-10-13 07:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005325
2020-10-13 07:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013664
2020-10-13 07:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7110.4140625Mb; avail=237444.01171875Mb
2020-10-13 07:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7110.91015625Mb; avail=237443.515625Mb
2020-10-13 07:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000626
2020-10-13 07:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7110.91015625Mb; avail=237443.515625Mb
2020-10-13 07:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006695
2020-10-13 07:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7110.91015625Mb; avail=237443.515625Mb
2020-10-13 07:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005362
2020-10-13 07:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013430
2020-10-13 07:10:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.06640625Mb; avail=237453.359375Mb
2020-10-13 07:10:01 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 8.537 | nll_loss 7.335 | ppl 161.47 | wps 38117.7 | wpb 1971 | bsz 82.7 | num_updates 3700 | best_loss 7.665
2020-10-13 07:10:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:10:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 74 @ 3700 updates, score 8.537) (writing took 2.5342696928419173 seconds)
2020-10-13 07:10:04 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2020-10-13 07:10:04 | INFO | train | epoch 074 | loss 3.856 | nll_loss 2.199 | ppl 4.59 | wps 13358.8 | ups 2.99 | wpb 4470.2 | bsz 180.3 | num_updates 3700 | lr 0.000185008 | gnorm 2.023 | clip 0 | train_wall 13 | wall 1322
2020-10-13 07:10:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=75/shard_epoch=74
2020-10-13 07:10:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=75/shard_epoch=75
2020-10-13 07:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7080.39453125Mb; avail=237474.0859375Mb
2020-10-13 07:10:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000230
2020-10-13 07:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002192
2020-10-13 07:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.39453125Mb; avail=237474.0859375Mb
2020-10-13 07:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 07:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.39453125Mb; avail=237474.0859375Mb
2020-10-13 07:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032269
2020-10-13 07:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035258
2020-10-13 07:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.39453125Mb; avail=237474.0859375Mb
2020-10-13 07:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7080.39453125Mb; avail=237474.0859375Mb
2020-10-13 07:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001703
2020-10-13 07:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.39453125Mb; avail=237474.0859375Mb
2020-10-13 07:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069
2020-10-13 07:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.39453125Mb; avail=237474.0859375Mb
2020-10-13 07:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032353
2020-10-13 07:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034845
2020-10-13 07:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.39453125Mb; avail=237474.0859375Mb
2020-10-13 07:10:04 | INFO | fairseq.trainer | begin training epoch 75
2020-10-13 07:10:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7101.12890625Mb; avail=237453.87890625Mb
2020-10-13 07:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000894
2020-10-13 07:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.12890625Mb; avail=237453.87890625Mb
2020-10-13 07:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006800
2020-10-13 07:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.32421875Mb; avail=237452.68359375Mb
2020-10-13 07:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005200
2020-10-13 07:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013683
2020-10-13 07:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7102.9296875Mb; avail=237452.078125Mb
2020-10-13 07:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7104.140625Mb; avail=237450.26171875Mb
2020-10-13 07:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000608
2020-10-13 07:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7104.74609375Mb; avail=237450.26171875Mb
2020-10-13 07:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006549
2020-10-13 07:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7104.74609375Mb; avail=237450.26171875Mb
2020-10-13 07:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005106
2020-10-13 07:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013015
2020-10-13 07:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7105.3515625Mb; avail=237449.65625Mb
2020-10-13 07:10:18 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 8.578 | nll_loss 7.384 | ppl 167.09 | wps 38262.5 | wpb 1971 | bsz 82.7 | num_updates 3750 | best_loss 7.665
2020-10-13 07:10:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:10:21 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 75 @ 3750 updates, score 8.578) (writing took 2.524604969890788 seconds)
2020-10-13 07:10:21 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2020-10-13 07:10:21 | INFO | train | epoch 075 | loss 3.797 | nll_loss 2.127 | ppl 4.37 | wps 13328.7 | ups 2.98 | wpb 4470.2 | bsz 180.3 | num_updates 3750 | lr 0.000187506 | gnorm 2.208 | clip 0 | train_wall 13 | wall 1338
2020-10-13 07:10:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=76/shard_epoch=75
2020-10-13 07:10:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=76/shard_epoch=76
2020-10-13 07:10:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7078.13671875Mb; avail=237476.33984375Mb
2020-10-13 07:10:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000223
2020-10-13 07:10:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002217
2020-10-13 07:10:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.13671875Mb; avail=237476.33984375Mb
2020-10-13 07:10:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000078
2020-10-13 07:10:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.13671875Mb; avail=237476.33984375Mb
2020-10-13 07:10:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032672
2020-10-13 07:10:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035680
2020-10-13 07:10:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.13671875Mb; avail=237476.33984375Mb
2020-10-13 07:10:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7078.13671875Mb; avail=237476.33984375Mb
2020-10-13 07:10:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001669
2020-10-13 07:10:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.13671875Mb; avail=237476.33984375Mb
2020-10-13 07:10:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 07:10:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.13671875Mb; avail=237476.33984375Mb
2020-10-13 07:10:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032290
2020-10-13 07:10:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034735
2020-10-13 07:10:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7078.13671875Mb; avail=237476.33984375Mb
2020-10-13 07:10:21 | INFO | fairseq.trainer | begin training epoch 76
2020-10-13 07:10:34 | INFO | train_inner | epoch 076:     50 / 50 loss=3.776, nll_loss=2.102, ppl=4.29, wps=13339.4, ups=2.98, wpb=4470.2, bsz=180.3, num_updates=3800, lr=0.000190005, gnorm=2.248, clip=0, train_wall=26, wall=1352
2020-10-13 07:10:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7101.42578125Mb; avail=237453.08203125Mb
2020-10-13 07:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000908
2020-10-13 07:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.42578125Mb; avail=237453.08203125Mb
2020-10-13 07:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006811
2020-10-13 07:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.42578125Mb; avail=237453.08203125Mb
2020-10-13 07:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005056
2020-10-13 07:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013583
2020-10-13 07:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.42578125Mb; avail=237453.08203125Mb
2020-10-13 07:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7101.42578125Mb; avail=237453.08203125Mb
2020-10-13 07:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000642
2020-10-13 07:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.42578125Mb; avail=237453.08203125Mb
2020-10-13 07:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006624
2020-10-13 07:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.42578125Mb; avail=237453.08203125Mb
2020-10-13 07:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005141
2020-10-13 07:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013244
2020-10-13 07:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7101.42578125Mb; avail=237453.08203125Mb
2020-10-13 07:10:35 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 8.617 | nll_loss 7.405 | ppl 169.44 | wps 38521 | wpb 1971 | bsz 82.7 | num_updates 3800 | best_loss 7.665
2020-10-13 07:10:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:10:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 76 @ 3800 updates, score 8.617) (writing took 2.52763345092535 seconds)
2020-10-13 07:10:37 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2020-10-13 07:10:37 | INFO | train | epoch 076 | loss 3.755 | nll_loss 2.076 | ppl 4.22 | wps 13365.5 | ups 2.99 | wpb 4470.2 | bsz 180.3 | num_updates 3800 | lr 0.000190005 | gnorm 2.288 | clip 0 | train_wall 13 | wall 1355
2020-10-13 07:10:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=77/shard_epoch=76
2020-10-13 07:10:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=77/shard_epoch=77
2020-10-13 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7080.765625Mb; avail=237473.7421875Mb
2020-10-13 07:10:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000243
2020-10-13 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002176
2020-10-13 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.765625Mb; avail=237473.7421875Mb
2020-10-13 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.515625Mb; avail=237473.9921875Mb
2020-10-13 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032349
2020-10-13 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035351
2020-10-13 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.515625Mb; avail=237473.9921875Mb
2020-10-13 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7080.515625Mb; avail=237473.9921875Mb
2020-10-13 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001703
2020-10-13 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.515625Mb; avail=237473.9921875Mb
2020-10-13 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069
2020-10-13 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.515625Mb; avail=237473.9921875Mb
2020-10-13 07:10:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032029
2020-10-13 07:10:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034514
2020-10-13 07:10:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7080.515625Mb; avail=237473.9921875Mb
2020-10-13 07:10:38 | INFO | fairseq.trainer | begin training epoch 77
2020-10-13 07:10:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:10:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7103.78515625Mb; avail=237450.51953125Mb
2020-10-13 07:10:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000928
2020-10-13 07:10:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.78515625Mb; avail=237450.51953125Mb
2020-10-13 07:10:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006873
2020-10-13 07:10:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.78515625Mb; avail=237450.51953125Mb
2020-10-13 07:10:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005164
2020-10-13 07:10:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013747
2020-10-13 07:10:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.78515625Mb; avail=237450.51953125Mb
2020-10-13 07:10:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7103.78515625Mb; avail=237450.51953125Mb
2020-10-13 07:10:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000610
2020-10-13 07:10:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.78515625Mb; avail=237450.51953125Mb
2020-10-13 07:10:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006852
2020-10-13 07:10:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.78515625Mb; avail=237450.51953125Mb
2020-10-13 07:10:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005096
2020-10-13 07:10:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013299
2020-10-13 07:10:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7103.78515625Mb; avail=237450.51953125Mb
2020-10-13 07:10:52 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 8.63 | nll_loss 7.424 | ppl 171.7 | wps 38456.3 | wpb 1971 | bsz 82.7 | num_updates 3850 | best_loss 7.665
2020-10-13 07:10:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:10:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 77 @ 3850 updates, score 8.63) (writing took 2.524116584099829 seconds)
2020-10-13 07:10:54 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2020-10-13 07:10:54 | INFO | train | epoch 077 | loss 3.65 | nll_loss 1.951 | ppl 3.87 | wps 13402.2 | ups 3 | wpb 4470.2 | bsz 180.3 | num_updates 3850 | lr 0.000192504 | gnorm 2.061 | clip 0 | train_wall 13 | wall 1372
2020-10-13 07:10:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=78/shard_epoch=77
2020-10-13 07:10:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=78/shard_epoch=78
2020-10-13 07:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7084.27734375Mb; avail=237470.20703125Mb
2020-10-13 07:10:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000229
2020-10-13 07:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002217
2020-10-13 07:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7083.77734375Mb; avail=237470.70703125Mb
2020-10-13 07:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 07:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7083.77734375Mb; avail=237470.70703125Mb
2020-10-13 07:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032281
2020-10-13 07:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035314
2020-10-13 07:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7083.77734375Mb; avail=237470.70703125Mb
2020-10-13 07:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7083.77734375Mb; avail=237470.70703125Mb
2020-10-13 07:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001697
2020-10-13 07:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7083.77734375Mb; avail=237470.70703125Mb
2020-10-13 07:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000070
2020-10-13 07:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7083.77734375Mb; avail=237470.70703125Mb
2020-10-13 07:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.031791
2020-10-13 07:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034269
2020-10-13 07:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7083.77734375Mb; avail=237470.70703125Mb
2020-10-13 07:10:54 | INFO | fairseq.trainer | begin training epoch 78
2020-10-13 07:11:08 | INFO | train_inner | epoch 078:     50 / 50 loss=3.612, nll_loss=1.908, ppl=3.75, wps=13361.3, ups=2.99, wpb=4470.2, bsz=180.3, num_updates=3900, lr=0.000195003, gnorm=2.039, clip=0, train_wall=26, wall=1386
2020-10-13 07:11:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7105.8984375Mb; avail=237448.41015625Mb
2020-10-13 07:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000890
2020-10-13 07:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7105.8984375Mb; avail=237448.41015625Mb
2020-10-13 07:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006825
2020-10-13 07:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7105.8984375Mb; avail=237448.41015625Mb
2020-10-13 07:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005166
2020-10-13 07:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013667
2020-10-13 07:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7105.8984375Mb; avail=237448.41015625Mb
2020-10-13 07:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7105.8984375Mb; avail=237448.41015625Mb
2020-10-13 07:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000605
2020-10-13 07:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7105.8984375Mb; avail=237448.41015625Mb
2020-10-13 07:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006883
2020-10-13 07:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7105.8984375Mb; avail=237448.41015625Mb
2020-10-13 07:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005021
2020-10-13 07:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013274
2020-10-13 07:11:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7105.8984375Mb; avail=237448.41015625Mb
2020-10-13 07:11:08 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 8.679 | nll_loss 7.476 | ppl 178.03 | wps 38183.3 | wpb 1971 | bsz 82.7 | num_updates 3900 | best_loss 7.665
2020-10-13 07:11:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:11:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 78 @ 3900 updates, score 8.679) (writing took 2.551565937930718 seconds)
2020-10-13 07:11:11 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2020-10-13 07:11:11 | INFO | train | epoch 078 | loss 3.575 | nll_loss 1.864 | ppl 3.64 | wps 13300 | ups 2.98 | wpb 4470.2 | bsz 180.3 | num_updates 3900 | lr 0.000195003 | gnorm 2.016 | clip 0 | train_wall 13 | wall 1389
2020-10-13 07:11:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=79/shard_epoch=78
2020-10-13 07:11:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=79/shard_epoch=79
2020-10-13 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7087.36328125Mb; avail=237467.140625Mb
2020-10-13 07:11:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000225
2020-10-13 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002181
2020-10-13 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7087.36328125Mb; avail=237467.140625Mb
2020-10-13 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000079
2020-10-13 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7087.36328125Mb; avail=237467.140625Mb
2020-10-13 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032420
2020-10-13 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035399
2020-10-13 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.84765625Mb; avail=237462.65625Mb
2020-10-13 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7091.84765625Mb; avail=237462.65625Mb
2020-10-13 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001687
2020-10-13 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.84765625Mb; avail=237462.65625Mb
2020-10-13 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000071
2020-10-13 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.84765625Mb; avail=237462.65625Mb
2020-10-13 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032021
2020-10-13 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034504
2020-10-13 07:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7082.49609375Mb; avail=237472.0078125Mb
2020-10-13 07:11:11 | INFO | fairseq.trainer | begin training epoch 79
2020-10-13 07:11:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:11:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7106.65234375Mb; avail=237447.84765625Mb
2020-10-13 07:11:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000896
2020-10-13 07:11:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.65234375Mb; avail=237447.84765625Mb
2020-10-13 07:11:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006787
2020-10-13 07:11:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.65234375Mb; avail=237447.84765625Mb
2020-10-13 07:11:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005154
2020-10-13 07:11:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013617
2020-10-13 07:11:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.65234375Mb; avail=237447.84765625Mb
2020-10-13 07:11:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7106.65234375Mb; avail=237447.84765625Mb
2020-10-13 07:11:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000605
2020-10-13 07:11:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.65234375Mb; avail=237447.84765625Mb
2020-10-13 07:11:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006509
2020-10-13 07:11:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.65234375Mb; avail=237447.84765625Mb
2020-10-13 07:11:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005060
2020-10-13 07:11:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.012901
2020-10-13 07:11:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7106.65234375Mb; avail=237447.84765625Mb
2020-10-13 07:11:25 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 8.726 | nll_loss 7.53 | ppl 184.87 | wps 38495.3 | wpb 1971 | bsz 82.7 | num_updates 3950 | best_loss 7.665
2020-10-13 07:11:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:11:28 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 79 @ 3950 updates, score 8.726) (writing took 2.5189360498916358 seconds)
2020-10-13 07:11:28 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2020-10-13 07:11:28 | INFO | train | epoch 079 | loss 3.491 | nll_loss 1.763 | ppl 3.4 | wps 13357.4 | ups 2.99 | wpb 4470.2 | bsz 180.3 | num_updates 3950 | lr 0.000197501 | gnorm 1.828 | clip 0 | train_wall 13 | wall 1405
2020-10-13 07:11:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=80/shard_epoch=79
2020-10-13 07:11:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=80/shard_epoch=80
2020-10-13 07:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7086.47265625Mb; avail=237468.0390625Mb
2020-10-13 07:11:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000235
2020-10-13 07:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002192
2020-10-13 07:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7086.47265625Mb; avail=237468.0390625Mb
2020-10-13 07:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000081
2020-10-13 07:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7086.47265625Mb; avail=237468.0390625Mb
2020-10-13 07:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032496
2020-10-13 07:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035535
2020-10-13 07:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7086.47265625Mb; avail=237468.0390625Mb
2020-10-13 07:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7088.78125Mb; avail=237465.73046875Mb
2020-10-13 07:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001709
2020-10-13 07:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7088.78125Mb; avail=237465.73046875Mb
2020-10-13 07:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000071
2020-10-13 07:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7088.78125Mb; avail=237465.73046875Mb
2020-10-13 07:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032005
2020-10-13 07:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.034494
2020-10-13 07:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7091.80859375Mb; avail=237462.703125Mb
2020-10-13 07:11:28 | INFO | fairseq.trainer | begin training epoch 80
2020-10-13 07:11:41 | INFO | train_inner | epoch 080:     50 / 50 loss=3.471, nll_loss=1.738, ppl=3.34, wps=13327.2, ups=2.98, wpb=4470.2, bsz=180.3, num_updates=4000, lr=0.0002, gnorm=1.935, clip=0, train_wall=26, wall=1419
2020-10-13 07:11:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-13 07:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7107.43359375Mb; avail=237446.7265625Mb
2020-10-13 07:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000882
2020-10-13 07:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7107.43359375Mb; avail=237446.7265625Mb
2020-10-13 07:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006897
2020-10-13 07:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7107.43359375Mb; avail=237446.7265625Mb
2020-10-13 07:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005094
2020-10-13 07:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013625
2020-10-13 07:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7107.43359375Mb; avail=237446.7265625Mb
2020-10-13 07:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7107.43359375Mb; avail=237446.7265625Mb
2020-10-13 07:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000603
2020-10-13 07:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7107.43359375Mb; avail=237446.7265625Mb
2020-10-13 07:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.006889
2020-10-13 07:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7107.43359375Mb; avail=237446.7265625Mb
2020-10-13 07:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.005153
2020-10-13 07:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.013432
2020-10-13 07:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7107.43359375Mb; avail=237446.7265625Mb
2020-10-13 07:11:42 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 8.764 | nll_loss 7.567 | ppl 189.61 | wps 38233 | wpb 1971 | bsz 82.7 | num_updates 4000 | best_loss 7.665
2020-10-13 07:11:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-13 07:11:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_belbep_sepspm8000/M2O/checkpoint_last.pt (epoch 80 @ 4000 updates, score 8.764) (writing took 2.525375318946317 seconds)
2020-10-13 07:11:44 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2020-10-13 07:11:44 | INFO | train | epoch 080 | loss 3.45 | nll_loss 1.712 | ppl 3.28 | wps 13317.6 | ups 2.98 | wpb 4470.2 | bsz 180.3 | num_updates 4000 | lr 0.0002 | gnorm 2.043 | clip 0 | train_wall 13 | wall 1422
2020-10-13 07:11:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=81/shard_epoch=80
2020-10-13 07:11:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=81/shard_epoch=81
2020-10-13 07:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7088.703125Mb; avail=237465.8125Mb
2020-10-13 07:11:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000225
2020-10-13 07:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002182
2020-10-13 07:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7088.703125Mb; avail=237465.8125Mb
2020-10-13 07:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000078
2020-10-13 07:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7088.703125Mb; avail=237465.8125Mb
2020-10-13 07:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.032433
2020-10-13 07:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.035462
2020-10-13 07:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7088.8046875Mb; avail=237465.94140625Mb
2020-10-13 07:11:44 | INFO | fairseq_cli.train | done training in 1422.1 seconds
