2020-10-11 04:55:17 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_azetur_sepspm8000/O2M/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok='tgt', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='eng-aze,eng-tur', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_azetur_sepspm8000/O2M/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-11 04:55:17 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-11 04:55:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['aze', 'eng', 'tur']
2020-10-11 04:55:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | [aze] dictionary: 19979 types
2020-10-11 04:55:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 19979 types
2020-10-11 04:55:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | [tur] dictionary: 19979 types
2020-10-11 04:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-11 04:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56927.0Mb; avail=135923.4609375Mb
2020-10-11 04:55:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-11 04:55:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:eng-aze': 1, 'main:eng-tur': 1}
2020-10-11 04:55:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-aze src_langtok: 19976; tgt_langtok: None
2020-10-11 04:55:17 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azetur_sepspm8000/O2M/valid.eng-aze.eng
2020-10-11 04:55:17 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azetur_sepspm8000/O2M/valid.eng-aze.aze
2020-10-11 04:55:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azetur_sepspm8000/O2M/ valid eng-aze 671 examples
2020-10-11 04:55:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-tur src_langtok: 19978; tgt_langtok: None
2020-10-11 04:55:17 | INFO | fairseq.data.data_utils | loaded 4045 examples from: fairseq/data-bin/ted_azetur_sepspm8000/O2M/valid.eng-tur.eng
2020-10-11 04:55:17 | INFO | fairseq.data.data_utils | loaded 4045 examples from: fairseq/data-bin/ted_azetur_sepspm8000/O2M/valid.eng-tur.tur
2020-10-11 04:55:17 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azetur_sepspm8000/O2M/ valid eng-tur 4045 examples
2020-10-11 04:55:19 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19979, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19979, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19979, bias=False)
  )
)
2020-10-11 04:55:19 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-11 04:55:19 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-11 04:55:19 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-11 04:55:19 | INFO | fairseq_cli.train | num. model params: 41772544 (num. trained: 41772544)
2020-10-11 04:55:30 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-11 04:55:30 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-11 04:55:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-11 04:55:30 | INFO | fairseq.utils | rank   0: capabilities =  5.2  ; total memory = 11.927 GB ; name = GeForce GTX TITAN X                     
2020-10-11 04:55:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-11 04:55:30 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-11 04:55:30 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-11 04:55:30 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_last.pt
2020-10-11 04:55:30 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-11 04:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-11 04:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60677.0625Mb; avail=132164.87109375Mb
2020-10-11 04:55:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('tgt', None)}
2020-10-11 04:55:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:eng-aze': 1, 'main:eng-tur': 1}
2020-10-11 04:55:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-aze src_langtok: 19976; tgt_langtok: None
2020-10-11 04:55:30 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azetur_sepspm8000/O2M/train.eng-aze.eng
2020-10-11 04:55:30 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azetur_sepspm8000/O2M/train.eng-aze.aze
2020-10-11 04:55:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azetur_sepspm8000/O2M/ train eng-aze 5946 examples
2020-10-11 04:55:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:eng-tur src_langtok: 19978; tgt_langtok: None
2020-10-11 04:55:30 | INFO | fairseq.data.data_utils | loaded 182419 examples from: fairseq/data-bin/ted_azetur_sepspm8000/O2M/train.eng-tur.eng
2020-10-11 04:55:30 | INFO | fairseq.data.data_utils | loaded 182419 examples from: fairseq/data-bin/ted_azetur_sepspm8000/O2M/train.eng-tur.tur
2020-10-11 04:55:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azetur_sepspm8000/O2M/ train eng-tur 182419 examples
2020-10-11 04:55:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:eng-aze', 5946), ('main:eng-tur', 182419)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-11 04:55:31 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-11 04:55:31 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 188365
2020-10-11 04:55:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 188365; virtual dataset size 188365
2020-10-11 04:55:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-11 04:55:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:eng-aze': 5946, 'main:eng-tur': 182419}; raw total size: 188365
2020-10-11 04:55:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:eng-aze': 5946, 'main:eng-tur': 182419}; resampled total size: 188365
2020-10-11 04:55:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-11 04:55:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.039678
2020-10-11 04:55:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60753.34765625Mb; avail=132088.23828125Mb
2020-10-11 04:55:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005763
2020-10-11 04:55:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.048803
2020-10-11 04:55:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60765.28125Mb; avail=132076.91015625Mb
2020-10-11 04:55:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002009
2020-10-11 04:55:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60765.88671875Mb; avail=132076.3046875Mb
2020-10-11 04:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.920714
2020-10-11 04:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.972772
2020-10-11 04:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60850.4375Mb; avail=131991.87890625Mb
2020-10-11 04:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60840.66796875Mb; avail=132001.62109375Mb
2020-10-11 04:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027001
2020-10-11 04:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60842.9609375Mb; avail=131999.28515625Mb
2020-10-11 04:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001321
2020-10-11 04:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60844.171875Mb; avail=131998.07421875Mb
2020-10-11 04:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.860310
2020-10-11 04:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.889821
2020-10-11 04:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60868.83203125Mb; avail=131973.5234375Mb
2020-10-11 04:55:32 | INFO | fairseq.trainer | begin training epoch 1
2020-10-11 04:57:59 | INFO | train_inner | epoch 001:    100 / 648 loss=14.126, nll_loss=14.026, ppl=16679.4, wps=4704.1, ups=0.69, wpb=6853.2, bsz=305.4, num_updates=100, lr=5.0975e-06, gnorm=3.872, clip=0, train_wall=145, wall=148
2020-10-11 05:00:27 | INFO | train_inner | epoch 001:    200 / 648 loss=12.958, nll_loss=12.719, ppl=6743.22, wps=4634.4, ups=0.67, wpb=6869.6, bsz=294.6, num_updates=200, lr=1.0095e-05, gnorm=1.651, clip=0, train_wall=147, wall=297
2020-10-11 05:02:57 | INFO | train_inner | epoch 001:    300 / 648 loss=12.344, nll_loss=12.034, ppl=4194.24, wps=4609.7, ups=0.67, wpb=6903.9, bsz=283.5, num_updates=300, lr=1.50925e-05, gnorm=1.393, clip=0, train_wall=149, wall=446
2020-10-11 05:05:24 | INFO | train_inner | epoch 001:    400 / 648 loss=11.677, nll_loss=11.279, ppl=2484.27, wps=4580.6, ups=0.68, wpb=6754.6, bsz=272.7, num_updates=400, lr=2.009e-05, gnorm=1.547, clip=0, train_wall=146, wall=594
2020-10-11 05:07:53 | INFO | train_inner | epoch 001:    500 / 648 loss=11.041, nll_loss=10.53, ppl=1478.6, wps=4650.4, ups=0.67, wpb=6920.6, bsz=303.4, num_updates=500, lr=2.50875e-05, gnorm=1.499, clip=0, train_wall=148, wall=743
2020-10-11 05:10:20 | INFO | train_inner | epoch 001:    600 / 648 loss=10.758, nll_loss=10.171, ppl=1153.04, wps=4698.1, ups=0.68, wpb=6928.2, bsz=282.9, num_updates=600, lr=3.0085e-05, gnorm=1.329, clip=0, train_wall=146, wall=890
2020-10-11 05:11:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 05:11:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60988.046875Mb; avail=131850.21484375Mb
2020-10-11 05:11:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001745
2020-10-11 05:11:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60989.33984375Mb; avail=131848.4140625Mb
2020-10-11 05:11:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077903
2020-10-11 05:11:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60983.265625Mb; avail=131854.51171875Mb
2020-10-11 05:11:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062289
2020-10-11 05:11:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.143221
2020-10-11 05:11:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60972.9296875Mb; avail=131864.84765625Mb
2020-10-11 05:11:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60972.9296875Mb; avail=131864.84765625Mb
2020-10-11 05:11:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001221
2020-10-11 05:11:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60972.9296875Mb; avail=131864.84765625Mb
2020-10-11 05:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076968
2020-10-11 05:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60972.9296875Mb; avail=131864.84765625Mb
2020-10-11 05:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061860
2020-10-11 05:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141130
2020-10-11 05:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60972.9296875Mb; avail=131864.84765625Mb
/usr1/home/rjoshi2/envs/torch160/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2020-10-11 05:11:42 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.442 | nll_loss 9.754 | ppl 863.21 | wps 11093.9 | wpb 2274.6 | bsz 96.2 | num_updates 648
2020-10-11 05:11:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 05:11:50 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 1 @ 648 updates, score 10.442) (writing took 7.996453646570444 seconds)
2020-10-11 05:11:50 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-11 05:11:50 | INFO | train | epoch 001 | loss 12.036 | nll_loss 11.658 | ppl 3231.07 | wps 4565.4 | ups 0.66 | wpb 6882.8 | bsz 290.7 | num_updates 648 | lr 3.24838e-05 | gnorm 1.835 | clip 0 | train_wall 951 | wall 980
2020-10-11 05:11:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-11 05:11:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-11 05:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60949.51953125Mb; avail=131888.7890625Mb
2020-10-11 05:11:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006687
2020-10-11 05:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.056438
2020-10-11 05:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60949.48828125Mb; avail=131888.80859375Mb
2020-10-11 05:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001727
2020-10-11 05:11:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60949.48828125Mb; avail=131888.80859375Mb
2020-10-11 05:11:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.930066
2020-10-11 05:11:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.989581
2020-10-11 05:11:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60953.17578125Mb; avail=131884.34765625Mb
2020-10-11 05:11:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60942.39453125Mb; avail=131895.12109375Mb
2020-10-11 05:11:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026779
2020-10-11 05:11:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60944.2109375Mb; avail=131893.3046875Mb
2020-10-11 05:11:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001344
2020-10-11 05:11:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60944.2109375Mb; avail=131893.3046875Mb
2020-10-11 05:11:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.886316
2020-10-11 05:11:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.915468
2020-10-11 05:11:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60952.73828125Mb; avail=131884.1953125Mb
2020-10-11 05:11:52 | INFO | fairseq.trainer | begin training epoch 2
2020-10-11 05:13:07 | INFO | train_inner | epoch 002:     52 / 648 loss=10.616, nll_loss=9.986, ppl=1014.39, wps=4108.9, ups=0.6, wpb=6857.9, bsz=290.7, num_updates=700, lr=3.50825e-05, gnorm=1.367, clip=0, train_wall=145, wall=1057
2020-10-11 05:15:34 | INFO | train_inner | epoch 002:    152 / 648 loss=10.559, nll_loss=9.912, ppl=963.43, wps=4707.2, ups=0.68, wpb=6895.7, bsz=304.2, num_updates=800, lr=4.008e-05, gnorm=1.301, clip=0, train_wall=145, wall=1204
2020-10-11 05:17:58 | INFO | train_inner | epoch 002:    252 / 648 loss=10.504, nll_loss=9.847, ppl=920.93, wps=4753.9, ups=0.69, wpb=6864.1, bsz=283.7, num_updates=900, lr=4.50775e-05, gnorm=1.347, clip=0, train_wall=143, wall=1348
2020-10-11 05:20:26 | INFO | train_inner | epoch 002:    352 / 648 loss=10.484, nll_loss=9.825, ppl=906.84, wps=4706.4, ups=0.68, wpb=6949.3, bsz=270, num_updates=1000, lr=5.0075e-05, gnorm=1.25, clip=0, train_wall=146, wall=1496
2020-10-11 05:22:51 | INFO | train_inner | epoch 002:    452 / 648 loss=10.345, nll_loss=9.667, ppl=812.81, wps=4696.4, ups=0.69, wpb=6832.5, bsz=306.4, num_updates=1100, lr=5.50725e-05, gnorm=1.347, clip=0, train_wall=144, wall=1641
2020-10-11 05:25:18 | INFO | train_inner | epoch 002:    552 / 648 loss=10.274, nll_loss=9.585, ppl=767.88, wps=4688.6, ups=0.68, wpb=6860.8, bsz=296.9, num_updates=1200, lr=6.007e-05, gnorm=1.233, clip=0, train_wall=145, wall=1787
2020-10-11 05:27:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 05:27:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=61035.09765625Mb; avail=131802.66796875Mb
2020-10-11 05:27:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001745
2020-10-11 05:27:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61035.09765625Mb; avail=131802.66796875Mb
2020-10-11 05:27:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078916
2020-10-11 05:27:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61031.15625Mb; avail=131806.609375Mb
2020-10-11 05:27:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.066427
2020-10-11 05:27:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.148230
2020-10-11 05:27:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61040.51171875Mb; avail=131797.25390625Mb
2020-10-11 05:27:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=61042.328125Mb; avail=131794.83203125Mb
2020-10-11 05:27:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001284
2020-10-11 05:27:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61042.93359375Mb; avail=131794.83203125Mb
2020-10-11 05:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082381
2020-10-11 05:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61029.05859375Mb; avail=131808.70703125Mb
2020-10-11 05:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063356
2020-10-11 05:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.148140
2020-10-11 05:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61029.07421875Mb; avail=131808.69140625Mb
2020-10-11 05:27:50 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.928 | nll_loss 9.175 | ppl 578.12 | wps 11195.6 | wpb 2274.6 | bsz 96.2 | num_updates 1296 | best_loss 9.928
2020-10-11 05:27:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 05:28:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 2 @ 1296 updates, score 9.928) (writing took 17.481063816696405 seconds)
2020-10-11 05:28:07 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-11 05:28:07 | INFO | train | epoch 002 | loss 10.411 | nll_loss 9.742 | ppl 856.21 | wps 4563.3 | ups 0.66 | wpb 6882.8 | bsz 290.7 | num_updates 1296 | lr 6.48676e-05 | gnorm 1.294 | clip 0 | train_wall 940 | wall 1957
2020-10-11 05:28:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-11 05:28:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-11 05:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60976.23046875Mb; avail=131860.72265625Mb
2020-10-11 05:28:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005798
2020-10-11 05:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044436
2020-10-11 05:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60968.9921875Mb; avail=131867.9609375Mb
2020-10-11 05:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001447
2020-10-11 05:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60968.9921875Mb; avail=131867.9609375Mb
2020-10-11 05:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.863532
2020-10-11 05:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.910605
2020-10-11 05:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60973.80859375Mb; avail=131863.5234375Mb
2020-10-11 05:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60969.57421875Mb; avail=131867.7578125Mb
2020-10-11 05:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026575
2020-10-11 05:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60969.21484375Mb; avail=131868.1171875Mb
2020-10-11 05:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001443
2020-10-11 05:28:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60969.21484375Mb; avail=131868.1171875Mb
2020-10-11 05:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.857820
2020-10-11 05:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.886952
2020-10-11 05:28:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60973.796875Mb; avail=131863.7734375Mb
2020-10-11 05:28:09 | INFO | fairseq.trainer | begin training epoch 3
2020-10-11 05:28:15 | INFO | train_inner | epoch 003:      4 / 648 loss=10.209, nll_loss=9.508, ppl=728.34, wps=3935.7, ups=0.56, wpb=6977.9, bsz=282.2, num_updates=1300, lr=6.50675e-05, gnorm=1.187, clip=0, train_wall=146, wall=1965
2020-10-11 05:30:42 | INFO | train_inner | epoch 003:    104 / 648 loss=10.068, nll_loss=9.345, ppl=650.37, wps=4829.1, ups=0.68, wpb=7095.7, bsz=281.4, num_updates=1400, lr=7.0065e-05, gnorm=1.156, clip=0, train_wall=146, wall=2112
2020-10-11 05:33:09 | INFO | train_inner | epoch 003:    204 / 648 loss=10.039, nll_loss=9.31, ppl=634.84, wps=4697.6, ups=0.68, wpb=6935.6, bsz=273.3, num_updates=1500, lr=7.50625e-05, gnorm=1.349, clip=0, train_wall=146, wall=2259
2020-10-11 05:35:37 | INFO | train_inner | epoch 003:    304 / 648 loss=9.993, nll_loss=9.257, ppl=611.65, wps=4657.6, ups=0.68, wpb=6850.5, bsz=263, num_updates=1600, lr=8.006e-05, gnorm=1.208, clip=0, train_wall=146, wall=2406
2020-10-11 05:38:03 | INFO | train_inner | epoch 003:    404 / 648 loss=9.726, nll_loss=8.955, ppl=496.11, wps=4709.8, ups=0.68, wpb=6913.6, bsz=318.1, num_updates=1700, lr=8.50575e-05, gnorm=1.382, clip=0, train_wall=146, wall=2553
2020-10-11 05:40:31 | INFO | train_inner | epoch 003:    504 / 648 loss=9.712, nll_loss=8.937, ppl=489.99, wps=4676.7, ups=0.68, wpb=6894.3, bsz=310, num_updates=1800, lr=9.0055e-05, gnorm=1.307, clip=0, train_wall=146, wall=2701
2020-10-11 05:42:57 | INFO | train_inner | epoch 003:    604 / 648 loss=9.58, nll_loss=8.784, ppl=440.76, wps=4572.2, ups=0.68, wpb=6693.7, bsz=294, num_updates=1900, lr=9.50525e-05, gnorm=1.304, clip=0, train_wall=145, wall=2847
2020-10-11 05:44:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 05:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=61068.6875Mb; avail=131768.98046875Mb
2020-10-11 05:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002008
2020-10-11 05:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61068.6875Mb; avail=131768.98046875Mb
2020-10-11 05:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.089041
2020-10-11 05:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61068.6875Mb; avail=131768.98046875Mb
2020-10-11 05:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062357
2020-10-11 05:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.154567
2020-10-11 05:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61071.625Mb; avail=131766.04296875Mb
2020-10-11 05:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=61071.625Mb; avail=131766.04296875Mb
2020-10-11 05:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001182
2020-10-11 05:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61071.625Mb; avail=131766.04296875Mb
2020-10-11 05:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079307
2020-10-11 05:44:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61078.73828125Mb; avail=131758.32421875Mb
2020-10-11 05:44:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062534
2020-10-11 05:44:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.144223
2020-10-11 05:44:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61069.12890625Mb; avail=131768.5390625Mb
2020-10-11 05:44:12 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.182 | nll_loss 8.312 | ppl 317.76 | wps 11159.4 | wpb 2274.6 | bsz 96.2 | num_updates 1944 | best_loss 9.182
2020-10-11 05:44:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 05:44:30 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 3 @ 1944 updates, score 9.182) (writing took 18.520064797252417 seconds)
2020-10-11 05:44:30 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-11 05:44:30 | INFO | train | epoch 003 | loss 9.831 | nll_loss 9.072 | ppl 538.34 | wps 4537.1 | ups 0.66 | wpb 6882.8 | bsz 290.7 | num_updates 1944 | lr 9.72514e-05 | gnorm 1.279 | clip 0 | train_wall 944 | wall 2940
2020-10-11 05:44:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-11 05:44:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-11 05:44:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=61027.171875Mb; avail=131810.6171875Mb
2020-10-11 05:44:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006893
2020-10-11 05:44:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.055447
2020-10-11 05:44:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61038.1171875Mb; avail=131799.671875Mb
2020-10-11 05:44:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002153
2020-10-11 05:44:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61038.1171875Mb; avail=131799.671875Mb
2020-10-11 05:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.869417
2020-10-11 05:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.928360
2020-10-11 05:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61025.3671875Mb; avail=131812.5859375Mb
2020-10-11 05:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=61026.32421875Mb; avail=131811.62890625Mb
2020-10-11 05:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027347
2020-10-11 05:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61015.8984375Mb; avail=131822.0546875Mb
2020-10-11 05:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001408
2020-10-11 05:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61015.8984375Mb; avail=131822.0546875Mb
2020-10-11 05:44:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.854473
2020-10-11 05:44:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.884443
2020-10-11 05:44:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61017.51171875Mb; avail=131820.23828125Mb
2020-10-11 05:44:32 | INFO | fairseq.trainer | begin training epoch 4
2020-10-11 05:45:55 | INFO | train_inner | epoch 004:     56 / 648 loss=9.423, nll_loss=8.604, ppl=389.22, wps=3852.4, ups=0.56, wpb=6847.4, bsz=293.7, num_updates=2000, lr=0.00010005, gnorm=1.226, clip=0, train_wall=146, wall=3025
2020-10-11 05:48:21 | INFO | train_inner | epoch 004:    156 / 648 loss=9.394, nll_loss=8.569, ppl=379.87, wps=4652, ups=0.68, wpb=6802.3, bsz=270.2, num_updates=2100, lr=0.000105048, gnorm=1.184, clip=0, train_wall=145, wall=3171
2020-10-11 05:50:48 | INFO | train_inner | epoch 004:    256 / 648 loss=9.242, nll_loss=8.395, ppl=336.61, wps=4714, ups=0.68, wpb=6925.5, bsz=307.5, num_updates=2200, lr=0.000110045, gnorm=1.284, clip=0, train_wall=146, wall=3318
2020-10-11 05:53:14 | INFO | train_inner | epoch 004:    356 / 648 loss=9.23, nll_loss=8.379, ppl=332.81, wps=4682.1, ups=0.69, wpb=6833.5, bsz=295, num_updates=2300, lr=0.000115043, gnorm=1.198, clip=0, train_wall=145, wall=3464
2020-10-11 05:55:41 | INFO | train_inner | epoch 004:    456 / 648 loss=9.148, nll_loss=8.284, ppl=311.6, wps=4771.6, ups=0.68, wpb=7022.1, bsz=296.7, num_updates=2400, lr=0.00012004, gnorm=1.14, clip=0, train_wall=146, wall=3611
2020-10-11 05:58:07 | INFO | train_inner | epoch 004:    556 / 648 loss=9.042, nll_loss=8.162, ppl=286.47, wps=4682.8, ups=0.68, wpb=6842.5, bsz=299.1, num_updates=2500, lr=0.000125037, gnorm=1.129, clip=0, train_wall=145, wall=3757
2020-10-11 06:00:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 06:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59367.15625Mb; avail=133471.296875Mb
2020-10-11 06:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002075
2020-10-11 06:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59367.15625Mb; avail=133471.296875Mb
2020-10-11 06:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.087416
2020-10-11 06:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59367.42578125Mb; avail=133471.15234375Mb
2020-10-11 06:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062252
2020-10-11 06:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.152924
2020-10-11 06:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59367.42578125Mb; avail=133471.15234375Mb
2020-10-11 06:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59367.42578125Mb; avail=133471.15234375Mb
2020-10-11 06:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001184
2020-10-11 06:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59367.42578125Mb; avail=133471.15234375Mb
2020-10-11 06:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077977
2020-10-11 06:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59367.42578125Mb; avail=133471.15234375Mb
2020-10-11 06:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061252
2020-10-11 06:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.147969
2020-10-11 06:00:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59369.84765625Mb; avail=133468.73046875Mb
2020-10-11 06:00:32 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.636 | nll_loss 7.684 | ppl 205.62 | wps 11289.9 | wpb 2274.6 | bsz 96.2 | num_updates 2592 | best_loss 8.636
2020-10-11 06:00:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 06:00:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 4 @ 2592 updates, score 8.636) (writing took 7.97392388805747 seconds)
2020-10-11 06:00:40 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-11 06:00:40 | INFO | train | epoch 004 | loss 9.197 | nll_loss 8.341 | ppl 324.27 | wps 4599.4 | ups 0.67 | wpb 6882.8 | bsz 290.7 | num_updates 2592 | lr 0.000129635 | gnorm 1.18 | clip 0 | train_wall 942 | wall 3910
2020-10-11 06:00:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-11 06:00:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-11 06:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59304.1875Mb; avail=133534.5859375Mb
2020-10-11 06:00:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008092
2020-10-11 06:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.063315
2020-10-11 06:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59306.1640625Mb; avail=133532.85546875Mb
2020-10-11 06:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001883
2020-10-11 06:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59306.1640625Mb; avail=133532.85546875Mb
2020-10-11 06:00:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.905889
2020-10-11 06:00:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.972648
2020-10-11 06:00:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59317.73828125Mb; avail=133521.31640625Mb
2020-10-11 06:00:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59318.7265625Mb; avail=133520.328125Mb
2020-10-11 06:00:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.029307
2020-10-11 06:00:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59307.8515625Mb; avail=133531.203125Mb
2020-10-11 06:00:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001601
2020-10-11 06:00:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59307.8515625Mb; avail=133531.203125Mb
2020-10-11 06:00:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.875184
2020-10-11 06:00:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.907432
2020-10-11 06:00:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59312.33203125Mb; avail=133526.65234375Mb
2020-10-11 06:00:42 | INFO | fairseq.trainer | begin training epoch 5
2020-10-11 06:00:54 | INFO | train_inner | epoch 005:      8 / 648 loss=8.991, nll_loss=8.103, ppl=274.87, wps=4089.9, ups=0.6, wpb=6806.6, bsz=274.1, num_updates=2600, lr=0.000130035, gnorm=1.112, clip=0, train_wall=145, wall=3924
2020-10-11 06:03:20 | INFO | train_inner | epoch 005:    108 / 648 loss=8.884, nll_loss=7.982, ppl=252.76, wps=4718.8, ups=0.68, wpb=6919.8, bsz=296.2, num_updates=2700, lr=0.000135032, gnorm=1.155, clip=0, train_wall=145, wall=4070
2020-10-11 06:05:46 | INFO | train_inner | epoch 005:    208 / 648 loss=8.771, nll_loss=7.852, ppl=231, wps=4682.6, ups=0.68, wpb=6839.4, bsz=307.1, num_updates=2800, lr=0.00014003, gnorm=1.18, clip=0, train_wall=145, wall=4216
2020-10-11 06:08:12 | INFO | train_inner | epoch 005:    308 / 648 loss=8.769, nll_loss=7.847, ppl=230.28, wps=4720.6, ups=0.69, wpb=6867.2, bsz=270.8, num_updates=2900, lr=0.000145028, gnorm=1.102, clip=0, train_wall=144, wall=4362
2020-10-11 06:10:39 | INFO | train_inner | epoch 005:    408 / 648 loss=8.653, nll_loss=7.714, ppl=209.94, wps=4779.8, ups=0.68, wpb=7031.7, bsz=283.2, num_updates=3000, lr=0.000150025, gnorm=1.066, clip=0, train_wall=146, wall=4509
2020-10-11 06:13:05 | INFO | train_inner | epoch 005:    508 / 648 loss=8.573, nll_loss=7.623, ppl=197.14, wps=4679.1, ups=0.69, wpb=6821.6, bsz=307.1, num_updates=3100, lr=0.000155023, gnorm=1.114, clip=0, train_wall=145, wall=4655
2020-10-11 06:15:31 | INFO | train_inner | epoch 005:    608 / 648 loss=8.49, nll_loss=7.525, ppl=184.23, wps=4660.5, ups=0.68, wpb=6818.6, bsz=289.7, num_updates=3200, lr=0.00016002, gnorm=1.147, clip=0, train_wall=145, wall=4801
2020-10-11 06:16:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 06:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59398.56640625Mb; avail=133440.39453125Mb
2020-10-11 06:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001967
2020-10-11 06:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59398.56640625Mb; avail=133440.39453125Mb
2020-10-11 06:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.084871
2020-10-11 06:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59398.56640625Mb; avail=133440.39453125Mb
2020-10-11 06:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061257
2020-10-11 06:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.149164
2020-10-11 06:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59398.56640625Mb; avail=133440.39453125Mb
2020-10-11 06:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59398.56640625Mb; avail=133440.39453125Mb
2020-10-11 06:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001199
2020-10-11 06:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59398.56640625Mb; avail=133440.39453125Mb
2020-10-11 06:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078922
2020-10-11 06:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59401.88671875Mb; avail=133437.07421875Mb
2020-10-11 06:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063724
2020-10-11 06:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.144884
2020-10-11 06:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59409.8359375Mb; avail=133429.1328125Mb
2020-10-11 06:16:40 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.137 | nll_loss 7.092 | ppl 136.42 | wps 11030.6 | wpb 2274.6 | bsz 96.2 | num_updates 3240 | best_loss 8.137
2020-10-11 06:16:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 06:16:59 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 5 @ 3240 updates, score 8.137) (writing took 18.806118089705706 seconds)
2020-10-11 06:16:59 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-11 06:16:59 | INFO | train | epoch 005 | loss 8.68 | nll_loss 7.745 | ppl 214.58 | wps 4554.6 | ups 0.66 | wpb 6882.8 | bsz 290.7 | num_updates 3240 | lr 0.000162019 | gnorm 1.125 | clip 0 | train_wall 940 | wall 4889
2020-10-11 06:16:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-11 06:16:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-11 06:16:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59342.71875Mb; avail=133496.0546875Mb
2020-10-11 06:16:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008303
2020-10-11 06:16:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.062164
2020-10-11 06:16:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59345.171875Mb; avail=133493.6015625Mb
2020-10-11 06:16:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001846
2020-10-11 06:16:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59345.171875Mb; avail=133493.6015625Mb
2020-10-11 06:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.876346
2020-10-11 06:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.941776
2020-10-11 06:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59355.45703125Mb; avail=133483.7890625Mb
2020-10-11 06:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59356.421875Mb; avail=133482.8046875Mb
2020-10-11 06:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027503
2020-10-11 06:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59358.84375Mb; avail=133480.3828125Mb
2020-10-11 06:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001480
2020-10-11 06:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59359.44921875Mb; avail=133479.77734375Mb
2020-10-11 06:17:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.865861
2020-10-11 06:17:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.895999
2020-10-11 06:17:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59348.0546875Mb; avail=133490.97265625Mb
2020-10-11 06:17:01 | INFO | fairseq.trainer | begin training epoch 6
2020-10-11 06:18:01 | INFO | train_inner | epoch 006:     60 / 648 loss=8.409, nll_loss=7.435, ppl=173.03, wps=4592.4, ups=0.67, wpb=6900.5, bsz=299.7, num_updates=3300, lr=0.000165018, gnorm=1.127, clip=0, train_wall=118, wall=4951
2020-10-11 06:19:12 | INFO | train_inner | epoch 006:    160 / 648 loss=8.3, nll_loss=7.309, ppl=158.53, wps=10092, ups=1.42, wpb=7087.5, bsz=298.5, num_updates=3400, lr=0.000170015, gnorm=1.105, clip=0, train_wall=69, wall=5021
2020-10-11 06:20:21 | INFO | train_inner | epoch 006:    260 / 648 loss=8.272, nll_loss=7.276, ppl=154.96, wps=9931.8, ups=1.44, wpb=6900.1, bsz=289, num_updates=3500, lr=0.000175013, gnorm=1.141, clip=0, train_wall=68, wall=5091
2020-10-11 06:21:31 | INFO | train_inner | epoch 006:    360 / 648 loss=8.152, nll_loss=7.138, ppl=140.85, wps=9794.6, ups=1.42, wpb=6885.5, bsz=285.5, num_updates=3600, lr=0.00018001, gnorm=1.069, clip=0, train_wall=69, wall=5161
2020-10-11 06:22:41 | INFO | train_inner | epoch 006:    460 / 648 loss=8.131, nll_loss=7.112, ppl=138.34, wps=9842.7, ups=1.43, wpb=6879, bsz=278.2, num_updates=3700, lr=0.000185008, gnorm=1.13, clip=0, train_wall=69, wall=5231
2020-10-11 06:23:51 | INFO | train_inner | epoch 006:    560 / 648 loss=8.012, nll_loss=6.977, ppl=125.96, wps=9792, ups=1.43, wpb=6835.3, bsz=290, num_updates=3800, lr=0.000190005, gnorm=1.097, clip=0, train_wall=69, wall=5301
2020-10-11 06:24:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 06:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57497.87109375Mb; avail=135340.71875Mb
2020-10-11 06:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002115
2020-10-11 06:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57497.87109375Mb; avail=135340.71875Mb
2020-10-11 06:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.092318
2020-10-11 06:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57497.93359375Mb; avail=135340.4765625Mb
2020-10-11 06:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060705
2020-10-11 06:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.156380
2020-10-11 06:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57498.171875Mb; avail=135340.4296875Mb
2020-10-11 06:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57498.171875Mb; avail=135340.4296875Mb
2020-10-11 06:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001171
2020-10-11 06:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57498.171875Mb; avail=135340.4296875Mb
2020-10-11 06:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077604
2020-10-11 06:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57500.8046875Mb; avail=135337.796875Mb
2020-10-11 06:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062843
2020-10-11 06:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142605
2020-10-11 06:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57503.83203125Mb; avail=135334.76953125Mb
2020-10-11 06:24:57 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.686 | nll_loss 6.563 | ppl 94.53 | wps 22166.5 | wpb 2274.6 | bsz 96.2 | num_updates 3888 | best_loss 7.686
2020-10-11 06:24:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 06:25:03 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 6 @ 3888 updates, score 7.686) (writing took 5.316562287509441 seconds)
2020-10-11 06:25:03 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-11 06:25:03 | INFO | train | epoch 006 | loss 8.164 | nll_loss 7.152 | ppl 142.17 | wps 9222.6 | ups 1.34 | wpb 6882.8 | bsz 290.7 | num_updates 3888 | lr 0.000194403 | gnorm 1.12 | clip 0 | train_wall 463 | wall 5373
2020-10-11 06:25:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-11 06:25:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-11 06:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57446.25Mb; avail=135392.42578125Mb
2020-10-11 06:25:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007197
2020-10-11 06:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.056920
2020-10-11 06:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57452.26953125Mb; avail=135386.40625Mb
2020-10-11 06:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001976
2020-10-11 06:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57452.26953125Mb; avail=135386.40625Mb
2020-10-11 06:25:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.887317
2020-10-11 06:25:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.947573
2020-10-11 06:25:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57456.85546875Mb; avail=135382.16796875Mb
2020-10-11 06:25:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57457.375Mb; avail=135381.6484375Mb
2020-10-11 06:25:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027714
2020-10-11 06:25:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57445.53125Mb; avail=135393.4921875Mb
2020-10-11 06:25:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001372
2020-10-11 06:25:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57445.53125Mb; avail=135393.4921875Mb
2020-10-11 06:25:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.868190
2020-10-11 06:25:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.898551
2020-10-11 06:25:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57454.63671875Mb; avail=135384.203125Mb
2020-10-11 06:25:05 | INFO | fairseq.trainer | begin training epoch 7
2020-10-11 06:25:13 | INFO | train_inner | epoch 007:     12 / 648 loss=7.919, nll_loss=6.87, ppl=116.94, wps=8207.2, ups=1.22, wpb=6748.5, bsz=300, num_updates=3900, lr=0.000195003, gnorm=1.183, clip=0, train_wall=68, wall=5383
2020-10-11 06:26:23 | INFO | train_inner | epoch 007:    112 / 648 loss=7.828, nll_loss=6.766, ppl=108.85, wps=9866.1, ups=1.43, wpb=6885.8, bsz=289.6, num_updates=4000, lr=0.0002, gnorm=1.123, clip=0, train_wall=69, wall=5453
2020-10-11 06:27:34 | INFO | train_inner | epoch 007:    212 / 648 loss=7.796, nll_loss=6.728, ppl=105.97, wps=9747, ups=1.41, wpb=6928.1, bsz=277.1, num_updates=4100, lr=0.000197546, gnorm=1.076, clip=0, train_wall=70, wall=5524
2020-10-11 06:28:45 | INFO | train_inner | epoch 007:    312 / 648 loss=7.641, nll_loss=6.55, ppl=93.72, wps=10001.9, ups=1.41, wpb=7096.2, bsz=291.3, num_updates=4200, lr=0.00019518, gnorm=1.094, clip=0, train_wall=70, wall=5595
2020-10-11 06:29:53 | INFO | train_inner | epoch 007:    412 / 648 loss=7.603, nll_loss=6.506, ppl=90.88, wps=10045.5, ups=1.47, wpb=6831.6, bsz=295.2, num_updates=4300, lr=0.000192897, gnorm=1.112, clip=0, train_wall=67, wall=5663
2020-10-11 06:31:03 | INFO | train_inner | epoch 007:    512 / 648 loss=7.546, nll_loss=6.44, ppl=86.84, wps=9726, ups=1.43, wpb=6813.2, bsz=296.2, num_updates=4400, lr=0.000190693, gnorm=1.109, clip=0, train_wall=69, wall=5733
2020-10-11 06:32:12 | INFO | train_inner | epoch 007:    612 / 648 loss=7.546, nll_loss=6.439, ppl=86.76, wps=9684.3, ups=1.45, wpb=6699.7, bsz=269.3, num_updates=4500, lr=0.000188562, gnorm=1.144, clip=0, train_wall=68, wall=5802
2020-10-11 06:32:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 06:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57490.1953125Mb; avail=135348.703125Mb
2020-10-11 06:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001647
2020-10-11 06:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57490.1953125Mb; avail=135348.703125Mb
2020-10-11 06:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076434
2020-10-11 06:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57490.55859375Mb; avail=135348.55078125Mb
2020-10-11 06:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060608
2020-10-11 06:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139743
2020-10-11 06:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57490.55859375Mb; avail=135348.55078125Mb
2020-10-11 06:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57490.55859375Mb; avail=135348.55078125Mb
2020-10-11 06:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001176
2020-10-11 06:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57490.55859375Mb; avail=135348.55078125Mb
2020-10-11 06:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079469
2020-10-11 06:32:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57494.25Mb; avail=135344.53515625Mb
2020-10-11 06:32:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061878
2020-10-11 06:32:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.143668
2020-10-11 06:32:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57497.27734375Mb; avail=135341.5078125Mb
2020-10-11 06:32:43 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.179 | nll_loss 5.953 | ppl 61.94 | wps 21596.5 | wpb 2274.6 | bsz 96.2 | num_updates 4536 | best_loss 7.179
2020-10-11 06:32:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 06:32:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 7 @ 4536 updates, score 7.179) (writing took 5.495745550841093 seconds)
2020-10-11 06:32:48 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-11 06:32:48 | INFO | train | epoch 007 | loss 7.646 | nll_loss 6.555 | ppl 94.01 | wps 9578.8 | ups 1.39 | wpb 6882.8 | bsz 290.7 | num_updates 4536 | lr 0.000187812 | gnorm 1.114 | clip 0 | train_wall 445 | wall 5838
2020-10-11 06:32:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-11 06:32:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-11 06:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57446.58984375Mb; avail=135392.1640625Mb
2020-10-11 06:32:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005178
2020-10-11 06:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.040972
2020-10-11 06:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57453.8984375Mb; avail=135384.85546875Mb
2020-10-11 06:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001670
2020-10-11 06:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57453.8984375Mb; avail=135384.85546875Mb
2020-10-11 06:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.876101
2020-10-11 06:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.920152
2020-10-11 06:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57448.015625Mb; avail=135391.15625Mb
2020-10-11 06:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57445.89453125Mb; avail=135393.27734375Mb
2020-10-11 06:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027673
2020-10-11 06:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57447.60546875Mb; avail=135391.56640625Mb
2020-10-11 06:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001450
2020-10-11 06:32:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57448.2109375Mb; avail=135390.9609375Mb
2020-10-11 06:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.857148
2020-10-11 06:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.887389
2020-10-11 06:32:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57459.5625Mb; avail=135379.625Mb
2020-10-11 06:32:50 | INFO | fairseq.trainer | begin training epoch 8
2020-10-11 06:33:35 | INFO | train_inner | epoch 008:     64 / 648 loss=7.374, nll_loss=6.243, ppl=75.72, wps=8421, ups=1.22, wpb=6925.1, bsz=291.4, num_updates=4600, lr=0.000186501, gnorm=1.095, clip=0, train_wall=68, wall=5884
2020-10-11 06:34:44 | INFO | train_inner | epoch 008:    164 / 648 loss=7.291, nll_loss=6.148, ppl=70.91, wps=9890, ups=1.44, wpb=6887.2, bsz=299.5, num_updates=4700, lr=0.000184506, gnorm=1.119, clip=0, train_wall=69, wall=5954
2020-10-11 06:35:56 | INFO | train_inner | epoch 008:    264 / 648 loss=7.223, nll_loss=6.068, ppl=67.1, wps=9864.9, ups=1.4, wpb=7036.3, bsz=305.5, num_updates=4800, lr=0.000182574, gnorm=1.091, clip=0, train_wall=70, wall=6025
2020-10-11 06:38:12 | INFO | train_inner | epoch 008:    364 / 648 loss=7.242, nll_loss=6.088, ppl=68.03, wps=4936.7, ups=0.73, wpb=6752, bsz=276.4, num_updates=4900, lr=0.000180702, gnorm=1.103, clip=0, train_wall=136, wall=6162
2020-10-11 06:40:40 | INFO | train_inner | epoch 008:    464 / 648 loss=7.189, nll_loss=6.027, ppl=65.19, wps=4635.9, ups=0.68, wpb=6822.8, bsz=281, num_updates=5000, lr=0.000178885, gnorm=1.11, clip=0, train_wall=146, wall=6309
2020-10-11 06:43:07 | INFO | train_inner | epoch 008:    564 / 648 loss=7.058, nll_loss=5.879, ppl=58.83, wps=4670.8, ups=0.68, wpb=6881, bsz=309.1, num_updates=5100, lr=0.000177123, gnorm=1.1, clip=0, train_wall=146, wall=6457
2020-10-11 06:45:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 06:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58188.3203125Mb; avail=134649.17578125Mb
2020-10-11 06:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001641
2020-10-11 06:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58188.3203125Mb; avail=134649.17578125Mb
2020-10-11 06:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080005
2020-10-11 06:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58192.03125Mb; avail=134645.46484375Mb
2020-10-11 06:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064165
2020-10-11 06:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.146881
2020-10-11 06:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58180.2578125Mb; avail=134657.23828125Mb
2020-10-11 06:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58180.2578125Mb; avail=134657.23828125Mb
2020-10-11 06:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001240
2020-10-11 06:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58180.76171875Mb; avail=134656.734375Mb
2020-10-11 06:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079061
2020-10-11 06:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58186.20703125Mb; avail=134651.2890625Mb
2020-10-11 06:45:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063102
2020-10-11 06:45:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.144413
2020-10-11 06:45:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58192.19140625Mb; avail=134645.51953125Mb
2020-10-11 06:45:21 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.849 | nll_loss 5.557 | ppl 47.07 | wps 11055.5 | wpb 2274.6 | bsz 96.2 | num_updates 5184 | best_loss 6.849
2020-10-11 06:45:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 06:45:31 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 8 @ 5184 updates, score 6.849) (writing took 9.774049870669842 seconds)
2020-10-11 06:45:31 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-11 06:45:31 | INFO | train | epoch 008 | loss 7.2 | nll_loss 6.042 | ppl 65.87 | wps 5851.3 | ups 0.85 | wpb 6882.8 | bsz 290.7 | num_updates 5184 | lr 0.000175682 | gnorm 1.096 | clip 0 | train_wall 732 | wall 6600
2020-10-11 06:45:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-11 06:45:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-11 06:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58135.26171875Mb; avail=134702.63671875Mb
2020-10-11 06:45:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007883
2020-10-11 06:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.061740
2020-10-11 06:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58137.171875Mb; avail=134700.7265625Mb
2020-10-11 06:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001856
2020-10-11 06:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58137.171875Mb; avail=134700.7265625Mb
2020-10-11 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.915184
2020-10-11 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.980141
2020-10-11 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58134.125Mb; avail=134703.61328125Mb
2020-10-11 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58125.58203125Mb; avail=134712.15625Mb
2020-10-11 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027189
2020-10-11 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58127.68359375Mb; avail=134710.0546875Mb
2020-10-11 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001361
2020-10-11 06:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58127.68359375Mb; avail=134710.0546875Mb
2020-10-11 06:45:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.856875
2020-10-11 06:45:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.886400
2020-10-11 06:45:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58135.01953125Mb; avail=134702.96875Mb
2020-10-11 06:45:33 | INFO | fairseq.trainer | begin training epoch 9
2020-10-11 06:45:56 | INFO | train_inner | epoch 009:     16 / 648 loss=7.049, nll_loss=5.866, ppl=58.32, wps=4048.8, ups=0.59, wpb=6830.8, bsz=288, num_updates=5200, lr=0.000175412, gnorm=1.057, clip=0, train_wall=145, wall=6625
2020-10-11 06:48:20 | INFO | train_inner | epoch 009:    116 / 648 loss=6.899, nll_loss=5.697, ppl=51.87, wps=4621, ups=0.69, wpb=6695.4, bsz=302.4, num_updates=5300, lr=0.000173749, gnorm=1.133, clip=0, train_wall=144, wall=6770
2020-10-11 06:50:48 | INFO | train_inner | epoch 009:    216 / 648 loss=6.959, nll_loss=5.764, ppl=54.33, wps=4637.5, ups=0.68, wpb=6844.8, bsz=268.5, num_updates=5400, lr=0.000172133, gnorm=1.105, clip=0, train_wall=146, wall=6918
2020-10-11 06:53:13 | INFO | train_inner | epoch 009:    316 / 648 loss=6.874, nll_loss=5.665, ppl=50.75, wps=4683.8, ups=0.69, wpb=6788.5, bsz=283.8, num_updates=5500, lr=0.000170561, gnorm=1.111, clip=0, train_wall=144, wall=7063
2020-10-11 06:55:41 | INFO | train_inner | epoch 009:    416 / 648 loss=6.887, nll_loss=5.677, ppl=51.17, wps=4805, ups=0.68, wpb=7094.7, bsz=283, num_updates=5600, lr=0.000169031, gnorm=1.126, clip=0, train_wall=146, wall=7210
2020-10-11 06:58:07 | INFO | train_inner | epoch 009:    516 / 648 loss=6.806, nll_loss=5.585, ppl=48.01, wps=4773.4, ups=0.68, wpb=6993.8, bsz=303, num_updates=5700, lr=0.000167542, gnorm=1.118, clip=0, train_wall=145, wall=7357
2020-10-11 07:00:34 | INFO | train_inner | epoch 009:    616 / 648 loss=6.771, nll_loss=5.546, ppl=46.72, wps=4713, ups=0.68, wpb=6916.8, bsz=301.4, num_updates=5800, lr=0.000166091, gnorm=1.085, clip=0, train_wall=146, wall=7504
2020-10-11 07:01:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 07:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58162.69921875Mb; avail=134675.4453125Mb
2020-10-11 07:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002028
2020-10-11 07:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58162.69921875Mb; avail=134675.4453125Mb
2020-10-11 07:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.090344
2020-10-11 07:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58164.796875Mb; avail=134672.98046875Mb
2020-10-11 07:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062657
2020-10-11 07:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.156107
2020-10-11 07:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58168.4296875Mb; avail=134669.34765625Mb
2020-10-11 07:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58169.03515625Mb; avail=134668.7421875Mb
2020-10-11 07:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001128
2020-10-11 07:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58169.03515625Mb; avail=134668.7421875Mb
2020-10-11 07:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075681
2020-10-11 07:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58164.03515625Mb; avail=134673.7421875Mb
2020-10-11 07:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059249
2020-10-11 07:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.137130
2020-10-11 07:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58163.9609375Mb; avail=134673.81640625Mb
2020-10-11 07:01:30 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.643 | nll_loss 5.306 | ppl 39.56 | wps 11214.8 | wpb 2274.6 | bsz 96.2 | num_updates 5832 | best_loss 6.643
2020-10-11 07:01:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 07:01:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 9 @ 5832 updates, score 6.643) (writing took 10.279445122927427 seconds)
2020-10-11 07:01:41 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-11 07:01:41 | INFO | train | epoch 009 | loss 6.861 | nll_loss 5.65 | ppl 50.2 | wps 4597.2 | ups 0.67 | wpb 6882.8 | bsz 290.7 | num_updates 5832 | lr 0.000165635 | gnorm 1.112 | clip 0 | train_wall 940 | wall 7571
2020-10-11 07:01:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-11 07:01:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-11 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58113.29296875Mb; avail=134724.44921875Mb
2020-10-11 07:01:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007494
2020-10-11 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.059816
2020-10-11 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58120.359375Mb; avail=134717.3828125Mb
2020-10-11 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002075
2020-10-11 07:01:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58120.08203125Mb; avail=134717.62890625Mb
2020-10-11 07:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.851461
2020-10-11 07:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.914727
2020-10-11 07:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58114.78515625Mb; avail=134723.203125Mb
2020-10-11 07:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58112.9140625Mb; avail=134725.07421875Mb
2020-10-11 07:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026512
2020-10-11 07:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58115.09765625Mb; avail=134722.875Mb
2020-10-11 07:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001358
2020-10-11 07:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58115.09765625Mb; avail=134722.875Mb
2020-10-11 07:01:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.868213
2020-10-11 07:01:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.897117
2020-10-11 07:01:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58122.22265625Mb; avail=134715.74609375Mb
2020-10-11 07:01:43 | INFO | fairseq.trainer | begin training epoch 10
2020-10-11 07:03:23 | INFO | train_inner | epoch 010:     68 / 648 loss=6.674, nll_loss=5.435, ppl=43.27, wps=4119.4, ups=0.59, wpb=6974.8, bsz=295.5, num_updates=5900, lr=0.000164677, gnorm=1.099, clip=0, train_wall=145, wall=7673
2020-10-11 07:05:52 | INFO | train_inner | epoch 010:    168 / 648 loss=6.625, nll_loss=5.379, ppl=41.6, wps=4711.9, ups=0.67, wpb=7002, bsz=297.1, num_updates=6000, lr=0.000163299, gnorm=1.095, clip=0, train_wall=147, wall=7822
2020-10-11 07:08:20 | INFO | train_inner | epoch 010:    268 / 648 loss=6.677, nll_loss=5.436, ppl=43.3, wps=4646.5, ups=0.67, wpb=6896.7, bsz=263.1, num_updates=6100, lr=0.000161955, gnorm=1.116, clip=0, train_wall=147, wall=7970
2020-10-11 07:10:44 | INFO | train_inner | epoch 010:    368 / 648 loss=6.493, nll_loss=5.228, ppl=37.47, wps=4680.6, ups=0.7, wpb=6721.6, bsz=319.8, num_updates=6200, lr=0.000160644, gnorm=1.058, clip=0, train_wall=142, wall=8114
2020-10-11 07:13:09 | INFO | train_inner | epoch 010:    468 / 648 loss=6.562, nll_loss=5.304, ppl=39.49, wps=4827.3, ups=0.69, wpb=6987.8, bsz=287.1, num_updates=6300, lr=0.000159364, gnorm=1.123, clip=0, train_wall=144, wall=8258
2020-10-11 07:15:33 | INFO | train_inner | epoch 010:    568 / 648 loss=6.481, nll_loss=5.212, ppl=37.07, wps=4677.2, ups=0.69, wpb=6753.7, bsz=299.2, num_updates=6400, lr=0.000158114, gnorm=1.116, clip=0, train_wall=143, wall=8403
2020-10-11 07:17:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 07:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57033.375Mb; avail=135805.41015625Mb
2020-10-11 07:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002068
2020-10-11 07:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57033.90625Mb; avail=135804.87890625Mb
2020-10-11 07:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.087625
2020-10-11 07:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57033.90625Mb; avail=135804.87890625Mb
2020-10-11 07:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061109
2020-10-11 07:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.151931
2020-10-11 07:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57033.90625Mb; avail=135804.87890625Mb
2020-10-11 07:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57033.90625Mb; avail=135804.87890625Mb
2020-10-11 07:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001269
2020-10-11 07:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57033.90625Mb; avail=135804.87890625Mb
2020-10-11 07:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076323
2020-10-11 07:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57034.14453125Mb; avail=135804.640625Mb
2020-10-11 07:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061056
2020-10-11 07:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139688
2020-10-11 07:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57034.16015625Mb; avail=135804.625Mb
2020-10-11 07:17:39 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.386 | nll_loss 5.008 | ppl 32.19 | wps 11019.5 | wpb 2274.6 | bsz 96.2 | num_updates 6480 | best_loss 6.386
2020-10-11 07:17:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 07:17:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 10 @ 6480 updates, score 6.386) (writing took 5.48402988538146 seconds)
2020-10-11 07:17:45 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-11 07:17:45 | INFO | train | epoch 010 | loss 6.573 | nll_loss 5.318 | ppl 39.88 | wps 4627.2 | ups 0.67 | wpb 6882.8 | bsz 290.7 | num_updates 6480 | lr 0.000157135 | gnorm 1.103 | clip 0 | train_wall 938 | wall 8534
2020-10-11 07:17:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-11 07:17:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-11 07:17:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57014.953125Mb; avail=135823.625Mb
2020-10-11 07:17:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008599
2020-10-11 07:17:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.063847
2020-10-11 07:17:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57020.38671875Mb; avail=135818.19140625Mb
2020-10-11 07:17:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001999
2020-10-11 07:17:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57020.38671875Mb; avail=135818.19140625Mb
2020-10-11 07:17:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.872510
2020-10-11 07:17:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.939878
2020-10-11 07:17:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57020.55859375Mb; avail=135818.64453125Mb
2020-10-11 07:17:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57016.12890625Mb; avail=135823.07421875Mb
2020-10-11 07:17:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026015
2020-10-11 07:17:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57016.12890625Mb; avail=135823.07421875Mb
2020-10-11 07:17:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001395
2020-10-11 07:17:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57016.12890625Mb; avail=135823.07421875Mb
2020-10-11 07:17:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.860154
2020-10-11 07:17:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.888669
2020-10-11 07:17:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57040.0859375Mb; avail=135798.76953125Mb
2020-10-11 07:17:46 | INFO | fairseq.trainer | begin training epoch 11
2020-10-11 07:18:16 | INFO | train_inner | epoch 011:     20 / 648 loss=6.523, nll_loss=5.259, ppl=38.3, wps=4176.9, ups=0.61, wpb=6792.6, bsz=263.9, num_updates=6500, lr=0.000156893, gnorm=1.12, clip=0, train_wall=143, wall=8565
2020-10-11 07:19:27 | INFO | train_inner | epoch 011:    120 / 648 loss=6.306, nll_loss=5.013, ppl=32.28, wps=9542.8, ups=1.4, wpb=6832.7, bsz=321.8, num_updates=6600, lr=0.0001557, gnorm=1.064, clip=0, train_wall=70, wall=8637
2020-10-11 07:20:37 | INFO | train_inner | epoch 011:    220 / 648 loss=6.371, nll_loss=5.086, ppl=33.97, wps=9843.8, ups=1.43, wpb=6901.4, bsz=282, num_updates=6700, lr=0.000154533, gnorm=1.117, clip=0, train_wall=69, wall=8707
2020-10-11 07:21:47 | INFO | train_inner | epoch 011:    320 / 648 loss=6.367, nll_loss=5.081, ppl=33.84, wps=9840.2, ups=1.45, wpb=6809.1, bsz=287.1, num_updates=6800, lr=0.000153393, gnorm=1.141, clip=0, train_wall=68, wall=8776
2020-10-11 07:22:56 | INFO | train_inner | epoch 011:    420 / 648 loss=6.344, nll_loss=5.054, ppl=33.21, wps=9845.1, ups=1.44, wpb=6829.7, bsz=278.7, num_updates=6900, lr=0.000152277, gnorm=1.085, clip=0, train_wall=68, wall=8846
2020-10-11 07:24:06 | INFO | train_inner | epoch 011:    520 / 648 loss=6.28, nll_loss=4.98, ppl=31.56, wps=9876.5, ups=1.42, wpb=6950.4, bsz=300.8, num_updates=7000, lr=0.000151186, gnorm=1.1, clip=0, train_wall=69, wall=8916
2020-10-11 07:25:17 | INFO | train_inner | epoch 011:    620 / 648 loss=6.316, nll_loss=5.02, ppl=32.45, wps=10004.3, ups=1.42, wpb=7032.8, bsz=278.1, num_updates=7100, lr=0.000150117, gnorm=1.08, clip=0, train_wall=69, wall=8986
2020-10-11 07:25:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 07:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57089.36328125Mb; avail=135750.453125Mb
2020-10-11 07:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002240
2020-10-11 07:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57089.36328125Mb; avail=135750.453125Mb
2020-10-11 07:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.094314
2020-10-11 07:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57089.36328125Mb; avail=135750.453125Mb
2020-10-11 07:25:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062555
2020-10-11 07:25:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.160325
2020-10-11 07:25:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57088.87109375Mb; avail=135750.9453125Mb
2020-10-11 07:25:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57090.6171875Mb; avail=135749.19921875Mb
2020-10-11 07:25:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001253
2020-10-11 07:25:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57090.6171875Mb; avail=135749.19921875Mb
2020-10-11 07:25:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079185
2020-10-11 07:25:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57101.6328125Mb; avail=135738.18359375Mb
2020-10-11 07:25:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.066140
2020-10-11 07:25:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.147816
2020-10-11 07:25:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57112.5Mb; avail=135727.31640625Mb
2020-10-11 07:25:42 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.18 | nll_loss 4.76 | ppl 27.1 | wps 22103.4 | wpb 2274.6 | bsz 96.2 | num_updates 7128 | best_loss 6.18
2020-10-11 07:25:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 07:25:52 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 11 @ 7128 updates, score 6.18) (writing took 10.481059059500694 seconds)
2020-10-11 07:25:52 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-11 07:25:52 | INFO | train | epoch 011 | loss 6.329 | nll_loss 5.037 | ppl 32.84 | wps 9143.3 | ups 1.33 | wpb 6882.8 | bsz 290.7 | num_updates 7128 | lr 0.000149822 | gnorm 1.097 | clip 0 | train_wall 462 | wall 9022
2020-10-11 07:25:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-11 07:25:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-11 07:25:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57038.2109375Mb; avail=135801.6796875Mb
2020-10-11 07:25:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008080
2020-10-11 07:25:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.063981
2020-10-11 07:25:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57043.90234375Mb; avail=135795.67578125Mb
2020-10-11 07:25:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002082
2020-10-11 07:25:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57043.90234375Mb; avail=135795.67578125Mb
2020-10-11 07:25:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.876399
2020-10-11 07:25:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.944031
2020-10-11 07:25:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57038.16015625Mb; avail=135802.4765625Mb
2020-10-11 07:25:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57033.84375Mb; avail=135806.4140625Mb
2020-10-11 07:25:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026489
2020-10-11 07:25:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57033.84375Mb; avail=135806.4140625Mb
2020-10-11 07:25:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001406
2020-10-11 07:25:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57033.84375Mb; avail=135806.4140625Mb
2020-10-11 07:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.879345
2020-10-11 07:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.908429
2020-10-11 07:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57044.5625Mb; avail=135795.265625Mb
2020-10-11 07:25:54 | INFO | fairseq.trainer | begin training epoch 12
2020-10-11 07:26:45 | INFO | train_inner | epoch 012:     72 / 648 loss=6.169, nll_loss=4.854, ppl=28.93, wps=7819.3, ups=1.13, wpb=6945.2, bsz=294, num_updates=7200, lr=0.000149071, gnorm=1.075, clip=0, train_wall=69, wall=9075
2020-10-11 07:27:58 | INFO | train_inner | epoch 012:    172 / 648 loss=6.178, nll_loss=4.863, ppl=29.11, wps=9544.3, ups=1.38, wpb=6929.2, bsz=276.2, num_updates=7300, lr=0.000148047, gnorm=1.104, clip=0, train_wall=71, wall=9148
2020-10-11 07:29:10 | INFO | train_inner | epoch 012:    272 / 648 loss=6.158, nll_loss=4.84, ppl=28.64, wps=9610.1, ups=1.39, wpb=6927, bsz=279.2, num_updates=7400, lr=0.000147043, gnorm=1.086, clip=0, train_wall=71, wall=9220
2020-10-11 07:30:21 | INFO | train_inner | epoch 012:    372 / 648 loss=6.101, nll_loss=4.775, ppl=27.38, wps=9653.9, ups=1.41, wpb=6838.8, bsz=313.1, num_updates=7500, lr=0.000146059, gnorm=1.071, clip=0, train_wall=70, wall=9291
2020-10-11 07:31:31 | INFO | train_inner | epoch 012:    472 / 648 loss=6.102, nll_loss=4.775, ppl=27.39, wps=9694.3, ups=1.42, wpb=6820, bsz=299.9, num_updates=7600, lr=0.000145095, gnorm=1.131, clip=0, train_wall=69, wall=9361
2020-10-11 07:32:42 | INFO | train_inner | epoch 012:    572 / 648 loss=6.11, nll_loss=4.785, ppl=27.56, wps=9730.6, ups=1.42, wpb=6869.4, bsz=279.3, num_updates=7700, lr=0.00014415, gnorm=1.109, clip=0, train_wall=69, wall=9432
2020-10-11 07:33:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 07:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57847.8984375Mb; avail=134991.17578125Mb
2020-10-11 07:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002045
2020-10-11 07:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57847.8984375Mb; avail=134991.17578125Mb
2020-10-11 07:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.093122
2020-10-11 07:33:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57847.8984375Mb; avail=134991.17578125Mb
2020-10-11 07:33:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062434
2020-10-11 07:33:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.158848
2020-10-11 07:33:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57851.02734375Mb; avail=134988.046875Mb
2020-10-11 07:33:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57851.6328125Mb; avail=134987.44140625Mb
2020-10-11 07:33:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001259
2020-10-11 07:33:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57852.23828125Mb; avail=134986.8359375Mb
2020-10-11 07:33:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080897
2020-10-11 07:33:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57860.71484375Mb; avail=134978.359375Mb
2020-10-11 07:33:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063533
2020-10-11 07:33:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.146851
2020-10-11 07:33:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57856.98046875Mb; avail=134982.09375Mb
2020-10-11 07:33:41 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.043 | nll_loss 4.606 | ppl 24.35 | wps 21817.6 | wpb 2274.6 | bsz 96.2 | num_updates 7776 | best_loss 6.043
2020-10-11 07:33:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 07:34:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 12 @ 7776 updates, score 6.043) (writing took 24.301067646592855 seconds)
2020-10-11 07:34:05 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-11 07:34:05 | INFO | train | epoch 012 | loss 6.123 | nll_loss 4.8 | ppl 27.87 | wps 9049.7 | ups 1.31 | wpb 6882.8 | bsz 290.7 | num_updates 7776 | lr 0.000143444 | gnorm 1.094 | clip 0 | train_wall 453 | wall 9515
2020-10-11 07:34:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-11 07:34:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-11 07:34:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57795.55078125Mb; avail=135043.265625Mb
2020-10-11 07:34:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004656
2020-10-11 07:34:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.040449
2020-10-11 07:34:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57787.55859375Mb; avail=135051.2578125Mb
2020-10-11 07:34:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001536
2020-10-11 07:34:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57787.55859375Mb; avail=135051.2578125Mb
2020-10-11 07:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.886522
2020-10-11 07:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.929731
2020-10-11 07:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57805.06640625Mb; avail=135033.8203125Mb
2020-10-11 07:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57798.36328125Mb; avail=135040.5234375Mb
2020-10-11 07:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.028148
2020-10-11 07:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57786.48046875Mb; avail=135052.40625Mb
2020-10-11 07:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001618
2020-10-11 07:34:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57786.48046875Mb; avail=135052.40625Mb
2020-10-11 07:34:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.855455
2020-10-11 07:34:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.886578
2020-10-11 07:34:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57787.65234375Mb; avail=135051.453125Mb
2020-10-11 07:34:07 | INFO | fairseq.trainer | begin training epoch 13
2020-10-11 07:34:24 | INFO | train_inner | epoch 013:     24 / 648 loss=6.029, nll_loss=4.693, ppl=25.87, wps=6753.3, ups=0.98, wpb=6864.9, bsz=300.4, num_updates=7800, lr=0.000143223, gnorm=1.091, clip=0, train_wall=68, wall=9533
2020-10-11 07:35:35 | INFO | train_inner | epoch 013:    124 / 648 loss=6.014, nll_loss=4.675, ppl=25.55, wps=9618, ups=1.4, wpb=6869.2, bsz=277.2, num_updates=7900, lr=0.000142314, gnorm=1.112, clip=0, train_wall=70, wall=9605
2020-10-11 07:36:46 | INFO | train_inner | epoch 013:    224 / 648 loss=5.967, nll_loss=4.623, ppl=24.64, wps=9636.6, ups=1.4, wpb=6859.4, bsz=280.4, num_updates=8000, lr=0.000141421, gnorm=1.065, clip=0, train_wall=70, wall=9676
2020-10-11 07:37:56 | INFO | train_inner | epoch 013:    324 / 648 loss=5.993, nll_loss=4.65, ppl=25.12, wps=9999.6, ups=1.43, wpb=6989, bsz=284.3, num_updates=8100, lr=0.000140546, gnorm=1.086, clip=0, train_wall=69, wall=9746
2020-10-11 07:39:06 | INFO | train_inner | epoch 013:    424 / 648 loss=5.931, nll_loss=4.581, ppl=23.94, wps=9817.9, ups=1.43, wpb=6857.2, bsz=303, num_updates=8200, lr=0.000139686, gnorm=1.116, clip=0, train_wall=69, wall=9816
2020-10-11 07:40:16 | INFO | train_inner | epoch 013:    524 / 648 loss=5.918, nll_loss=4.565, ppl=23.67, wps=9945.9, ups=1.43, wpb=6971, bsz=292.8, num_updates=8300, lr=0.000138842, gnorm=1.067, clip=0, train_wall=69, wall=9886
2020-10-11 07:41:25 | INFO | train_inner | epoch 013:    624 / 648 loss=5.913, nll_loss=4.559, ppl=23.58, wps=9770.3, ups=1.44, wpb=6786.4, bsz=292.6, num_updates=8400, lr=0.000138013, gnorm=1.069, clip=0, train_wall=68, wall=9955
2020-10-11 07:41:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 07:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58434.83984375Mb; avail=134403.95703125Mb
2020-10-11 07:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001730
2020-10-11 07:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58434.83984375Mb; avail=134403.95703125Mb
2020-10-11 07:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080599
2020-10-11 07:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58442.0546875Mb; avail=134396.9375Mb
2020-10-11 07:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.067575
2020-10-11 07:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.151868
2020-10-11 07:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58444.9609375Mb; avail=134394.03125Mb
2020-10-11 07:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58444.9609375Mb; avail=134394.03125Mb
2020-10-11 07:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001465
2020-10-11 07:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58444.9609375Mb; avail=134394.03125Mb
2020-10-11 07:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082389
2020-10-11 07:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58453.6796875Mb; avail=134385.08984375Mb
2020-10-11 07:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.065727
2020-10-11 07:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.151017
2020-10-11 07:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58448.57421875Mb; avail=134390.1953125Mb
2020-10-11 07:41:48 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 5.927 | nll_loss 4.473 | ppl 22.21 | wps 21401.9 | wpb 2274.6 | bsz 96.2 | num_updates 8424 | best_loss 5.927
2020-10-11 07:41:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 07:42:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 13 @ 8424 updates, score 5.927) (writing took 37.40495461970568 seconds)
2020-10-11 07:42:25 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-11 07:42:25 | INFO | train | epoch 013 | loss 5.952 | nll_loss 4.605 | ppl 24.33 | wps 8924.5 | ups 1.3 | wpb 6882.8 | bsz 290.7 | num_updates 8424 | lr 0.000137816 | gnorm 1.092 | clip 0 | train_wall 447 | wall 10015
2020-10-11 07:42:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-11 07:42:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-11 07:42:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58873.65625Mb; avail=133965.171875Mb
2020-10-11 07:42:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004587
2020-10-11 07:42:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.039012
2020-10-11 07:42:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58878.3046875Mb; avail=133960.5234375Mb
2020-10-11 07:42:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002090
2020-10-11 07:42:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58878.3046875Mb; avail=133960.5234375Mb
2020-10-11 07:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.897817
2020-10-11 07:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.940367
2020-10-11 07:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58887.37109375Mb; avail=133951.3515625Mb
2020-10-11 07:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58887.0625Mb; avail=133951.86328125Mb
2020-10-11 07:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.028168
2020-10-11 07:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58887.0625Mb; avail=133951.86328125Mb
2020-10-11 07:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001964
2020-10-11 07:42:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58887.0625Mb; avail=133951.86328125Mb
2020-10-11 07:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.883309
2020-10-11 07:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.914930
2020-10-11 07:42:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58898.95703125Mb; avail=133939.96875Mb
2020-10-11 07:42:27 | INFO | fairseq.trainer | begin training epoch 14
2020-10-11 07:44:03 | INFO | train_inner | epoch 014:     76 / 648 loss=5.856, nll_loss=4.495, ppl=22.55, wps=4302, ups=0.63, wpb=6795.8, bsz=280.3, num_updates=8500, lr=0.000137199, gnorm=1.096, clip=0, train_wall=112, wall=10113
2020-10-11 07:46:34 | INFO | train_inner | epoch 014:    176 / 648 loss=5.757, nll_loss=4.382, ppl=20.86, wps=4603.6, ups=0.66, wpb=6922.8, bsz=316.3, num_updates=8600, lr=0.000136399, gnorm=1.054, clip=0, train_wall=149, wall=10264
2020-10-11 07:49:02 | INFO | train_inner | epoch 014:    276 / 648 loss=5.829, nll_loss=4.462, ppl=22.04, wps=4791.1, ups=0.68, wpb=7096.1, bsz=287.4, num_updates=8700, lr=0.000135613, gnorm=1.075, clip=0, train_wall=147, wall=10412
2020-10-11 07:51:29 | INFO | train_inner | epoch 014:    376 / 648 loss=5.792, nll_loss=4.42, ppl=21.41, wps=4673.2, ups=0.68, wpb=6894.6, bsz=298, num_updates=8800, lr=0.00013484, gnorm=1.097, clip=0, train_wall=146, wall=10559
2020-10-11 07:53:57 | INFO | train_inner | epoch 014:    476 / 648 loss=5.786, nll_loss=4.414, ppl=21.32, wps=4683.3, ups=0.68, wpb=6924.2, bsz=295.6, num_updates=8900, lr=0.00013408, gnorm=1.063, clip=0, train_wall=147, wall=10707
2020-10-11 07:56:23 | INFO | train_inner | epoch 014:    576 / 648 loss=5.823, nll_loss=4.457, ppl=21.96, wps=4561.9, ups=0.69, wpb=6627.1, bsz=276.6, num_updates=9000, lr=0.000133333, gnorm=1.134, clip=0, train_wall=144, wall=10852
2020-10-11 07:58:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 07:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58928.515625Mb; avail=133909.2734375Mb
2020-10-11 07:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001689
2020-10-11 07:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58928.515625Mb; avail=133909.2734375Mb
2020-10-11 07:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078042
2020-10-11 07:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58906.4609375Mb; avail=133931.16015625Mb
2020-10-11 07:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061716
2020-10-11 07:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142626
2020-10-11 07:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58906.4296875Mb; avail=133931.19140625Mb
2020-10-11 07:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58906.4296875Mb; avail=133931.19140625Mb
2020-10-11 07:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001200
2020-10-11 07:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58906.4296875Mb; avail=133931.19140625Mb
2020-10-11 07:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076774
2020-10-11 07:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58906.4296875Mb; avail=133931.19140625Mb
2020-10-11 07:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.105971
2020-10-11 07:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.185186
2020-10-11 07:58:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58906.4296875Mb; avail=133931.19140625Mb
2020-10-11 07:58:18 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 5.811 | nll_loss 4.333 | ppl 20.16 | wps 11034.4 | wpb 2274.6 | bsz 96.2 | num_updates 9072 | best_loss 5.811
2020-10-11 07:58:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 07:58:56 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 14 @ 9072 updates, score 5.811) (writing took 37.79097067564726 seconds)
2020-10-11 07:58:56 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-11 07:58:56 | INFO | train | epoch 014 | loss 5.803 | nll_loss 4.434 | ppl 21.61 | wps 4501 | ups 0.65 | wpb 6882.8 | bsz 290.7 | num_updates 9072 | lr 0.000132803 | gnorm 1.081 | clip 0 | train_wall 932 | wall 11006
2020-10-11 07:58:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-11 07:58:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-11 07:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58850.47265625Mb; avail=133987.078125Mb
2020-10-11 07:58:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008445
2020-10-11 07:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.062437
2020-10-11 07:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58852.28125Mb; avail=133985.50390625Mb
2020-10-11 07:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001875
2020-10-11 07:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58852.28125Mb; avail=133985.50390625Mb
2020-10-11 07:58:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.899215
2020-10-11 07:58:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.965236
2020-10-11 07:58:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58863.1015625Mb; avail=133974.88671875Mb
2020-10-11 07:58:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58864.42578125Mb; avail=133973.6171875Mb
2020-10-11 07:58:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.028204
2020-10-11 07:58:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58867.26953125Mb; avail=133970.7734375Mb
2020-10-11 07:58:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001553
2020-10-11 07:58:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58867.26953125Mb; avail=133970.7734375Mb
2020-10-11 07:58:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.872953
2020-10-11 07:58:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.904079
2020-10-11 07:58:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58868.140625Mb; avail=133969.85546875Mb
2020-10-11 07:58:58 | INFO | fairseq.trainer | begin training epoch 15
2020-10-11 07:59:39 | INFO | train_inner | epoch 015:     28 / 648 loss=5.754, nll_loss=4.379, ppl=20.8, wps=3494.5, ups=0.51, wpb=6855.9, bsz=293.6, num_updates=9100, lr=0.000132599, gnorm=1.071, clip=0, train_wall=145, wall=11049
2020-10-11 08:02:06 | INFO | train_inner | epoch 015:    128 / 648 loss=5.692, nll_loss=4.307, ppl=19.79, wps=4713.4, ups=0.68, wpb=6953.2, bsz=287, num_updates=9200, lr=0.000131876, gnorm=1.062, clip=0, train_wall=146, wall=11196
2020-10-11 08:04:32 | INFO | train_inner | epoch 015:    228 / 648 loss=5.654, nll_loss=4.264, ppl=19.21, wps=4725.8, ups=0.69, wpb=6890, bsz=304.7, num_updates=9300, lr=0.000131165, gnorm=1.058, clip=0, train_wall=145, wall=11342
2020-10-11 08:06:58 | INFO | train_inner | epoch 015:    328 / 648 loss=5.696, nll_loss=4.311, ppl=19.84, wps=4688, ups=0.68, wpb=6852.2, bsz=287.5, num_updates=9400, lr=0.000130466, gnorm=1.087, clip=0, train_wall=145, wall=11488
2020-10-11 08:09:23 | INFO | train_inner | epoch 015:    428 / 648 loss=5.676, nll_loss=4.288, ppl=19.54, wps=4817.5, ups=0.69, wpb=6984.7, bsz=284.6, num_updates=9500, lr=0.000129777, gnorm=1.056, clip=0, train_wall=144, wall=11633
2020-10-11 08:11:48 | INFO | train_inner | epoch 015:    528 / 648 loss=5.66, nll_loss=4.271, ppl=19.3, wps=4687.8, ups=0.69, wpb=6800.7, bsz=292.3, num_updates=9600, lr=0.000129099, gnorm=1.102, clip=0, train_wall=144, wall=11778
2020-10-11 08:14:15 | INFO | train_inner | epoch 015:    628 / 648 loss=5.663, nll_loss=4.273, ppl=19.33, wps=4663.2, ups=0.68, wpb=6854.7, bsz=288.1, num_updates=9700, lr=0.000128432, gnorm=1.079, clip=0, train_wall=146, wall=11925
2020-10-11 08:14:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 08:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58951.32421875Mb; avail=133886.171875Mb
2020-10-11 08:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001563
2020-10-11 08:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58951.32421875Mb; avail=133886.171875Mb
2020-10-11 08:14:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080759
2020-10-11 08:14:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58952.68359375Mb; avail=133884.8125Mb
2020-10-11 08:14:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061950
2020-10-11 08:14:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.145490
2020-10-11 08:14:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58943.50390625Mb; avail=133893.9921875Mb
2020-10-11 08:14:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58943.50390625Mb; avail=133893.9921875Mb
2020-10-11 08:14:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001167
2020-10-11 08:14:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58943.50390625Mb; avail=133893.9921875Mb
2020-10-11 08:14:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076652
2020-10-11 08:14:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58943.50390625Mb; avail=133893.9921875Mb
2020-10-11 08:14:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062723
2020-10-11 08:14:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141560
2020-10-11 08:14:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58943.50390625Mb; avail=133893.9921875Mb
2020-10-11 08:14:55 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 5.729 | nll_loss 4.241 | ppl 18.91 | wps 10935.4 | wpb 2274.6 | bsz 96.2 | num_updates 9720 | best_loss 5.729
2020-10-11 08:14:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 08:15:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 15 @ 9720 updates, score 5.729) (writing took 31.875023368746042 seconds)
2020-10-11 08:15:27 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-11 08:15:27 | INFO | train | epoch 015 | loss 5.674 | nll_loss 4.286 | ppl 19.51 | wps 4500.4 | ups 0.65 | wpb 6882.8 | bsz 290.7 | num_updates 9720 | lr 0.0001283 | gnorm 1.074 | clip 0 | train_wall 939 | wall 11997
2020-10-11 08:15:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-11 08:15:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-11 08:15:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58876.234375Mb; avail=133961.375Mb
2020-10-11 08:15:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006317
2020-10-11 08:15:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.049881
2020-10-11 08:15:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58878.578125Mb; avail=133959.03125Mb
2020-10-11 08:15:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001656
2020-10-11 08:15:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58878.578125Mb; avail=133959.03125Mb
2020-10-11 08:15:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.849954
2020-10-11 08:15:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.902795
2020-10-11 08:15:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58876.2265625Mb; avail=133961.60546875Mb
2020-10-11 08:15:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58878.37109375Mb; avail=133959.4609375Mb
2020-10-11 08:15:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027272
2020-10-11 08:15:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58881.62109375Mb; avail=133956.2109375Mb
2020-10-11 08:15:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001740
2020-10-11 08:15:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58881.62109375Mb; avail=133955.60546875Mb
2020-10-11 08:15:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.854757
2020-10-11 08:15:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.885003
2020-10-11 08:15:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58887.5234375Mb; avail=133950.48828125Mb
2020-10-11 08:15:29 | INFO | fairseq.trainer | begin training epoch 16
2020-10-11 08:17:26 | INFO | train_inner | epoch 016:     80 / 648 loss=5.602, nll_loss=4.204, ppl=18.43, wps=3559.6, ups=0.53, wpb=6777.9, bsz=273.3, num_updates=9800, lr=0.000127775, gnorm=1.09, clip=0, train_wall=145, wall=12116
2020-10-11 08:19:53 | INFO | train_inner | epoch 016:    180 / 648 loss=5.516, nll_loss=4.107, ppl=17.23, wps=4754, ups=0.68, wpb=7004.8, bsz=318.8, num_updates=9900, lr=0.000127128, gnorm=1.055, clip=0, train_wall=146, wall=12263
2020-10-11 08:22:19 | INFO | train_inner | epoch 016:    280 / 648 loss=5.591, nll_loss=4.192, ppl=18.27, wps=4602.3, ups=0.68, wpb=6720.8, bsz=275.4, num_updates=10000, lr=0.000126491, gnorm=1.095, clip=0, train_wall=145, wall=12409
2020-10-11 08:24:47 | INFO | train_inner | epoch 016:    380 / 648 loss=5.573, nll_loss=4.17, ppl=18, wps=4785.6, ups=0.68, wpb=7066.2, bsz=288.2, num_updates=10100, lr=0.000125863, gnorm=1.048, clip=0, train_wall=146, wall=12557
2020-10-11 08:27:14 | INFO | train_inner | epoch 016:    480 / 648 loss=5.539, nll_loss=4.13, ppl=17.51, wps=4710, ups=0.68, wpb=6913, bsz=306.2, num_updates=10200, lr=0.000125245, gnorm=1.044, clip=0, train_wall=146, wall=12703
2020-10-11 08:29:37 | INFO | train_inner | epoch 016:    580 / 648 loss=5.573, nll_loss=4.17, ppl=17.99, wps=4839.5, ups=0.7, wpb=6941.7, bsz=283.4, num_updates=10300, lr=0.000124635, gnorm=1.075, clip=0, train_wall=142, wall=12847
2020-10-11 08:31:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 08:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58120.19140625Mb; avail=134718.53515625Mb
2020-10-11 08:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001732
2020-10-11 08:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58120.19140625Mb; avail=134718.53515625Mb
2020-10-11 08:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.118887
2020-10-11 08:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58115.7890625Mb; avail=134722.9375Mb
2020-10-11 08:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064462
2020-10-11 08:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.186367
2020-10-11 08:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58121.6171875Mb; avail=134717.109375Mb
2020-10-11 08:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58110.7890625Mb; avail=134727.9375Mb
2020-10-11 08:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001341
2020-10-11 08:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58110.7890625Mb; avail=134727.9375Mb
2020-10-11 08:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079699
2020-10-11 08:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58110.7890625Mb; avail=134727.9375Mb
2020-10-11 08:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062045
2020-10-11 08:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.144252
2020-10-11 08:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58110.7890625Mb; avail=134727.9375Mb
2020-10-11 08:31:26 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 5.654 | nll_loss 4.153 | ppl 17.79 | wps 11171.6 | wpb 2274.6 | bsz 96.2 | num_updates 10368 | best_loss 5.654
2020-10-11 08:31:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 08:31:48 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 16 @ 10368 updates, score 5.654) (writing took 22.83708056434989 seconds)
2020-10-11 08:31:48 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-11 08:31:48 | INFO | train | epoch 016 | loss 5.564 | nll_loss 4.161 | ppl 17.89 | wps 4544.2 | ups 0.66 | wpb 6882.8 | bsz 290.7 | num_updates 10368 | lr 0.000124226 | gnorm 1.073 | clip 0 | train_wall 938 | wall 12978
2020-10-11 08:31:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-11 08:31:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-11 08:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58068.2265625Mb; avail=134770.5703125Mb
2020-10-11 08:31:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006915
2020-10-11 08:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.054348
2020-10-11 08:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58072.38671875Mb; avail=134766.41015625Mb
2020-10-11 08:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001995
2020-10-11 08:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58072.38671875Mb; avail=134766.41015625Mb
2020-10-11 08:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.866630
2020-10-11 08:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.924361
2020-10-11 08:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58072.63671875Mb; avail=134765.37890625Mb
2020-10-11 08:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58076.26953125Mb; avail=134762.3515625Mb
2020-10-11 08:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027478
2020-10-11 08:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58079.78125Mb; avail=134758.234375Mb
2020-10-11 08:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001517
2020-10-11 08:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58082.11328125Mb; avail=134756.5078125Mb
2020-10-11 08:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.863769
2020-10-11 08:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.893992
2020-10-11 08:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58072.94140625Mb; avail=134766.08984375Mb
2020-10-11 08:31:50 | INFO | fairseq.trainer | begin training epoch 17
2020-10-11 08:32:37 | INFO | train_inner | epoch 017:     32 / 648 loss=5.539, nll_loss=4.133, ppl=17.54, wps=3760.3, ups=0.56, wpb=6769.9, bsz=296, num_updates=10400, lr=0.000124035, gnorm=1.086, clip=0, train_wall=143, wall=13027
2020-10-11 08:35:02 | INFO | train_inner | epoch 017:    132 / 648 loss=5.518, nll_loss=4.108, ppl=17.24, wps=4721.8, ups=0.69, wpb=6856.2, bsz=263.5, num_updates=10500, lr=0.000123443, gnorm=1.045, clip=0, train_wall=144, wall=13172
2020-10-11 08:37:29 | INFO | train_inner | epoch 017:    232 / 648 loss=5.503, nll_loss=4.09, ppl=17.03, wps=4751.8, ups=0.68, wpb=6955.8, bsz=274.3, num_updates=10600, lr=0.000122859, gnorm=1.072, clip=0, train_wall=145, wall=13318
2020-10-11 08:39:54 | INFO | train_inner | epoch 017:    332 / 648 loss=5.444, nll_loss=4.025, ppl=16.27, wps=4721.6, ups=0.69, wpb=6871.5, bsz=294, num_updates=10700, lr=0.000122284, gnorm=1.047, clip=0, train_wall=144, wall=13464
2020-10-11 08:42:20 | INFO | train_inner | epoch 017:    432 / 648 loss=5.452, nll_loss=4.031, ppl=16.35, wps=4734.5, ups=0.68, wpb=6918.5, bsz=298.1, num_updates=10800, lr=0.000121716, gnorm=1.082, clip=0, train_wall=145, wall=13610
2020-10-11 08:44:47 | INFO | train_inner | epoch 017:    532 / 648 loss=5.421, nll_loss=3.998, ppl=15.98, wps=4604.8, ups=0.68, wpb=6738.5, bsz=310.1, num_updates=10900, lr=0.000121157, gnorm=1.048, clip=0, train_wall=145, wall=13756
2020-10-11 08:46:24 | INFO | train_inner | epoch 017:    632 / 648 loss=5.461, nll_loss=4.042, ppl=16.48, wps=7020.1, ups=1.02, wpb=6862.8, bsz=294.8, num_updates=11000, lr=0.000120605, gnorm=1.069, clip=0, train_wall=97, wall=13854
2020-10-11 08:46:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 08:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57793.58203125Mb; avail=135044.953125Mb
2020-10-11 08:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001774
2020-10-11 08:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57793.58203125Mb; avail=135044.953125Mb
2020-10-11 08:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077417
2020-10-11 08:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57796.609375Mb; avail=135041.92578125Mb
2020-10-11 08:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063117
2020-10-11 08:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.143392
2020-10-11 08:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57803.83984375Mb; avail=135034.91796875Mb
2020-10-11 08:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57804.01953125Mb; avail=135034.73828125Mb
2020-10-11 08:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001250
2020-10-11 08:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57804.01953125Mb; avail=135034.73828125Mb
2020-10-11 08:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080710
2020-10-11 08:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57792.484375Mb; avail=135046.2734375Mb
2020-10-11 08:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063412
2020-10-11 08:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.146561
2020-10-11 08:46:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57792.56640625Mb; avail=135046.2109375Mb
2020-10-11 08:46:41 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 5.579 | nll_loss 4.065 | ppl 16.74 | wps 22002.8 | wpb 2274.6 | bsz 96.2 | num_updates 11016 | best_loss 5.579
2020-10-11 08:46:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 08:46:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 17 @ 11016 updates, score 5.579) (writing took 5.852076455950737 seconds)
2020-10-11 08:46:47 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-11 08:46:47 | INFO | train | epoch 017 | loss 5.465 | nll_loss 4.048 | ppl 16.54 | wps 4962.3 | ups 0.72 | wpb 6882.8 | bsz 290.7 | num_updates 11016 | lr 0.000120517 | gnorm 1.058 | clip 0 | train_wall 878 | wall 13877
2020-10-11 08:46:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-11 08:46:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-11 08:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57747.640625Mb; avail=135091.12890625Mb
2020-10-11 08:46:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008506
2020-10-11 08:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.065254
2020-10-11 08:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57760.6953125Mb; avail=135078.07421875Mb
2020-10-11 08:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002487
2020-10-11 08:46:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57738.80078125Mb; avail=135099.96875Mb
2020-10-11 08:46:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.879690
2020-10-11 08:46:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.949076
2020-10-11 08:46:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57744.81640625Mb; avail=135094.22265625Mb
2020-10-11 08:46:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57740.9765625Mb; avail=135098.0703125Mb
2020-10-11 08:46:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027017
2020-10-11 08:46:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57742.6875Mb; avail=135096.359375Mb
2020-10-11 08:46:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001533
2020-10-11 08:46:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57742.6875Mb; avail=135096.359375Mb
2020-10-11 08:46:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.866681
2020-10-11 08:46:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.896427
2020-10-11 08:46:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57743.30859375Mb; avail=135095.734375Mb
2020-10-11 08:46:49 | INFO | fairseq.trainer | begin training epoch 18
2020-10-11 08:47:48 | INFO | train_inner | epoch 018:     84 / 648 loss=5.378, nll_loss=3.948, ppl=15.43, wps=8401.7, ups=1.2, wpb=7003.3, bsz=303.6, num_updates=11100, lr=0.00012006, gnorm=1.039, clip=0, train_wall=69, wall=13938
2020-10-11 08:48:58 | INFO | train_inner | epoch 018:    184 / 648 loss=5.402, nll_loss=3.976, ppl=15.74, wps=9787.4, ups=1.43, wpb=6836.8, bsz=269.2, num_updates=11200, lr=0.000119523, gnorm=1.1, clip=0, train_wall=69, wall=14007
2020-10-11 08:50:08 | INFO | train_inner | epoch 018:    284 / 648 loss=5.407, nll_loss=3.98, ppl=15.78, wps=9798.4, ups=1.42, wpb=6904.5, bsz=280.6, num_updates=11300, lr=0.000118993, gnorm=1.059, clip=0, train_wall=69, wall=14078
2020-10-11 08:51:18 | INFO | train_inner | epoch 018:    384 / 648 loss=5.366, nll_loss=3.935, ppl=15.3, wps=9857.9, ups=1.42, wpb=6925, bsz=300.5, num_updates=11400, lr=0.00011847, gnorm=1.068, clip=0, train_wall=69, wall=14148
2020-10-11 08:52:28 | INFO | train_inner | epoch 018:    484 / 648 loss=5.356, nll_loss=3.923, ppl=15.17, wps=9630.8, ups=1.43, wpb=6743.5, bsz=301.3, num_updates=11500, lr=0.000117954, gnorm=1.066, clip=0, train_wall=69, wall=14218
2020-10-11 08:53:38 | INFO | train_inner | epoch 018:    584 / 648 loss=5.378, nll_loss=3.948, ppl=15.43, wps=10065.7, ups=1.44, wpb=6972.6, bsz=296.6, num_updates=11600, lr=0.000117444, gnorm=1.047, clip=0, train_wall=68, wall=14287
2020-10-11 08:54:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 08:54:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58463.3515625Mb; avail=134375.2265625Mb
2020-10-11 08:54:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002136
2020-10-11 08:54:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58463.3515625Mb; avail=134375.2265625Mb
2020-10-11 08:54:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.092005
2020-10-11 08:54:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58463.16796875Mb; avail=134375.41015625Mb
2020-10-11 08:54:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061668
2020-10-11 08:54:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.156941
2020-10-11 08:54:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58463.1796875Mb; avail=134375.39453125Mb
2020-10-11 08:54:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58463.1796875Mb; avail=134375.39453125Mb
2020-10-11 08:54:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001226
2020-10-11 08:54:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58463.1796875Mb; avail=134375.39453125Mb
2020-10-11 08:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078331
2020-10-11 08:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58464.890625Mb; avail=134373.68359375Mb
2020-10-11 08:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063677
2020-10-11 08:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.144412
2020-10-11 08:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58470.82421875Mb; avail=134367.75Mb
2020-10-11 08:54:28 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 5.537 | nll_loss 4.019 | ppl 16.21 | wps 21741.1 | wpb 2274.6 | bsz 96.2 | num_updates 11664 | best_loss 5.537
2020-10-11 08:54:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 08:55:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 18 @ 11664 updates, score 5.537) (writing took 37.60536302626133 seconds)
2020-10-11 08:55:05 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-11 08:55:05 | INFO | train | epoch 018 | loss 5.381 | nll_loss 3.951 | ppl 15.47 | wps 8951.3 | ups 1.3 | wpb 6882.8 | bsz 290.7 | num_updates 11664 | lr 0.000117121 | gnorm 1.066 | clip 0 | train_wall 445 | wall 14375
2020-10-11 08:55:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-11 08:55:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-11 08:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58388.97265625Mb; avail=134449.98046875Mb
2020-10-11 08:55:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004603
2020-10-11 08:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.037236
2020-10-11 08:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58381.52734375Mb; avail=134457.42578125Mb
2020-10-11 08:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001351
2020-10-11 08:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58381.52734375Mb; avail=134457.42578125Mb
2020-10-11 08:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.847461
2020-10-11 08:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.887392
2020-10-11 08:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58388.03125Mb; avail=134451.203125Mb
2020-10-11 08:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58384.09375Mb; avail=134455.140625Mb
2020-10-11 08:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026318
2020-10-11 08:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58384.09375Mb; avail=134455.140625Mb
2020-10-11 08:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001508
2020-10-11 08:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58384.09375Mb; avail=134455.140625Mb
2020-10-11 08:55:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.864289
2020-10-11 08:55:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.893197
2020-10-11 08:55:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58338.72265625Mb; avail=134500.765625Mb
2020-10-11 08:55:07 | INFO | fairseq.trainer | begin training epoch 19
2020-10-11 08:55:32 | INFO | train_inner | epoch 019:     36 / 648 loss=5.378, nll_loss=3.949, ppl=15.44, wps=5983.7, ups=0.88, wpb=6837.7, bsz=268.2, num_updates=11700, lr=0.000116941, gnorm=1.067, clip=0, train_wall=68, wall=14402
2020-10-11 08:56:43 | INFO | train_inner | epoch 019:    136 / 648 loss=5.267, nll_loss=3.822, ppl=14.15, wps=9665.3, ups=1.4, wpb=6907.8, bsz=306.2, num_updates=11800, lr=0.000116445, gnorm=1.042, clip=0, train_wall=70, wall=14473
2020-10-11 08:57:54 | INFO | train_inner | epoch 019:    236 / 648 loss=5.3, nll_loss=3.86, ppl=14.52, wps=9676, ups=1.41, wpb=6885.3, bsz=294.4, num_updates=11900, lr=0.000115954, gnorm=1.058, clip=0, train_wall=70, wall=14544
2020-10-11 08:59:04 | INFO | train_inner | epoch 019:    336 / 648 loss=5.305, nll_loss=3.865, ppl=14.57, wps=9814.8, ups=1.43, wpb=6865.8, bsz=283.8, num_updates=12000, lr=0.00011547, gnorm=1.047, clip=0, train_wall=69, wall=14614
2020-10-11 09:00:15 | INFO | train_inner | epoch 019:    436 / 648 loss=5.314, nll_loss=3.874, ppl=14.66, wps=9777.2, ups=1.41, wpb=6925.4, bsz=293.9, num_updates=12100, lr=0.000114992, gnorm=1.069, clip=0, train_wall=70, wall=14685
2020-10-11 09:01:26 | INFO | train_inner | epoch 019:    536 / 648 loss=5.322, nll_loss=3.883, ppl=14.75, wps=9751.3, ups=1.42, wpb=6874.9, bsz=286.7, num_updates=12200, lr=0.00011452, gnorm=1.061, clip=0, train_wall=69, wall=14756
2020-10-11 09:02:36 | INFO | train_inner | epoch 019:    636 / 648 loss=5.292, nll_loss=3.85, ppl=14.42, wps=9747.5, ups=1.42, wpb=6870.3, bsz=298, num_updates=12300, lr=0.000114053, gnorm=1.065, clip=0, train_wall=69, wall=14826
2020-10-11 09:02:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 09:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58403.2578125Mb; avail=134435.171875Mb
2020-10-11 09:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001689
2020-10-11 09:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58403.2578125Mb; avail=134435.171875Mb
2020-10-11 09:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082488
2020-10-11 09:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58402.6171875Mb; avail=134435.8125Mb
2020-10-11 09:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.066257
2020-10-11 09:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.151734
2020-10-11 09:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58409.71484375Mb; avail=134428.71484375Mb
2020-10-11 09:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58399.0078125Mb; avail=134439.421875Mb
2020-10-11 09:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001231
2020-10-11 09:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58399.0078125Mb; avail=134439.6640625Mb
2020-10-11 09:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079405
2020-10-11 09:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58394.33984375Mb; avail=134444.29296875Mb
2020-10-11 09:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062083
2020-10-11 09:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.143946
2020-10-11 09:02:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58394.33984375Mb; avail=134444.29296875Mb
2020-10-11 09:02:50 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 5.489 | nll_loss 3.954 | ppl 15.49 | wps 21774 | wpb 2274.6 | bsz 96.2 | num_updates 12312 | best_loss 5.489
2020-10-11 09:02:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 09:03:09 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 19 @ 12312 updates, score 5.489) (writing took 19.24401791766286 seconds)
2020-10-11 09:03:09 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-11 09:03:09 | INFO | train | epoch 019 | loss 5.303 | nll_loss 3.863 | ppl 14.55 | wps 9215.3 | ups 1.34 | wpb 6882.8 | bsz 290.7 | num_updates 12312 | lr 0.000113998 | gnorm 1.056 | clip 0 | train_wall 450 | wall 14859
2020-10-11 09:03:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-11 09:03:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-11 09:03:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58386.66015625Mb; avail=134451.84375Mb
2020-10-11 09:03:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006367
2020-10-11 09:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.055072
2020-10-11 09:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58393.42578125Mb; avail=134444.59375Mb
2020-10-11 09:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002156
2020-10-11 09:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58393.42578125Mb; avail=134444.59375Mb
2020-10-11 09:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.883724
2020-10-11 09:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.942553
2020-10-11 09:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58426.17578125Mb; avail=134412.84375Mb
2020-10-11 09:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58430.4140625Mb; avail=134408.60546875Mb
2020-10-11 09:03:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027774
2020-10-11 09:03:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58431.55859375Mb; avail=134407.4609375Mb
2020-10-11 09:03:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001359
2020-10-11 09:03:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58431.55859375Mb; avail=134407.4609375Mb
2020-10-11 09:03:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.865179
2020-10-11 09:03:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.895487
2020-10-11 09:03:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58446.84765625Mb; avail=134392.33984375Mb
2020-10-11 09:03:11 | INFO | fairseq.trainer | begin training epoch 20
2020-10-11 09:04:15 | INFO | train_inner | epoch 020:     88 / 648 loss=5.208, nll_loss=3.755, ppl=13.5, wps=7156.3, ups=1.02, wpb=7035.3, bsz=307.9, num_updates=12400, lr=0.000113592, gnorm=1.037, clip=0, train_wall=70, wall=14924
2020-10-11 09:05:26 | INFO | train_inner | epoch 020:    188 / 648 loss=5.255, nll_loss=3.808, ppl=14.01, wps=9623.5, ups=1.39, wpb=6906.8, bsz=277, num_updates=12500, lr=0.000113137, gnorm=1.048, clip=0, train_wall=71, wall=14996
2020-10-11 09:06:38 | INFO | train_inner | epoch 020:    288 / 648 loss=5.24, nll_loss=3.79, ppl=13.83, wps=9820.7, ups=1.39, wpb=7063.9, bsz=294.2, num_updates=12600, lr=0.000112687, gnorm=1.077, clip=0, train_wall=71, wall=15068
2020-10-11 09:07:48 | INFO | train_inner | epoch 020:    388 / 648 loss=5.242, nll_loss=3.794, ppl=13.87, wps=9704.2, ups=1.43, wpb=6780.1, bsz=285.2, num_updates=12700, lr=0.000112243, gnorm=1.082, clip=0, train_wall=69, wall=15138
2020-10-11 09:08:58 | INFO | train_inner | epoch 020:    488 / 648 loss=5.239, nll_loss=3.789, ppl=13.83, wps=9755.6, ups=1.44, wpb=6795.8, bsz=294.1, num_updates=12800, lr=0.000111803, gnorm=1.055, clip=0, train_wall=69, wall=15208
2020-10-11 09:10:33 | INFO | train_inner | epoch 020:    588 / 648 loss=5.217, nll_loss=3.765, ppl=13.59, wps=7108.5, ups=1.05, wpb=6799, bsz=292.8, num_updates=12900, lr=0.000111369, gnorm=1.075, clip=0, train_wall=94, wall=15303
2020-10-11 09:12:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 09:12:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59774.59765625Mb; avail=133063.359375Mb
2020-10-11 09:12:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002265
2020-10-11 09:12:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59776.4140625Mb; avail=133061.54296875Mb
2020-10-11 09:12:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.101229
2020-10-11 09:12:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59779.6328125Mb; avail=133058.3203125Mb
2020-10-11 09:12:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.065848
2020-10-11 09:12:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.170780
2020-10-11 09:12:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59779.6171875Mb; avail=133058.3359375Mb
2020-10-11 09:12:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59779.6171875Mb; avail=133058.3359375Mb
2020-10-11 09:12:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001530
2020-10-11 09:12:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59779.6171875Mb; avail=133058.3359375Mb
2020-10-11 09:12:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080617
2020-10-11 09:12:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59747.5546875Mb; avail=133089.96875Mb
2020-10-11 09:12:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062065
2020-10-11 09:12:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.145682
2020-10-11 09:12:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59736.44140625Mb; avail=133101.18359375Mb
2020-10-11 09:12:11 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 5.455 | nll_loss 3.914 | ppl 15.07 | wps 11246 | wpb 2274.6 | bsz 96.2 | num_updates 12960 | best_loss 5.455
2020-10-11 09:12:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 09:12:34 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 20 @ 12960 updates, score 5.455) (writing took 22.58052223548293 seconds)
2020-10-11 09:12:34 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-11 09:12:34 | INFO | train | epoch 020 | loss 5.236 | nll_loss 3.787 | ppl 13.8 | wps 7900.1 | ups 1.15 | wpb 6882.8 | bsz 290.7 | num_updates 12960 | lr 0.000111111 | gnorm 1.063 | clip 0 | train_wall 522 | wall 15424
2020-10-11 09:12:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-11 09:12:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-11 09:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59143.4765625Mb; avail=133694.52734375Mb
2020-10-11 09:12:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007002
2020-10-11 09:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.053061
2020-10-11 09:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59145.7265625Mb; avail=133692.25390625Mb
2020-10-11 09:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001818
2020-10-11 09:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59145.7265625Mb; avail=133692.25390625Mb
2020-10-11 09:12:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.877285
2020-10-11 09:12:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.933445
2020-10-11 09:12:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59168.7265625Mb; avail=133669.2578125Mb
2020-10-11 09:12:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59148.109375Mb; avail=133689.76953125Mb
2020-10-11 09:12:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027518
2020-10-11 09:12:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59148.109375Mb; avail=133689.76953125Mb
2020-10-11 09:12:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001462
2020-10-11 09:12:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59148.109375Mb; avail=133689.76953125Mb
2020-10-11 09:12:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.856454
2020-10-11 09:12:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.886588
2020-10-11 09:12:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59147.2109375Mb; avail=133690.77734375Mb
2020-10-11 09:12:36 | INFO | fairseq.trainer | begin training epoch 21
2020-10-11 09:13:35 | INFO | train_inner | epoch 021:     40 / 648 loss=5.213, nll_loss=3.76, ppl=13.55, wps=3677.1, ups=0.55, wpb=6675.1, bsz=297, num_updates=13000, lr=0.00011094, gnorm=1.053, clip=0, train_wall=145, wall=15485
2020-10-11 09:16:03 | INFO | train_inner | epoch 021:    140 / 648 loss=5.183, nll_loss=3.726, ppl=13.23, wps=4664.8, ups=0.68, wpb=6908.4, bsz=272.9, num_updates=13100, lr=0.000110516, gnorm=1.06, clip=0, train_wall=147, wall=15633
2020-10-11 09:18:29 | INFO | train_inner | epoch 021:    240 / 648 loss=5.162, nll_loss=3.701, ppl=13.01, wps=4701.6, ups=0.68, wpb=6874.6, bsz=296.5, num_updates=13200, lr=0.000110096, gnorm=1.043, clip=0, train_wall=145, wall=15779
2020-10-11 09:20:57 | INFO | train_inner | epoch 021:    340 / 648 loss=5.141, nll_loss=3.678, ppl=12.8, wps=4754.6, ups=0.68, wpb=7017, bsz=311, num_updates=13300, lr=0.000109682, gnorm=1.025, clip=0, train_wall=146, wall=15927
2020-10-11 09:23:22 | INFO | train_inner | epoch 021:    440 / 648 loss=5.219, nll_loss=3.767, ppl=13.62, wps=4628.4, ups=0.69, wpb=6705.2, bsz=254.8, num_updates=13400, lr=0.000109272, gnorm=1.073, clip=0, train_wall=144, wall=16072
2020-10-11 09:25:49 | INFO | train_inner | epoch 021:    540 / 648 loss=5.167, nll_loss=3.706, ppl=13.05, wps=4759.3, ups=0.68, wpb=7014.5, bsz=299.6, num_updates=13500, lr=0.000108866, gnorm=1.053, clip=0, train_wall=146, wall=16219
2020-10-11 09:28:16 | INFO | train_inner | epoch 021:    640 / 648 loss=5.187, nll_loss=3.73, ppl=13.27, wps=4733.5, ups=0.68, wpb=6972.6, bsz=294.1, num_updates=13600, lr=0.000108465, gnorm=1.06, clip=0, train_wall=146, wall=16366
2020-10-11 09:28:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 09:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58335.54296875Mb; avail=134503.20703125Mb
2020-10-11 09:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001963
2020-10-11 09:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58336.1484375Mb; avail=134502.6015625Mb
2020-10-11 09:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.086629
2020-10-11 09:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58329.78125Mb; avail=134508.96875Mb
2020-10-11 09:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062535
2020-10-11 09:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.152416
2020-10-11 09:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58329.78125Mb; avail=134508.96875Mb
2020-10-11 09:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58329.78125Mb; avail=134508.96875Mb
2020-10-11 09:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001236
2020-10-11 09:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58329.78125Mb; avail=134508.96875Mb
2020-10-11 09:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077840
2020-10-11 09:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58329.78125Mb; avail=134508.96875Mb
2020-10-11 09:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062959
2020-10-11 09:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.143090
2020-10-11 09:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58329.78125Mb; avail=134508.96875Mb
2020-10-11 09:28:38 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 5.428 | nll_loss 3.883 | ppl 14.75 | wps 11058.7 | wpb 2274.6 | bsz 96.2 | num_updates 13608 | best_loss 5.428
2020-10-11 09:28:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 09:28:44 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 21 @ 13608 updates, score 5.428) (writing took 5.550978552550077 seconds)
2020-10-11 09:28:44 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-11 09:28:44 | INFO | train | epoch 021 | loss 5.171 | nll_loss 3.712 | ppl 13.11 | wps 4599.9 | ups 0.67 | wpb 6882.8 | bsz 290.7 | num_updates 13608 | lr 0.000108433 | gnorm 1.052 | clip 0 | train_wall 943 | wall 16393
2020-10-11 09:28:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-11 09:28:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-11 09:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58274.48046875Mb; avail=134564.1015625Mb
2020-10-11 09:28:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004870
2020-10-11 09:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.038193
2020-10-11 09:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58276.296875Mb; avail=134562.28515625Mb
2020-10-11 09:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001581
2020-10-11 09:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58276.296875Mb; avail=134562.28515625Mb
2020-10-11 09:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.852067
2020-10-11 09:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.893003
2020-10-11 09:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58291.34765625Mb; avail=134547.45703125Mb
2020-10-11 09:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58295.9140625Mb; avail=134543.3828125Mb
2020-10-11 09:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.028280
2020-10-11 09:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58296.82421875Mb; avail=134541.98046875Mb
2020-10-11 09:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001652
2020-10-11 09:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58296.82421875Mb; avail=134541.98046875Mb
2020-10-11 09:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.842446
2020-10-11 09:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.873617
2020-10-11 09:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58282.33203125Mb; avail=134556.63671875Mb
2020-10-11 09:28:45 | INFO | fairseq.trainer | begin training epoch 22
2020-10-11 09:31:02 | INFO | train_inner | epoch 022:     92 / 648 loss=5.089, nll_loss=3.62, ppl=12.29, wps=4174.1, ups=0.6, wpb=6901.2, bsz=299.8, num_updates=13700, lr=0.000108069, gnorm=1.028, clip=0, train_wall=146, wall=16532
2020-10-11 09:33:27 | INFO | train_inner | epoch 022:    192 / 648 loss=5.14, nll_loss=3.677, ppl=12.79, wps=4635.7, ups=0.69, wpb=6749.4, bsz=269.4, num_updates=13800, lr=0.000107676, gnorm=1.073, clip=0, train_wall=144, wall=16677
2020-10-11 09:35:54 | INFO | train_inner | epoch 022:    292 / 648 loss=5.138, nll_loss=3.674, ppl=12.77, wps=4631.8, ups=0.68, wpb=6810.1, bsz=266.7, num_updates=13900, lr=0.000107288, gnorm=1.063, clip=0, train_wall=146, wall=16824
2020-10-11 09:38:20 | INFO | train_inner | epoch 022:    392 / 648 loss=5.156, nll_loss=3.695, ppl=12.95, wps=4632.9, ups=0.69, wpb=6759.3, bsz=275.7, num_updates=14000, lr=0.000106904, gnorm=1.08, clip=0, train_wall=145, wall=16970
2020-10-11 09:40:48 | INFO | train_inner | epoch 022:    492 / 648 loss=5.087, nll_loss=3.615, ppl=12.25, wps=4743.4, ups=0.68, wpb=6982.4, bsz=314.2, num_updates=14100, lr=0.000106525, gnorm=1.023, clip=0, train_wall=146, wall=17117
2020-10-11 09:43:12 | INFO | train_inner | epoch 022:    592 / 648 loss=5.094, nll_loss=3.625, ppl=12.33, wps=4767.6, ups=0.69, wpb=6887.5, bsz=305.4, num_updates=14200, lr=0.000106149, gnorm=1.043, clip=0, train_wall=143, wall=17262
2020-10-11 09:44:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 09:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58388.3515625Mb; avail=134450.08203125Mb
2020-10-11 09:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001656
2020-10-11 09:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58388.3515625Mb; avail=134450.08203125Mb
2020-10-11 09:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077321
2020-10-11 09:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58388.3203125Mb; avail=134450.32421875Mb
2020-10-11 09:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061534
2020-10-11 09:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141595
2020-10-11 09:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58387.828125Mb; avail=134450.81640625Mb
2020-10-11 09:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58387.94140625Mb; avail=134450.703125Mb
2020-10-11 09:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001151
2020-10-11 09:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58387.94140625Mb; avail=134450.703125Mb
2020-10-11 09:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076463
2020-10-11 09:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58387.94140625Mb; avail=134450.703125Mb
2020-10-11 09:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060058
2020-10-11 09:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138614
2020-10-11 09:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58387.94140625Mb; avail=134450.5Mb
2020-10-11 09:44:43 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 5.385 | nll_loss 3.829 | ppl 14.21 | wps 11217.1 | wpb 2274.6 | bsz 96.2 | num_updates 14256 | best_loss 5.385
2020-10-11 09:44:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 09:45:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 22 @ 14256 updates, score 5.385) (writing took 24.62037878111005 seconds)
2020-10-11 09:45:08 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-11 09:45:08 | INFO | train | epoch 022 | loss 5.114 | nll_loss 3.647 | ppl 12.53 | wps 4532.1 | ups 0.66 | wpb 6882.8 | bsz 290.7 | num_updates 14256 | lr 0.00010594 | gnorm 1.05 | clip 0 | train_wall 939 | wall 17378
2020-10-11 09:45:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-11 09:45:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-11 09:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58359.1953125Mb; avail=134479.3671875Mb
2020-10-11 09:45:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007140
2020-10-11 09:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.053447
2020-10-11 09:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58361.48828125Mb; avail=134477.07421875Mb
2020-10-11 09:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001778
2020-10-11 09:45:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58361.48828125Mb; avail=134477.07421875Mb
2020-10-11 09:45:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.850724
2020-10-11 09:45:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.907313
2020-10-11 09:45:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58366.9765625Mb; avail=134472.02734375Mb
2020-10-11 09:45:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58371.21484375Mb; avail=134467.7890625Mb
2020-10-11 09:45:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026366
2020-10-11 09:45:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58371.72265625Mb; avail=134467.28125Mb
2020-10-11 09:45:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001354
2020-10-11 09:45:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58368.76953125Mb; avail=134471.7109375Mb
2020-10-11 09:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.828519
2020-10-11 09:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.857384
2020-10-11 09:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58363.078125Mb; avail=134475.9375Mb
2020-10-11 09:45:10 | INFO | fairseq.trainer | begin training epoch 23
2020-10-11 09:46:15 | INFO | train_inner | epoch 023:     44 / 648 loss=5.067, nll_loss=3.593, ppl=12.07, wps=3852.8, ups=0.55, wpb=7057.9, bsz=306.7, num_updates=14300, lr=0.000105777, gnorm=1.033, clip=0, train_wall=145, wall=17445
2020-10-11 09:48:43 | INFO | train_inner | epoch 023:    144 / 648 loss=5.06, nll_loss=3.585, ppl=12, wps=4734.2, ups=0.68, wpb=7001.8, bsz=280.3, num_updates=14400, lr=0.000105409, gnorm=1.06, clip=0, train_wall=147, wall=17593
2020-10-11 09:51:07 | INFO | train_inner | epoch 023:    244 / 648 loss=5.079, nll_loss=3.607, ppl=12.19, wps=4716.3, ups=0.69, wpb=6798.4, bsz=278.5, num_updates=14500, lr=0.000105045, gnorm=1.055, clip=0, train_wall=143, wall=17737
2020-10-11 09:53:32 | INFO | train_inner | epoch 023:    344 / 648 loss=5.058, nll_loss=3.583, ppl=11.99, wps=4754.5, ups=0.69, wpb=6896.7, bsz=291.6, num_updates=14600, lr=0.000104685, gnorm=1.048, clip=0, train_wall=144, wall=17882
2020-10-11 09:55:56 | INFO | train_inner | epoch 023:    444 / 648 loss=5.064, nll_loss=3.59, ppl=12.04, wps=4759.6, ups=0.7, wpb=6829.1, bsz=286, num_updates=14700, lr=0.000104328, gnorm=1.057, clip=0, train_wall=142, wall=18026
2020-10-11 09:58:23 | INFO | train_inner | epoch 023:    544 / 648 loss=5.049, nll_loss=3.574, ppl=11.91, wps=4680.9, ups=0.68, wpb=6889.7, bsz=299.7, num_updates=14800, lr=0.000103975, gnorm=1.034, clip=0, train_wall=146, wall=18173
2020-10-11 10:00:49 | INFO | train_inner | epoch 023:    644 / 648 loss=5.067, nll_loss=3.594, ppl=12.07, wps=4656.3, ups=0.68, wpb=6820, bsz=304.3, num_updates=14900, lr=0.000103626, gnorm=1.088, clip=0, train_wall=145, wall=18319
2020-10-11 10:00:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 10:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59093.046875Mb; avail=133744.6953125Mb
2020-10-11 10:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001647
2020-10-11 10:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59093.046875Mb; avail=133744.6953125Mb
2020-10-11 10:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076580
2020-10-11 10:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59093.046875Mb; avail=133744.6953125Mb
2020-10-11 10:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061159
2020-10-11 10:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140398
2020-10-11 10:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59093.046875Mb; avail=133744.6953125Mb
2020-10-11 10:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59093.046875Mb; avail=133744.6953125Mb
2020-10-11 10:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001165
2020-10-11 10:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59093.046875Mb; avail=133744.6953125Mb
2020-10-11 10:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076177
2020-10-11 10:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59093.046875Mb; avail=133744.6953125Mb
2020-10-11 10:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060691
2020-10-11 10:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138996
2020-10-11 10:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59093.046875Mb; avail=133744.6953125Mb
2020-10-11 10:01:05 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 5.36 | nll_loss 3.805 | ppl 13.98 | wps 11385.5 | wpb 2274.6 | bsz 96.2 | num_updates 14904 | best_loss 5.36
2020-10-11 10:01:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 10:01:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 23 @ 14904 updates, score 5.36) (writing took 32.9921812787652 seconds)
2020-10-11 10:01:38 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-11 10:01:38 | INFO | train | epoch 023 | loss 5.062 | nll_loss 3.587 | ppl 12.02 | wps 4501.9 | ups 0.65 | wpb 6882.8 | bsz 290.7 | num_updates 14904 | lr 0.000103612 | gnorm 1.055 | clip 0 | train_wall 937 | wall 18368
2020-10-11 10:01:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-11 10:01:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-11 10:01:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59053.77734375Mb; avail=133784.0390625Mb
2020-10-11 10:01:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007574
2020-10-11 10:01:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.042966
2020-10-11 10:01:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59048.44140625Mb; avail=133789.265625Mb
2020-10-11 10:01:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001635
2020-10-11 10:01:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59048.44140625Mb; avail=133789.265625Mb
2020-10-11 10:01:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.838642
2020-10-11 10:01:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.884494
2020-10-11 10:01:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59041.46484375Mb; avail=133796.609375Mb
2020-10-11 10:01:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59041.74609375Mb; avail=133796.328125Mb
2020-10-11 10:01:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026282
2020-10-11 10:01:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59041.74609375Mb; avail=133796.328125Mb
2020-10-11 10:01:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001378
2020-10-11 10:01:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59041.74609375Mb; avail=133796.328125Mb
2020-10-11 10:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.859875
2020-10-11 10:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.888621
2020-10-11 10:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59041.875Mb; avail=133795.96484375Mb
2020-10-11 10:01:40 | INFO | fairseq.trainer | begin training epoch 24
2020-10-11 10:04:00 | INFO | train_inner | epoch 024:     96 / 648 loss=5.014, nll_loss=3.532, ppl=11.57, wps=3637.2, ups=0.52, wpb=6932.8, bsz=275.1, num_updates=15000, lr=0.00010328, gnorm=1.029, clip=0, train_wall=144, wall=18510
2020-10-11 10:06:26 | INFO | train_inner | epoch 024:    196 / 648 loss=4.981, nll_loss=3.495, ppl=11.28, wps=4753.9, ups=0.69, wpb=6920.4, bsz=305, num_updates=15100, lr=0.000102937, gnorm=1.033, clip=0, train_wall=144, wall=18655
2020-10-11 10:07:38 | INFO | train_inner | epoch 024:    296 / 648 loss=5.016, nll_loss=3.535, ppl=11.59, wps=9458.9, ups=1.37, wpb=6882.1, bsz=286.2, num_updates=15200, lr=0.000102598, gnorm=1.054, clip=0, train_wall=72, wall=18728
2020-10-11 10:08:48 | INFO | train_inner | epoch 024:    396 / 648 loss=5.048, nll_loss=3.572, ppl=11.89, wps=9799.2, ups=1.43, wpb=6837.2, bsz=277.8, num_updates=15300, lr=0.000102262, gnorm=1.074, clip=0, train_wall=69, wall=18798
2020-10-11 10:09:59 | INFO | train_inner | epoch 024:    496 / 648 loss=5.035, nll_loss=3.556, ppl=11.76, wps=9891.5, ups=1.42, wpb=6966.8, bsz=279.2, num_updates=15400, lr=0.000101929, gnorm=1.046, clip=0, train_wall=69, wall=18868
2020-10-11 10:11:09 | INFO | train_inner | epoch 024:    596 / 648 loss=4.996, nll_loss=3.511, ppl=11.4, wps=9714.8, ups=1.41, wpb=6874.5, bsz=311.2, num_updates=15500, lr=0.0001016, gnorm=1.041, clip=0, train_wall=70, wall=18939
2020-10-11 10:11:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 10:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57691.23828125Mb; avail=135147.515625Mb
2020-10-11 10:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002062
2020-10-11 10:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57691.23828125Mb; avail=135147.515625Mb
2020-10-11 10:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.089701
2020-10-11 10:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57691.23828125Mb; avail=135147.515625Mb
2020-10-11 10:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061637
2020-10-11 10:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.154517
2020-10-11 10:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57691.23828125Mb; avail=135147.515625Mb
2020-10-11 10:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57691.23828125Mb; avail=135147.515625Mb
2020-10-11 10:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001205
2020-10-11 10:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57691.23828125Mb; avail=135147.515625Mb
2020-10-11 10:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077437
2020-10-11 10:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57691.23828125Mb; avail=135147.515625Mb
2020-10-11 10:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061546
2020-10-11 10:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141184
2020-10-11 10:11:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57691.23828125Mb; avail=135147.515625Mb
2020-10-11 10:11:51 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.343 | nll_loss 3.787 | ppl 13.8 | wps 22188.2 | wpb 2274.6 | bsz 96.2 | num_updates 15552 | best_loss 5.343
2020-10-11 10:11:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 10:12:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 24 @ 15552 updates, score 5.343) (writing took 15.694996744394302 seconds)
2020-10-11 10:12:07 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-11 10:12:07 | INFO | train | epoch 024 | loss 5.012 | nll_loss 3.53 | ppl 11.56 | wps 7098.5 | ups 1.03 | wpb 6882.8 | bsz 290.7 | num_updates 15552 | lr 0.00010143 | gnorm 1.047 | clip 0 | train_wall 597 | wall 18997
2020-10-11 10:12:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-11 10:12:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-11 10:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57627.96484375Mb; avail=135210.79296875Mb
2020-10-11 10:12:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008031
2020-10-11 10:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.063083
2020-10-11 10:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57634.01953125Mb; avail=135203.99609375Mb
2020-10-11 10:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002060
2020-10-11 10:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57637.09375Mb; avail=135201.6640625Mb
2020-10-11 10:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.867829
2020-10-11 10:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.934572
2020-10-11 10:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57638.8046875Mb; avail=135200.22265625Mb
2020-10-11 10:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57639.82421875Mb; avail=135199.203125Mb
2020-10-11 10:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027356
2020-10-11 10:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57640.32421875Mb; avail=135198.703125Mb
2020-10-11 10:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001581
2020-10-11 10:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57640.32421875Mb; avail=135198.703125Mb
2020-10-11 10:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.848636
2020-10-11 10:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.878844
2020-10-11 10:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57631.6484375Mb; avail=135207.3671875Mb
2020-10-11 10:12:09 | INFO | fairseq.trainer | begin training epoch 25
2020-10-11 10:12:42 | INFO | train_inner | epoch 025:     48 / 648 loss=4.979, nll_loss=3.493, ppl=11.26, wps=7296.5, ups=1.08, wpb=6725.9, bsz=288.2, num_updates=15600, lr=0.000101274, gnorm=1.062, clip=0, train_wall=68, wall=19031
2020-10-11 10:13:55 | INFO | train_inner | epoch 025:    148 / 648 loss=4.9, nll_loss=3.402, ppl=10.57, wps=9696.4, ups=1.37, wpb=7087, bsz=330, num_updates=15700, lr=0.000100951, gnorm=1.007, clip=0, train_wall=72, wall=19104
2020-10-11 10:15:04 | INFO | train_inner | epoch 025:    248 / 648 loss=4.978, nll_loss=3.493, ppl=11.26, wps=9580.2, ups=1.44, wpb=6667.1, bsz=287.9, num_updates=15800, lr=0.000100631, gnorm=1.066, clip=0, train_wall=68, wall=19174
2020-10-11 10:16:16 | INFO | train_inner | epoch 025:    348 / 648 loss=4.963, nll_loss=3.474, ppl=11.11, wps=9726, ups=1.39, wpb=6988, bsz=286.2, num_updates=15900, lr=0.000100314, gnorm=1.036, clip=0, train_wall=71, wall=19246
2020-10-11 10:17:26 | INFO | train_inner | epoch 025:    448 / 648 loss=4.967, nll_loss=3.48, ppl=11.16, wps=9565.2, ups=1.43, wpb=6687.1, bsz=285.8, num_updates=16000, lr=0.0001, gnorm=1.07, clip=0, train_wall=69, wall=19316
2020-10-11 10:18:37 | INFO | train_inner | epoch 025:    548 / 648 loss=5.002, nll_loss=3.518, ppl=11.46, wps=9733, ups=1.41, wpb=6881.2, bsz=278.2, num_updates=16100, lr=9.9689e-05, gnorm=1.05, clip=0, train_wall=69, wall=19387
2020-10-11 10:19:48 | INFO | train_inner | epoch 025:    648 / 648 loss=4.987, nll_loss=3.501, ppl=11.32, wps=9900.3, ups=1.41, wpb=7025.9, bsz=288, num_updates=16200, lr=9.93808e-05, gnorm=1.04, clip=0, train_wall=70, wall=19457
2020-10-11 10:19:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 10:19:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57669.65625Mb; avail=135169.10546875Mb
2020-10-11 10:19:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002098
2020-10-11 10:19:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57669.65625Mb; avail=135169.10546875Mb
2020-10-11 10:19:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.096394
2020-10-11 10:19:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57669.1875Mb; avail=135169.57421875Mb
2020-10-11 10:19:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.065218
2020-10-11 10:19:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.165030
2020-10-11 10:19:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57670.32421875Mb; avail=135167.83203125Mb
2020-10-11 10:19:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57671.79296875Mb; avail=135166.96875Mb
2020-10-11 10:19:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001278
2020-10-11 10:19:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57671.79296875Mb; avail=135166.96875Mb
2020-10-11 10:19:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082824
2020-10-11 10:19:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57676.39453125Mb; avail=135162.3671875Mb
2020-10-11 10:19:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.066572
2020-10-11 10:19:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.151865
2020-10-11 10:19:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57669.2109375Mb; avail=135169.43359375Mb
2020-10-11 10:19:53 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.309 | nll_loss 3.746 | ppl 13.42 | wps 21729.9 | wpb 2274.6 | bsz 96.2 | num_updates 16200 | best_loss 5.309
2020-10-11 10:19:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 10:20:11 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 25 @ 16200 updates, score 5.309) (writing took 17.987937454134226 seconds)
2020-10-11 10:20:11 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-11 10:20:11 | INFO | train | epoch 025 | loss 4.966 | nll_loss 3.478 | ppl 11.14 | wps 9205.2 | ups 1.34 | wpb 6882.8 | bsz 290.7 | num_updates 16200 | lr 9.93808e-05 | gnorm 1.047 | clip 0 | train_wall 451 | wall 19481
2020-10-11 10:20:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-11 10:20:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-11 10:20:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57595.1796875Mb; avail=135243.6171875Mb
2020-10-11 10:20:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004529
2020-10-11 10:20:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.037601
2020-10-11 10:20:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57597.58984375Mb; avail=135241.20703125Mb
2020-10-11 10:20:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001546
2020-10-11 10:20:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57597.58984375Mb; avail=135241.20703125Mb
2020-10-11 10:20:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.941426
2020-10-11 10:20:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.981681
2020-10-11 10:20:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57597.3671875Mb; avail=135241.66015625Mb
2020-10-11 10:20:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57599.7890625Mb; avail=135238.6328125Mb
2020-10-11 10:20:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.030088
2020-10-11 10:20:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57601.0234375Mb; avail=135238.00390625Mb
2020-10-11 10:20:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002368
2020-10-11 10:20:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57601.0234375Mb; avail=135238.00390625Mb
2020-10-11 10:20:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.894550
2020-10-11 10:20:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.928566
2020-10-11 10:20:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57606.34765625Mb; avail=135232.6796875Mb
2020-10-11 10:20:13 | INFO | fairseq.trainer | begin training epoch 26
2020-10-11 10:21:24 | INFO | train_inner | epoch 026:    100 / 648 loss=4.897, nll_loss=3.399, ppl=10.55, wps=7223.6, ups=1.04, wpb=6945.3, bsz=284.6, num_updates=16300, lr=9.90755e-05, gnorm=1.024, clip=0, train_wall=69, wall=19554
2020-10-11 10:22:34 | INFO | train_inner | epoch 026:    200 / 648 loss=4.91, nll_loss=3.415, ppl=10.66, wps=9586.9, ups=1.42, wpb=6730.6, bsz=305.5, num_updates=16400, lr=9.8773e-05, gnorm=1.045, clip=0, train_wall=69, wall=19624
2020-10-11 10:23:44 | INFO | train_inner | epoch 026:    300 / 648 loss=4.943, nll_loss=3.451, ppl=10.94, wps=9660, ups=1.42, wpb=6807.6, bsz=273.4, num_updates=16500, lr=9.84732e-05, gnorm=1.065, clip=0, train_wall=69, wall=19694
2020-10-11 10:25:30 | INFO | train_inner | epoch 026:    400 / 648 loss=4.918, nll_loss=3.423, ppl=10.73, wps=6674.2, ups=0.95, wpb=7035.1, bsz=301, num_updates=16600, lr=9.81761e-05, gnorm=1.032, clip=0, train_wall=104, wall=19800
2020-10-11 10:27:56 | INFO | train_inner | epoch 026:    500 / 648 loss=4.926, nll_loss=3.433, ppl=10.8, wps=4646.1, ups=0.69, wpb=6776.3, bsz=284.3, num_updates=16700, lr=9.78818e-05, gnorm=1.046, clip=0, train_wall=145, wall=19946
2020-10-11 10:30:22 | INFO | train_inner | epoch 026:    600 / 648 loss=4.945, nll_loss=3.453, ppl=10.95, wps=4808.8, ups=0.68, wpb=7043.9, bsz=299.9, num_updates=16800, lr=9.759e-05, gnorm=1.046, clip=0, train_wall=145, wall=20092
2020-10-11 10:31:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 10:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58385.484375Mb; avail=134452.26171875Mb
2020-10-11 10:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002170
2020-10-11 10:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58385.484375Mb; avail=134452.26171875Mb
2020-10-11 10:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.090559
2020-10-11 10:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58385.484375Mb; avail=134452.26171875Mb
2020-10-11 10:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062593
2020-10-11 10:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.156455
2020-10-11 10:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58384.82421875Mb; avail=134452.921875Mb
2020-10-11 10:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58385.06640625Mb; avail=134452.6796875Mb
2020-10-11 10:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001191
2020-10-11 10:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58385.06640625Mb; avail=134452.6796875Mb
2020-10-11 10:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080849
2020-10-11 10:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58388.7890625Mb; avail=134448.96484375Mb
2020-10-11 10:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064850
2020-10-11 10:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.148085
2020-10-11 10:31:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58396.93359375Mb; avail=134440.8203125Mb
2020-10-11 10:31:42 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.278 | nll_loss 3.71 | ppl 13.09 | wps 11129.4 | wpb 2274.6 | bsz 96.2 | num_updates 16848 | best_loss 5.278
2020-10-11 10:31:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 10:32:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 26 @ 16848 updates, score 5.278) (writing took 20.205888751894236 seconds)
2020-10-11 10:32:02 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-11 10:32:02 | INFO | train | epoch 026 | loss 4.924 | nll_loss 3.43 | ppl 10.78 | wps 6272.3 | ups 0.91 | wpb 6882.8 | bsz 290.7 | num_updates 16848 | lr 9.74509e-05 | gnorm 1.043 | clip 0 | train_wall 670 | wall 20192
2020-10-11 10:32:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-11 10:32:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-11 10:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58339.90625Mb; avail=134497.4453125Mb
2020-10-11 10:32:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005614
2020-10-11 10:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.040584
2020-10-11 10:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58342.5078125Mb; avail=134495.0234375Mb
2020-10-11 10:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001466
2020-10-11 10:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58342.5078125Mb; avail=134495.0234375Mb
2020-10-11 10:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.878291
2020-10-11 10:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.921415
2020-10-11 10:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58344.69140625Mb; avail=134492.87109375Mb
2020-10-11 10:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58349.53515625Mb; avail=134488.02734375Mb
2020-10-11 10:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.028933
2020-10-11 10:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58351.95703125Mb; avail=134485.60546875Mb
2020-10-11 10:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001716
2020-10-11 10:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58351.95703125Mb; avail=134485.60546875Mb
2020-10-11 10:32:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.895016
2020-10-11 10:32:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.926927
2020-10-11 10:32:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58337.6015625Mb; avail=134500.390625Mb
2020-10-11 10:32:04 | INFO | fairseq.trainer | begin training epoch 27
2020-10-11 10:33:20 | INFO | train_inner | epoch 027:     52 / 648 loss=4.884, nll_loss=3.385, ppl=10.45, wps=3827.4, ups=0.56, wpb=6798.5, bsz=284.4, num_updates=16900, lr=9.73009e-05, gnorm=1.034, clip=0, train_wall=144, wall=20270
2020-10-11 10:35:45 | INFO | train_inner | epoch 027:    152 / 648 loss=4.882, nll_loss=3.381, ppl=10.42, wps=4827.8, ups=0.69, wpb=7009.9, bsz=278.6, num_updates=17000, lr=9.70143e-05, gnorm=1.032, clip=0, train_wall=144, wall=20415
2020-10-11 10:38:11 | INFO | train_inner | epoch 027:    252 / 648 loss=4.878, nll_loss=3.378, ppl=10.4, wps=4734.2, ups=0.69, wpb=6890.1, bsz=284.3, num_updates=17100, lr=9.67302e-05, gnorm=1.057, clip=0, train_wall=144, wall=20560
2020-10-11 10:40:36 | INFO | train_inner | epoch 027:    352 / 648 loss=4.897, nll_loss=3.4, ppl=10.56, wps=4746.2, ups=0.69, wpb=6906.6, bsz=277.7, num_updates=17200, lr=9.64486e-05, gnorm=1.06, clip=0, train_wall=144, wall=20706
2020-10-11 10:43:01 | INFO | train_inner | epoch 027:    452 / 648 loss=4.895, nll_loss=3.396, ppl=10.53, wps=4768.8, ups=0.69, wpb=6907, bsz=310.2, num_updates=17300, lr=9.61694e-05, gnorm=1.068, clip=0, train_wall=144, wall=20851
2020-10-11 10:45:26 | INFO | train_inner | epoch 027:    552 / 648 loss=4.901, nll_loss=3.402, ppl=10.57, wps=4782, ups=0.69, wpb=6956.3, bsz=294.7, num_updates=17400, lr=9.58927e-05, gnorm=1.03, clip=0, train_wall=144, wall=20996
2020-10-11 10:47:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 10:47:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66110.83203125Mb; avail=126723.984375Mb
2020-10-11 10:47:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001657
2020-10-11 10:47:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66110.83203125Mb; avail=126723.984375Mb
2020-10-11 10:47:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.084679
2020-10-11 10:47:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66117.01953125Mb; avail=126717.38671875Mb
2020-10-11 10:47:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.067186
2020-10-11 10:47:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.154801
2020-10-11 10:47:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66105.41796875Mb; avail=126729.203125Mb
2020-10-11 10:47:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66105.41796875Mb; avail=126729.203125Mb
2020-10-11 10:47:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001230
2020-10-11 10:47:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66105.41796875Mb; avail=126729.203125Mb
2020-10-11 10:47:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082163
2020-10-11 10:47:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66107.73046875Mb; avail=126726.8515625Mb
2020-10-11 10:47:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.065446
2020-10-11 10:47:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.149921
2020-10-11 10:47:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66115.99609375Mb; avail=126718.796875Mb
2020-10-11 10:47:54 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.278 | nll_loss 3.71 | ppl 13.09 | wps 11199.9 | wpb 2274.6 | bsz 96.2 | num_updates 17496 | best_loss 5.278
2020-10-11 10:47:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 10:48:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 27 @ 17496 updates, score 5.278) (writing took 17.465932939201593 seconds)
2020-10-11 10:48:12 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-11 10:48:12 | INFO | train | epoch 027 | loss 4.886 | nll_loss 3.387 | ppl 10.46 | wps 4600.5 | ups 0.67 | wpb 6882.8 | bsz 290.7 | num_updates 17496 | lr 9.56292e-05 | gnorm 1.049 | clip 0 | train_wall 932 | wall 21162
2020-10-11 10:48:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-11 10:48:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-11 10:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66683.4375Mb; avail=126150.4140625Mb
2020-10-11 10:48:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004424
2020-10-11 10:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.037728
2020-10-11 10:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66686.72265625Mb; avail=126147.12890625Mb
2020-10-11 10:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001601
2020-10-11 10:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66686.72265625Mb; avail=126147.12890625Mb
2020-10-11 10:48:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.890041
2020-10-11 10:48:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.930468
2020-10-11 10:48:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66690.99609375Mb; avail=126143.23046875Mb
2020-10-11 10:48:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66686.9375Mb; avail=126147.2890625Mb
2020-10-11 10:48:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026766
2020-10-11 10:48:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66686.9375Mb; avail=126147.2890625Mb
2020-10-11 10:48:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001368
2020-10-11 10:48:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66686.9375Mb; avail=126147.2890625Mb
2020-10-11 10:48:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.892888
2020-10-11 10:48:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.922144
2020-10-11 10:48:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66699.99609375Mb; avail=126134.02734375Mb
2020-10-11 10:48:14 | INFO | fairseq.trainer | begin training epoch 28
2020-10-11 10:48:20 | INFO | train_inner | epoch 028:      4 / 648 loss=4.885, nll_loss=3.385, ppl=10.45, wps=3864.7, ups=0.58, wpb=6693.5, bsz=298.3, num_updates=17500, lr=9.56183e-05, gnorm=1.054, clip=0, train_wall=142, wall=21169
2020-10-11 10:50:44 | INFO | train_inner | epoch 028:    104 / 648 loss=4.829, nll_loss=3.322, ppl=10, wps=4679, ups=0.69, wpb=6753.5, bsz=288.2, num_updates=17600, lr=9.53463e-05, gnorm=1.045, clip=0, train_wall=143, wall=21314
2020-10-11 10:53:09 | INFO | train_inner | epoch 028:    204 / 648 loss=4.849, nll_loss=3.344, ppl=10.15, wps=4728.6, ups=0.69, wpb=6846, bsz=290.9, num_updates=17700, lr=9.50765e-05, gnorm=1.044, clip=0, train_wall=144, wall=21459
2020-10-11 10:55:35 | INFO | train_inner | epoch 028:    304 / 648 loss=4.833, nll_loss=3.325, ppl=10.02, wps=4742.2, ups=0.68, wpb=6938, bsz=302.4, num_updates=17800, lr=9.48091e-05, gnorm=1.055, clip=0, train_wall=145, wall=21605
2020-10-11 10:58:01 | INFO | train_inner | epoch 028:    404 / 648 loss=4.849, nll_loss=3.344, ppl=10.15, wps=4772.6, ups=0.69, wpb=6954.2, bsz=291.7, num_updates=17900, lr=9.45439e-05, gnorm=1.028, clip=0, train_wall=144, wall=21751
2020-10-11 11:00:27 | INFO | train_inner | epoch 028:    504 / 648 loss=4.888, nll_loss=3.389, ppl=10.48, wps=4703.2, ups=0.68, wpb=6876.1, bsz=270.9, num_updates=18000, lr=9.42809e-05, gnorm=1.058, clip=0, train_wall=145, wall=21897
2020-10-11 11:02:54 | INFO | train_inner | epoch 028:    604 / 648 loss=4.826, nll_loss=3.32, ppl=9.98, wps=4672, ups=0.68, wpb=6848, bsz=315.5, num_updates=18100, lr=9.40201e-05, gnorm=1.051, clip=0, train_wall=145, wall=22043
2020-10-11 11:03:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 11:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66710.08203125Mb; avail=126123.61328125Mb
2020-10-11 11:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002101
2020-10-11 11:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66710.08203125Mb; avail=126123.61328125Mb
2020-10-11 11:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.092021
2020-10-11 11:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66711.125Mb; avail=126122.5703125Mb
2020-10-11 11:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.065048
2020-10-11 11:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.160530
2020-10-11 11:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66700.58203125Mb; avail=126133.11328125Mb
2020-10-11 11:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66700.58203125Mb; avail=126133.11328125Mb
2020-10-11 11:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001296
2020-10-11 11:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66700.58203125Mb; avail=126133.11328125Mb
2020-10-11 11:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079671
2020-10-11 11:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66700.58203125Mb; avail=126133.11328125Mb
2020-10-11 11:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062443
2020-10-11 11:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.144457
2020-10-11 11:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66700.58203125Mb; avail=126133.11328125Mb
2020-10-11 11:04:09 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.261 | nll_loss 3.69 | ppl 12.9 | wps 11114.7 | wpb 2274.6 | bsz 96.2 | num_updates 18144 | best_loss 5.261
2020-10-11 11:04:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 11:04:18 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 28 @ 18144 updates, score 5.261) (writing took 9.874525148421526 seconds)
2020-10-11 11:04:19 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-11 11:04:19 | INFO | train | epoch 028 | loss 4.848 | nll_loss 3.343 | ppl 10.15 | wps 4613.3 | ups 0.67 | wpb 6882.8 | bsz 290.7 | num_updates 18144 | lr 9.3906e-05 | gnorm 1.048 | clip 0 | train_wall 936 | wall 22128
2020-10-11 11:04:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-11 11:04:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-11 11:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66661.8984375Mb; avail=126171.84375Mb
2020-10-11 11:04:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004572
2020-10-11 11:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.039633
2020-10-11 11:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66660.77734375Mb; avail=126172.96484375Mb
2020-10-11 11:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001472
2020-10-11 11:04:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66660.77734375Mb; avail=126172.96484375Mb
2020-10-11 11:04:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.882832
2020-10-11 11:04:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.925357
2020-10-11 11:04:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66646.6171875Mb; avail=126187.3828125Mb
2020-10-11 11:04:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66646.6171875Mb; avail=126187.3828125Mb
2020-10-11 11:04:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027723
2020-10-11 11:04:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66646.6171875Mb; avail=126187.3828125Mb
2020-10-11 11:04:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001426
2020-10-11 11:04:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66646.6171875Mb; avail=126187.3828125Mb
2020-10-11 11:04:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.893461
2020-10-11 11:04:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.923910
2020-10-11 11:04:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66650.46875Mb; avail=126183.50390625Mb
2020-10-11 11:04:20 | INFO | fairseq.trainer | begin training epoch 29
2020-10-11 11:05:43 | INFO | train_inner | epoch 029:     56 / 648 loss=4.825, nll_loss=3.317, ppl=9.96, wps=4112.1, ups=0.59, wpb=6966.2, bsz=272.6, num_updates=18200, lr=9.37614e-05, gnorm=1.037, clip=0, train_wall=146, wall=22213
2020-10-11 11:08:07 | INFO | train_inner | epoch 029:    156 / 648 loss=4.799, nll_loss=3.288, ppl=9.77, wps=4769, ups=0.69, wpb=6872.4, bsz=290.9, num_updates=18300, lr=9.35049e-05, gnorm=1.049, clip=0, train_wall=143, wall=22357
2020-10-11 11:09:55 | INFO | train_inner | epoch 029:    256 / 648 loss=4.8, nll_loss=3.289, ppl=9.78, wps=6304.6, ups=0.93, wpb=6813.1, bsz=292.6, num_updates=18400, lr=9.32505e-05, gnorm=1.055, clip=0, train_wall=107, wall=22465
2020-10-11 11:11:04 | INFO | train_inner | epoch 029:    356 / 648 loss=4.785, nll_loss=3.271, ppl=9.66, wps=10068.9, ups=1.45, wpb=6947.6, bsz=314.2, num_updates=18500, lr=9.29981e-05, gnorm=1.042, clip=0, train_wall=68, wall=22534
2020-10-11 11:12:13 | INFO | train_inner | epoch 029:    456 / 648 loss=4.837, nll_loss=3.33, ppl=10.06, wps=10024.8, ups=1.44, wpb=6943.4, bsz=282.3, num_updates=18600, lr=9.27478e-05, gnorm=1.048, clip=0, train_wall=68, wall=22603
2020-10-11 11:13:23 | INFO | train_inner | epoch 029:    556 / 648 loss=4.852, nll_loss=3.347, ppl=10.18, wps=9706.4, ups=1.43, wpb=6772.9, bsz=274.2, num_updates=18700, lr=9.24995e-05, gnorm=1.053, clip=0, train_wall=69, wall=22673
2020-10-11 11:14:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 11:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66262.71484375Mb; avail=126571.80078125Mb
2020-10-11 11:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002140
2020-10-11 11:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66262.71484375Mb; avail=126571.80078125Mb
2020-10-11 11:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.093652
2020-10-11 11:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66262.68359375Mb; avail=126571.83203125Mb
2020-10-11 11:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063608
2020-10-11 11:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.160645
2020-10-11 11:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66262.68359375Mb; avail=126571.83203125Mb
2020-10-11 11:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66262.68359375Mb; avail=126571.83203125Mb
2020-10-11 11:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001222
2020-10-11 11:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66262.68359375Mb; avail=126571.83203125Mb
2020-10-11 11:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078822
2020-10-11 11:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66262.68359375Mb; avail=126571.83203125Mb
2020-10-11 11:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063571
2020-10-11 11:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.144634
2020-10-11 11:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66262.5Mb; avail=126572.21484375Mb
2020-10-11 11:14:32 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.238 | nll_loss 3.662 | ppl 12.66 | wps 21910.5 | wpb 2274.6 | bsz 96.2 | num_updates 18792 | best_loss 5.238
2020-10-11 11:14:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 11:14:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 29 @ 18792 updates, score 5.238) (writing took 21.322563726454973 seconds)
2020-10-11 11:14:53 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-11 11:14:53 | INFO | train | epoch 029 | loss 4.814 | nll_loss 3.304 | ppl 9.88 | wps 7024.6 | ups 1.02 | wpb 6882.8 | bsz 290.7 | num_updates 18792 | lr 9.22728e-05 | gnorm 1.047 | clip 0 | train_wall 598 | wall 22763
2020-10-11 11:14:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-11 11:14:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-11 11:14:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66207.37890625Mb; avail=126627.28515625Mb
2020-10-11 11:14:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006834
2020-10-11 11:14:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.054196
2020-10-11 11:14:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66199.33984375Mb; avail=126635.32421875Mb
2020-10-11 11:14:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001828
2020-10-11 11:14:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66199.33984375Mb; avail=126635.32421875Mb
2020-10-11 11:14:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.885425
2020-10-11 11:14:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.942903
2020-10-11 11:14:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66201.30859375Mb; avail=126633.6796875Mb
2020-10-11 11:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66201.546875Mb; avail=126633.44140625Mb
2020-10-11 11:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027253
2020-10-11 11:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66202.68359375Mb; avail=126632.3046875Mb
2020-10-11 11:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001499
2020-10-11 11:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66202.68359375Mb; avail=126632.3046875Mb
2020-10-11 11:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.886477
2020-10-11 11:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.916355
2020-10-11 11:14:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66211.1171875Mb; avail=126623.90234375Mb
2020-10-11 11:14:55 | INFO | fairseq.trainer | begin training epoch 30
2020-10-11 11:15:01 | INFO | train_inner | epoch 030:      8 / 648 loss=4.822, nll_loss=3.314, ppl=9.94, wps=7056.2, ups=1.02, wpb=6892.2, bsz=295.3, num_updates=18800, lr=9.22531e-05, gnorm=1.053, clip=0, train_wall=68, wall=22771
2020-10-11 11:16:11 | INFO | train_inner | epoch 030:    108 / 648 loss=4.75, nll_loss=3.232, ppl=9.4, wps=9739.2, ups=1.43, wpb=6815.2, bsz=293.5, num_updates=18900, lr=9.20087e-05, gnorm=1.034, clip=0, train_wall=69, wall=22841
2020-10-11 11:17:22 | INFO | train_inner | epoch 030:    208 / 648 loss=4.766, nll_loss=3.249, ppl=9.5, wps=9822, ups=1.4, wpb=7034.2, bsz=295.9, num_updates=19000, lr=9.17663e-05, gnorm=1.017, clip=0, train_wall=70, wall=22912
2020-10-11 11:18:33 | INFO | train_inner | epoch 030:    308 / 648 loss=4.779, nll_loss=3.264, ppl=9.61, wps=9749.6, ups=1.42, wpb=6854.3, bsz=295.5, num_updates=19100, lr=9.15258e-05, gnorm=1.054, clip=0, train_wall=69, wall=22983
2020-10-11 11:19:42 | INFO | train_inner | epoch 030:    408 / 648 loss=4.802, nll_loss=3.291, ppl=9.79, wps=9723, ups=1.44, wpb=6732.9, bsz=273.8, num_updates=19200, lr=9.12871e-05, gnorm=1.085, clip=0, train_wall=68, wall=23052
2020-10-11 11:20:53 | INFO | train_inner | epoch 030:    508 / 648 loss=4.791, nll_loss=3.278, ppl=9.7, wps=9712.9, ups=1.42, wpb=6851.6, bsz=294, num_updates=19300, lr=9.10503e-05, gnorm=1.04, clip=0, train_wall=69, wall=23122
2020-10-11 11:22:03 | INFO | train_inner | epoch 030:    608 / 648 loss=4.81, nll_loss=3.297, ppl=9.83, wps=9880.9, ups=1.42, wpb=6977.3, bsz=280.2, num_updates=19400, lr=9.08153e-05, gnorm=1.033, clip=0, train_wall=69, wall=23193
2020-10-11 11:22:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 11:22:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=65684.3828125Mb; avail=127150.4765625Mb
2020-10-11 11:22:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001632
2020-10-11 11:22:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65684.3828125Mb; avail=127150.4765625Mb
2020-10-11 11:22:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081365
2020-10-11 11:22:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65718.609375Mb; avail=127115.64453125Mb
2020-10-11 11:22:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.065859
2020-10-11 11:22:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.150056
2020-10-11 11:22:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65741.65625Mb; avail=127092.85546875Mb
2020-10-11 11:22:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=65747.10546875Mb; avail=127087.40625Mb
2020-10-11 11:22:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001261
2020-10-11 11:22:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65747.7109375Mb; avail=127086.80078125Mb
2020-10-11 11:22:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081866
2020-10-11 11:22:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65790.55078125Mb; avail=127043.35546875Mb
2020-10-11 11:22:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064362
2020-10-11 11:22:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.148756
2020-10-11 11:22:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65808.9296875Mb; avail=127024.9765625Mb
2020-10-11 11:22:37 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.231 | nll_loss 3.656 | ppl 12.6 | wps 21824.4 | wpb 2274.6 | bsz 96.2 | num_updates 19440 | best_loss 5.231
2020-10-11 11:22:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 11:22:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 30 @ 19440 updates, score 5.231) (writing took 9.916107777506113 seconds)
2020-10-11 11:22:47 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-11 11:22:47 | INFO | train | epoch 030 | loss 4.781 | nll_loss 3.266 | ppl 9.62 | wps 9420 | ups 1.37 | wpb 6882.8 | bsz 290.7 | num_updates 19440 | lr 9.07218e-05 | gnorm 1.043 | clip 0 | train_wall 448 | wall 23237
2020-10-11 11:22:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-11 11:22:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-11 11:22:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66259.1796875Mb; avail=126575.96484375Mb
2020-10-11 11:22:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005492
2020-10-11 11:22:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.044406
2020-10-11 11:22:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66261.328125Mb; avail=126573.81640625Mb
2020-10-11 11:22:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001823
2020-10-11 11:22:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66261.328125Mb; avail=126573.81640625Mb
2020-10-11 11:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.882619
2020-10-11 11:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.930260
2020-10-11 11:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66266.00390625Mb; avail=126569.04296875Mb
2020-10-11 11:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66264.8046875Mb; avail=126569.63671875Mb
2020-10-11 11:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.029485
2020-10-11 11:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66268.78125Mb; avail=126566.15234375Mb
2020-10-11 11:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001755
2020-10-11 11:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66269.83984375Mb; avail=126564.6015625Mb
2020-10-11 11:22:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.901666
2020-10-11 11:22:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.934235
2020-10-11 11:22:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66216.09375Mb; avail=126618.953125Mb
2020-10-11 11:22:49 | INFO | fairseq.trainer | begin training epoch 31
2020-10-11 11:23:31 | INFO | train_inner | epoch 031:     60 / 648 loss=4.741, nll_loss=3.222, ppl=9.33, wps=7875.2, ups=1.14, wpb=6887.5, bsz=297, num_updates=19500, lr=9.05822e-05, gnorm=1.026, clip=0, train_wall=69, wall=23280
2020-10-11 11:24:42 | INFO | train_inner | epoch 031:    160 / 648 loss=4.752, nll_loss=3.233, ppl=9.4, wps=9534.2, ups=1.4, wpb=6812.3, bsz=275.8, num_updates=19600, lr=9.03508e-05, gnorm=1.059, clip=0, train_wall=70, wall=23352
2020-10-11 11:25:52 | INFO | train_inner | epoch 031:    260 / 648 loss=4.759, nll_loss=3.241, ppl=9.46, wps=9616.9, ups=1.42, wpb=6755.3, bsz=281, num_updates=19700, lr=9.01212e-05, gnorm=1.065, clip=0, train_wall=69, wall=23422
2020-10-11 11:27:03 | INFO | train_inner | epoch 031:    360 / 648 loss=4.772, nll_loss=3.257, ppl=9.56, wps=9551.4, ups=1.41, wpb=6778.6, bsz=272.1, num_updates=19800, lr=8.98933e-05, gnorm=1.068, clip=0, train_wall=70, wall=23493
2020-10-11 11:28:14 | INFO | train_inner | epoch 031:    460 / 648 loss=4.775, nll_loss=3.259, ppl=9.57, wps=9852, ups=1.41, wpb=6971.5, bsz=279.3, num_updates=19900, lr=8.96672e-05, gnorm=1.05, clip=0, train_wall=70, wall=23564
2020-10-11 11:29:25 | INFO | train_inner | epoch 031:    560 / 648 loss=4.718, nll_loss=3.195, ppl=9.15, wps=9951.5, ups=1.41, wpb=7050.1, bsz=322.5, num_updates=20000, lr=8.94427e-05, gnorm=1.029, clip=0, train_wall=70, wall=23635
2020-10-11 11:30:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 11:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66869.84765625Mb; avail=125964.89453125Mb
2020-10-11 11:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002060
2020-10-11 11:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66869.84765625Mb; avail=125964.89453125Mb
2020-10-11 11:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.088794
2020-10-11 11:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66876.90625Mb; avail=125958.0546875Mb
2020-10-11 11:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063913
2020-10-11 11:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.156218
2020-10-11 11:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66866.046875Mb; avail=125968.9140625Mb
2020-10-11 11:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66866.046875Mb; avail=125968.9140625Mb
2020-10-11 11:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001486
2020-10-11 11:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66866.046875Mb; avail=125968.9140625Mb
2020-10-11 11:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079009
2020-10-11 11:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66870.6484375Mb; avail=125964.09765625Mb
2020-10-11 11:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064848
2020-10-11 11:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.146671
2020-10-11 11:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66874.26171875Mb; avail=125960.484375Mb
2020-10-11 11:31:09 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 5.193 | nll_loss 3.614 | ppl 12.24 | wps 10883.6 | wpb 2274.6 | bsz 96.2 | num_updates 20088 | best_loss 5.193
2020-10-11 11:31:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 11:31:19 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 31 @ 20088 updates, score 5.193) (writing took 10.623757414519787 seconds)
2020-10-11 11:31:19 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-11 11:31:19 | INFO | train | epoch 031 | loss 4.751 | nll_loss 3.232 | ppl 9.39 | wps 8706.7 | ups 1.27 | wpb 6882.8 | bsz 290.7 | num_updates 20088 | lr 8.92466e-05 | gnorm 1.049 | clip 0 | train_wall 481 | wall 23749
2020-10-11 11:31:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-11 11:31:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-11 11:31:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66810.015625Mb; avail=126024.75390625Mb
2020-10-11 11:31:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.010141
2020-10-11 11:31:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.067744
2020-10-11 11:31:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66794.265625Mb; avail=126040.50390625Mb
2020-10-11 11:31:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001970
2020-10-11 11:31:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66794.265625Mb; avail=126040.50390625Mb
2020-10-11 11:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.897474
2020-10-11 11:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.968935
2020-10-11 11:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66790.8828125Mb; avail=126044.5234375Mb
2020-10-11 11:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66790.50390625Mb; avail=126044.5234375Mb
2020-10-11 11:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026705
2020-10-11 11:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66790.50390625Mb; avail=126044.5234375Mb
2020-10-11 11:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001337
2020-10-11 11:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66790.50390625Mb; avail=126044.5234375Mb
2020-10-11 11:31:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.876346
2020-10-11 11:31:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.905773
2020-10-11 11:31:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66790.171875Mb; avail=126044.6015625Mb
2020-10-11 11:31:21 | INFO | fairseq.trainer | begin training epoch 32
2020-10-11 11:31:39 | INFO | train_inner | epoch 032:     12 / 648 loss=4.731, nll_loss=3.21, ppl=9.25, wps=5241.2, ups=0.75, wpb=7025.8, bsz=322.6, num_updates=20100, lr=8.92199e-05, gnorm=1.036, clip=0, train_wall=109, wall=23769
2020-10-11 11:34:07 | INFO | train_inner | epoch 032:    112 / 648 loss=4.699, nll_loss=3.173, ppl=9.02, wps=4637.9, ups=0.68, wpb=6844.8, bsz=281.8, num_updates=20200, lr=8.89988e-05, gnorm=1.041, clip=0, train_wall=146, wall=23916
2020-10-11 11:36:33 | INFO | train_inner | epoch 032:    212 / 648 loss=4.696, nll_loss=3.169, ppl=8.99, wps=4752.6, ups=0.68, wpb=6940.4, bsz=312.4, num_updates=20300, lr=8.87794e-05, gnorm=1.048, clip=0, train_wall=145, wall=24062
2020-10-11 11:38:59 | INFO | train_inner | epoch 032:    312 / 648 loss=4.766, nll_loss=3.248, ppl=9.5, wps=4727.5, ups=0.68, wpb=6945.2, bsz=256, num_updates=20400, lr=8.85615e-05, gnorm=1.047, clip=0, train_wall=146, wall=24209
2020-10-11 11:41:25 | INFO | train_inner | epoch 032:    412 / 648 loss=4.717, nll_loss=3.193, ppl=9.15, wps=4756.3, ups=0.69, wpb=6905.1, bsz=283.4, num_updates=20500, lr=8.83452e-05, gnorm=1.032, clip=0, train_wall=144, wall=24354
2020-10-11 11:43:49 | INFO | train_inner | epoch 032:    512 / 648 loss=4.718, nll_loss=3.194, ppl=9.15, wps=4700.7, ups=0.69, wpb=6779.9, bsz=313.3, num_updates=20600, lr=8.81305e-05, gnorm=1.078, clip=0, train_wall=143, wall=24499
2020-10-11 11:46:14 | INFO | train_inner | epoch 032:    612 / 648 loss=4.739, nll_loss=3.218, ppl=9.3, wps=4735.8, ups=0.69, wpb=6890.8, bsz=285.6, num_updates=20700, lr=8.79174e-05, gnorm=1.03, clip=0, train_wall=144, wall=24644
2020-10-11 11:47:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 11:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=77227.9609375Mb; avail=115605.5546875Mb
2020-10-11 11:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001797
2020-10-11 11:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=77228.56640625Mb; avail=115604.34375Mb
2020-10-11 11:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080118
2020-10-11 11:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=77265.4453125Mb; avail=115567.46484375Mb
2020-10-11 11:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064640
2020-10-11 11:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.147881
2020-10-11 11:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=77296.82421875Mb; avail=115536.69140625Mb
2020-10-11 11:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=77301.65234375Mb; avail=115531.2578125Mb
2020-10-11 11:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001271
2020-10-11 11:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=77302.86328125Mb; avail=115530.65234375Mb
2020-10-11 11:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083087
2020-10-11 11:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=77337.13671875Mb; avail=115495.7734375Mb
2020-10-11 11:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064712
2020-10-11 11:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.150349
2020-10-11 11:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=77357.63671875Mb; avail=115476.08984375Mb
2020-10-11 11:47:15 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.2 | nll_loss 3.617 | ppl 12.27 | wps 11382.5 | wpb 2274.6 | bsz 96.2 | num_updates 20736 | best_loss 5.193
2020-10-11 11:47:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 11:47:35 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_last.pt (epoch 32 @ 20736 updates, score 5.2) (writing took 20.590034291148186 seconds)
2020-10-11 11:47:35 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-11 11:47:35 | INFO | train | epoch 032 | loss 4.72 | nll_loss 3.197 | ppl 9.17 | wps 4568.9 | ups 0.66 | wpb 6882.8 | bsz 290.7 | num_updates 20736 | lr 8.7841e-05 | gnorm 1.047 | clip 0 | train_wall 935 | wall 24725
2020-10-11 11:47:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-11 11:47:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-11 11:47:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=73693.6875Mb; avail=119140.0625Mb
2020-10-11 11:47:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004597
2020-10-11 11:47:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.041413
2020-10-11 11:47:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=73695.765625Mb; avail=119137.984375Mb
2020-10-11 11:47:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001823
2020-10-11 11:47:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=73695.765625Mb; avail=119137.984375Mb
2020-10-11 11:47:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.948030
2020-10-11 11:47:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.992521
2020-10-11 11:47:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=73712.203125Mb; avail=119121.66015625Mb
2020-10-11 11:47:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=73713.96484375Mb; avail=119119.8984375Mb
2020-10-11 11:47:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.029774
2020-10-11 11:47:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=73713.47265625Mb; avail=119120.390625Mb
2020-10-11 11:47:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002086
2020-10-11 11:47:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=73713.47265625Mb; avail=119120.390625Mb
2020-10-11 11:47:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.906717
2020-10-11 11:47:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.940244
2020-10-11 11:47:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=73725.9140625Mb; avail=119108.046875Mb
2020-10-11 11:47:37 | INFO | fairseq.trainer | begin training epoch 33
2020-10-11 11:49:11 | INFO | train_inner | epoch 033:     64 / 648 loss=4.69, nll_loss=3.162, ppl=8.95, wps=3877.7, ups=0.57, wpb=6841, bsz=279.4, num_updates=20800, lr=8.77058e-05, gnorm=1.056, clip=0, train_wall=142, wall=24821
2020-10-11 11:51:36 | INFO | train_inner | epoch 033:    164 / 648 loss=4.661, nll_loss=3.129, ppl=8.75, wps=4754.9, ups=0.69, wpb=6900.5, bsz=318.2, num_updates=20900, lr=8.74957e-05, gnorm=1.041, clip=0, train_wall=144, wall=24966
2020-10-11 11:54:00 | INFO | train_inner | epoch 033:    264 / 648 loss=4.668, nll_loss=3.138, ppl=8.8, wps=4857.6, ups=0.69, wpb=7020.8, bsz=297.6, num_updates=21000, lr=8.72872e-05, gnorm=1.048, clip=0, train_wall=143, wall=25110
2020-10-11 11:56:24 | INFO | train_inner | epoch 033:    364 / 648 loss=4.691, nll_loss=3.163, ppl=8.96, wps=4779.1, ups=0.7, wpb=6862.2, bsz=279, num_updates=21100, lr=8.70801e-05, gnorm=1.062, clip=0, train_wall=142, wall=25254
2020-10-11 11:58:47 | INFO | train_inner | epoch 033:    464 / 648 loss=4.692, nll_loss=3.165, ppl=8.97, wps=4724.1, ups=0.7, wpb=6744.5, bsz=307.3, num_updates=21200, lr=8.68744e-05, gnorm=1.049, clip=0, train_wall=142, wall=25397
2020-10-11 12:01:13 | INFO | train_inner | epoch 033:    564 / 648 loss=4.711, nll_loss=3.187, ppl=9.11, wps=4696.4, ups=0.69, wpb=6854.6, bsz=289.3, num_updates=21300, lr=8.66703e-05, gnorm=1.059, clip=0, train_wall=145, wall=25543
2020-10-11 12:03:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 12:03:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67622.62109375Mb; avail=125211.15625Mb
2020-10-11 12:03:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001696
2020-10-11 12:03:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67622.62109375Mb; avail=125211.15625Mb
2020-10-11 12:03:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080614
2020-10-11 12:03:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67622.62109375Mb; avail=125211.15625Mb
2020-10-11 12:03:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062359
2020-10-11 12:03:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.145734
2020-10-11 12:03:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67622.62109375Mb; avail=125211.15625Mb
2020-10-11 12:03:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67622.62109375Mb; avail=125211.15625Mb
2020-10-11 12:03:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001232
2020-10-11 12:03:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67622.62109375Mb; avail=125211.15625Mb
2020-10-11 12:03:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080823
2020-10-11 12:03:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67625.19140625Mb; avail=125209.5703125Mb
2020-10-11 12:03:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.126540
2020-10-11 12:03:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.209983
2020-10-11 12:03:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67643.43359375Mb; avail=125190.33984375Mb
2020-10-11 12:03:26 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 5.201 | nll_loss 3.618 | ppl 12.28 | wps 11143.3 | wpb 2274.6 | bsz 96.2 | num_updates 21384 | best_loss 5.193
2020-10-11 12:03:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 12:03:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_last.pt (epoch 33 @ 21384 updates, score 5.201) (writing took 18.89001752436161 seconds)
2020-10-11 12:03:45 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-11 12:03:45 | INFO | train | epoch 033 | loss 4.692 | nll_loss 3.165 | ppl 8.97 | wps 4600.3 | ups 0.67 | wpb 6882.8 | bsz 290.7 | num_updates 21384 | lr 8.64999e-05 | gnorm 1.052 | clip 0 | train_wall 930 | wall 25695
2020-10-11 12:03:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-11 12:03:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-11 12:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67628.453125Mb; avail=125205.26953125Mb
2020-10-11 12:03:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007545
2020-10-11 12:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.062345
2020-10-11 12:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67631.07421875Mb; avail=125202.65625Mb
2020-10-11 12:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002191
2020-10-11 12:03:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67631.07421875Mb; avail=125202.65625Mb
2020-10-11 12:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.872545
2020-10-11 12:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.938564
2020-10-11 12:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67632.81640625Mb; avail=125201.19140625Mb
2020-10-11 12:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67634.84375Mb; avail=125199.1640625Mb
2020-10-11 12:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027653
2020-10-11 12:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67635.34375Mb; avail=125198.6640625Mb
2020-10-11 12:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001542
2020-10-11 12:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67635.34375Mb; avail=125198.6640625Mb
2020-10-11 12:03:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.867575
2020-10-11 12:03:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.898049
2020-10-11 12:03:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67628.921875Mb; avail=125205.05859375Mb
2020-10-11 12:03:47 | INFO | fairseq.trainer | begin training epoch 34
2020-10-11 12:04:10 | INFO | train_inner | epoch 034:     16 / 648 loss=4.72, nll_loss=3.196, ppl=9.17, wps=3914.8, ups=0.56, wpb=6955.1, bsz=276.1, num_updates=21400, lr=8.64675e-05, gnorm=1.043, clip=0, train_wall=145, wall=25720
2020-10-11 12:06:36 | INFO | train_inner | epoch 034:    116 / 648 loss=4.628, nll_loss=3.092, ppl=8.53, wps=4708.7, ups=0.69, wpb=6842.6, bsz=292.1, num_updates=21500, lr=8.62662e-05, gnorm=1.035, clip=0, train_wall=144, wall=25866
2020-10-11 12:08:59 | INFO | train_inner | epoch 034:    216 / 648 loss=4.661, nll_loss=3.129, ppl=8.75, wps=4728, ups=0.7, wpb=6777.1, bsz=283.3, num_updates=21600, lr=8.60663e-05, gnorm=1.044, clip=0, train_wall=142, wall=26009
2020-10-11 12:11:24 | INFO | train_inner | epoch 034:    316 / 648 loss=4.681, nll_loss=3.153, ppl=8.89, wps=4675.1, ups=0.69, wpb=6764.6, bsz=291.7, num_updates=21700, lr=8.58678e-05, gnorm=1.071, clip=0, train_wall=144, wall=26154
2020-10-11 12:13:50 | INFO | train_inner | epoch 034:    416 / 648 loss=4.679, nll_loss=3.148, ppl=8.87, wps=4757.6, ups=0.68, wpb=6976.8, bsz=289.5, num_updates=21800, lr=8.56706e-05, gnorm=1.043, clip=0, train_wall=145, wall=26300
2020-10-11 12:16:16 | INFO | train_inner | epoch 034:    516 / 648 loss=4.643, nll_loss=3.109, ppl=8.63, wps=4744.4, ups=0.69, wpb=6902.8, bsz=311.9, num_updates=21900, lr=8.54748e-05, gnorm=1.036, clip=0, train_wall=144, wall=26446
2020-10-11 12:18:43 | INFO | train_inner | epoch 034:    616 / 648 loss=4.684, nll_loss=3.154, ppl=8.9, wps=4768.7, ups=0.68, wpb=7008.7, bsz=284.6, num_updates=22000, lr=8.52803e-05, gnorm=1.044, clip=0, train_wall=146, wall=26593
2020-10-11 12:19:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 12:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67937.78125Mb; avail=124896.18359375Mb
2020-10-11 12:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001736
2020-10-11 12:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67937.78125Mb; avail=124896.18359375Mb
2020-10-11 12:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083970
2020-10-11 12:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67930.07421875Mb; avail=124903.67578125Mb
2020-10-11 12:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.067933
2020-10-11 12:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.155068
2020-10-11 12:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67930.078125Mb; avail=124903.87890625Mb
2020-10-11 12:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67930.68359375Mb; avail=124903.2734375Mb
2020-10-11 12:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001544
2020-10-11 12:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67930.68359375Mb; avail=124903.2734375Mb
2020-10-11 12:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.084273
2020-10-11 12:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67935.9375Mb; avail=124898.01953125Mb
2020-10-11 12:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.068722
2020-10-11 12:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.155912
2020-10-11 12:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67939.875Mb; avail=124893.87109375Mb
2020-10-11 12:19:40 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 5.179 | nll_loss 3.594 | ppl 12.07 | wps 11146.2 | wpb 2274.6 | bsz 96.2 | num_updates 22032 | best_loss 5.179
2020-10-11 12:19:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 12:20:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 34 @ 22032 updates, score 5.179) (writing took 26.713868521153927 seconds)
2020-10-11 12:20:07 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-11 12:20:07 | INFO | train | epoch 034 | loss 4.665 | nll_loss 3.134 | ppl 8.78 | wps 4541 | ups 0.66 | wpb 6882.8 | bsz 290.7 | num_updates 22032 | lr 8.52183e-05 | gnorm 1.046 | clip 0 | train_wall 935 | wall 26677
2020-10-11 12:20:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-11 12:20:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-11 12:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=69582.17578125Mb; avail=123251.55078125Mb
2020-10-11 12:20:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.010912
2020-10-11 12:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.068923
2020-10-11 12:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=69584.83984375Mb; avail=123249.03125Mb
2020-10-11 12:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002428
2020-10-11 12:20:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=69585.8515625Mb; avail=123247.875Mb
2020-10-11 12:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.946709
2020-10-11 12:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.019749
2020-10-11 12:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=69732.54296875Mb; avail=123101.7890625Mb
2020-10-11 12:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=69738.75Mb; avail=123095.58203125Mb
2020-10-11 12:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.029845
2020-10-11 12:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=69744.05859375Mb; avail=123090.27734375Mb
2020-10-11 12:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001858
2020-10-11 12:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=69744.05859375Mb; avail=123090.27734375Mb
2020-10-11 12:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.927363
2020-10-11 12:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.960744
2020-10-11 12:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=69888.6171875Mb; avail=122945.36328125Mb
2020-10-11 12:20:09 | INFO | fairseq.trainer | begin training epoch 35
2020-10-11 12:21:36 | INFO | train_inner | epoch 035:     68 / 648 loss=4.657, nll_loss=3.126, ppl=8.73, wps=3863, ups=0.58, wpb=6684.1, bsz=268.7, num_updates=22100, lr=8.50871e-05, gnorm=1.068, clip=0, train_wall=132, wall=26766
2020-10-11 12:23:46 | INFO | train_inner | epoch 035:    168 / 648 loss=4.629, nll_loss=3.092, ppl=8.53, wps=5364.4, ups=0.77, wpb=6951.4, bsz=284.2, num_updates=22200, lr=8.48953e-05, gnorm=1.041, clip=0, train_wall=128, wall=26895
2020-10-11 12:24:57 | INFO | train_inner | epoch 035:    268 / 648 loss=4.604, nll_loss=3.064, ppl=8.36, wps=9572.2, ups=1.39, wpb=6862.6, bsz=321.1, num_updates=22300, lr=8.47047e-05, gnorm=1.05, clip=0, train_wall=70, wall=26967
2020-10-11 12:26:09 | INFO | train_inner | epoch 035:    368 / 648 loss=4.656, nll_loss=3.123, ppl=8.71, wps=9660.5, ups=1.4, wpb=6902.5, bsz=289.2, num_updates=22400, lr=8.45154e-05, gnorm=1.047, clip=0, train_wall=70, wall=27039
2020-10-11 12:27:20 | INFO | train_inner | epoch 035:    468 / 648 loss=4.672, nll_loss=3.141, ppl=8.82, wps=9626.8, ups=1.41, wpb=6829.6, bsz=272.1, num_updates=22500, lr=8.43274e-05, gnorm=1.06, clip=0, train_wall=69, wall=27109
2020-10-11 12:28:31 | INFO | train_inner | epoch 035:    568 / 648 loss=4.652, nll_loss=3.118, ppl=8.68, wps=9826.6, ups=1.4, wpb=7012.2, bsz=288.3, num_updates=22600, lr=8.41406e-05, gnorm=1.041, clip=0, train_wall=70, wall=27181
2020-10-11 12:29:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 12:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=78273.234375Mb; avail=114561.31640625Mb
2020-10-11 12:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002367
2020-10-11 12:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=78273.234375Mb; avail=114561.31640625Mb
2020-10-11 12:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.161191
2020-10-11 12:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=78316.65234375Mb; avail=114518.4375Mb
2020-10-11 12:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.101184
2020-10-11 12:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.266582
2020-10-11 12:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=78334.265625Mb; avail=114500.7109375Mb
2020-10-11 12:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=78341.41015625Mb; avail=114493.56640625Mb
2020-10-11 12:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001472
2020-10-11 12:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=78341.41015625Mb; avail=114493.56640625Mb
2020-10-11 12:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.085430
2020-10-11 12:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=78264.28125Mb; avail=114570.6953125Mb
2020-10-11 12:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.067493
2020-10-11 12:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.155884
2020-10-11 12:29:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=78292.59765625Mb; avail=114542.37890625Mb
2020-10-11 12:29:34 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 5.175 | nll_loss 3.586 | ppl 12.01 | wps 21298.3 | wpb 2274.6 | bsz 96.2 | num_updates 22680 | best_loss 5.175
2020-10-11 12:29:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 12:29:41 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 35 @ 22680 updates, score 5.175) (writing took 6.89574521780014 seconds)
2020-10-11 12:29:41 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-11 12:29:41 | INFO | train | epoch 035 | loss 4.64 | nll_loss 3.105 | ppl 8.6 | wps 7776.4 | ups 1.13 | wpb 6882.8 | bsz 290.7 | num_updates 22680 | lr 8.39921e-05 | gnorm 1.05 | clip 0 | train_wall 547 | wall 27250
2020-10-11 12:29:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-11 12:29:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-11 12:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=75094.9296875Mb; avail=117739.25390625Mb
2020-10-11 12:29:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008028
2020-10-11 12:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.071621
2020-10-11 12:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=75127.32421875Mb; avail=117707.46484375Mb
2020-10-11 12:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.004097
2020-10-11 12:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=75104.6484375Mb; avail=117730.140625Mb
2020-10-11 12:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.980946
2020-10-11 12:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.059751
2020-10-11 12:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=75497.1484375Mb; avail=117337.8828125Mb
2020-10-11 12:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=75507.03125Mb; avail=117328.0Mb
2020-10-11 12:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.031606
2020-10-11 12:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=75521.49609375Mb; avail=117313.53515625Mb
2020-10-11 12:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002129
2020-10-11 12:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=75522.70703125Mb; avail=117312.32421875Mb
2020-10-11 12:29:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.043424
2020-10-11 12:29:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.078733
2020-10-11 12:29:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=75684.66796875Mb; avail=117150.3828125Mb
2020-10-11 12:29:43 | INFO | fairseq.trainer | begin training epoch 36
2020-10-11 12:29:57 | INFO | train_inner | epoch 036:     20 / 648 loss=4.636, nll_loss=3.101, ppl=8.58, wps=7964.2, ups=1.17, wpb=6812.9, bsz=293, num_updates=22700, lr=8.39551e-05, gnorm=1.059, clip=0, train_wall=69, wall=27266
2020-10-11 12:31:08 | INFO | train_inner | epoch 036:    120 / 648 loss=4.62, nll_loss=3.081, ppl=8.46, wps=9580.4, ups=1.4, wpb=6828.8, bsz=272.8, num_updates=22800, lr=8.37708e-05, gnorm=1.048, clip=0, train_wall=69, wall=27338
2020-10-11 12:32:21 | INFO | train_inner | epoch 036:    220 / 648 loss=4.584, nll_loss=3.042, ppl=8.23, wps=9448.7, ups=1.38, wpb=6871, bsz=305.4, num_updates=22900, lr=8.35877e-05, gnorm=1.049, clip=0, train_wall=71, wall=27410
2020-10-11 12:33:32 | INFO | train_inner | epoch 036:    320 / 648 loss=4.617, nll_loss=3.077, ppl=8.44, wps=9641.3, ups=1.39, wpb=6930, bsz=293.3, num_updates=23000, lr=8.34058e-05, gnorm=1.047, clip=0, train_wall=70, wall=27482
2020-10-11 12:34:45 | INFO | train_inner | epoch 036:    420 / 648 loss=4.62, nll_loss=3.083, ppl=8.47, wps=9483.5, ups=1.38, wpb=6857.2, bsz=302, num_updates=23100, lr=8.3225e-05, gnorm=1.078, clip=0, train_wall=70, wall=27555
2020-10-11 12:35:57 | INFO | train_inner | epoch 036:    520 / 648 loss=4.621, nll_loss=3.083, ppl=8.47, wps=9691.8, ups=1.39, wpb=6984.1, bsz=291.3, num_updates=23200, lr=8.30455e-05, gnorm=1.038, clip=0, train_wall=70, wall=27627
2020-10-11 12:37:09 | INFO | train_inner | epoch 036:    620 / 648 loss=4.638, nll_loss=3.103, ppl=8.59, wps=9587.2, ups=1.39, wpb=6897.7, bsz=279.9, num_updates=23300, lr=8.28671e-05, gnorm=1.052, clip=0, train_wall=70, wall=27699
2020-10-11 12:37:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 12:37:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67277.95703125Mb; avail=125561.234375Mb
2020-10-11 12:37:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002368
2020-10-11 12:37:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67255.4296875Mb; avail=125587.3203125Mb
2020-10-11 12:37:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.120905
2020-10-11 12:37:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67264.703125Mb; avail=125575.09375Mb
2020-10-11 12:37:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.132426
2020-10-11 12:37:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.257172
2020-10-11 12:37:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67231.48828125Mb; avail=125608.30859375Mb
2020-10-11 12:37:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67229.234375Mb; avail=125610.5625Mb
2020-10-11 12:37:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001503
2020-10-11 12:37:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67229.234375Mb; avail=125610.5625Mb
2020-10-11 12:37:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.084947
2020-10-11 12:37:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67268.18359375Mb; avail=125571.83203125Mb
2020-10-11 12:37:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.067072
2020-10-11 12:37:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.154771
2020-10-11 12:37:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67305.58984375Mb; avail=125533.8203125Mb
2020-10-11 12:37:34 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 5.176 | nll_loss 3.586 | ppl 12.01 | wps 21490.3 | wpb 2274.6 | bsz 96.2 | num_updates 23328 | best_loss 5.175
2020-10-11 12:37:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 12:37:43 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_last.pt (epoch 36 @ 23328 updates, score 5.176) (writing took 8.602235246449709 seconds)
2020-10-11 12:37:43 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-11 12:37:43 | INFO | train | epoch 036 | loss 4.617 | nll_loss 3.078 | ppl 8.45 | wps 9245.4 | ups 1.34 | wpb 6882.8 | bsz 290.7 | num_updates 23328 | lr 8.28173e-05 | gnorm 1.056 | clip 0 | train_wall 454 | wall 27733
2020-10-11 12:37:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-11 12:37:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-11 12:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=68275.75Mb; avail=124564.10546875Mb
2020-10-11 12:37:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004292
2020-10-11 12:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.046873
2020-10-11 12:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68193.6796875Mb; avail=124647.45703125Mb
2020-10-11 12:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002210
2020-10-11 12:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68188.4921875Mb; avail=124651.66015625Mb
2020-10-11 12:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.033055
2020-10-11 12:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.083784
2020-10-11 12:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68263.5390625Mb; avail=124576.51171875Mb
2020-10-11 12:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=68263.5390625Mb; avail=124576.51171875Mb
2020-10-11 12:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.031956
2020-10-11 12:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68271.125Mb; avail=124568.92578125Mb
2020-10-11 12:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002258
2020-10-11 12:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68271.71875Mb; avail=124568.33203125Mb
2020-10-11 12:37:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.191270
2020-10-11 12:37:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.226943
2020-10-11 12:37:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68475.3828125Mb; avail=124364.453125Mb
2020-10-11 12:37:45 | INFO | fairseq.trainer | begin training epoch 37
2020-10-11 12:38:36 | INFO | train_inner | epoch 037:     72 / 648 loss=4.588, nll_loss=3.046, ppl=8.26, wps=7863.8, ups=1.15, wpb=6861.4, bsz=300.2, num_updates=23400, lr=8.26898e-05, gnorm=1.102, clip=0, train_wall=69, wall=27786
2020-10-11 12:39:49 | INFO | train_inner | epoch 037:    172 / 648 loss=4.561, nll_loss=3.015, ppl=8.08, wps=9320.8, ups=1.37, wpb=6799.9, bsz=290.7, num_updates=23500, lr=8.25137e-05, gnorm=1.044, clip=0, train_wall=71, wall=27859
2020-10-11 12:41:01 | INFO | train_inner | epoch 037:    272 / 648 loss=4.597, nll_loss=3.055, ppl=8.31, wps=9600.4, ups=1.39, wpb=6926.6, bsz=285.8, num_updates=23600, lr=8.23387e-05, gnorm=1.066, clip=0, train_wall=71, wall=27931
2020-10-11 12:42:14 | INFO | train_inner | epoch 037:    372 / 648 loss=4.581, nll_loss=3.037, ppl=8.21, wps=9656.7, ups=1.37, wpb=7032.8, bsz=307.7, num_updates=23700, lr=8.21648e-05, gnorm=1.054, clip=0, train_wall=71, wall=28004
2020-10-11 12:43:26 | INFO | train_inner | epoch 037:    472 / 648 loss=4.61, nll_loss=3.07, ppl=8.4, wps=9632.9, ups=1.39, wpb=6933.3, bsz=282.4, num_updates=23800, lr=8.1992e-05, gnorm=1.047, clip=0, train_wall=70, wall=28076
2020-10-11 12:44:37 | INFO | train_inner | epoch 037:    572 / 648 loss=4.598, nll_loss=3.056, ppl=8.32, wps=9685.1, ups=1.41, wpb=6886.7, bsz=297.4, num_updates=23900, lr=8.18203e-05, gnorm=1.046, clip=0, train_wall=70, wall=28147
2020-10-11 12:45:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 12:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=71500.5859375Mb; avail=121339.25Mb
2020-10-11 12:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002481
2020-10-11 12:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=71501.19140625Mb; avail=121338.64453125Mb
2020-10-11 12:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.133299
2020-10-11 12:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=71576.6640625Mb; avail=121262.56640625Mb
2020-10-11 12:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.074118
2020-10-11 12:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.211719
2020-10-11 12:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=71581.73046875Mb; avail=121258.10546875Mb
2020-10-11 12:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=71590.8125Mb; avail=121249.0234375Mb
2020-10-11 12:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002226
2020-10-11 12:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=71592.0234375Mb; avail=121247.8125Mb
2020-10-11 12:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.149704
2020-10-11 12:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=71630.4921875Mb; avail=121209.34375Mb
2020-10-11 12:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.067434
2020-10-11 12:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.220956
2020-10-11 12:45:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=71684.73046875Mb; avail=121155.10546875Mb
2020-10-11 12:45:37 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 5.17 | nll_loss 3.582 | ppl 11.97 | wps 21522.6 | wpb 2274.6 | bsz 96.2 | num_updates 23976 | best_loss 5.17
2020-10-11 12:45:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 12:45:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 37 @ 23976 updates, score 5.17) (writing took 12.502961549907923 seconds)
2020-10-11 12:45:49 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-11 12:45:49 | INFO | train | epoch 037 | loss 4.595 | nll_loss 3.053 | ppl 8.3 | wps 9169.3 | ups 1.33 | wpb 6882.8 | bsz 290.7 | num_updates 23976 | lr 8.16905e-05 | gnorm 1.066 | clip 0 | train_wall 455 | wall 28219
2020-10-11 12:45:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-11 12:45:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-11 12:45:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=69382.30859375Mb; avail=123457.53515625Mb
2020-10-11 12:45:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006213
2020-10-11 12:45:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.051027
2020-10-11 12:45:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=69402.5703125Mb; avail=123437.2734375Mb
2020-10-11 12:45:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001608
2020-10-11 12:45:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=69403.17578125Mb; avail=123436.66796875Mb
2020-10-11 12:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.137198
2020-10-11 12:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.191186
2020-10-11 12:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=69540.4609375Mb; avail=123299.35546875Mb
2020-10-11 12:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=69584.57421875Mb; avail=123255.2421875Mb
2020-10-11 12:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035454
2020-10-11 12:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=69580.41796875Mb; avail=123259.3984375Mb
2020-10-11 12:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.003519
2020-10-11 12:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=69586.22265625Mb; avail=123253.59375Mb
2020-10-11 12:45:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.120141
2020-10-11 12:45:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.161617
2020-10-11 12:45:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=69949.953125Mb; avail=122890.5078125Mb
2020-10-11 12:45:52 | INFO | fairseq.trainer | begin training epoch 38
2020-10-11 12:46:08 | INFO | train_inner | epoch 038:     24 / 648 loss=4.628, nll_loss=3.092, ppl=8.52, wps=7428.1, ups=1.09, wpb=6789.3, bsz=276.5, num_updates=24000, lr=8.16497e-05, gnorm=1.107, clip=0, train_wall=69, wall=28238
2020-10-11 12:47:21 | INFO | train_inner | epoch 038:    124 / 648 loss=4.524, nll_loss=2.972, ppl=7.84, wps=9449.8, ups=1.37, wpb=6886.6, bsz=294.2, num_updates=24100, lr=8.14801e-05, gnorm=1.042, clip=0, train_wall=71, wall=28311
2020-10-11 12:48:33 | INFO | train_inner | epoch 038:    224 / 648 loss=4.561, nll_loss=3.014, ppl=8.08, wps=9801.2, ups=1.4, wpb=7008.3, bsz=280.3, num_updates=24200, lr=8.13116e-05, gnorm=1.047, clip=0, train_wall=70, wall=28383
2020-10-11 12:49:44 | INFO | train_inner | epoch 038:    324 / 648 loss=4.58, nll_loss=3.035, ppl=8.2, wps=9658.6, ups=1.4, wpb=6904.5, bsz=291.4, num_updates=24300, lr=8.11441e-05, gnorm=1.057, clip=0, train_wall=70, wall=28454
2020-10-11 12:50:55 | INFO | train_inner | epoch 038:    424 / 648 loss=4.594, nll_loss=3.051, ppl=8.29, wps=9618.8, ups=1.41, wpb=6845.7, bsz=291, num_updates=24400, lr=8.09776e-05, gnorm=1.077, clip=0, train_wall=70, wall=28525
2020-10-11 12:52:07 | INFO | train_inner | epoch 038:    524 / 648 loss=4.583, nll_loss=3.04, ppl=8.22, wps=9779.5, ups=1.41, wpb=6947.8, bsz=301.1, num_updates=24500, lr=8.08122e-05, gnorm=1.057, clip=0, train_wall=69, wall=28596
2020-10-11 12:53:16 | INFO | train_inner | epoch 038:    624 / 648 loss=4.591, nll_loss=3.049, ppl=8.28, wps=9602.3, ups=1.44, wpb=6690.4, bsz=275.6, num_updates=24600, lr=8.06478e-05, gnorm=1.068, clip=0, train_wall=68, wall=28666
2020-10-11 12:53:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 12:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=64715.046875Mb; avail=128124.75390625Mb
2020-10-11 12:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001923
2020-10-11 12:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64705.2890625Mb; avail=128134.51171875Mb
2020-10-11 12:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.084585
2020-10-11 12:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64708.359375Mb; avail=128131.44140625Mb
2020-10-11 12:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.066099
2020-10-11 12:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.154193
2020-10-11 12:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64730.9609375Mb; avail=128108.83984375Mb
2020-10-11 12:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=64738.0078125Mb; avail=128102.01171875Mb
2020-10-11 12:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001196
2020-10-11 12:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64738.0078125Mb; avail=128102.01171875Mb
2020-10-11 12:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082261
2020-10-11 12:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64676.21875Mb; avail=128163.796875Mb
2020-10-11 12:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.067282
2020-10-11 12:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.152047
2020-10-11 12:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64733.984375Mb; avail=128106.03125Mb
2020-10-11 12:53:39 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 5.146 | nll_loss 3.552 | ppl 11.73 | wps 21571.5 | wpb 2274.6 | bsz 96.2 | num_updates 24624 | best_loss 5.146
2020-10-11 12:53:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 12:53:49 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 38 @ 24624 updates, score 5.146) (writing took 10.434192895889282 seconds)
2020-10-11 12:53:49 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-11 12:53:49 | INFO | train | epoch 038 | loss 4.571 | nll_loss 3.026 | ppl 8.14 | wps 9297 | ups 1.35 | wpb 6882.8 | bsz 290.7 | num_updates 24624 | lr 8.06085e-05 | gnorm 1.056 | clip 0 | train_wall 451 | wall 28699
2020-10-11 12:53:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-11 12:53:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-11 12:53:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=64655.77734375Mb; avail=128184.27734375Mb
2020-10-11 12:53:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007424
2020-10-11 12:53:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.064438
2020-10-11 12:53:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64627.76171875Mb; avail=128212.29296875Mb
2020-10-11 12:53:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.003612
2020-10-11 12:53:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64628.25390625Mb; avail=128211.80078125Mb
2020-10-11 12:53:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.946489
2020-10-11 12:53:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.016789
2020-10-11 12:53:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64617.1484375Mb; avail=128222.8828125Mb
2020-10-11 12:53:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=64663.796875Mb; avail=128176.234375Mb
2020-10-11 12:53:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.029593
2020-10-11 12:53:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64676.1640625Mb; avail=128163.8671875Mb
2020-10-11 12:53:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001474
2020-10-11 12:53:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64676.1640625Mb; avail=128163.8671875Mb
2020-10-11 12:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.915925
2020-10-11 12:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.948366
2020-10-11 12:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64635.8125Mb; avail=128204.43359375Mb
2020-10-11 12:53:51 | INFO | fairseq.trainer | begin training epoch 39
2020-10-11 12:54:45 | INFO | train_inner | epoch 039:     76 / 648 loss=4.542, nll_loss=2.992, ppl=7.96, wps=7869.8, ups=1.13, wpb=6982.6, bsz=289.2, num_updates=24700, lr=8.04844e-05, gnorm=1.034, clip=0, train_wall=69, wall=28755
2020-10-11 12:55:57 | INFO | train_inner | epoch 039:    176 / 648 loss=4.538, nll_loss=2.989, ppl=7.94, wps=9581.8, ups=1.39, wpb=6870.6, bsz=285.9, num_updates=24800, lr=8.03219e-05, gnorm=1.065, clip=0, train_wall=70, wall=28826
2020-10-11 12:57:07 | INFO | train_inner | epoch 039:    276 / 648 loss=4.574, nll_loss=3.029, ppl=8.16, wps=9611.9, ups=1.42, wpb=6748.6, bsz=267.3, num_updates=24900, lr=8.01605e-05, gnorm=1.085, clip=0, train_wall=69, wall=28897
2020-10-11 12:58:17 | INFO | train_inner | epoch 039:    376 / 648 loss=4.546, nll_loss=2.997, ppl=7.99, wps=9716.3, ups=1.42, wpb=6826.1, bsz=298.6, num_updates=25000, lr=8e-05, gnorm=1.092, clip=0, train_wall=69, wall=28967
2020-10-11 12:59:28 | INFO | train_inner | epoch 039:    476 / 648 loss=4.549, nll_loss=3, ppl=8, wps=9793.5, ups=1.42, wpb=6900.2, bsz=305.2, num_updates=25100, lr=7.98405e-05, gnorm=1.053, clip=0, train_wall=69, wall=29037
2020-10-11 13:01:08 | INFO | train_inner | epoch 039:    576 / 648 loss=4.546, nll_loss=2.997, ppl=7.98, wps=6917.7, ups=0.99, wpb=6968.3, bsz=310.2, num_updates=25200, lr=7.96819e-05, gnorm=1.049, clip=0, train_wall=99, wall=29138
2020-10-11 13:02:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 13:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66783.75Mb; avail=126056.0234375Mb
2020-10-11 13:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.006405
2020-10-11 13:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66785.109375Mb; avail=126053.6796875Mb
2020-10-11 13:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.090385
2020-10-11 13:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66803.80078125Mb; avail=126035.19921875Mb
2020-10-11 13:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.068159
2020-10-11 13:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.167050
2020-10-11 13:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66794.26171875Mb; avail=126044.75390625Mb
2020-10-11 13:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66794.74609375Mb; avail=126044.26953125Mb
2020-10-11 13:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001533
2020-10-11 13:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66794.74609375Mb; avail=126044.26953125Mb
2020-10-11 13:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.084528
2020-10-11 13:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66754.9921875Mb; avail=126083.91796875Mb
2020-10-11 13:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.066017
2020-10-11 13:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.153409
2020-10-11 13:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66760.7734375Mb; avail=126078.02734375Mb
2020-10-11 13:02:51 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 5.139 | nll_loss 3.544 | ppl 11.67 | wps 12124 | wpb 2274.6 | bsz 96.2 | num_updates 25272 | best_loss 5.139
2020-10-11 13:02:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 13:03:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 39 @ 25272 updates, score 5.139) (writing took 10.28956451639533 seconds)
2020-10-11 13:03:02 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-11 13:03:02 | INFO | train | epoch 039 | loss 4.551 | nll_loss 3.003 | ppl 8.02 | wps 8072.8 | ups 1.17 | wpb 6882.8 | bsz 290.7 | num_updates 25272 | lr 7.95683e-05 | gnorm 1.064 | clip 0 | train_wall 521 | wall 29251
2020-10-11 13:03:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-11 13:03:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-11 13:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67668.67578125Mb; avail=125170.0703125Mb
2020-10-11 13:03:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004092
2020-10-11 13:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.049764
2020-10-11 13:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67691.078125Mb; avail=125147.0625Mb
2020-10-11 13:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002735
2020-10-11 13:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67694.65625Mb; avail=125141.31640625Mb
2020-10-11 13:03:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.142384
2020-10-11 13:03:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.196515
2020-10-11 13:03:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67799.9140625Mb; avail=125038.95703125Mb
2020-10-11 13:03:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67824.35546875Mb; avail=125014.4140625Mb
2020-10-11 13:03:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.029074
2020-10-11 13:03:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67842.453125Mb; avail=124996.3671875Mb
2020-10-11 13:03:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001480
2020-10-11 13:03:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67843.64453125Mb; avail=124995.17578125Mb
2020-10-11 13:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.003314
2020-10-11 13:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.035151
2020-10-11 13:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67870.0703125Mb; avail=124968.44921875Mb
2020-10-11 13:03:04 | INFO | fairseq.trainer | begin training epoch 40
2020-10-11 13:03:40 | INFO | train_inner | epoch 040:     28 / 648 loss=4.558, nll_loss=3.01, ppl=8.06, wps=4539.7, ups=0.66, wpb=6909.9, bsz=284.8, num_updates=25300, lr=7.95243e-05, gnorm=1.064, clip=0, train_wall=128, wall=29290
2020-10-11 13:05:48 | INFO | train_inner | epoch 040:    128 / 648 loss=4.508, nll_loss=2.953, ppl=7.74, wps=5394, ups=0.78, wpb=6876.4, bsz=285.2, num_updates=25400, lr=7.93676e-05, gnorm=1.052, clip=0, train_wall=126, wall=29418
2020-10-11 13:07:54 | INFO | train_inner | epoch 040:    228 / 648 loss=4.522, nll_loss=2.97, ppl=7.83, wps=5458.3, ups=0.79, wpb=6901.1, bsz=291.9, num_updates=25500, lr=7.92118e-05, gnorm=1.066, clip=0, train_wall=125, wall=29544
2020-10-11 13:10:04 | INFO | train_inner | epoch 040:    328 / 648 loss=4.514, nll_loss=2.96, ppl=7.78, wps=5257.8, ups=0.77, wpb=6804.1, bsz=294.2, num_updates=25600, lr=7.90569e-05, gnorm=1.079, clip=0, train_wall=128, wall=29674
2020-10-11 13:12:11 | INFO | train_inner | epoch 040:    428 / 648 loss=4.531, nll_loss=2.978, ppl=7.88, wps=5599.8, ups=0.78, wpb=7135.4, bsz=305, num_updates=25700, lr=7.8903e-05, gnorm=1.034, clip=0, train_wall=126, wall=29801
2020-10-11 13:14:16 | INFO | train_inner | epoch 040:    528 / 648 loss=4.558, nll_loss=3.01, ppl=8.06, wps=5416.3, ups=0.8, wpb=6755, bsz=291.7, num_updates=25800, lr=7.87499e-05, gnorm=1.079, clip=0, train_wall=123, wall=29926
2020-10-11 13:16:22 | INFO | train_inner | epoch 040:    628 / 648 loss=4.555, nll_loss=3.008, ppl=8.05, wps=5405.9, ups=0.79, wpb=6806.5, bsz=281.4, num_updates=25900, lr=7.85977e-05, gnorm=1.085, clip=0, train_wall=124, wall=30052
2020-10-11 13:16:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 13:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=65655.453125Mb; avail=127183.3125Mb
2020-10-11 13:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002506
2020-10-11 13:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65655.453125Mb; avail=127183.3125Mb
2020-10-11 13:16:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.153856
2020-10-11 13:16:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65627.45703125Mb; avail=127211.30859375Mb
2020-10-11 13:16:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.071310
2020-10-11 13:16:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.229142
2020-10-11 13:16:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65650.78515625Mb; avail=127187.98046875Mb
2020-10-11 13:16:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=65650.78515625Mb; avail=127187.98046875Mb
2020-10-11 13:16:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001709
2020-10-11 13:16:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65650.78515625Mb; avail=127187.98046875Mb
2020-10-11 13:16:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.093087
2020-10-11 13:16:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65571.859375Mb; avail=127266.90625Mb
2020-10-11 13:16:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.065809
2020-10-11 13:16:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.161924
2020-10-11 13:16:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65571.76171875Mb; avail=127267.00390625Mb
2020-10-11 13:16:57 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 5.137 | nll_loss 3.543 | ppl 11.66 | wps 12164.9 | wpb 2274.6 | bsz 96.2 | num_updates 25920 | best_loss 5.137
2020-10-11 13:16:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 13:17:07 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/O2M/checkpoint_best.pt (epoch 40 @ 25920 updates, score 5.137) (writing took 10.175200633704662 seconds)
2020-10-11 13:17:07 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-11 13:17:07 | INFO | train | epoch 040 | loss 4.531 | nll_loss 2.98 | ppl 7.89 | wps 5274.2 | ups 0.77 | wpb 6882.8 | bsz 290.7 | num_updates 25920 | lr 7.85674e-05 | gnorm 1.066 | clip 0 | train_wall 813 | wall 30097
2020-10-11 13:17:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-11 13:17:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-11 13:17:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=65555.2578125Mb; avail=127283.4921875Mb
2020-10-11 13:17:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004451
2020-10-11 13:17:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.043466
2020-10-11 13:17:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65572.8828125Mb; avail=127265.8671875Mb
2020-10-11 13:17:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001832
2020-10-11 13:17:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65578.0078125Mb; avail=127260.7421875Mb
2020-10-11 13:17:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.019573
2020-10-11 13:17:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.066478
2020-10-11 13:17:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65507.45703125Mb; avail=127331.5390625Mb
2020-10-11 13:17:08 | INFO | fairseq_cli.train | done training in 30096.8 seconds
