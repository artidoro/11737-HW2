2020-10-11 04:55:18 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='fairseq/data-bin/ted_azetur_sepspm8000/M2O/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_langtok=False, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_langtok=None, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='aze-eng,tur-eng', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=40, max_source_positions=1024, max_target_positions=1024, max_tokens=4500, max_tokens_valid=4500, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='fairseq/checkpoints/ted_azetur_sepspm8000/M2O/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[2], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-10-11 04:55:18 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.
2020-10-11 04:55:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['aze', 'eng', 'tur']
2020-10-11 04:55:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | [aze] dictionary: 19979 types
2020-10-11 04:55:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | [eng] dictionary: 19979 types
2020-10-11 04:55:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | [tur] dictionary: 19979 types
2020-10-11 04:55:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2020-10-11 04:55:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56999.33984375Mb; avail=135851.07421875Mb
2020-10-11 04:55:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-11 04:55:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:aze-eng': 1, 'main:tur-eng': 1}
2020-10-11 04:55:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-11 04:55:18 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azetur_sepspm8000/M2O/valid.aze-eng.aze
2020-10-11 04:55:18 | INFO | fairseq.data.data_utils | loaded 671 examples from: fairseq/data-bin/ted_azetur_sepspm8000/M2O/valid.aze-eng.eng
2020-10-11 04:55:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azetur_sepspm8000/M2O/ valid aze-eng 671 examples
2020-10-11 04:55:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:tur-eng src_langtok: None; tgt_langtok: None
2020-10-11 04:55:18 | INFO | fairseq.data.data_utils | loaded 4045 examples from: fairseq/data-bin/ted_azetur_sepspm8000/M2O/valid.tur-eng.tur
2020-10-11 04:55:18 | INFO | fairseq.data.data_utils | loaded 4045 examples from: fairseq/data-bin/ted_azetur_sepspm8000/M2O/valid.tur-eng.eng
2020-10-11 04:55:18 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azetur_sepspm8000/M2O/ valid tur-eng 4045 examples
2020-10-11 04:55:20 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19979, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19979, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19979, bias=False)
  )
)
2020-10-11 04:55:20 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)
2020-10-11 04:55:20 | INFO | fairseq_cli.train | model: transformer_iwslt_de_en (TransformerModel)
2020-10-11 04:55:20 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-10-11 04:55:20 | INFO | fairseq_cli.train | num. model params: 41772544 (num. trained: 41772544)
2020-10-11 04:55:33 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-10-11 04:55:33 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-10-11 04:55:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-11 04:55:33 | INFO | fairseq.utils | rank   0: capabilities =  5.2  ; total memory = 11.927 GB ; name = GeForce GTX TITAN X                     
2020-10-11 04:55:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-10-11 04:55:33 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-10-11 04:55:33 | INFO | fairseq_cli.train | max tokens per GPU = 4500 and max sentences per GPU = None
2020-10-11 04:55:33 | INFO | fairseq.trainer | no existing checkpoint found fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_last.pt
2020-10-11 04:55:33 | INFO | fairseq.trainer | loading train data for epoch 1
2020-10-11 04:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2020-10-11 04:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60868.265625Mb; avail=131974.08984375Mb
2020-10-11 04:55:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': (None, None)}
2020-10-11 04:55:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:aze-eng': 1, 'main:tur-eng': 1}
2020-10-11 04:55:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aze-eng src_langtok: None; tgt_langtok: None
2020-10-11 04:55:33 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azetur_sepspm8000/M2O/train.aze-eng.aze
2020-10-11 04:55:33 | INFO | fairseq.data.data_utils | loaded 5946 examples from: fairseq/data-bin/ted_azetur_sepspm8000/M2O/train.aze-eng.eng
2020-10-11 04:55:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azetur_sepspm8000/M2O/ train aze-eng 5946 examples
2020-10-11 04:55:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:tur-eng src_langtok: None; tgt_langtok: None
2020-10-11 04:55:33 | INFO | fairseq.data.data_utils | loaded 182419 examples from: fairseq/data-bin/ted_azetur_sepspm8000/M2O/train.tur-eng.tur
2020-10-11 04:55:33 | INFO | fairseq.data.data_utils | loaded 182419 examples from: fairseq/data-bin/ted_azetur_sepspm8000/M2O/train.tur-eng.eng
2020-10-11 04:55:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | fairseq/data-bin/ted_azetur_sepspm8000/M2O/ train tur-eng 182419 examples
2020-10-11 04:55:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:aze-eng', 5946), ('main:tur-eng', 182419)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2020-10-11 04:55:33 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat
2020-10-11 04:55:33 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 188365
2020-10-11 04:55:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 188365; virtual dataset size 188365
2020-10-11 04:55:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1
2020-10-11 04:55:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:aze-eng': 5946, 'main:tur-eng': 182419}; raw total size: 188365
2020-10-11 04:55:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:aze-eng': 5946, 'main:tur-eng': 182419}; resampled total size: 188365
2020-10-11 04:55:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset
2020-10-11 04:55:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.022522
2020-10-11 04:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60881.49609375Mb; avail=131958.3359375Mb
2020-10-11 04:55:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.003548
2020-10-11 04:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.034618
2020-10-11 04:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60885.5625Mb; avail=131954.26953125Mb
2020-10-11 04:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001347
2020-10-11 04:55:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60885.5625Mb; avail=131954.26953125Mb
2020-10-11 04:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.906968
2020-10-11 04:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.943946
2020-10-11 04:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60889.55078125Mb; avail=131950.44140625Mb
2020-10-11 04:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60886.21875Mb; avail=131953.7734375Mb
2020-10-11 04:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026521
2020-10-11 04:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60886.21875Mb; avail=131953.7734375Mb
2020-10-11 04:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001502
2020-10-11 04:55:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60886.21875Mb; avail=131953.7734375Mb
2020-10-11 04:55:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.851259
2020-10-11 04:55:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.880532
2020-10-11 04:55:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60891.32421875Mb; avail=131948.625Mb
2020-10-11 04:55:35 | INFO | fairseq.trainer | begin training epoch 1
2020-10-11 04:58:10 | INFO | train_inner | epoch 001:    100 / 640 loss=13.922, nll_loss=13.799, ppl=14257.8, wps=5136.1, ups=0.65, wpb=7904.5, bsz=319.7, num_updates=100, lr=5.0975e-06, gnorm=4.646, clip=0, train_wall=152, wall=157
2020-10-11 05:00:41 | INFO | train_inner | epoch 001:    200 / 640 loss=12.234, nll_loss=11.915, ppl=3860.98, wps=5168.5, ups=0.66, wpb=7851.2, bsz=301.8, num_updates=200, lr=1.0095e-05, gnorm=2.009, clip=0, train_wall=150, wall=309
2020-10-11 05:03:12 | INFO | train_inner | epoch 001:    300 / 640 loss=11.409, nll_loss=10.992, ppl=2036.66, wps=5164.1, ups=0.66, wpb=7774.7, bsz=279.8, num_updates=300, lr=1.50925e-05, gnorm=1.535, clip=0, train_wall=149, wall=459
2020-10-11 05:05:43 | INFO | train_inner | epoch 001:    400 / 640 loss=10.549, nll_loss=10.015, ppl=1034.4, wps=5182.1, ups=0.66, wpb=7806.2, bsz=270.2, num_updates=400, lr=2.009e-05, gnorm=1.651, clip=0, train_wall=149, wall=610
2020-10-11 05:08:14 | INFO | train_inner | epoch 001:    500 / 640 loss=9.609, nll_loss=8.908, ppl=480.32, wps=5214.1, ups=0.66, wpb=7896.8, bsz=310.1, num_updates=500, lr=2.50875e-05, gnorm=1.545, clip=0, train_wall=150, wall=762
2020-10-11 05:10:44 | INFO | train_inner | epoch 001:    600 / 640 loss=9.292, nll_loss=8.506, ppl=363.67, wps=5183.8, ups=0.67, wpb=7746.2, bsz=292.9, num_updates=600, lr=3.0085e-05, gnorm=1.495, clip=0, train_wall=148, wall=911
2020-10-11 05:11:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 05:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=61243.2890625Mb; avail=131594.03125Mb
2020-10-11 05:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001735
2020-10-11 05:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61245.015625Mb; avail=131592.8203125Mb
2020-10-11 05:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078116
2020-10-11 05:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61250.31640625Mb; avail=131587.51953125Mb
2020-10-11 05:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064449
2020-10-11 05:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.145600
2020-10-11 05:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61253.56640625Mb; avail=131584.2578125Mb
2020-10-11 05:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=61257.2578125Mb; avail=131580.56640625Mb
2020-10-11 05:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001530
2020-10-11 05:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61257.83984375Mb; avail=131579.984375Mb
2020-10-11 05:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078895
2020-10-11 05:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61252.66015625Mb; avail=131585.3515625Mb
2020-10-11 05:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063124
2020-10-11 05:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.144814
2020-10-11 05:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61251.875Mb; avail=131586.484375Mb
/usr1/home/rjoshi2/envs/torch160/fairseq/fairseq/utils.py:341: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  "amp_C fused kernels unavailable, disabling multi_tensor_l2norm; "
2020-10-11 05:11:54 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.929 | nll_loss 8.022 | ppl 260.01 | wps 12163.1 | wpb 2380 | bsz 90.7 | num_updates 640
2020-10-11 05:11:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 05:12:16 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 1 @ 640 updates, score 8.929) (writing took 22.413618676364422 seconds)
2020-10-11 05:12:16 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-10-11 05:12:16 | INFO | train | epoch 001 | loss 11.058 | nll_loss 10.559 | ppl 1508.26 | wps 5000.6 | ups 0.64 | wpb 7819.3 | bsz 294.3 | num_updates 640 | lr 3.2084e-05 | gnorm 2.106 | clip 0 | train_wall 957 | wall 1004
2020-10-11 05:12:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1
2020-10-11 05:12:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2
2020-10-11 05:12:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60897.625Mb; avail=131940.1796875Mb
2020-10-11 05:12:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007805
2020-10-11 05:12:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.059161
2020-10-11 05:12:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60899.44140625Mb; avail=131938.36328125Mb
2020-10-11 05:12:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002155
2020-10-11 05:12:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60899.44140625Mb; avail=131938.36328125Mb
2020-10-11 05:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.863679
2020-10-11 05:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.926417
2020-10-11 05:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60905.77734375Mb; avail=131931.828125Mb
2020-10-11 05:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60904.5078125Mb; avail=131934.1953125Mb
2020-10-11 05:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026839
2020-10-11 05:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60906.05859375Mb; avail=131932.15234375Mb
2020-10-11 05:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001350
2020-10-11 05:12:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60906.05859375Mb; avail=131932.15234375Mb
2020-10-11 05:12:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.867489
2020-10-11 05:12:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.896759
2020-10-11 05:12:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60915.20703125Mb; avail=131922.68359375Mb
2020-10-11 05:12:18 | INFO | fairseq.trainer | begin training epoch 2
2020-10-11 05:13:48 | INFO | train_inner | epoch 002:     60 / 640 loss=9.184, nll_loss=8.357, ppl=327.91, wps=4113.9, ups=0.54, wpb=7603.2, bsz=279.5, num_updates=700, lr=3.50825e-05, gnorm=1.481, clip=0, train_wall=148, wall=1096
2020-10-11 05:16:19 | INFO | train_inner | epoch 002:    160 / 640 loss=8.993, nll_loss=8.133, ppl=280.63, wps=5266.6, ups=0.66, wpb=7951.7, bsz=295.8, num_updates=800, lr=4.008e-05, gnorm=1.615, clip=0, train_wall=149, wall=1247
2020-10-11 05:18:50 | INFO | train_inner | epoch 002:    260 / 640 loss=8.924, nll_loss=8.051, ppl=265.18, wps=5211, ups=0.66, wpb=7863.1, bsz=300.3, num_updates=900, lr=4.50775e-05, gnorm=1.566, clip=0, train_wall=149, wall=1398
2020-10-11 05:21:22 | INFO | train_inner | epoch 002:    360 / 640 loss=8.782, nll_loss=7.89, ppl=237.28, wps=5160.8, ups=0.66, wpb=7817.7, bsz=285.9, num_updates=1000, lr=5.0075e-05, gnorm=1.43, clip=0, train_wall=150, wall=1549
2020-10-11 05:23:52 | INFO | train_inner | epoch 002:    460 / 640 loss=8.569, nll_loss=7.647, ppl=200.47, wps=5136.5, ups=0.66, wpb=7740.4, bsz=319, num_updates=1100, lr=5.50725e-05, gnorm=1.614, clip=0, train_wall=149, wall=1700
2020-10-11 05:26:24 | INFO | train_inner | epoch 002:    560 / 640 loss=8.509, nll_loss=7.577, ppl=190.98, wps=5104.3, ups=0.66, wpb=7711.6, bsz=276.9, num_updates=1200, lr=6.007e-05, gnorm=1.38, clip=0, train_wall=149, wall=1851
2020-10-11 05:28:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 05:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=61009.28125Mb; avail=131828.69140625Mb
2020-10-11 05:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002041
2020-10-11 05:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61009.28125Mb; avail=131828.69140625Mb
2020-10-11 05:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.086779
2020-10-11 05:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60998.953125Mb; avail=131839.01953125Mb
2020-10-11 05:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062483
2020-10-11 05:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.152521
2020-10-11 05:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60998.953125Mb; avail=131839.01953125Mb
2020-10-11 05:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60998.953125Mb; avail=131839.01953125Mb
2020-10-11 05:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001261
2020-10-11 05:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60998.953125Mb; avail=131839.01953125Mb
2020-10-11 05:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076256
2020-10-11 05:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60998.890625Mb; avail=131839.08203125Mb
2020-10-11 05:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062221
2020-10-11 05:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140697
2020-10-11 05:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60999.05859375Mb; avail=131838.6875Mb
2020-10-11 05:28:36 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.019 | nll_loss 6.973 | ppl 125.66 | wps 12031.8 | wpb 2380 | bsz 90.7 | num_updates 1280 | best_loss 8.019
2020-10-11 05:28:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 05:28:56 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 2 @ 1280 updates, score 8.019) (writing took 19.566256877034903 seconds)
2020-10-11 05:28:56 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-10-11 05:28:56 | INFO | train | epoch 002 | loss 8.745 | nll_loss 7.848 | ppl 230.43 | wps 5006.5 | ups 0.64 | wpb 7819.3 | bsz 294.3 | num_updates 1280 | lr 6.4068e-05 | gnorm 1.513 | clip 0 | train_wall 956 | wall 2003
2020-10-11 05:28:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2
2020-10-11 05:28:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3
2020-10-11 05:28:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60947.83984375Mb; avail=131890.0078125Mb
2020-10-11 05:28:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004833
2020-10-11 05:28:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.037451
2020-10-11 05:28:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60950.1640625Mb; avail=131887.64453125Mb
2020-10-11 05:28:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001362
2020-10-11 05:28:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60950.1640625Mb; avail=131887.64453125Mb
2020-10-11 05:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.886934
2020-10-11 05:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.927050
2020-10-11 05:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60961.71875Mb; avail=131876.2578125Mb
2020-10-11 05:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60956.41796875Mb; avail=131881.55859375Mb
2020-10-11 05:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027051
2020-10-11 05:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60958.65625Mb; avail=131879.3203125Mb
2020-10-11 05:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001420
2020-10-11 05:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60958.65625Mb; avail=131879.3203125Mb
2020-10-11 05:28:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.879050
2020-10-11 05:28:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.908777
2020-10-11 05:28:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60956.72265625Mb; avail=131881.3125Mb
2020-10-11 05:28:58 | INFO | fairseq.trainer | begin training epoch 3
2020-10-11 05:29:28 | INFO | train_inner | epoch 003:     20 / 640 loss=8.375, nll_loss=7.426, ppl=171.92, wps=4306.7, ups=0.54, wpb=7952.6, bsz=296.2, num_updates=1300, lr=6.50675e-05, gnorm=1.452, clip=0, train_wall=151, wall=2036
2020-10-11 05:31:59 | INFO | train_inner | epoch 003:    120 / 640 loss=8.284, nll_loss=7.323, ppl=160.06, wps=5186.6, ups=0.66, wpb=7837.2, bsz=301.1, num_updates=1400, lr=7.0065e-05, gnorm=1.359, clip=0, train_wall=149, wall=2187
2020-10-11 05:34:30 | INFO | train_inner | epoch 003:    220 / 640 loss=8.172, nll_loss=7.192, ppl=146.2, wps=5208.7, ups=0.67, wpb=7831.8, bsz=269.8, num_updates=1500, lr=7.50625e-05, gnorm=1.356, clip=0, train_wall=149, wall=2337
2020-10-11 05:37:00 | INFO | train_inner | epoch 003:    320 / 640 loss=8.084, nll_loss=7.093, ppl=136.5, wps=5124.7, ups=0.66, wpb=7707.9, bsz=274.7, num_updates=1600, lr=8.006e-05, gnorm=1.253, clip=0, train_wall=149, wall=2487
2020-10-11 05:39:32 | INFO | train_inner | epoch 003:    420 / 640 loss=7.844, nll_loss=6.819, ppl=112.89, wps=5209.2, ups=0.66, wpb=7914.5, bsz=318.7, num_updates=1700, lr=8.50575e-05, gnorm=1.367, clip=0, train_wall=150, wall=2639
2020-10-11 05:42:03 | INFO | train_inner | epoch 003:    520 / 640 loss=7.876, nll_loss=6.853, ppl=115.59, wps=5171.4, ups=0.66, wpb=7826.6, bsz=295.5, num_updates=1800, lr=9.0055e-05, gnorm=1.294, clip=0, train_wall=150, wall=2791
2020-10-11 05:44:35 | INFO | train_inner | epoch 003:    620 / 640 loss=7.817, nll_loss=6.786, ppl=110.35, wps=5164.2, ups=0.66, wpb=7819.2, bsz=295, num_updates=1900, lr=9.50525e-05, gnorm=1.311, clip=0, train_wall=150, wall=2942
2020-10-11 05:45:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 05:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=61057.4609375Mb; avail=131780.4609375Mb
2020-10-11 05:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001707
2020-10-11 05:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61058.06640625Mb; avail=131779.85546875Mb
2020-10-11 05:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074540
2020-10-11 05:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61063.15625Mb; avail=131774.765625Mb
2020-10-11 05:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063228
2020-10-11 05:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140469
2020-10-11 05:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61052.40234375Mb; avail=131785.51953125Mb
2020-10-11 05:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=61052.40234375Mb; avail=131785.51953125Mb
2020-10-11 05:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001242
2020-10-11 05:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61052.40234375Mb; avail=131785.51953125Mb
2020-10-11 05:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076329
2020-10-11 05:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61052.40234375Mb; avail=131785.51953125Mb
2020-10-11 05:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061213
2020-10-11 05:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139794
2020-10-11 05:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61052.40234375Mb; avail=131785.51953125Mb
2020-10-11 05:45:16 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.455 | nll_loss 6.33 | ppl 80.46 | wps 12041.6 | wpb 2380 | bsz 90.7 | num_updates 1920 | best_loss 7.455
2020-10-11 05:45:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 05:45:39 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 3 @ 1920 updates, score 7.455) (writing took 22.402786314487457 seconds)
2020-10-11 05:45:39 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-10-11 05:45:39 | INFO | train | epoch 003 | loss 8.018 | nll_loss 7.017 | ppl 129.5 | wps 4990.5 | ups 0.64 | wpb 7819.3 | bsz 294.3 | num_updates 1920 | lr 9.6052e-05 | gnorm 1.316 | clip 0 | train_wall 956 | wall 3006
2020-10-11 05:45:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3
2020-10-11 05:45:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4
2020-10-11 05:45:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60991.3515625Mb; avail=131846.421875Mb
2020-10-11 05:45:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.009735
2020-10-11 05:45:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.065362
2020-10-11 05:45:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60993.6796875Mb; avail=131844.09375Mb
2020-10-11 05:45:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001821
2020-10-11 05:45:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60993.6796875Mb; avail=131844.09375Mb
2020-10-11 05:45:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.866750
2020-10-11 05:45:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.935435
2020-10-11 05:45:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60993.76953125Mb; avail=131844.2734375Mb
2020-10-11 05:45:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60995.71875Mb; avail=131842.32421875Mb
2020-10-11 05:45:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026951
2020-10-11 05:45:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61000.5625Mb; avail=131837.48046875Mb
2020-10-11 05:45:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001628
2020-10-11 05:45:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61000.5625Mb; avail=131837.48046875Mb
2020-10-11 05:45:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.862643
2020-10-11 05:45:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.892468
2020-10-11 05:45:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61003.37109375Mb; avail=131834.58984375Mb
2020-10-11 05:45:41 | INFO | fairseq.trainer | begin training epoch 4
2020-10-11 05:47:41 | INFO | train_inner | epoch 004:     80 / 640 loss=7.753, nll_loss=6.713, ppl=104.89, wps=4124.2, ups=0.54, wpb=7701.3, bsz=316, num_updates=2000, lr=0.00010005, gnorm=1.276, clip=0, train_wall=150, wall=3129
2020-10-11 05:50:13 | INFO | train_inner | epoch 004:    180 / 640 loss=7.677, nll_loss=6.625, ppl=98.72, wps=5143.1, ups=0.66, wpb=7774.1, bsz=279.6, num_updates=2100, lr=0.000105048, gnorm=1.167, clip=0, train_wall=149, wall=3280
2020-10-11 05:52:43 | INFO | train_inner | epoch 004:    280 / 640 loss=7.549, nll_loss=6.477, ppl=89.1, wps=5295.5, ups=0.67, wpb=7950.6, bsz=285.8, num_updates=2200, lr=0.000110045, gnorm=1.168, clip=0, train_wall=148, wall=3430
2020-10-11 05:55:16 | INFO | train_inner | epoch 004:    380 / 640 loss=7.706, nll_loss=6.657, ppl=100.88, wps=5133.1, ups=0.65, wpb=7883, bsz=303.8, num_updates=2300, lr=0.000115043, gnorm=1.171, clip=0, train_wall=152, wall=3584
2020-10-11 05:57:36 | INFO | train_inner | epoch 004:    480 / 640 loss=7.48, nll_loss=6.398, ppl=84.35, wps=5676.4, ups=0.71, wpb=7940.1, bsz=308.6, num_updates=2400, lr=0.00012004, gnorm=1.216, clip=0, train_wall=138, wall=3724
2020-10-11 05:58:48 | INFO | train_inner | epoch 004:    580 / 640 loss=7.484, nll_loss=6.402, ppl=84.55, wps=10781, ups=1.39, wpb=7773.2, bsz=280.7, num_updates=2500, lr=0.000125037, gnorm=1.077, clip=0, train_wall=70, wall=3796
2020-10-11 05:59:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 05:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59396.23828125Mb; avail=133442.578125Mb
2020-10-11 05:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001652
2020-10-11 05:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59396.23828125Mb; avail=133442.578125Mb
2020-10-11 05:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079947
2020-10-11 05:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59410.94921875Mb; avail=133427.8671875Mb
2020-10-11 05:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.065876
2020-10-11 05:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.148755
2020-10-11 05:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59406.0703125Mb; avail=133432.74609375Mb
2020-10-11 05:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59407.88671875Mb; avail=133430.9296875Mb
2020-10-11 05:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001315
2020-10-11 05:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59407.88671875Mb; avail=133430.9296875Mb
2020-10-11 05:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078661
2020-10-11 05:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59396.90625Mb; avail=133441.91015625Mb
2020-10-11 05:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062141
2020-10-11 05:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.143248
2020-10-11 05:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59396.90625Mb; avail=133441.91015625Mb
2020-10-11 05:59:37 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.137 | nll_loss 5.937 | ppl 61.27 | wps 23961.9 | wpb 2380 | bsz 90.7 | num_updates 2560 | best_loss 7.137
2020-10-11 05:59:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 05:59:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 4 @ 2560 updates, score 7.137) (writing took 7.969907481223345 seconds)
2020-10-11 05:59:45 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-10-11 05:59:45 | INFO | train | epoch 004 | loss 7.584 | nll_loss 6.517 | ppl 91.59 | wps 5914.7 | ups 0.76 | wpb 7819.3 | bsz 294.3 | num_updates 2560 | lr 0.000128036 | gnorm 1.179 | clip 0 | train_wall 819 | wall 3852
2020-10-11 05:59:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4
2020-10-11 05:59:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5
2020-10-11 05:59:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59337.3359375Mb; avail=133501.56640625Mb
2020-10-11 05:59:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.010020
2020-10-11 05:59:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.073572
2020-10-11 05:59:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59345.4453125Mb; avail=133493.46875Mb
2020-10-11 05:59:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002374
2020-10-11 05:59:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59345.4453125Mb; avail=133493.46875Mb
2020-10-11 05:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.882289
2020-10-11 05:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.959926
2020-10-11 05:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59334.59375Mb; avail=133504.328125Mb
2020-10-11 05:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59335.19921875Mb; avail=133504.21484375Mb
2020-10-11 05:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.025999
2020-10-11 05:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59334.91796875Mb; avail=133504.00390625Mb
2020-10-11 05:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001461
2020-10-11 05:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59334.91796875Mb; avail=133504.00390625Mb
2020-10-11 05:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.866270
2020-10-11 05:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.894823
2020-10-11 05:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59352.0078125Mb; avail=133486.98046875Mb
2020-10-11 05:59:47 | INFO | fairseq.trainer | begin training epoch 5
2020-10-11 06:00:15 | INFO | train_inner | epoch 005:     40 / 640 loss=7.446, nll_loss=6.358, ppl=82.03, wps=8905, ups=1.15, wpb=7761.6, bsz=296.6, num_updates=2600, lr=0.000130035, gnorm=1.169, clip=0, train_wall=70, wall=3883
2020-10-11 06:01:30 | INFO | train_inner | epoch 005:    140 / 640 loss=7.346, nll_loss=6.243, ppl=75.75, wps=10577.5, ups=1.34, wpb=7883.7, bsz=296.9, num_updates=2700, lr=0.000135032, gnorm=1.13, clip=0, train_wall=73, wall=3957
2020-10-11 06:02:45 | INFO | train_inner | epoch 005:    240 / 640 loss=7.264, nll_loss=6.151, ppl=71.06, wps=10474.2, ups=1.34, wpb=7803, bsz=313, num_updates=2800, lr=0.00014003, gnorm=1.123, clip=0, train_wall=73, wall=4032
2020-10-11 06:03:59 | INFO | train_inner | epoch 005:    340 / 640 loss=7.297, nll_loss=6.185, ppl=72.78, wps=10796.8, ups=1.35, wpb=8003.9, bsz=275.2, num_updates=2900, lr=0.000145028, gnorm=1.012, clip=0, train_wall=72, wall=4106
2020-10-11 06:05:12 | INFO | train_inner | epoch 005:    440 / 640 loss=7.266, nll_loss=6.15, ppl=71.01, wps=10463.3, ups=1.37, wpb=7644.7, bsz=298.7, num_updates=3000, lr=0.000150025, gnorm=1.192, clip=0, train_wall=71, wall=4179
2020-10-11 06:06:25 | INFO | train_inner | epoch 005:    540 / 640 loss=7.196, nll_loss=6.071, ppl=67.22, wps=10818.9, ups=1.37, wpb=7924.9, bsz=300.7, num_updates=3100, lr=0.000155023, gnorm=1.036, clip=0, train_wall=71, wall=4252
2020-10-11 06:07:37 | INFO | train_inner | epoch 005:    640 / 640 loss=7.166, nll_loss=6.036, ppl=65.61, wps=10533.8, ups=1.38, wpb=7615.1, bsz=279.7, num_updates=3200, lr=0.00016002, gnorm=1.108, clip=0, train_wall=71, wall=4325
2020-10-11 06:07:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 06:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59404.75390625Mb; avail=133434.265625Mb
2020-10-11 06:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002170
2020-10-11 06:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59404.75390625Mb; avail=133434.265625Mb
2020-10-11 06:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.091996
2020-10-11 06:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59404.75390625Mb; avail=133434.265625Mb
2020-10-11 06:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062544
2020-10-11 06:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.157801
2020-10-11 06:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59404.5625Mb; avail=133434.45703125Mb
2020-10-11 06:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59404.5625Mb; avail=133434.45703125Mb
2020-10-11 06:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001226
2020-10-11 06:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59404.5625Mb; avail=133434.45703125Mb
2020-10-11 06:07:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076143
2020-10-11 06:07:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59403.78125Mb; avail=133434.83203125Mb
2020-10-11 06:07:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.107485
2020-10-11 06:07:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.202982
2020-10-11 06:07:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59420.8515625Mb; avail=133417.76171875Mb
2020-10-11 06:07:43 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.805 | nll_loss 5.575 | ppl 47.68 | wps 24017.8 | wpb 2380 | bsz 90.7 | num_updates 3200 | best_loss 6.805
2020-10-11 06:07:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 06:08:05 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 5 @ 3200 updates, score 6.805) (writing took 21.94109646230936 seconds)
2020-10-11 06:08:05 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-10-11 06:08:05 | INFO | train | epoch 005 | loss 7.267 | nll_loss 6.153 | ppl 71.14 | wps 10006.4 | ups 1.28 | wpb 7819.3 | bsz 294.3 | num_updates 3200 | lr 0.00016002 | gnorm 1.104 | clip 0 | train_wall 459 | wall 4352
2020-10-11 06:08:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5
2020-10-11 06:08:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6
2020-10-11 06:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59355.12890625Mb; avail=133483.5625Mb
2020-10-11 06:08:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.010203
2020-10-11 06:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.060011
2020-10-11 06:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59361.08203125Mb; avail=133477.609375Mb
2020-10-11 06:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001610
2020-10-11 06:08:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59361.08203125Mb; avail=133477.609375Mb
2020-10-11 06:08:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.864451
2020-10-11 06:08:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.927503
2020-10-11 06:08:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59363.28125Mb; avail=133475.515625Mb
2020-10-11 06:08:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59361.2734375Mb; avail=133477.5234375Mb
2020-10-11 06:08:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.029628
2020-10-11 06:08:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59362.984375Mb; avail=133475.8125Mb
2020-10-11 06:08:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001427
2020-10-11 06:08:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59363.57421875Mb; avail=133475.22265625Mb
2020-10-11 06:08:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.919610
2020-10-11 06:08:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.951967
2020-10-11 06:08:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59361.19140625Mb; avail=133477.87109375Mb
2020-10-11 06:08:07 | INFO | fairseq.trainer | begin training epoch 6
2020-10-11 06:09:20 | INFO | train_inner | epoch 006:    100 / 640 loss=7.047, nll_loss=5.903, ppl=59.82, wps=7622.2, ups=0.97, wpb=7830.2, bsz=322.5, num_updates=3300, lr=0.000165018, gnorm=1.021, clip=0, train_wall=71, wall=4427
2020-10-11 06:10:35 | INFO | train_inner | epoch 006:    200 / 640 loss=7.058, nll_loss=5.912, ppl=60.23, wps=10570.6, ups=1.34, wpb=7912.4, bsz=304.4, num_updates=3400, lr=0.000170015, gnorm=1.037, clip=0, train_wall=73, wall=4502
2020-10-11 06:11:48 | INFO | train_inner | epoch 006:    300 / 640 loss=6.969, nll_loss=5.81, ppl=56.09, wps=10565.3, ups=1.37, wpb=7703.5, bsz=275.1, num_updates=3500, lr=0.000175013, gnorm=1.01, clip=0, train_wall=71, wall=4575
2020-10-11 06:13:01 | INFO | train_inner | epoch 006:    400 / 640 loss=6.974, nll_loss=5.815, ppl=56.32, wps=10598.8, ups=1.37, wpb=7732.9, bsz=267.7, num_updates=3600, lr=0.00018001, gnorm=1.011, clip=0, train_wall=71, wall=4648
2020-10-11 06:14:13 | INFO | train_inner | epoch 006:    500 / 640 loss=6.867, nll_loss=5.694, ppl=51.76, wps=10816.8, ups=1.38, wpb=7819.1, bsz=295.2, num_updates=3700, lr=0.000185008, gnorm=1.061, clip=0, train_wall=71, wall=4720
2020-10-11 06:15:26 | INFO | train_inner | epoch 006:    600 / 640 loss=6.869, nll_loss=5.695, ppl=51.79, wps=10830.9, ups=1.37, wpb=7884.9, bsz=297.5, num_updates=3800, lr=0.000190005, gnorm=0.977, clip=0, train_wall=71, wall=4793
2020-10-11 06:15:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 06:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59435.56640625Mb; avail=133403.23046875Mb
2020-10-11 06:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002107
2020-10-11 06:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59435.56640625Mb; avail=133403.23046875Mb
2020-10-11 06:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.093995
2020-10-11 06:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59437.23046875Mb; avail=133401.56640625Mb
2020-10-11 06:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064960
2020-10-11 06:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.162185
2020-10-11 06:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59444.7421875Mb; avail=133394.0546875Mb
2020-10-11 06:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59449.4375Mb; avail=133389.55859375Mb
2020-10-11 06:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001326
2020-10-11 06:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59449.4375Mb; avail=133389.55859375Mb
2020-10-11 06:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078931
2020-10-11 06:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59447.546875Mb; avail=133391.44921875Mb
2020-10-11 06:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063606
2020-10-11 06:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.144918
2020-10-11 06:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59437.7734375Mb; avail=133401.00390625Mb
2020-10-11 06:16:00 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.494 | nll_loss 5.202 | ppl 36.82 | wps 23801.9 | wpb 2380 | bsz 90.7 | num_updates 3840 | best_loss 6.494
2020-10-11 06:16:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 06:16:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 6 @ 3840 updates, score 6.494) (writing took 22.293245278298855 seconds)
2020-10-11 06:16:22 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-10-11 06:16:22 | INFO | train | epoch 006 | loss 6.952 | nll_loss 5.791 | ppl 55.36 | wps 10063.9 | ups 1.29 | wpb 7819.3 | bsz 294.3 | num_updates 3840 | lr 0.000192004 | gnorm 1.021 | clip 0 | train_wall 456 | wall 4850
2020-10-11 06:16:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6
2020-10-11 06:16:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7
2020-10-11 06:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59376.40234375Mb; avail=133462.61328125Mb
2020-10-11 06:16:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008825
2020-10-11 06:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.070545
2020-10-11 06:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59370.20703125Mb; avail=133468.80859375Mb
2020-10-11 06:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002179
2020-10-11 06:16:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59369.87109375Mb; avail=133469.14453125Mb
2020-10-11 06:16:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.884436
2020-10-11 06:16:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.958642
2020-10-11 06:16:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59385.25Mb; avail=133453.796875Mb
2020-10-11 06:16:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59385.58984375Mb; avail=133458.37890625Mb
2020-10-11 06:16:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026782
2020-10-11 06:16:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59374.109375Mb; avail=133464.9375Mb
2020-10-11 06:16:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001294
2020-10-11 06:16:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59374.109375Mb; avail=133464.9375Mb
2020-10-11 06:16:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.847826
2020-10-11 06:16:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.876876
2020-10-11 06:16:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59380.18359375Mb; avail=133458.875Mb
2020-10-11 06:16:24 | INFO | fairseq.trainer | begin training epoch 7
2020-10-11 06:17:08 | INFO | train_inner | epoch 007:     60 / 640 loss=6.659, nll_loss=5.457, ppl=43.91, wps=7812.2, ups=0.98, wpb=7972.2, bsz=324.6, num_updates=3900, lr=0.000195003, gnorm=1.046, clip=0, train_wall=70, wall=4895
2020-10-11 06:18:47 | INFO | train_inner | epoch 007:    160 / 640 loss=6.75, nll_loss=5.558, ppl=47.11, wps=8045.7, ups=1.01, wpb=7954.2, bsz=269.3, num_updates=4000, lr=0.0002, gnorm=1.041, clip=0, train_wall=97, wall=4994
2020-10-11 06:21:21 | INFO | train_inner | epoch 007:    260 / 640 loss=6.664, nll_loss=5.461, ppl=44.06, wps=5092.4, ups=0.65, wpb=7849.5, bsz=277, num_updates=4100, lr=0.000197546, gnorm=0.971, clip=0, train_wall=152, wall=5148
2020-10-11 06:23:53 | INFO | train_inner | epoch 007:    360 / 640 loss=6.599, nll_loss=5.387, ppl=41.83, wps=5136, ups=0.66, wpb=7835.8, bsz=316.5, num_updates=4200, lr=0.00019518, gnorm=1.011, clip=0, train_wall=151, wall=5301
2020-10-11 06:26:24 | INFO | train_inner | epoch 007:    460 / 640 loss=6.542, nll_loss=5.321, ppl=39.98, wps=5134.8, ups=0.66, wpb=7738.1, bsz=299, num_updates=4300, lr=0.000192897, gnorm=0.996, clip=0, train_wall=149, wall=5452
2020-10-11 06:28:56 | INFO | train_inner | epoch 007:    560 / 640 loss=6.502, nll_loss=5.274, ppl=38.7, wps=5093.2, ups=0.66, wpb=7709.7, bsz=289.7, num_updates=4400, lr=0.000190693, gnorm=0.978, clip=0, train_wall=150, wall=5603
2020-10-11 06:30:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 06:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57503.03125Mb; avail=135335.82421875Mb
2020-10-11 06:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002234
2020-10-11 06:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57503.03125Mb; avail=135335.82421875Mb
2020-10-11 06:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.139077
2020-10-11 06:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57503.03125Mb; avail=135335.82421875Mb
2020-10-11 06:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.113236
2020-10-11 06:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.256065
2020-10-11 06:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57503.03125Mb; avail=135335.82421875Mb
2020-10-11 06:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57503.03125Mb; avail=135335.82421875Mb
2020-10-11 06:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001816
2020-10-11 06:30:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57503.03125Mb; avail=135335.82421875Mb
2020-10-11 06:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.148593
2020-10-11 06:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57503.5390625Mb; avail=135335.31640625Mb
2020-10-11 06:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.119400
2020-10-11 06:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.271362
2020-10-11 06:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57524.05859375Mb; avail=135314.796875Mb
2020-10-11 06:31:08 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.122 | nll_loss 4.768 | ppl 27.25 | wps 12021.2 | wpb 2380 | bsz 90.7 | num_updates 4480 | best_loss 6.122
2020-10-11 06:31:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 06:31:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 7 @ 4480 updates, score 6.122) (writing took 5.998452909290791 seconds)
2020-10-11 06:31:14 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-10-11 06:31:14 | INFO | train | epoch 007 | loss 6.605 | nll_loss 5.393 | ppl 42.02 | wps 5610.5 | ups 0.72 | wpb 7819.3 | bsz 294.3 | num_updates 4480 | lr 0.000188982 | gnorm 1.006 | clip 0 | train_wall 861 | wall 5742
2020-10-11 06:31:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7
2020-10-11 06:31:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8
2020-10-11 06:31:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57449.73828125Mb; avail=135388.953125Mb
2020-10-11 06:31:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004815
2020-10-11 06:31:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.038974
2020-10-11 06:31:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57455.046875Mb; avail=135383.64453125Mb
2020-10-11 06:31:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001549
2020-10-11 06:31:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57458.07421875Mb; avail=135380.6171875Mb
2020-10-11 06:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.862589
2020-10-11 06:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.904330
2020-10-11 06:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57451.4140625Mb; avail=135387.56640625Mb
2020-10-11 06:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57452.07421875Mb; avail=135387.1484375Mb
2020-10-11 06:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027179
2020-10-11 06:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57453.8125Mb; avail=135385.45703125Mb
2020-10-11 06:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001337
2020-10-11 06:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57453.8125Mb; avail=135385.45703125Mb
2020-10-11 06:31:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.874484
2020-10-11 06:31:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.904009
2020-10-11 06:31:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57452.37890625Mb; avail=135386.8984375Mb
2020-10-11 06:31:16 | INFO | fairseq.trainer | begin training epoch 8
2020-10-11 06:31:46 | INFO | train_inner | epoch 008:     20 / 640 loss=6.529, nll_loss=5.304, ppl=39.51, wps=4517.1, ups=0.59, wpb=7719.7, bsz=280.2, num_updates=4500, lr=0.000188562, gnorm=0.998, clip=0, train_wall=150, wall=5774
2020-10-11 06:34:18 | INFO | train_inner | epoch 008:    120 / 640 loss=6.326, nll_loss=5.074, ppl=33.69, wps=5177.7, ups=0.66, wpb=7859.2, bsz=308.2, num_updates=4600, lr=0.000186501, gnorm=0.981, clip=0, train_wall=150, wall=5926
2020-10-11 06:36:50 | INFO | train_inner | epoch 008:    220 / 640 loss=6.303, nll_loss=5.046, ppl=33.04, wps=5171.2, ups=0.66, wpb=7847.4, bsz=301.9, num_updates=4700, lr=0.000184506, gnorm=0.972, clip=0, train_wall=150, wall=6077
2020-10-11 06:39:20 | INFO | train_inner | epoch 008:    320 / 640 loss=6.295, nll_loss=5.036, ppl=32.81, wps=5190.6, ups=0.67, wpb=7782.9, bsz=285.8, num_updates=4800, lr=0.000182574, gnorm=1.002, clip=0, train_wall=148, wall=6227
2020-10-11 06:41:51 | INFO | train_inner | epoch 008:    420 / 640 loss=6.227, nll_loss=4.958, ppl=31.09, wps=5191.8, ups=0.66, wpb=7835.7, bsz=275.1, num_updates=4900, lr=0.000180702, gnorm=0.981, clip=0, train_wall=149, wall=6378
2020-10-11 06:44:24 | INFO | train_inner | epoch 008:    520 / 640 loss=6.169, nll_loss=4.891, ppl=29.68, wps=5116.7, ups=0.65, wpb=7856.2, bsz=316.5, num_updates=5000, lr=0.000178885, gnorm=0.972, clip=0, train_wall=152, wall=6532
2020-10-11 06:46:55 | INFO | train_inner | epoch 008:    620 / 640 loss=6.163, nll_loss=4.884, ppl=29.53, wps=5104.8, ups=0.66, wpb=7709.5, bsz=283.9, num_updates=5100, lr=0.000177123, gnorm=1.006, clip=0, train_wall=149, wall=6683
2020-10-11 06:47:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 06:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58146.33984375Mb; avail=134691.2109375Mb
2020-10-11 06:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002243
2020-10-11 06:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58146.33984375Mb; avail=134691.2109375Mb
2020-10-11 06:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.134525
2020-10-11 06:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58146.89453125Mb; avail=134690.65625Mb
2020-10-11 06:47:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.113000
2020-10-11 06:47:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.251031
2020-10-11 06:47:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58147.0Mb; avail=134690.55078125Mb
2020-10-11 06:47:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58147.9453125Mb; avail=134689.60546875Mb
2020-10-11 06:47:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001990
2020-10-11 06:47:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58147.9453125Mb; avail=134689.60546875Mb
2020-10-11 06:47:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.140005
2020-10-11 06:47:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58161.85546875Mb; avail=134675.08984375Mb
2020-10-11 06:47:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.119784
2020-10-11 06:47:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263319
2020-10-11 06:47:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58157.85546875Mb; avail=134679.6953125Mb
2020-10-11 06:47:36 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.837 | nll_loss 4.416 | ppl 21.35 | wps 12227.4 | wpb 2380 | bsz 90.7 | num_updates 5120 | best_loss 5.837
2020-10-11 06:47:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 06:47:46 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 8 @ 5120 updates, score 5.837) (writing took 9.984761327505112 seconds)
2020-10-11 06:47:46 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-10-11 06:47:46 | INFO | train | epoch 008 | loss 6.244 | nll_loss 4.979 | ppl 31.53 | wps 5044.8 | ups 0.65 | wpb 7819.3 | bsz 294.3 | num_updates 5120 | lr 0.000176777 | gnorm 0.981 | clip 0 | train_wall 958 | wall 6734
2020-10-11 06:47:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8
2020-10-11 06:47:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9
2020-10-11 06:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58088.79296875Mb; avail=134748.89453125Mb
2020-10-11 06:47:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007739
2020-10-11 06:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.056319
2020-10-11 06:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58094.9765625Mb; avail=134743.203125Mb
2020-10-11 06:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001942
2020-10-11 06:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58094.484375Mb; avail=134743.203125Mb
2020-10-11 06:47:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.858516
2020-10-11 06:47:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.918151
2020-10-11 06:47:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58098.91796875Mb; avail=134738.91015625Mb
2020-10-11 06:47:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58098.16796875Mb; avail=134739.67578125Mb
2020-10-11 06:47:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027076
2020-10-11 06:47:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58087.6796875Mb; avail=134750.36328125Mb
2020-10-11 06:47:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001421
2020-10-11 06:47:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58087.6796875Mb; avail=134750.36328125Mb
2020-10-11 06:47:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.854173
2020-10-11 06:47:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.883802
2020-10-11 06:47:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58090.74609375Mb; avail=134747.48046875Mb
2020-10-11 06:47:48 | INFO | fairseq.trainer | begin training epoch 9
2020-10-11 06:49:51 | INFO | train_inner | epoch 009:     80 / 640 loss=5.957, nll_loss=4.65, ppl=25.11, wps=4581.8, ups=0.57, wpb=8028.8, bsz=328.2, num_updates=5200, lr=0.000175412, gnorm=0.961, clip=0, train_wall=150, wall=6858
2020-10-11 06:52:23 | INFO | train_inner | epoch 009:    180 / 640 loss=6.06, nll_loss=4.767, ppl=27.22, wps=5096.3, ups=0.66, wpb=7771.8, bsz=283.3, num_updates=5300, lr=0.000173749, gnorm=0.976, clip=0, train_wall=151, wall=7011
2020-10-11 06:54:55 | INFO | train_inner | epoch 009:    280 / 640 loss=6.016, nll_loss=4.715, ppl=26.26, wps=5131, ups=0.66, wpb=7815, bsz=277.4, num_updates=5400, lr=0.000172133, gnorm=0.979, clip=0, train_wall=151, wall=7163
2020-10-11 06:57:28 | INFO | train_inner | epoch 009:    380 / 640 loss=5.971, nll_loss=4.664, ppl=25.34, wps=5085.6, ups=0.65, wpb=7784.1, bsz=304.6, num_updates=5500, lr=0.000170561, gnorm=0.99, clip=0, train_wall=151, wall=7316
2020-10-11 07:00:00 | INFO | train_inner | epoch 009:    480 / 640 loss=5.886, nll_loss=4.566, ppl=23.69, wps=5052, ups=0.66, wpb=7678.7, bsz=298.8, num_updates=5600, lr=0.000169031, gnorm=0.991, clip=0, train_wall=150, wall=7468
2020-10-11 07:02:33 | INFO | train_inner | epoch 009:    580 / 640 loss=5.909, nll_loss=4.591, ppl=24.1, wps=5162.4, ups=0.65, wpb=7895.4, bsz=285.5, num_updates=5700, lr=0.000167542, gnorm=0.972, clip=0, train_wall=151, wall=7621
2020-10-11 07:04:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 07:04:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58138.921875Mb; avail=134698.94140625Mb
2020-10-11 07:04:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002163
2020-10-11 07:04:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58138.921875Mb; avail=134698.94140625Mb
2020-10-11 07:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.087772
2020-10-11 07:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58142.0390625Mb; avail=134695.82421875Mb
2020-10-11 07:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060271
2020-10-11 07:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.151273
2020-10-11 07:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58145.55078125Mb; avail=134692.3125Mb
2020-10-11 07:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58149.18359375Mb; avail=134688.6796875Mb
2020-10-11 07:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001239
2020-10-11 07:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58149.18359375Mb; avail=134688.6796875Mb
2020-10-11 07:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075195
2020-10-11 07:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58139.34765625Mb; avail=134698.515625Mb
2020-10-11 07:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.059173
2020-10-11 07:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136632
2020-10-11 07:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58139.34765625Mb; avail=134698.515625Mb
2020-10-11 07:04:16 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.621 | nll_loss 4.157 | ppl 17.84 | wps 11922.7 | wpb 2380 | bsz 90.7 | num_updates 5760 | best_loss 5.621
2020-10-11 07:04:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 07:04:27 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 9 @ 5760 updates, score 5.621) (writing took 10.381217960268259 seconds)
2020-10-11 07:04:27 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-10-11 07:04:27 | INFO | train | epoch 009 | loss 5.958 | nll_loss 4.648 | ppl 25.08 | wps 5001.3 | ups 0.64 | wpb 7819.3 | bsz 294.3 | num_updates 5760 | lr 0.000166667 | gnorm 0.978 | clip 0 | train_wall 966 | wall 7734
2020-10-11 07:04:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9
2020-10-11 07:04:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10
2020-10-11 07:04:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58108.40625Mb; avail=134729.859375Mb
2020-10-11 07:04:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004831
2020-10-11 07:04:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.039271
2020-10-11 07:04:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58114.65234375Mb; avail=134723.61328125Mb
2020-10-11 07:04:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001839
2020-10-11 07:04:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58104.80859375Mb; avail=134734.44140625Mb
2020-10-11 07:04:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.830205
2020-10-11 07:04:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.872592
2020-10-11 07:04:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58096.1484375Mb; avail=134741.80859375Mb
2020-10-11 07:04:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58092.703125Mb; avail=134745.25390625Mb
2020-10-11 07:04:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026072
2020-10-11 07:04:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58092.703125Mb; avail=134745.25390625Mb
2020-10-11 07:04:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001250
2020-10-11 07:04:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58092.703125Mb; avail=134745.25390625Mb
2020-10-11 07:04:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.831136
2020-10-11 07:04:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.859524
2020-10-11 07:04:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58095.20703125Mb; avail=134742.94140625Mb
2020-10-11 07:04:28 | INFO | fairseq.trainer | begin training epoch 10
2020-10-11 07:05:30 | INFO | train_inner | epoch 010:     40 / 640 loss=5.849, nll_loss=4.523, ppl=23, wps=4421.7, ups=0.57, wpb=7800.5, bsz=287.8, num_updates=5800, lr=0.000166091, gnorm=0.933, clip=0, train_wall=152, wall=7797
2020-10-11 07:08:01 | INFO | train_inner | epoch 010:    140 / 640 loss=5.777, nll_loss=4.441, ppl=21.71, wps=5131.1, ups=0.66, wpb=7771.1, bsz=298.8, num_updates=5900, lr=0.000164677, gnorm=0.976, clip=0, train_wall=150, wall=7949
2020-10-11 07:10:34 | INFO | train_inner | epoch 010:    240 / 640 loss=5.758, nll_loss=4.418, ppl=21.38, wps=5187.3, ups=0.65, wpb=7939.6, bsz=269.5, num_updates=6000, lr=0.000163299, gnorm=0.969, clip=0, train_wall=151, wall=8102
2020-10-11 07:13:07 | INFO | train_inner | epoch 010:    340 / 640 loss=5.65, nll_loss=4.295, ppl=19.64, wps=5126.9, ups=0.66, wpb=7824.5, bsz=328.1, num_updates=6100, lr=0.000161955, gnorm=0.976, clip=0, train_wall=151, wall=8254
2020-10-11 07:14:21 | INFO | train_inner | epoch 010:    440 / 640 loss=5.696, nll_loss=4.347, ppl=20.35, wps=10683.6, ups=1.36, wpb=7857.4, bsz=281.8, num_updates=6200, lr=0.000160644, gnorm=0.969, clip=0, train_wall=72, wall=8328
2020-10-11 07:15:34 | INFO | train_inner | epoch 010:    540 / 640 loss=5.714, nll_loss=4.367, ppl=20.63, wps=10683.1, ups=1.37, wpb=7794.9, bsz=301.3, num_updates=6300, lr=0.000159364, gnorm=0.998, clip=0, train_wall=71, wall=8401
2020-10-11 07:16:44 | INFO | train_inner | epoch 010:    640 / 640 loss=5.68, nll_loss=4.327, ppl=20.07, wps=10882, ups=1.41, wpb=7701.3, bsz=282.9, num_updates=6400, lr=0.000158114, gnorm=0.948, clip=0, train_wall=69, wall=8472
2020-10-11 07:16:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 07:16:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57058.390625Mb; avail=135780.28515625Mb
2020-10-11 07:16:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002460
2020-10-11 07:16:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57058.390625Mb; avail=135780.28515625Mb
2020-10-11 07:16:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077758
2020-10-11 07:16:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57058.390625Mb; avail=135780.28515625Mb
2020-10-11 07:16:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063954
2020-10-11 07:16:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.145459
2020-10-11 07:16:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57058.390625Mb; avail=135780.28515625Mb
2020-10-11 07:16:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57058.390625Mb; avail=135780.28515625Mb
2020-10-11 07:16:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001536
2020-10-11 07:16:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57058.390625Mb; avail=135780.28515625Mb
2020-10-11 07:16:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077717
2020-10-11 07:16:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57059.99609375Mb; avail=135778.6796875Mb
2020-10-11 07:16:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063580
2020-10-11 07:16:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.144040
2020-10-11 07:16:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57063.3671875Mb; avail=135775.30859375Mb
2020-10-11 07:16:50 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.447 | nll_loss 3.95 | ppl 15.45 | wps 23959.5 | wpb 2380 | bsz 90.7 | num_updates 6400 | best_loss 5.447
2020-10-11 07:16:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 07:16:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 10 @ 6400 updates, score 5.447) (writing took 5.459068704396486 seconds)
2020-10-11 07:16:55 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-10-11 07:16:55 | INFO | train | epoch 010 | loss 5.716 | nll_loss 4.37 | ppl 20.68 | wps 6684.3 | ups 0.85 | wpb 7819.3 | bsz 294.3 | num_updates 6400 | lr 0.000158114 | gnorm 0.969 | clip 0 | train_wall 724 | wall 8483
2020-10-11 07:16:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10
2020-10-11 07:16:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11
2020-10-11 07:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57010.1484375Mb; avail=135828.46875Mb
2020-10-11 07:16:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.009055
2020-10-11 07:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.067185
2020-10-11 07:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57017.65625Mb; avail=135820.9609375Mb
2020-10-11 07:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002116
2020-10-11 07:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57017.65625Mb; avail=135820.9609375Mb
2020-10-11 07:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.869054
2020-10-11 07:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.939867
2020-10-11 07:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57013.5625Mb; avail=135825.453125Mb
2020-10-11 07:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57008.62890625Mb; avail=135830.38671875Mb
2020-10-11 07:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026854
2020-10-11 07:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57009.18359375Mb; avail=135829.83203125Mb
2020-10-11 07:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001496
2020-10-11 07:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57009.7890625Mb; avail=135829.2265625Mb
2020-10-11 07:16:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.845519
2020-10-11 07:16:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.874890
2020-10-11 07:16:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57012.27734375Mb; avail=135826.54296875Mb
2020-10-11 07:16:57 | INFO | fairseq.trainer | begin training epoch 11
2020-10-11 07:18:09 | INFO | train_inner | epoch 011:    100 / 640 loss=5.515, nll_loss=4.142, ppl=17.65, wps=9134.3, ups=1.18, wpb=7764.1, bsz=297.9, num_updates=6500, lr=0.000156893, gnorm=0.984, clip=0, train_wall=70, wall=8557
2020-10-11 07:19:22 | INFO | train_inner | epoch 011:    200 / 640 loss=5.559, nll_loss=4.189, ppl=18.24, wps=10819.1, ups=1.37, wpb=7877, bsz=282.4, num_updates=6600, lr=0.0001557, gnorm=0.93, clip=0, train_wall=71, wall=8630
2020-10-11 07:20:35 | INFO | train_inner | epoch 011:    300 / 640 loss=5.55, nll_loss=4.179, ppl=18.11, wps=10622.1, ups=1.37, wpb=7749.6, bsz=293.5, num_updates=6700, lr=0.000154533, gnorm=1.013, clip=0, train_wall=71, wall=8702
2020-10-11 07:21:48 | INFO | train_inner | epoch 011:    400 / 640 loss=5.51, nll_loss=4.132, ppl=17.54, wps=10807.2, ups=1.37, wpb=7915.6, bsz=287.6, num_updates=6800, lr=0.000153393, gnorm=0.906, clip=0, train_wall=71, wall=8776
2020-10-11 07:23:01 | INFO | train_inner | epoch 011:    500 / 640 loss=5.478, nll_loss=4.095, ppl=17.09, wps=10746.8, ups=1.38, wpb=7766, bsz=314.2, num_updates=6900, lr=0.000152277, gnorm=0.947, clip=0, train_wall=71, wall=8848
2020-10-11 07:24:14 | INFO | train_inner | epoch 011:    600 / 640 loss=5.513, nll_loss=4.134, ppl=17.56, wps=10825.3, ups=1.37, wpb=7914, bsz=295.1, num_updates=7000, lr=0.000151186, gnorm=0.954, clip=0, train_wall=71, wall=8921
2020-10-11 07:24:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 07:24:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57108.90625Mb; avail=135730.6640625Mb
2020-10-11 07:24:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001630
2020-10-11 07:24:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57108.90625Mb; avail=135730.6640625Mb
2020-10-11 07:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079388
2020-10-11 07:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57116.25390625Mb; avail=135723.31640625Mb
2020-10-11 07:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064318
2020-10-11 07:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.146551
2020-10-11 07:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57108.79296875Mb; avail=135730.77734375Mb
2020-10-11 07:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57108.79296875Mb; avail=135730.77734375Mb
2020-10-11 07:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001362
2020-10-11 07:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57108.79296875Mb; avail=135730.77734375Mb
2020-10-11 07:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075912
2020-10-11 07:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57108.79296875Mb; avail=135730.77734375Mb
2020-10-11 07:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060433
2020-10-11 07:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138720
2020-10-11 07:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57108.79296875Mb; avail=135730.77734375Mb
2020-10-11 07:24:48 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 5.293 | nll_loss 3.777 | ppl 13.71 | wps 24068.4 | wpb 2380 | bsz 90.7 | num_updates 7040 | best_loss 5.293
2020-10-11 07:24:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 07:24:59 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 11 @ 7040 updates, score 5.293) (writing took 10.629857916384935 seconds)
2020-10-11 07:24:59 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-10-11 07:24:59 | INFO | train | epoch 011 | loss 5.518 | nll_loss 4.142 | ppl 17.66 | wps 10354.6 | ups 1.32 | wpb 7819.3 | bsz 294.3 | num_updates 7040 | lr 0.000150756 | gnorm 0.954 | clip 0 | train_wall 454 | wall 8966
2020-10-11 07:24:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11
2020-10-11 07:24:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12
2020-10-11 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57038.7890625Mb; avail=135801.00390625Mb
2020-10-11 07:24:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004820
2020-10-11 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.040052
2020-10-11 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57041.6171875Mb; avail=135798.17578125Mb
2020-10-11 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001630
2020-10-11 07:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57041.6171875Mb; avail=135798.17578125Mb
2020-10-11 07:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.868054
2020-10-11 07:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.911006
2020-10-11 07:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57059.76171875Mb; avail=135780.515625Mb
2020-10-11 07:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57048.2578125Mb; avail=135792.01953125Mb
2020-10-11 07:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.028140
2020-10-11 07:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57051.109375Mb; avail=135789.16796875Mb
2020-10-11 07:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001569
2020-10-11 07:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57051.609375Mb; avail=135788.66796875Mb
2020-10-11 07:25:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.861246
2020-10-11 07:25:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.892203
2020-10-11 07:25:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57048.94921875Mb; avail=135791.13671875Mb
2020-10-11 07:25:01 | INFO | fairseq.trainer | begin training epoch 12
2020-10-11 07:25:43 | INFO | train_inner | epoch 012:     60 / 640 loss=5.449, nll_loss=4.062, ppl=16.7, wps=8713.9, ups=1.12, wpb=7804.7, bsz=279.6, num_updates=7100, lr=0.000150117, gnorm=0.944, clip=0, train_wall=70, wall=9011
2020-10-11 07:26:58 | INFO | train_inner | epoch 012:    160 / 640 loss=5.413, nll_loss=4.021, ppl=16.23, wps=10585.8, ups=1.33, wpb=7962.8, bsz=296, num_updates=7200, lr=0.000149071, gnorm=0.966, clip=0, train_wall=73, wall=9086
2020-10-11 07:28:12 | INFO | train_inner | epoch 012:    260 / 640 loss=5.371, nll_loss=3.973, ppl=15.71, wps=10636.3, ups=1.36, wpb=7810.3, bsz=273.8, num_updates=7300, lr=0.000148047, gnorm=0.942, clip=0, train_wall=72, wall=9159
2020-10-11 07:29:25 | INFO | train_inner | epoch 012:    360 / 640 loss=5.338, nll_loss=3.935, ppl=15.29, wps=10574.1, ups=1.37, wpb=7741.8, bsz=291.4, num_updates=7400, lr=0.000147043, gnorm=0.967, clip=0, train_wall=71, wall=9233
2020-10-11 07:30:38 | INFO | train_inner | epoch 012:    460 / 640 loss=5.294, nll_loss=3.884, ppl=14.76, wps=10754.3, ups=1.37, wpb=7878.4, bsz=313, num_updates=7500, lr=0.000146059, gnorm=0.919, clip=0, train_wall=71, wall=9306
2020-10-11 07:31:51 | INFO | train_inner | epoch 012:    560 / 640 loss=5.362, nll_loss=3.961, ppl=15.57, wps=10601.3, ups=1.38, wpb=7671.6, bsz=276.2, num_updates=7600, lr=0.000145095, gnorm=0.972, clip=0, train_wall=71, wall=9378
2020-10-11 07:32:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 07:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57894.015625Mb; avail=134945.01953125Mb
2020-10-11 07:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001595
2020-10-11 07:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57894.015625Mb; avail=134945.0546875Mb
2020-10-11 07:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076850
2020-10-11 07:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57892.62890625Mb; avail=134946.40625Mb
2020-10-11 07:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063781
2020-10-11 07:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.143295
2020-10-11 07:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57895.9453125Mb; avail=134943.08984375Mb
2020-10-11 07:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57884.60546875Mb; avail=134954.4296875Mb
2020-10-11 07:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001332
2020-10-11 07:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57884.60546875Mb; avail=134954.4296875Mb
2020-10-11 07:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075586
2020-10-11 07:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57884.9765625Mb; avail=134954.26953125Mb
2020-10-11 07:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061678
2020-10-11 07:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139612
2020-10-11 07:32:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57884.9765625Mb; avail=134954.26953125Mb
2020-10-11 07:33:06 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.179 | nll_loss 3.64 | ppl 12.46 | wps 11885 | wpb 2380 | bsz 90.7 | num_updates 7680 | best_loss 5.179
2020-10-11 07:33:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 07:33:29 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 12 @ 7680 updates, score 5.179) (writing took 23.14275210723281 seconds)
2020-10-11 07:33:29 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-10-11 07:33:29 | INFO | train | epoch 012 | loss 5.351 | nll_loss 3.949 | ppl 15.45 | wps 9808.3 | ups 1.25 | wpb 7819.3 | bsz 294.3 | num_updates 7680 | lr 0.000144338 | gnorm 0.951 | clip 0 | train_wall 463 | wall 9476
2020-10-11 07:33:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12
2020-10-11 07:33:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13
2020-10-11 07:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57823.56640625Mb; avail=135015.15234375Mb
2020-10-11 07:33:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006967
2020-10-11 07:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.054628
2020-10-11 07:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57829.64453125Mb; avail=135009.2890625Mb
2020-10-11 07:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001908
2020-10-11 07:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57829.64453125Mb; avail=135009.2890625Mb
2020-10-11 07:33:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.855099
2020-10-11 07:33:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.913070
2020-10-11 07:33:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57838.8203125Mb; avail=135000.11328125Mb
2020-10-11 07:33:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57828.11328125Mb; avail=135010.8203125Mb
2020-10-11 07:33:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027283
2020-10-11 07:33:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57828.11328125Mb; avail=135010.8203125Mb
2020-10-11 07:33:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001384
2020-10-11 07:33:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57828.11328125Mb; avail=135010.8203125Mb
2020-10-11 07:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.839873
2020-10-11 07:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.869696
2020-10-11 07:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57842.109375Mb; avail=134996.9375Mb
2020-10-11 07:33:31 | INFO | fairseq.trainer | begin training epoch 13
2020-10-11 07:34:01 | INFO | train_inner | epoch 013:     20 / 640 loss=5.257, nll_loss=3.842, ppl=14.34, wps=5994.3, ups=0.77, wpb=7814.6, bsz=323, num_updates=7700, lr=0.00014415, gnorm=0.943, clip=0, train_wall=93, wall=9509
2020-10-11 07:36:32 | INFO | train_inner | epoch 013:    120 / 640 loss=5.254, nll_loss=3.839, ppl=14.31, wps=5056.1, ups=0.66, wpb=7644.8, bsz=272.6, num_updates=7800, lr=0.000143223, gnorm=0.958, clip=0, train_wall=149, wall=9660
2020-10-11 07:39:04 | INFO | train_inner | epoch 013:    220 / 640 loss=5.219, nll_loss=3.799, ppl=13.91, wps=5218.5, ups=0.66, wpb=7918.4, bsz=286.1, num_updates=7900, lr=0.000142314, gnorm=0.925, clip=0, train_wall=150, wall=9811
2020-10-11 07:41:32 | INFO | train_inner | epoch 013:    320 / 640 loss=5.2, nll_loss=3.776, ppl=13.7, wps=5276.4, ups=0.67, wpb=7829.5, bsz=295.2, num_updates=8000, lr=0.000141421, gnorm=0.955, clip=0, train_wall=147, wall=9960
2020-10-11 07:44:05 | INFO | train_inner | epoch 013:    420 / 640 loss=5.221, nll_loss=3.8, ppl=13.93, wps=5171.6, ups=0.66, wpb=7885.6, bsz=310.8, num_updates=8100, lr=0.000140546, gnorm=0.958, clip=0, train_wall=151, wall=10112
2020-10-11 07:46:36 | INFO | train_inner | epoch 013:    520 / 640 loss=5.173, nll_loss=3.745, ppl=13.41, wps=5132.2, ups=0.66, wpb=7779.4, bsz=306.6, num_updates=8200, lr=0.000139686, gnorm=0.929, clip=0, train_wall=150, wall=10264
2020-10-11 07:49:07 | INFO | train_inner | epoch 013:    620 / 640 loss=5.198, nll_loss=3.772, ppl=13.66, wps=5230.2, ups=0.66, wpb=7874.5, bsz=296.9, num_updates=8300, lr=0.000138842, gnorm=0.916, clip=0, train_wall=149, wall=10414
2020-10-11 07:49:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 07:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58919.61328125Mb; avail=133918.25Mb
2020-10-11 07:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002155
2020-10-11 07:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58919.265625Mb; avail=133918.25Mb
2020-10-11 07:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.092813
2020-10-11 07:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58927.109375Mb; avail=133910.421875Mb
2020-10-11 07:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062628
2020-10-11 07:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.158893
2020-10-11 07:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58926.65234375Mb; avail=133910.87890625Mb
2020-10-11 07:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58918.28515625Mb; avail=133919.24609375Mb
2020-10-11 07:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001441
2020-10-11 07:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58918.28515625Mb; avail=133919.24609375Mb
2020-10-11 07:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079205
2020-10-11 07:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58918.05859375Mb; avail=133919.90234375Mb
2020-10-11 07:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.065856
2020-10-11 07:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.147725
2020-10-11 07:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58928.51171875Mb; avail=133909.44921875Mb
2020-10-11 07:49:47 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 5.068 | nll_loss 3.522 | ppl 11.49 | wps 12143.5 | wpb 2380 | bsz 90.7 | num_updates 8320 | best_loss 5.068
2020-10-11 07:49:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 07:50:15 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 13 @ 8320 updates, score 5.068) (writing took 27.22191008552909 seconds)
2020-10-11 07:50:15 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-10-11 07:50:15 | INFO | train | epoch 013 | loss 5.209 | nll_loss 3.786 | ppl 13.79 | wps 4975.7 | ups 0.64 | wpb 7819.3 | bsz 294.3 | num_updates 8320 | lr 0.000138675 | gnorm 0.94 | clip 0 | train_wall 954 | wall 10482
2020-10-11 07:50:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13
2020-10-11 07:50:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14
2020-10-11 07:50:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58852.75Mb; avail=133985.0Mb
2020-10-11 07:50:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006990
2020-10-11 07:50:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.055782
2020-10-11 07:50:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58855.09375Mb; avail=133982.65625Mb
2020-10-11 07:50:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001956
2020-10-11 07:50:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58855.09375Mb; avail=133982.65625Mb
2020-10-11 07:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.886851
2020-10-11 07:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.945973
2020-10-11 07:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58863.0234375Mb; avail=133974.97265625Mb
2020-10-11 07:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58850.71875Mb; avail=133987.27734375Mb
2020-10-11 07:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027015
2020-10-11 07:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58850.2109375Mb; avail=133987.78515625Mb
2020-10-11 07:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001336
2020-10-11 07:50:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58850.2109375Mb; avail=133987.78515625Mb
2020-10-11 07:50:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.890335
2020-10-11 07:50:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.919988
2020-10-11 07:50:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58863.82421875Mb; avail=133974.3828125Mb
2020-10-11 07:50:17 | INFO | fairseq.trainer | begin training epoch 14
2020-10-11 07:52:17 | INFO | train_inner | epoch 014:     80 / 640 loss=5.12, nll_loss=3.685, ppl=12.86, wps=4039.6, ups=0.53, wpb=7687.9, bsz=275.5, num_updates=8400, lr=0.000138013, gnorm=0.941, clip=0, train_wall=149, wall=10605
2020-10-11 07:54:49 | INFO | train_inner | epoch 014:    180 / 640 loss=5.087, nll_loss=3.646, ppl=12.52, wps=5074.8, ups=0.66, wpb=7682.8, bsz=312.7, num_updates=8500, lr=0.000137199, gnorm=0.975, clip=0, train_wall=150, wall=10756
2020-10-11 07:57:20 | INFO | train_inner | epoch 014:    280 / 640 loss=5.112, nll_loss=3.675, ppl=12.77, wps=5214.1, ups=0.66, wpb=7904.6, bsz=292.8, num_updates=8600, lr=0.000136399, gnorm=0.942, clip=0, train_wall=150, wall=10908
2020-10-11 07:59:52 | INFO | train_inner | epoch 014:    380 / 640 loss=5.058, nll_loss=3.614, ppl=12.24, wps=5124.9, ups=0.66, wpb=7792.3, bsz=299.6, num_updates=8700, lr=0.000135613, gnorm=0.919, clip=0, train_wall=150, wall=11060
2020-10-11 08:02:25 | INFO | train_inner | epoch 014:    480 / 640 loss=5.119, nll_loss=3.682, ppl=12.83, wps=5147.9, ups=0.66, wpb=7845.1, bsz=293.3, num_updates=8800, lr=0.00013484, gnorm=0.931, clip=0, train_wall=151, wall=11212
2020-10-11 08:04:56 | INFO | train_inner | epoch 014:    580 / 640 loss=5.069, nll_loss=3.625, ppl=12.34, wps=5164.8, ups=0.66, wpb=7828.2, bsz=290.3, num_updates=8900, lr=0.00013408, gnorm=0.971, clip=0, train_wall=150, wall=11364
2020-10-11 08:06:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 08:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58914.76171875Mb; avail=133922.51953125Mb
2020-10-11 08:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001808
2020-10-11 08:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58915.2734375Mb; avail=133921.9140625Mb
2020-10-11 08:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.084993
2020-10-11 08:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58929.515625Mb; avail=133908.27734375Mb
2020-10-11 08:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064555
2020-10-11 08:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.152597
2020-10-11 08:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58923.1171875Mb; avail=133918.1328125Mb
2020-10-11 08:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58913.2734375Mb; avail=133924.53125Mb
2020-10-11 08:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001306
2020-10-11 08:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58913.2734375Mb; avail=133924.53125Mb
2020-10-11 08:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075225
2020-10-11 08:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58913.421875Mb; avail=133924.57421875Mb
2020-10-11 08:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061414
2020-10-11 08:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138982
2020-10-11 08:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58913.4296875Mb; avail=133924.56640625Mb
2020-10-11 08:06:39 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 4.996 | nll_loss 3.432 | ppl 10.79 | wps 11969.6 | wpb 2380 | bsz 90.7 | num_updates 8960 | best_loss 4.996
2020-10-11 08:06:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 08:07:17 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 14 @ 8960 updates, score 4.996) (writing took 38.65104245766997 seconds)
2020-10-11 08:07:17 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-10-11 08:07:17 | INFO | train | epoch 014 | loss 5.091 | nll_loss 3.651 | ppl 12.56 | wps 4893.3 | ups 0.63 | wpb 7819.3 | bsz 294.3 | num_updates 8960 | lr 0.000133631 | gnorm 0.946 | clip 0 | train_wall 960 | wall 11505
2020-10-11 08:07:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14
2020-10-11 08:07:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15
2020-10-11 08:07:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58869.234375Mb; avail=133968.515625Mb
2020-10-11 08:07:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004911
2020-10-11 08:07:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.038397
2020-10-11 08:07:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58862.92578125Mb; avail=133974.82421875Mb
2020-10-11 08:07:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001546
2020-10-11 08:07:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58862.92578125Mb; avail=133974.82421875Mb
2020-10-11 08:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.871192
2020-10-11 08:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.912277
2020-10-11 08:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58863.9140625Mb; avail=133974.1328125Mb
2020-10-11 08:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58860.08984375Mb; avail=133977.95703125Mb
2020-10-11 08:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026738
2020-10-11 08:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58860.08984375Mb; avail=133977.95703125Mb
2020-10-11 08:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001283
2020-10-11 08:07:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58860.08984375Mb; avail=133977.95703125Mb
2020-10-11 08:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.863345
2020-10-11 08:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.892431
2020-10-11 08:07:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58876.30078125Mb; avail=133961.51953125Mb
2020-10-11 08:07:19 | INFO | fairseq.trainer | begin training epoch 15
2020-10-11 08:08:19 | INFO | train_inner | epoch 015:     40 / 640 loss=5.072, nll_loss=3.629, ppl=12.37, wps=3804.2, ups=0.49, wpb=7719.7, bsz=282.1, num_updates=9000, lr=0.000133333, gnorm=0.956, clip=0, train_wall=150, wall=11567
2020-10-11 08:10:51 | INFO | train_inner | epoch 015:    140 / 640 loss=4.941, nll_loss=3.481, ppl=11.16, wps=5199.1, ups=0.66, wpb=7909.6, bsz=297.9, num_updates=9100, lr=0.000132599, gnorm=0.894, clip=0, train_wall=150, wall=11719
2020-10-11 08:13:25 | INFO | train_inner | epoch 015:    240 / 640 loss=4.978, nll_loss=3.521, ppl=11.48, wps=5197.7, ups=0.65, wpb=7978.5, bsz=330.8, num_updates=9200, lr=0.000131876, gnorm=0.914, clip=0, train_wall=152, wall=11872
2020-10-11 08:15:55 | INFO | train_inner | epoch 015:    340 / 640 loss=4.999, nll_loss=3.546, ppl=11.68, wps=5206.1, ups=0.67, wpb=7821.4, bsz=268.6, num_updates=9300, lr=0.000131165, gnorm=0.948, clip=0, train_wall=148, wall=12023
2020-10-11 08:18:27 | INFO | train_inner | epoch 015:    440 / 640 loss=4.99, nll_loss=3.535, ppl=11.59, wps=5071.7, ups=0.66, wpb=7720.7, bsz=316.9, num_updates=9400, lr=0.000130466, gnorm=0.966, clip=0, train_wall=150, wall=12175
2020-10-11 08:20:59 | INFO | train_inner | epoch 015:    540 / 640 loss=4.981, nll_loss=3.524, ppl=11.5, wps=5224.7, ups=0.66, wpb=7915.5, bsz=293.1, num_updates=9500, lr=0.000129777, gnorm=0.91, clip=0, train_wall=150, wall=12326
2020-10-11 08:22:14 | INFO | train_inner | epoch 015:    640 / 640 loss=4.99, nll_loss=3.535, ppl=11.59, wps=10379.5, ups=1.33, wpb=7807.3, bsz=272.6, num_updates=9600, lr=0.000129099, gnorm=0.941, clip=0, train_wall=73, wall=12402
2020-10-11 08:22:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 08:22:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58064.15625Mb; avail=134774.6328125Mb
2020-10-11 08:22:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002684
2020-10-11 08:22:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58064.15625Mb; avail=134774.6328125Mb
2020-10-11 08:22:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.127858
2020-10-11 08:22:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58064.27734375Mb; avail=134774.51171875Mb
2020-10-11 08:22:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.066229
2020-10-11 08:22:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.198313
2020-10-11 08:22:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58064.27734375Mb; avail=134774.51171875Mb
2020-10-11 08:22:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58065.44140625Mb; avail=134773.34765625Mb
2020-10-11 08:22:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001292
2020-10-11 08:22:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58065.44140625Mb; avail=134773.34765625Mb
2020-10-11 08:22:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081123
2020-10-11 08:22:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58071.49609375Mb; avail=134767.29296875Mb
2020-10-11 08:22:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062579
2020-10-11 08:22:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.146172
2020-10-11 08:22:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58064.60546875Mb; avail=134774.18359375Mb
2020-10-11 08:22:20 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 4.916 | nll_loss 3.339 | ppl 10.12 | wps 24280.8 | wpb 2380 | bsz 90.7 | num_updates 9600 | best_loss 4.916
2020-10-11 08:22:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 08:22:26 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 15 @ 9600 updates, score 4.916) (writing took 5.8288694098591805 seconds)
2020-10-11 08:22:26 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-10-11 08:22:26 | INFO | train | epoch 015 | loss 4.985 | nll_loss 3.53 | ppl 11.55 | wps 5509.7 | ups 0.7 | wpb 7819.3 | bsz 294.3 | num_updates 9600 | lr 0.000129099 | gnorm 0.933 | clip 0 | train_wall 883 | wall 12413
2020-10-11 08:22:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15
2020-10-11 08:22:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16
2020-10-11 08:22:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58013.8203125Mb; avail=134824.96875Mb
2020-10-11 08:22:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008703
2020-10-11 08:22:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.064692
2020-10-11 08:22:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58026.05078125Mb; avail=134812.73828125Mb
2020-10-11 08:22:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002096
2020-10-11 08:22:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58026.05078125Mb; avail=134812.73828125Mb
2020-10-11 08:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.863878
2020-10-11 08:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.932275
2020-10-11 08:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58015.17578125Mb; avail=134823.84375Mb
2020-10-11 08:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58011.3515625Mb; avail=134827.66796875Mb
2020-10-11 08:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.026640
2020-10-11 08:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58011.3515625Mb; avail=134827.66796875Mb
2020-10-11 08:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001397
2020-10-11 08:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58011.3515625Mb; avail=134827.66796875Mb
2020-10-11 08:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.847340
2020-10-11 08:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.876497
2020-10-11 08:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58024.203125Mb; avail=134814.81640625Mb
2020-10-11 08:22:27 | INFO | fairseq.trainer | begin training epoch 16
2020-10-11 08:23:40 | INFO | train_inner | epoch 016:    100 / 640 loss=4.931, nll_loss=3.468, ppl=11.07, wps=8953.5, ups=1.16, wpb=7725.4, bsz=275.2, num_updates=9700, lr=0.000128432, gnorm=0.934, clip=0, train_wall=71, wall=12488
2020-10-11 08:24:53 | INFO | train_inner | epoch 016:    200 / 640 loss=4.86, nll_loss=3.387, ppl=10.46, wps=10610.8, ups=1.37, wpb=7742, bsz=323.4, num_updates=9800, lr=0.000127775, gnorm=0.931, clip=0, train_wall=71, wall=12561
2020-10-11 08:26:06 | INFO | train_inner | epoch 016:    300 / 640 loss=4.873, nll_loss=3.402, ppl=10.57, wps=10767.7, ups=1.37, wpb=7863.3, bsz=295.8, num_updates=9900, lr=0.000127128, gnorm=0.929, clip=0, train_wall=71, wall=12634
2020-10-11 08:27:20 | INFO | train_inner | epoch 016:    400 / 640 loss=4.888, nll_loss=3.42, ppl=10.7, wps=10792, ups=1.36, wpb=7925.8, bsz=300.2, num_updates=10000, lr=0.000126491, gnorm=0.909, clip=0, train_wall=72, wall=12707
2020-10-11 08:28:33 | INFO | train_inner | epoch 016:    500 / 640 loss=4.904, nll_loss=3.436, ppl=10.82, wps=10854.2, ups=1.37, wpb=7904, bsz=279.9, num_updates=10100, lr=0.000125863, gnorm=0.956, clip=0, train_wall=71, wall=12780
2020-10-11 08:29:45 | INFO | train_inner | epoch 016:    600 / 640 loss=4.89, nll_loss=3.421, ppl=10.71, wps=10607.9, ups=1.38, wpb=7700.8, bsz=302, num_updates=10200, lr=0.000125245, gnorm=0.942, clip=0, train_wall=71, wall=12853
2020-10-11 08:30:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 08:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58154.20703125Mb; avail=134684.7734375Mb
2020-10-11 08:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001659
2020-10-11 08:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58154.20703125Mb; avail=134684.7734375Mb
2020-10-11 08:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077521
2020-10-11 08:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58160.43359375Mb; avail=134678.546875Mb
2020-10-11 08:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.066450
2020-10-11 08:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.146809
2020-10-11 08:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58171.46875Mb; avail=134667.51171875Mb
2020-10-11 08:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58167.47265625Mb; avail=134671.30078125Mb
2020-10-11 08:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001999
2020-10-11 08:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58168.5625Mb; avail=134670.2109375Mb
2020-10-11 08:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082197
2020-10-11 08:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58170.8359375Mb; avail=134667.9375Mb
2020-10-11 08:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.065512
2020-10-11 08:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.151062
2020-10-11 08:30:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58151.30859375Mb; avail=134687.3671875Mb
2020-10-11 08:30:20 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.854 | nll_loss 3.269 | ppl 9.64 | wps 24078.6 | wpb 2380 | bsz 90.7 | num_updates 10240 | best_loss 4.854
2020-10-11 08:30:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 08:30:42 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 16 @ 10240 updates, score 4.854) (writing took 21.30378533527255 seconds)
2020-10-11 08:30:42 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-10-11 08:30:42 | INFO | train | epoch 016 | loss 4.895 | nll_loss 3.427 | ppl 10.76 | wps 10088.7 | ups 1.29 | wpb 7819.3 | bsz 294.3 | num_updates 10240 | lr 0.000125 | gnorm 0.932 | clip 0 | train_wall 455 | wall 12909
2020-10-11 08:30:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16
2020-10-11 08:30:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17
2020-10-11 08:30:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58094.5Mb; avail=134744.23828125Mb
2020-10-11 08:30:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007246
2020-10-11 08:30:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.053781
2020-10-11 08:30:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58096.8359375Mb; avail=134741.90234375Mb
2020-10-11 08:30:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002413
2020-10-11 08:30:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58096.8359375Mb; avail=134741.90234375Mb
2020-10-11 08:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.940003
2020-10-11 08:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.997854
2020-10-11 08:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58108.859375Mb; avail=134730.36328125Mb
2020-10-11 08:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58099.25Mb; avail=134739.97265625Mb
2020-10-11 08:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027251
2020-10-11 08:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58101.01171875Mb; avail=134738.19921875Mb
2020-10-11 08:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001460
2020-10-11 08:30:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58101.01171875Mb; avail=134738.19921875Mb
2020-10-11 08:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.965956
2020-10-11 08:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.996002
2020-10-11 08:30:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58098.53125Mb; avail=134740.53125Mb
2020-10-11 08:30:44 | INFO | fairseq.trainer | begin training epoch 17
2020-10-11 08:31:27 | INFO | train_inner | epoch 017:     60 / 640 loss=4.895, nll_loss=3.427, ppl=10.75, wps=7698, ups=0.98, wpb=7846.6, bsz=269.8, num_updates=10300, lr=0.000124635, gnorm=0.906, clip=0, train_wall=71, wall=12955
2020-10-11 08:32:42 | INFO | train_inner | epoch 017:    160 / 640 loss=4.88, nll_loss=3.408, ppl=10.62, wps=10361.6, ups=1.34, wpb=7755.7, bsz=265.2, num_updates=10400, lr=0.000124035, gnorm=0.912, clip=0, train_wall=73, wall=13030
2020-10-11 08:33:56 | INFO | train_inner | epoch 017:    260 / 640 loss=4.818, nll_loss=3.339, ppl=10.12, wps=10452.8, ups=1.35, wpb=7766.6, bsz=305.4, num_updates=10500, lr=0.000123443, gnorm=0.927, clip=0, train_wall=73, wall=13104
2020-10-11 08:35:10 | INFO | train_inner | epoch 017:    360 / 640 loss=4.832, nll_loss=3.354, ppl=10.22, wps=10607, ups=1.37, wpb=7761.7, bsz=287, num_updates=10600, lr=0.000122859, gnorm=0.934, clip=0, train_wall=71, wall=13177
2020-10-11 08:36:23 | INFO | train_inner | epoch 017:    460 / 640 loss=4.757, nll_loss=3.27, ppl=9.64, wps=10811.2, ups=1.36, wpb=7937.6, bsz=311.5, num_updates=10700, lr=0.000122284, gnorm=0.891, clip=0, train_wall=72, wall=13250
2020-10-11 08:37:36 | INFO | train_inner | epoch 017:    560 / 640 loss=4.788, nll_loss=3.306, ppl=9.89, wps=10732.2, ups=1.37, wpb=7835.6, bsz=305.8, num_updates=10800, lr=0.000121716, gnorm=0.929, clip=0, train_wall=71, wall=13323
2020-10-11 08:38:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 08:38:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58220.62109375Mb; avail=134618.265625Mb
2020-10-11 08:38:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002363
2020-10-11 08:38:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58220.62109375Mb; avail=134618.265625Mb
2020-10-11 08:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.097713
2020-10-11 08:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58219.96484375Mb; avail=134618.921875Mb
2020-10-11 08:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061450
2020-10-11 08:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.162680
2020-10-11 08:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58220.078125Mb; avail=134618.80859375Mb
2020-10-11 08:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58220.078125Mb; avail=134618.80859375Mb
2020-10-11 08:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001177
2020-10-11 08:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58220.078125Mb; avail=134618.80859375Mb
2020-10-11 08:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077935
2020-10-11 08:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58223.578125Mb; avail=134615.203125Mb
2020-10-11 08:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.066112
2020-10-11 08:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.146296
2020-10-11 08:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58233.17578125Mb; avail=134605.60546875Mb
2020-10-11 08:38:40 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.807 | nll_loss 3.212 | ppl 9.27 | wps 23912.4 | wpb 2380 | bsz 90.7 | num_updates 10880 | best_loss 4.807
2020-10-11 08:38:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 08:39:04 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 17 @ 10880 updates, score 4.807) (writing took 23.509829312562943 seconds)
2020-10-11 08:39:04 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-10-11 08:39:04 | INFO | train | epoch 017 | loss 4.814 | nll_loss 3.334 | ppl 10.08 | wps 9968.4 | ups 1.27 | wpb 7819.3 | bsz 294.3 | num_updates 10880 | lr 0.000121268 | gnorm 0.916 | clip 0 | train_wall 459 | wall 13411
2020-10-11 08:39:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17
2020-10-11 08:39:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18
2020-10-11 08:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58352.734375Mb; avail=134486.28125Mb
2020-10-11 08:39:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007176
2020-10-11 08:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.057747
2020-10-11 08:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58339.58984375Mb; avail=134499.42578125Mb
2020-10-11 08:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002232
2020-10-11 08:39:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58339.58984375Mb; avail=134499.42578125Mb
2020-10-11 08:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.882842
2020-10-11 08:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.944356
2020-10-11 08:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58356.87109375Mb; avail=134481.59375Mb
2020-10-11 08:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58367.9140625Mb; avail=134471.15625Mb
2020-10-11 08:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027357
2020-10-11 08:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58378.20703125Mb; avail=134460.86328125Mb
2020-10-11 08:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001711
2020-10-11 08:39:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58378.8125Mb; avail=134460.2578125Mb
2020-10-11 08:39:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.872124
2020-10-11 08:39:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.902500
2020-10-11 08:39:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58452.65234375Mb; avail=134386.359375Mb
2020-10-11 08:39:06 | INFO | fairseq.trainer | begin training epoch 18
2020-10-11 08:39:20 | INFO | train_inner | epoch 018:     20 / 640 loss=4.797, nll_loss=3.315, ppl=9.95, wps=7541.4, ups=0.97, wpb=7812.8, bsz=296.8, num_updates=10900, lr=0.000121157, gnorm=0.932, clip=0, train_wall=71, wall=13427
2020-10-11 08:41:07 | INFO | train_inner | epoch 018:    120 / 640 loss=4.715, nll_loss=3.221, ppl=9.32, wps=7399.6, ups=0.93, wpb=7978.4, bsz=308.6, num_updates=11000, lr=0.000120605, gnorm=0.869, clip=0, train_wall=106, wall=13535
2020-10-11 08:43:40 | INFO | train_inner | epoch 018:    220 / 640 loss=4.767, nll_loss=3.28, ppl=9.72, wps=5082, ups=0.66, wpb=7746.2, bsz=281, num_updates=11100, lr=0.00012006, gnorm=0.924, clip=0, train_wall=151, wall=13687
2020-10-11 08:46:12 | INFO | train_inner | epoch 018:    320 / 640 loss=4.769, nll_loss=3.283, ppl=9.74, wps=5083.5, ups=0.66, wpb=7720.2, bsz=288.2, num_updates=11200, lr=0.000119523, gnorm=0.912, clip=0, train_wall=150, wall=13839
2020-10-11 08:48:45 | INFO | train_inner | epoch 018:    420 / 640 loss=4.724, nll_loss=3.231, ppl=9.39, wps=5147, ups=0.65, wpb=7891.6, bsz=309.3, num_updates=11300, lr=0.000118993, gnorm=0.896, clip=0, train_wall=152, wall=13992
2020-10-11 08:51:17 | INFO | train_inner | epoch 018:    520 / 640 loss=4.737, nll_loss=3.246, ppl=9.49, wps=5078.2, ups=0.66, wpb=7713.6, bsz=299.6, num_updates=11400, lr=0.00011847, gnorm=0.924, clip=0, train_wall=150, wall=14144
2020-10-11 08:53:49 | INFO | train_inner | epoch 018:    620 / 640 loss=4.717, nll_loss=3.224, ppl=9.34, wps=5273.5, ups=0.66, wpb=8029.8, bsz=295, num_updates=11500, lr=0.000117954, gnorm=0.883, clip=0, train_wall=150, wall=14297
2020-10-11 08:54:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 08:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58453.0859375Mb; avail=134386.0625Mb
2020-10-11 08:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001625
2020-10-11 08:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58453.41796875Mb; avail=134385.73046875Mb
2020-10-11 08:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077476
2020-10-11 08:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58442.9296875Mb; avail=134396.21875Mb
2020-10-11 08:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062630
2020-10-11 08:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142900
2020-10-11 08:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58442.765625Mb; avail=134396.60546875Mb
2020-10-11 08:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58442.36328125Mb; avail=134397.0078125Mb
2020-10-11 08:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001274
2020-10-11 08:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58442.36328125Mb; avail=134397.0078125Mb
2020-10-11 08:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077340
2020-10-11 08:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58441.6796875Mb; avail=134397.69140625Mb
2020-10-11 08:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062477
2020-10-11 08:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142138
2020-10-11 08:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58445.51953125Mb; avail=134393.8515625Mb
2020-10-11 08:54:30 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.758 | nll_loss 3.162 | ppl 8.95 | wps 12014 | wpb 2380 | bsz 90.7 | num_updates 11520 | best_loss 4.758
2020-10-11 08:54:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 08:55:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 18 @ 11520 updates, score 4.758) (writing took 36.96210551261902 seconds)
2020-10-11 08:55:07 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-10-11 08:55:07 | INFO | train | epoch 018 | loss 4.742 | nll_loss 3.252 | ppl 9.53 | wps 5197 | ups 0.66 | wpb 7819.3 | bsz 294.3 | num_updates 11520 | lr 0.000117851 | gnorm 0.906 | clip 0 | train_wall 902 | wall 14374
2020-10-11 08:55:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18
2020-10-11 08:55:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19
2020-10-11 08:55:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58327.9140625Mb; avail=134511.3203125Mb
2020-10-11 08:55:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008060
2020-10-11 08:55:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.053782
2020-10-11 08:55:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58334.546875Mb; avail=134504.6875Mb
2020-10-11 08:55:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002267
2020-10-11 08:55:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58335.09765625Mb; avail=134504.140625Mb
2020-10-11 08:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.879455
2020-10-11 08:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.937088
2020-10-11 08:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58352.55078125Mb; avail=134486.9375Mb
2020-10-11 08:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58352.01953125Mb; avail=134487.46875Mb
2020-10-11 08:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027116
2020-10-11 08:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58354.44140625Mb; avail=134485.046875Mb
2020-10-11 08:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001411
2020-10-11 08:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58354.44140625Mb; avail=134485.046875Mb
2020-10-11 08:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.850020
2020-10-11 08:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.879634
2020-10-11 08:55:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58355.703125Mb; avail=134483.19921875Mb
2020-10-11 08:55:08 | INFO | fairseq.trainer | begin training epoch 19
2020-10-11 08:57:10 | INFO | train_inner | epoch 019:     80 / 640 loss=4.695, nll_loss=3.199, ppl=9.18, wps=3855, ups=0.5, wpb=7729.9, bsz=271.6, num_updates=11600, lr=0.000117444, gnorm=0.937, clip=0, train_wall=149, wall=14497
2020-10-11 08:59:40 | INFO | train_inner | epoch 019:    180 / 640 loss=4.624, nll_loss=3.118, ppl=8.68, wps=5194.4, ups=0.67, wpb=7805.3, bsz=321.8, num_updates=11700, lr=0.000116941, gnorm=0.916, clip=0, train_wall=148, wall=14647
2020-10-11 09:02:13 | INFO | train_inner | epoch 019:    280 / 640 loss=4.714, nll_loss=3.22, ppl=9.32, wps=5135, ups=0.65, wpb=7841.5, bsz=288, num_updates=11800, lr=0.000116445, gnorm=0.922, clip=0, train_wall=151, wall=14800
2020-10-11 09:04:44 | INFO | train_inner | epoch 019:    380 / 640 loss=4.656, nll_loss=3.155, ppl=8.91, wps=5194.2, ups=0.66, wpb=7874.3, bsz=306.1, num_updates=11900, lr=0.000115954, gnorm=0.902, clip=0, train_wall=150, wall=14952
2020-10-11 09:07:16 | INFO | train_inner | epoch 019:    480 / 640 loss=4.697, nll_loss=3.201, ppl=9.2, wps=5137.8, ups=0.66, wpb=7772.4, bsz=288.6, num_updates=12000, lr=0.00011547, gnorm=0.916, clip=0, train_wall=149, wall=15103
2020-10-11 09:09:49 | INFO | train_inner | epoch 019:    580 / 640 loss=4.695, nll_loss=3.197, ppl=9.17, wps=5149.8, ups=0.65, wpb=7880.7, bsz=288.8, num_updates=12100, lr=0.000114992, gnorm=0.9, clip=0, train_wall=151, wall=15256
2020-10-11 09:11:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 09:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59207.6484375Mb; avail=133630.36328125Mb
2020-10-11 09:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002153
2020-10-11 09:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59208.25390625Mb; avail=133629.7578125Mb
2020-10-11 09:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.093820
2020-10-11 09:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59185.234375Mb; avail=133652.77734375Mb
2020-10-11 09:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062924
2020-10-11 09:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.160122
2020-10-11 09:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59185.0703125Mb; avail=133652.609375Mb
2020-10-11 09:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59185.0703125Mb; avail=133652.609375Mb
2020-10-11 09:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001221
2020-10-11 09:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59185.0703125Mb; avail=133652.609375Mb
2020-10-11 09:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076283
2020-10-11 09:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59185.0703125Mb; avail=133652.609375Mb
2020-10-11 09:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062087
2020-10-11 09:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.140533
2020-10-11 09:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59184.578125Mb; avail=133653.1015625Mb
2020-10-11 09:11:30 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.72 | nll_loss 3.115 | ppl 8.67 | wps 12064.7 | wpb 2380 | bsz 90.7 | num_updates 12160 | best_loss 4.72
2020-10-11 09:11:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 09:12:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 19 @ 12160 updates, score 4.72) (writing took 35.92986473441124 seconds)
2020-10-11 09:12:06 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-10-11 09:12:06 | INFO | train | epoch 019 | loss 4.682 | nll_loss 3.184 | ppl 9.09 | wps 4909.7 | ups 0.63 | wpb 7819.3 | bsz 294.3 | num_updates 12160 | lr 0.000114708 | gnorm 0.917 | clip 0 | train_wall 959 | wall 15393
2020-10-11 09:12:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19
2020-10-11 09:12:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20
2020-10-11 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59141.5625Mb; avail=133695.8046875Mb
2020-10-11 09:12:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006962
2020-10-11 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.056086
2020-10-11 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59148.69921875Mb; avail=133688.66796875Mb
2020-10-11 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.003267
2020-10-11 09:12:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59149.19140625Mb; avail=133688.17578125Mb
2020-10-11 09:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.415756
2020-10-11 09:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.477267
2020-10-11 09:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59149.01171875Mb; avail=133688.72265625Mb
2020-10-11 09:12:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59148.0703125Mb; avail=133689.6640625Mb
2020-10-11 09:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027630
2020-10-11 09:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59137.38671875Mb; avail=133700.2734375Mb
2020-10-11 09:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001349
2020-10-11 09:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59137.38671875Mb; avail=133700.2734375Mb
2020-10-11 09:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.861550
2020-10-11 09:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.891806
2020-10-11 09:12:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59140.85546875Mb; avail=133696.9921875Mb
2020-10-11 09:12:08 | INFO | fairseq.trainer | begin training epoch 20
2020-10-11 09:13:10 | INFO | train_inner | epoch 020:     40 / 640 loss=4.686, nll_loss=3.188, ppl=9.11, wps=3829.1, ups=0.5, wpb=7694.2, bsz=287.4, num_updates=12200, lr=0.00011452, gnorm=0.928, clip=0, train_wall=150, wall=15457
2020-10-11 09:15:42 | INFO | train_inner | epoch 020:    140 / 640 loss=4.588, nll_loss=3.078, ppl=8.44, wps=5137.7, ups=0.66, wpb=7835, bsz=295.5, num_updates=12300, lr=0.000114053, gnorm=0.888, clip=0, train_wall=151, wall=15609
2020-10-11 09:18:13 | INFO | train_inner | epoch 020:    240 / 640 loss=4.622, nll_loss=3.116, ppl=8.67, wps=5176.8, ups=0.66, wpb=7825.5, bsz=281, num_updates=12400, lr=0.000113592, gnorm=0.885, clip=0, train_wall=149, wall=15761
2020-10-11 09:20:45 | INFO | train_inner | epoch 020:    340 / 640 loss=4.644, nll_loss=3.14, ppl=8.82, wps=5080.9, ups=0.66, wpb=7710.1, bsz=295.5, num_updates=12500, lr=0.000113137, gnorm=0.94, clip=0, train_wall=150, wall=15912
2020-10-11 09:23:17 | INFO | train_inner | epoch 020:    440 / 640 loss=4.639, nll_loss=3.135, ppl=8.78, wps=5154.8, ups=0.66, wpb=7836.2, bsz=281.7, num_updates=12600, lr=0.000112687, gnorm=0.917, clip=0, train_wall=150, wall=16064
2020-10-11 09:25:50 | INFO | train_inner | epoch 020:    540 / 640 loss=4.607, nll_loss=3.098, ppl=8.56, wps=5135.4, ups=0.65, wpb=7856.8, bsz=321.1, num_updates=12700, lr=0.000112243, gnorm=0.925, clip=0, train_wall=151, wall=16217
2020-10-11 09:27:43 | INFO | train_inner | epoch 020:    640 / 640 loss=4.632, nll_loss=3.126, ppl=8.73, wps=7007.9, ups=0.89, wpb=7896.9, bsz=291.4, num_updates=12800, lr=0.000111803, gnorm=0.897, clip=0, train_wall=111, wall=16330
2020-10-11 09:27:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 09:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58608.94921875Mb; avail=134229.796875Mb
2020-10-11 09:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001615
2020-10-11 09:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58608.94921875Mb; avail=134229.796875Mb
2020-10-11 09:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080554
2020-10-11 09:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58609.46875Mb; avail=134229.27734375Mb
2020-10-11 09:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.065819
2020-10-11 09:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.149309
2020-10-11 09:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58617.3125Mb; avail=134221.43359375Mb
2020-10-11 09:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58618.8125Mb; avail=134220.12109375Mb
2020-10-11 09:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001370
2020-10-11 09:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58618.8125Mb; avail=134220.12109375Mb
2020-10-11 09:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082271
2020-10-11 09:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58632.0078125Mb; avail=134206.8125Mb
2020-10-11 09:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064516
2020-10-11 09:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.149341
2020-10-11 09:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58636.421875Mb; avail=134202.828125Mb
2020-10-11 09:27:48 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.688 | nll_loss 3.076 | ppl 8.43 | wps 23910 | wpb 2380 | bsz 90.7 | num_updates 12800 | best_loss 4.688
2020-10-11 09:27:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 09:27:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 20 @ 12800 updates, score 4.688) (writing took 6.127689104527235 seconds)
2020-10-11 09:27:54 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-10-11 09:27:54 | INFO | train | epoch 020 | loss 4.622 | nll_loss 3.116 | ppl 8.67 | wps 5276 | ups 0.67 | wpb 7819.3 | bsz 294.3 | num_updates 12800 | lr 0.000111803 | gnorm 0.909 | clip 0 | train_wall 922 | wall 16342
2020-10-11 09:27:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20
2020-10-11 09:27:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21
2020-10-11 09:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58519.3125Mb; avail=134319.46875Mb
2020-10-11 09:27:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005066
2020-10-11 09:27:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.041098
2020-10-11 09:27:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58512.76953125Mb; avail=134326.01171875Mb
2020-10-11 09:27:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001811
2020-10-11 09:27:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58512.76953125Mb; avail=134326.01171875Mb
2020-10-11 09:27:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.883840
2020-10-11 09:27:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.927998
2020-10-11 09:27:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58314.38671875Mb; avail=134524.8671875Mb
2020-10-11 09:27:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58317.8984375Mb; avail=134521.35546875Mb
2020-10-11 09:27:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027221
2020-10-11 09:27:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58321.1875Mb; avail=134518.06640625Mb
2020-10-11 09:27:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001408
2020-10-11 09:27:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58321.1875Mb; avail=134518.06640625Mb
2020-10-11 09:27:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.859860
2020-10-11 09:27:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.889815
2020-10-11 09:27:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58307.93359375Mb; avail=134531.05078125Mb
2020-10-11 09:27:56 | INFO | fairseq.trainer | begin training epoch 21
2020-10-11 09:29:09 | INFO | train_inner | epoch 021:    100 / 640 loss=4.551, nll_loss=3.035, ppl=8.19, wps=9073.7, ups=1.16, wpb=7805.1, bsz=293.5, num_updates=12900, lr=0.000111369, gnorm=0.904, clip=0, train_wall=70, wall=16416
2020-10-11 09:30:22 | INFO | train_inner | epoch 021:    200 / 640 loss=4.539, nll_loss=3.021, ppl=8.12, wps=10618.3, ups=1.36, wpb=7784, bsz=303.8, num_updates=13000, lr=0.00011094, gnorm=0.892, clip=0, train_wall=72, wall=16489
2020-10-11 09:31:35 | INFO | train_inner | epoch 021:    300 / 640 loss=4.56, nll_loss=3.045, ppl=8.25, wps=10613.1, ups=1.37, wpb=7773.7, bsz=298.7, num_updates=13100, lr=0.000110516, gnorm=0.888, clip=0, train_wall=71, wall=16563
2020-10-11 09:32:48 | INFO | train_inner | epoch 021:    400 / 640 loss=4.602, nll_loss=3.093, ppl=8.53, wps=10720.7, ups=1.37, wpb=7823.3, bsz=275.6, num_updates=13200, lr=0.000110096, gnorm=0.901, clip=0, train_wall=71, wall=16636
2020-10-11 09:34:02 | INFO | train_inner | epoch 021:    500 / 640 loss=4.587, nll_loss=3.075, ppl=8.42, wps=10801.4, ups=1.35, wpb=7975.6, bsz=307.7, num_updates=13300, lr=0.000109682, gnorm=0.902, clip=0, train_wall=72, wall=16710
2020-10-11 09:35:15 | INFO | train_inner | epoch 021:    600 / 640 loss=4.582, nll_loss=3.07, ppl=8.4, wps=10791.8, ups=1.38, wpb=7822.9, bsz=286.4, num_updates=13400, lr=0.000109272, gnorm=0.915, clip=0, train_wall=71, wall=16782
2020-10-11 09:35:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 09:35:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58382.48046875Mb; avail=134456.08984375Mb
2020-10-11 09:35:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002067
2020-10-11 09:35:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58382.48046875Mb; avail=134456.08984375Mb
2020-10-11 09:35:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.087896
2020-10-11 09:35:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58376.15234375Mb; avail=134462.41796875Mb
2020-10-11 09:35:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063188
2020-10-11 09:35:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.154416
2020-10-11 09:35:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58380.28515625Mb; avail=134458.28515625Mb
2020-10-11 09:35:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58382.1015625Mb; avail=134456.46875Mb
2020-10-11 09:35:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001378
2020-10-11 09:35:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58382.1015625Mb; avail=134456.46875Mb
2020-10-11 09:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077820
2020-10-11 09:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58377.11328125Mb; avail=134461.45703125Mb
2020-10-11 09:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061641
2020-10-11 09:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141947
2020-10-11 09:35:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58377.11328125Mb; avail=134461.45703125Mb
2020-10-11 09:35:49 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 4.651 | nll_loss 3.032 | ppl 8.18 | wps 23604.8 | wpb 2380 | bsz 90.7 | num_updates 13440 | best_loss 4.651
2020-10-11 09:35:49 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 09:35:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 21 @ 13440 updates, score 4.651) (writing took 4.973351307213306 seconds)
2020-10-11 09:35:54 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-10-11 09:35:54 | INFO | train | epoch 021 | loss 4.569 | nll_loss 3.055 | ppl 8.31 | wps 10436.1 | ups 1.33 | wpb 7819.3 | bsz 294.3 | num_updates 13440 | lr 0.000109109 | gnorm 0.9 | clip 0 | train_wall 455 | wall 16821
2020-10-11 09:35:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21
2020-10-11 09:35:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22
2020-10-11 09:35:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58327.7734375Mb; avail=134511.328125Mb
2020-10-11 09:35:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005673
2020-10-11 09:35:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.042936
2020-10-11 09:35:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58330.1171875Mb; avail=134508.984375Mb
2020-10-11 09:35:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001622
2020-10-11 09:35:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58330.05078125Mb; avail=134509.05078125Mb
2020-10-11 09:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.845752
2020-10-11 09:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.891477
2020-10-11 09:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58339.00390625Mb; avail=134500.01171875Mb
2020-10-11 09:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58336.37890625Mb; avail=134502.4375Mb
2020-10-11 09:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027697
2020-10-11 09:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58337.58984375Mb; avail=134501.2265625Mb
2020-10-11 09:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001555
2020-10-11 09:35:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58337.58984375Mb; avail=134501.2265625Mb
2020-10-11 09:35:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.853832
2020-10-11 09:35:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.884286
2020-10-11 09:35:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58332.58984375Mb; avail=134506.43359375Mb
2020-10-11 09:35:56 | INFO | fairseq.trainer | begin training epoch 22
2020-10-11 09:36:39 | INFO | train_inner | epoch 022:     60 / 640 loss=4.492, nll_loss=2.969, ppl=7.83, wps=9114.1, ups=1.19, wpb=7655.6, bsz=304.2, num_updates=13500, lr=0.000108866, gnorm=0.904, clip=0, train_wall=70, wall=16866
2020-10-11 09:37:52 | INFO | train_inner | epoch 022:    160 / 640 loss=4.547, nll_loss=3.03, ppl=8.17, wps=10530.1, ups=1.36, wpb=7766.6, bsz=274.9, num_updates=13600, lr=0.000108465, gnorm=0.898, clip=0, train_wall=72, wall=16940
2020-10-11 09:39:06 | INFO | train_inner | epoch 022:    260 / 640 loss=4.527, nll_loss=3.007, ppl=8.04, wps=10659.9, ups=1.36, wpb=7863.4, bsz=296.3, num_updates=13700, lr=0.000108069, gnorm=0.893, clip=0, train_wall=72, wall=17014
2020-10-11 09:40:19 | INFO | train_inner | epoch 022:    360 / 640 loss=4.554, nll_loss=3.037, ppl=8.21, wps=10613.5, ups=1.37, wpb=7731.7, bsz=280.4, num_updates=13800, lr=0.000107676, gnorm=0.914, clip=0, train_wall=71, wall=17086
2020-10-11 09:41:32 | INFO | train_inner | epoch 022:    460 / 640 loss=4.555, nll_loss=3.038, ppl=8.21, wps=10727.8, ups=1.37, wpb=7855.1, bsz=271.4, num_updates=13900, lr=0.000107288, gnorm=0.928, clip=0, train_wall=72, wall=17160
2020-10-11 09:42:46 | INFO | train_inner | epoch 022:    560 / 640 loss=4.497, nll_loss=2.973, ppl=7.85, wps=10775.6, ups=1.35, wpb=7968.4, bsz=325.9, num_updates=14000, lr=0.000106904, gnorm=0.898, clip=0, train_wall=72, wall=17234
2020-10-11 09:43:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 09:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58371.296875Mb; avail=134467.53125Mb
2020-10-11 09:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001856
2020-10-11 09:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58371.296875Mb; avail=134467.53125Mb
2020-10-11 09:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076967
2020-10-11 09:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58375.4765625Mb; avail=134463.3515625Mb
2020-10-11 09:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.065201
2020-10-11 09:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.145134
2020-10-11 09:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58381.40234375Mb; avail=134457.42578125Mb
2020-10-11 09:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58386.24609375Mb; avail=134452.58203125Mb
2020-10-11 09:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001442
2020-10-11 09:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58386.24609375Mb; avail=134452.58203125Mb
2020-10-11 09:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080608
2020-10-11 09:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58384.265625Mb; avail=134454.5625Mb
2020-10-11 09:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063953
2020-10-11 09:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.147295
2020-10-11 09:43:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58372.76953125Mb; avail=134466.05859375Mb
2020-10-11 09:43:49 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 4.641 | nll_loss 3.024 | ppl 8.13 | wps 23928.2 | wpb 2380 | bsz 90.7 | num_updates 14080 | best_loss 4.641
2020-10-11 09:43:49 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 09:44:12 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 22 @ 14080 updates, score 4.641) (writing took 22.627711463719606 seconds)
2020-10-11 09:44:12 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-10-11 09:44:12 | INFO | train | epoch 022 | loss 4.522 | nll_loss 3.001 | ppl 8.01 | wps 10052.6 | ups 1.29 | wpb 7819.3 | bsz 294.3 | num_updates 14080 | lr 0.0001066 | gnorm 0.905 | clip 0 | train_wall 456 | wall 17319
2020-10-11 09:44:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22
2020-10-11 09:44:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23
2020-10-11 09:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58355.375Mb; avail=134483.41796875Mb
2020-10-11 09:44:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004268
2020-10-11 09:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.035346
2020-10-11 09:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58355.375Mb; avail=134483.41796875Mb
2020-10-11 09:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001454
2020-10-11 09:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58355.375Mb; avail=134483.41796875Mb
2020-10-11 09:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.833101
2020-10-11 09:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.871141
2020-10-11 09:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58365.30859375Mb; avail=134473.75Mb
2020-10-11 09:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58364.890625Mb; avail=134474.16796875Mb
2020-10-11 09:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.029212
2020-10-11 09:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58360.953125Mb; avail=134478.10546875Mb
2020-10-11 09:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001788
2020-10-11 09:44:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58360.953125Mb; avail=134478.10546875Mb
2020-10-11 09:44:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.856462
2020-10-11 09:44:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.888755
2020-10-11 09:44:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58356.5390625Mb; avail=134482.50390625Mb
2020-10-11 09:44:14 | INFO | fairseq.trainer | begin training epoch 23
2020-10-11 09:44:27 | INFO | train_inner | epoch 023:     20 / 640 loss=4.487, nll_loss=2.962, ppl=7.79, wps=7700, ups=0.99, wpb=7807.4, bsz=305.7, num_updates=14100, lr=0.000106525, gnorm=0.897, clip=0, train_wall=69, wall=17335
2020-10-11 09:45:48 | INFO | train_inner | epoch 023:    120 / 640 loss=4.446, nll_loss=2.914, ppl=7.54, wps=9940.9, ups=1.25, wpb=7978.1, bsz=304.6, num_updates=14200, lr=0.000106149, gnorm=0.875, clip=0, train_wall=78, wall=17415
2020-10-11 09:48:21 | INFO | train_inner | epoch 023:    220 / 640 loss=4.502, nll_loss=2.979, ppl=7.88, wps=5085.4, ups=0.65, wpb=7787, bsz=276.9, num_updates=14300, lr=0.000105777, gnorm=0.936, clip=0, train_wall=151, wall=17568
2020-10-11 09:50:53 | INFO | train_inner | epoch 023:    320 / 640 loss=4.478, nll_loss=2.952, ppl=7.74, wps=5076.3, ups=0.66, wpb=7702.5, bsz=283.8, num_updates=14400, lr=0.000105409, gnorm=0.899, clip=0, train_wall=150, wall=17720
2020-10-11 09:53:25 | INFO | train_inner | epoch 023:    420 / 640 loss=4.486, nll_loss=2.96, ppl=7.78, wps=5153, ups=0.66, wpb=7848.2, bsz=298, num_updates=14500, lr=0.000105045, gnorm=0.878, clip=0, train_wall=151, wall=17872
2020-10-11 09:55:57 | INFO | train_inner | epoch 023:    520 / 640 loss=4.481, nll_loss=2.955, ppl=7.75, wps=5197.8, ups=0.66, wpb=7888.5, bsz=296.6, num_updates=14600, lr=0.000104685, gnorm=0.883, clip=0, train_wall=150, wall=18024
2020-10-11 09:58:29 | INFO | train_inner | epoch 023:    620 / 640 loss=4.454, nll_loss=2.925, ppl=7.6, wps=5101.8, ups=0.66, wpb=7763.4, bsz=320.3, num_updates=14700, lr=0.000104328, gnorm=0.895, clip=0, train_wall=150, wall=18176
2020-10-11 09:58:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 09:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59136.15234375Mb; avail=133701.6328125Mb
2020-10-11 09:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001904
2020-10-11 09:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59136.15234375Mb; avail=133701.6328125Mb
2020-10-11 09:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.085081
2020-10-11 09:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59140.390625Mb; avail=133697.39453125Mb
2020-10-11 09:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062943
2020-10-11 09:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.151191
2020-10-11 09:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59145.20703125Mb; avail=133692.578125Mb
2020-10-11 09:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59134.37890625Mb; avail=133703.40625Mb
2020-10-11 09:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001221
2020-10-11 09:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59134.375Mb; avail=133703.41015625Mb
2020-10-11 09:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073687
2020-10-11 09:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59134.375Mb; avail=133703.41015625Mb
2020-10-11 09:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062216
2020-10-11 09:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.138109
2020-10-11 09:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59134.375Mb; avail=133703.41015625Mb
2020-10-11 09:59:09 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 4.609 | nll_loss 2.991 | ppl 7.95 | wps 11992.4 | wpb 2380 | bsz 90.7 | num_updates 14720 | best_loss 4.609
2020-10-11 09:59:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 09:59:47 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 23 @ 14720 updates, score 4.609) (writing took 37.42139859497547 seconds)
2020-10-11 09:59:47 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-10-11 09:59:47 | INFO | train | epoch 023 | loss 4.478 | nll_loss 2.951 | ppl 7.74 | wps 5352.1 | ups 0.68 | wpb 7819.3 | bsz 294.3 | num_updates 14720 | lr 0.000104257 | gnorm 0.895 | clip 0 | train_wall 874 | wall 18254
2020-10-11 09:59:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23
2020-10-11 09:59:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24
2020-10-11 09:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59033.46484375Mb; avail=133804.3125Mb
2020-10-11 09:59:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007972
2020-10-11 09:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.067734
2020-10-11 09:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59042.3515625Mb; avail=133795.42578125Mb
2020-10-11 09:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001852
2020-10-11 09:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59042.3515625Mb; avail=133795.42578125Mb
2020-10-11 09:59:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.876704
2020-10-11 09:59:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.947772
2020-10-11 09:59:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59040.890625Mb; avail=133796.8984375Mb
2020-10-11 09:59:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59040.16796875Mb; avail=133797.62109375Mb
2020-10-11 09:59:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.028175
2020-10-11 09:59:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59041.37890625Mb; avail=133796.41015625Mb
2020-10-11 09:59:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001501
2020-10-11 09:59:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59041.37890625Mb; avail=133796.41015625Mb
2020-10-11 09:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.860970
2020-10-11 09:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.891869
2020-10-11 09:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59043.26953125Mb; avail=133794.94921875Mb
2020-10-11 09:59:49 | INFO | fairseq.trainer | begin training epoch 24
2020-10-11 10:01:50 | INFO | train_inner | epoch 024:     80 / 640 loss=4.487, nll_loss=2.962, ppl=7.79, wps=3812.8, ups=0.5, wpb=7688.2, bsz=255, num_updates=14800, lr=0.000103975, gnorm=0.91, clip=0, train_wall=150, wall=18378
2020-10-11 10:04:23 | INFO | train_inner | epoch 024:    180 / 640 loss=4.404, nll_loss=2.867, ppl=7.3, wps=5189.5, ups=0.66, wpb=7909.6, bsz=308.1, num_updates=14900, lr=0.000103626, gnorm=0.874, clip=0, train_wall=151, wall=18530
2020-10-11 10:06:55 | INFO | train_inner | epoch 024:    280 / 640 loss=4.458, nll_loss=2.928, ppl=7.61, wps=5114.1, ups=0.66, wpb=7802, bsz=288.9, num_updates=15000, lr=0.00010328, gnorm=0.909, clip=0, train_wall=151, wall=18683
2020-10-11 10:09:26 | INFO | train_inner | epoch 024:    380 / 640 loss=4.454, nll_loss=2.925, ppl=7.59, wps=5279.4, ups=0.67, wpb=7921.8, bsz=283.6, num_updates=15100, lr=0.000102937, gnorm=0.907, clip=0, train_wall=148, wall=18833
2020-10-11 10:11:54 | INFO | train_inner | epoch 024:    480 / 640 loss=4.442, nll_loss=2.91, ppl=7.52, wps=5223.4, ups=0.67, wpb=7773.6, bsz=296, num_updates=15200, lr=0.000102598, gnorm=0.907, clip=0, train_wall=147, wall=18982
2020-10-11 10:14:25 | INFO | train_inner | epoch 024:    580 / 640 loss=4.415, nll_loss=2.881, ppl=7.37, wps=5218.7, ups=0.67, wpb=7846.3, bsz=305.1, num_updates=15300, lr=0.000102262, gnorm=0.88, clip=0, train_wall=149, wall=19132
2020-10-11 10:15:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 10:15:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57698.21484375Mb; avail=135140.58203125Mb
2020-10-11 10:15:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001965
2020-10-11 10:15:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57698.21484375Mb; avail=135140.58203125Mb
2020-10-11 10:15:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.086841
2020-10-11 10:15:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57687.78125Mb; avail=135151.015625Mb
2020-10-11 10:15:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063797
2020-10-11 10:15:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.153906
2020-10-11 10:15:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57687.765625Mb; avail=135151.03125Mb
2020-10-11 10:15:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57687.765625Mb; avail=135151.03125Mb
2020-10-11 10:15:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001247
2020-10-11 10:15:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57687.765625Mb; avail=135151.03125Mb
2020-10-11 10:15:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076439
2020-10-11 10:15:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57687.52734375Mb; avail=135151.26953125Mb
2020-10-11 10:15:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063346
2020-10-11 10:15:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.142123
2020-10-11 10:15:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57691.1328125Mb; avail=135147.6640625Mb
2020-10-11 10:16:06 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 4.597 | nll_loss 2.971 | ppl 7.84 | wps 12007.6 | wpb 2380 | bsz 90.7 | num_updates 15360 | best_loss 4.597
2020-10-11 10:16:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 10:16:25 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 24 @ 15360 updates, score 4.597) (writing took 19.198809046298265 seconds)
2020-10-11 10:16:25 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-10-11 10:16:25 | INFO | train | epoch 024 | loss 4.437 | nll_loss 2.905 | ppl 7.49 | wps 5012.1 | ups 0.64 | wpb 7819.3 | bsz 294.3 | num_updates 15360 | lr 0.000102062 | gnorm 0.895 | clip 0 | train_wall 955 | wall 19253
2020-10-11 10:16:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24
2020-10-11 10:16:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25
2020-10-11 10:16:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57625.796875Mb; avail=135212.97265625Mb
2020-10-11 10:16:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008589
2020-10-11 10:16:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.063385
2020-10-11 10:16:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57627.51171875Mb; avail=135211.2578125Mb
2020-10-11 10:16:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002132
2020-10-11 10:16:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57627.51171875Mb; avail=135211.2578125Mb
2020-10-11 10:16:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.892114
2020-10-11 10:16:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.959119
2020-10-11 10:16:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57628.73828125Mb; avail=135210.2734375Mb
2020-10-11 10:16:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57628.67578125Mb; avail=135210.3359375Mb
2020-10-11 10:16:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027648
2020-10-11 10:16:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57628.67578125Mb; avail=135210.3359375Mb
2020-10-11 10:16:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001407
2020-10-11 10:16:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57628.67578125Mb; avail=135210.3359375Mb
2020-10-11 10:16:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.877162
2020-10-11 10:16:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.907350
2020-10-11 10:16:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57634.7265625Mb; avail=135203.96875Mb
2020-10-11 10:16:27 | INFO | fairseq.trainer | begin training epoch 25
2020-10-11 10:17:28 | INFO | train_inner | epoch 025:     40 / 640 loss=4.415, nll_loss=2.881, ppl=7.36, wps=4245.8, ups=0.55, wpb=7773, bsz=299.6, num_updates=15400, lr=0.000101929, gnorm=0.88, clip=0, train_wall=149, wall=19315
2020-10-11 10:19:58 | INFO | train_inner | epoch 025:    140 / 640 loss=4.323, nll_loss=2.777, ppl=6.85, wps=5150.9, ups=0.66, wpb=7757.1, bsz=339.4, num_updates=15500, lr=0.0001016, gnorm=0.881, clip=0, train_wall=149, wall=19466
2020-10-11 10:22:30 | INFO | train_inner | epoch 025:    240 / 640 loss=4.375, nll_loss=2.834, ppl=7.13, wps=5280.3, ups=0.66, wpb=8011.2, bsz=308.6, num_updates=15600, lr=0.000101274, gnorm=0.855, clip=0, train_wall=150, wall=19618
2020-10-11 10:25:00 | INFO | train_inner | epoch 025:    340 / 640 loss=4.426, nll_loss=2.893, ppl=7.43, wps=5141.1, ups=0.67, wpb=7728.9, bsz=277.4, num_updates=15700, lr=0.000100951, gnorm=0.913, clip=0, train_wall=149, wall=19768
2020-10-11 10:27:31 | INFO | train_inner | epoch 025:    440 / 640 loss=4.404, nll_loss=2.868, ppl=7.3, wps=5220, ups=0.67, wpb=7846.9, bsz=295.3, num_updates=15800, lr=0.000100631, gnorm=0.89, clip=0, train_wall=149, wall=19918
2020-10-11 10:30:02 | INFO | train_inner | epoch 025:    540 / 640 loss=4.435, nll_loss=2.903, ppl=7.48, wps=5102.9, ups=0.66, wpb=7703.8, bsz=273, num_updates=15900, lr=0.000100314, gnorm=0.914, clip=0, train_wall=149, wall=20069
2020-10-11 10:32:22 | INFO | train_inner | epoch 025:    640 / 640 loss=4.428, nll_loss=2.895, ppl=7.44, wps=5581.2, ups=0.71, wpb=7853.3, bsz=283.5, num_updates=16000, lr=0.0001, gnorm=0.9, clip=0, train_wall=139, wall=20210
2020-10-11 10:32:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 10:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=54118.28125Mb; avail=138724.484375Mb
2020-10-11 10:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002115
2020-10-11 10:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=54118.28125Mb; avail=138724.484375Mb
2020-10-11 10:32:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.093255
2020-10-11 10:32:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=54126.13671875Mb; avail=138716.62890625Mb
2020-10-11 10:32:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064727
2020-10-11 10:32:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.161398
2020-10-11 10:32:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=54131.5859375Mb; avail=138711.1796875Mb
2020-10-11 10:32:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=54131.5859375Mb; avail=138711.1796875Mb
2020-10-11 10:32:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001192
2020-10-11 10:32:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=54131.5859375Mb; avail=138711.1796875Mb
2020-10-11 10:32:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075915
2020-10-11 10:32:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=54137.03515625Mb; avail=138705.73046875Mb
2020-10-11 10:32:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063045
2020-10-11 10:32:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.141297
2020-10-11 10:32:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=54140.45703125Mb; avail=138702.30859375Mb
2020-10-11 10:32:28 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 4.556 | nll_loss 2.929 | ppl 7.62 | wps 23786.2 | wpb 2380 | bsz 90.7 | num_updates 16000 | best_loss 4.556
2020-10-11 10:32:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 10:32:38 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 25 @ 16000 updates, score 4.556) (writing took 9.531600657850504 seconds)
2020-10-11 10:32:38 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-10-11 10:32:38 | INFO | train | epoch 025 | loss 4.4 | nll_loss 2.862 | ppl 7.27 | wps 5146.5 | ups 0.66 | wpb 7819.3 | bsz 294.3 | num_updates 16000 | lr 0.0001 | gnorm 0.892 | clip 0 | train_wall 944 | wall 20225
2020-10-11 10:32:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25
2020-10-11 10:32:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26
2020-10-11 10:32:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=56610.95703125Mb; avail=136227.36328125Mb
2020-10-11 10:32:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007099
2020-10-11 10:32:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.057633
2020-10-11 10:32:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56638.28125Mb; avail=136200.64453125Mb
2020-10-11 10:32:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002226
2020-10-11 10:32:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56639.4921875Mb; avail=136199.43359375Mb
2020-10-11 10:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.882978
2020-10-11 10:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.944349
2020-10-11 10:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56798.89453125Mb; avail=136040.140625Mb
2020-10-11 10:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=56812.19921875Mb; avail=136026.8359375Mb
2020-10-11 10:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027259
2020-10-11 10:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56823.703125Mb; avail=136015.33203125Mb
2020-10-11 10:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001674
2020-10-11 10:32:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56824.9140625Mb; avail=136014.12109375Mb
2020-10-11 10:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.863114
2020-10-11 10:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.893334
2020-10-11 10:32:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56913.25Mb; avail=135925.140625Mb
2020-10-11 10:32:40 | INFO | fairseq.trainer | begin training epoch 26
2020-10-11 10:33:52 | INFO | train_inner | epoch 026:    100 / 640 loss=4.34, nll_loss=2.795, ppl=6.94, wps=8868.2, ups=1.11, wpb=7981.3, bsz=290.7, num_updates=16100, lr=9.9689e-05, gnorm=0.867, clip=0, train_wall=71, wall=20300
2020-10-11 10:35:06 | INFO | train_inner | epoch 026:    200 / 640 loss=4.358, nll_loss=2.816, ppl=7.04, wps=10430.2, ups=1.35, wpb=7712.5, bsz=290.6, num_updates=16200, lr=9.93808e-05, gnorm=0.907, clip=0, train_wall=72, wall=20374
2020-10-11 10:36:20 | INFO | train_inner | epoch 026:    300 / 640 loss=4.382, nll_loss=2.842, ppl=7.17, wps=10623.1, ups=1.35, wpb=7854.8, bsz=279.9, num_updates=16300, lr=9.90755e-05, gnorm=0.903, clip=0, train_wall=72, wall=20448
2020-10-11 10:37:33 | INFO | train_inner | epoch 026:    400 / 640 loss=4.347, nll_loss=2.802, ppl=6.98, wps=10710.6, ups=1.37, wpb=7821.6, bsz=306.9, num_updates=16400, lr=9.8773e-05, gnorm=0.892, clip=0, train_wall=71, wall=20521
2020-10-11 10:38:46 | INFO | train_inner | epoch 026:    500 / 640 loss=4.383, nll_loss=2.844, ppl=7.18, wps=10501.5, ups=1.37, wpb=7646.3, bsz=295.6, num_updates=16500, lr=9.84732e-05, gnorm=0.92, clip=0, train_wall=71, wall=20594
2020-10-11 10:39:59 | INFO | train_inner | epoch 026:    600 / 640 loss=4.369, nll_loss=2.828, ppl=7.1, wps=10690.5, ups=1.37, wpb=7799.6, bsz=298.1, num_updates=16600, lr=9.81761e-05, gnorm=0.877, clip=0, train_wall=71, wall=20667
2020-10-11 10:40:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 10:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66091.78515625Mb; avail=126742.91015625Mb
2020-10-11 10:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002239
2020-10-11 10:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66091.78515625Mb; avail=126742.91015625Mb
2020-10-11 10:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.093916
2020-10-11 10:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66073.91796875Mb; avail=126760.77734375Mb
2020-10-11 10:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.068741
2020-10-11 10:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.166426
2020-10-11 10:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66077.265625Mb; avail=126757.4296875Mb
2020-10-11 10:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66078.96484375Mb; avail=126755.82421875Mb
2020-10-11 10:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001354
2020-10-11 10:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66078.96484375Mb; avail=126755.82421875Mb
2020-10-11 10:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083615
2020-10-11 10:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66084.2109375Mb; avail=126750.578125Mb
2020-10-11 10:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.066867
2020-10-11 10:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.153013
2020-10-11 10:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66075.765625Mb; avail=126759.0234375Mb
2020-10-11 10:40:34 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 4.552 | nll_loss 2.921 | ppl 7.58 | wps 23627.5 | wpb 2380 | bsz 90.7 | num_updates 16640 | best_loss 4.552
2020-10-11 10:40:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 10:40:52 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 26 @ 16640 updates, score 4.552) (writing took 17.306347247213125 seconds)
2020-10-11 10:40:52 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-10-11 10:40:52 | INFO | train | epoch 026 | loss 4.364 | nll_loss 2.822 | ppl 7.07 | wps 10130.5 | ups 1.3 | wpb 7819.3 | bsz 294.3 | num_updates 16640 | lr 9.80581e-05 | gnorm 0.892 | clip 0 | train_wall 457 | wall 20719
2020-10-11 10:40:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26
2020-10-11 10:40:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27
2020-10-11 10:40:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66014.89453125Mb; avail=126819.65625Mb
2020-10-11 10:40:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005557
2020-10-11 10:40:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.041356
2020-10-11 10:40:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66016.6015625Mb; avail=126817.94921875Mb
2020-10-11 10:40:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001645
2020-10-11 10:40:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66016.6015625Mb; avail=126817.94921875Mb
2020-10-11 10:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.906863
2020-10-11 10:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.951155
2020-10-11 10:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66028.359375Mb; avail=126806.0625Mb
2020-10-11 10:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66026.2421875Mb; avail=126808.78515625Mb
2020-10-11 10:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.031730
2020-10-11 10:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66026.734375Mb; avail=126808.29296875Mb
2020-10-11 10:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002271
2020-10-11 10:40:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66026.734375Mb; avail=126808.29296875Mb
2020-10-11 10:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.923778
2020-10-11 10:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.959293
2020-10-11 10:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66031.25390625Mb; avail=126803.7734375Mb
2020-10-11 10:40:54 | INFO | fairseq.trainer | begin training epoch 27
2020-10-11 10:41:37 | INFO | train_inner | epoch 027:     60 / 640 loss=4.361, nll_loss=2.818, ppl=7.05, wps=8060, ups=1.02, wpb=7906.9, bsz=283, num_updates=16700, lr=9.78818e-05, gnorm=0.902, clip=0, train_wall=71, wall=20765
2020-10-11 10:42:51 | INFO | train_inner | epoch 027:    160 / 640 loss=4.314, nll_loss=2.765, ppl=6.8, wps=10481.7, ups=1.35, wpb=7779.2, bsz=294.8, num_updates=16800, lr=9.759e-05, gnorm=0.887, clip=0, train_wall=72, wall=20839
2020-10-11 10:44:05 | INFO | train_inner | epoch 027:    260 / 640 loss=4.319, nll_loss=2.771, ppl=6.82, wps=10612.5, ups=1.35, wpb=7856.8, bsz=295.4, num_updates=16900, lr=9.73009e-05, gnorm=0.885, clip=0, train_wall=72, wall=20913
2020-10-11 10:45:19 | INFO | train_inner | epoch 027:    360 / 640 loss=4.357, nll_loss=2.814, ppl=7.03, wps=10577.9, ups=1.36, wpb=7775.6, bsz=289.6, num_updates=17000, lr=9.70143e-05, gnorm=0.93, clip=0, train_wall=72, wall=20986
2020-10-11 10:46:32 | INFO | train_inner | epoch 027:    460 / 640 loss=4.323, nll_loss=2.775, ppl=6.85, wps=10730.5, ups=1.37, wpb=7805.3, bsz=303.9, num_updates=17100, lr=9.67302e-05, gnorm=0.884, clip=0, train_wall=71, wall=21059
2020-10-11 10:47:44 | INFO | train_inner | epoch 027:    560 / 640 loss=4.339, nll_loss=2.793, ppl=6.93, wps=10939.3, ups=1.39, wpb=7897.9, bsz=301.1, num_updates=17200, lr=9.64486e-05, gnorm=0.894, clip=0, train_wall=70, wall=21131
2020-10-11 10:49:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 10:49:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66763.40625Mb; avail=126070.56640625Mb
2020-10-11 10:49:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001593
2020-10-11 10:49:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66763.40625Mb; avail=126070.56640625Mb
2020-10-11 10:49:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079941
2020-10-11 10:49:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66763.40625Mb; avail=126070.56640625Mb
2020-10-11 10:49:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063345
2020-10-11 10:49:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.145996
2020-10-11 10:49:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66763.40625Mb; avail=126070.56640625Mb
2020-10-11 10:49:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66763.40625Mb; avail=126070.56640625Mb
2020-10-11 10:49:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001339
2020-10-11 10:49:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66763.40625Mb; avail=126070.56640625Mb
2020-10-11 10:49:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.077396
2020-10-11 10:49:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66763.40625Mb; avail=126070.56640625Mb
2020-10-11 10:49:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064368
2020-10-11 10:49:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.144302
2020-10-11 10:49:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66768.76171875Mb; avail=126065.2109375Mb
2020-10-11 10:49:25 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 4.539 | nll_loss 2.914 | ppl 7.54 | wps 12214.5 | wpb 2380 | bsz 90.7 | num_updates 17280 | best_loss 4.539
2020-10-11 10:49:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 10:49:45 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 27 @ 17280 updates, score 4.539) (writing took 20.37027519568801 seconds)
2020-10-11 10:49:45 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-10-11 10:49:45 | INFO | train | epoch 027 | loss 4.332 | nll_loss 2.786 | ppl 6.9 | wps 9380 | ups 1.2 | wpb 7819.3 | bsz 294.3 | num_updates 17280 | lr 9.6225e-05 | gnorm 0.897 | clip 0 | train_wall 489 | wall 21253
2020-10-11 10:49:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27
2020-10-11 10:49:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28
2020-10-11 10:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66692.76953125Mb; avail=126140.953125Mb
2020-10-11 10:49:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004759
2020-10-11 10:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.038657
2020-10-11 10:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66695.16015625Mb; avail=126138.5625Mb
2020-10-11 10:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001481
2020-10-11 10:49:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66695.16015625Mb; avail=126138.5625Mb
2020-10-11 10:49:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.875828
2020-10-11 10:49:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.917258
2020-10-11 10:49:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66704.953125Mb; avail=126129.03515625Mb
2020-10-11 10:49:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66704.421875Mb; avail=126129.56640625Mb
2020-10-11 10:49:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.030381
2020-10-11 10:49:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66706.01171875Mb; avail=126127.9765625Mb
2020-10-11 10:49:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002442
2020-10-11 10:49:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66706.01171875Mb; avail=126127.9765625Mb
2020-10-11 10:49:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.884692
2020-10-11 10:49:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.919079
2020-10-11 10:49:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66701.6171875Mb; avail=126132.33203125Mb
2020-10-11 10:49:47 | INFO | fairseq.trainer | begin training epoch 28
2020-10-11 10:50:18 | INFO | train_inner | epoch 028:     20 / 640 loss=4.306, nll_loss=2.756, ppl=6.76, wps=5104.2, ups=0.65, wpb=7841.4, bsz=312, num_updates=17300, lr=9.61694e-05, gnorm=0.871, clip=0, train_wall=119, wall=21285
2020-10-11 10:52:45 | INFO | train_inner | epoch 028:    120 / 640 loss=4.299, nll_loss=2.747, ppl=6.72, wps=5275.2, ups=0.68, wpb=7779.7, bsz=281, num_updates=17400, lr=9.58927e-05, gnorm=0.891, clip=0, train_wall=146, wall=21432
2020-10-11 10:55:14 | INFO | train_inner | epoch 028:    220 / 640 loss=4.296, nll_loss=2.744, ppl=6.7, wps=5324.4, ups=0.67, wpb=7911.6, bsz=303.3, num_updates=17500, lr=9.56183e-05, gnorm=0.886, clip=0, train_wall=147, wall=21581
2020-10-11 10:57:40 | INFO | train_inner | epoch 028:    320 / 640 loss=4.288, nll_loss=2.737, ppl=6.67, wps=5297.6, ups=0.68, wpb=7745.5, bsz=286.9, num_updates=17600, lr=9.53463e-05, gnorm=0.893, clip=0, train_wall=145, wall=21727
2020-10-11 11:00:09 | INFO | train_inner | epoch 028:    420 / 640 loss=4.286, nll_loss=2.732, ppl=6.64, wps=5271.3, ups=0.67, wpb=7851.8, bsz=313, num_updates=17700, lr=9.50765e-05, gnorm=0.877, clip=0, train_wall=147, wall=21876
2020-10-11 11:02:37 | INFO | train_inner | epoch 028:    520 / 640 loss=4.346, nll_loss=2.801, ppl=6.97, wps=5269.1, ups=0.67, wpb=7833.1, bsz=278.4, num_updates=17800, lr=9.48091e-05, gnorm=0.917, clip=0, train_wall=147, wall=22025
2020-10-11 11:05:06 | INFO | train_inner | epoch 028:    620 / 640 loss=4.306, nll_loss=2.756, ppl=6.75, wps=5200.7, ups=0.67, wpb=7728.9, bsz=291, num_updates=17900, lr=9.45439e-05, gnorm=0.883, clip=0, train_wall=147, wall=22174
2020-10-11 11:05:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66657.05859375Mb; avail=126176.7109375Mb
2020-10-11 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001772
2020-10-11 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66657.05859375Mb; avail=126176.7109375Mb
2020-10-11 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078117
2020-10-11 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66656.90625Mb; avail=126176.86328125Mb
2020-10-11 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063240
2020-10-11 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.144117
2020-10-11 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66656.875Mb; avail=126176.69140625Mb
2020-10-11 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66656.88671875Mb; avail=126176.640625Mb
2020-10-11 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001187
2020-10-11 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66656.88671875Mb; avail=126176.640625Mb
2020-10-11 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080275
2020-10-11 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66662.40234375Mb; avail=126171.125Mb
2020-10-11 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.066440
2020-10-11 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.149050
2020-10-11 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66673.1953125Mb; avail=126160.33203125Mb
2020-10-11 11:05:46 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 4.52 | nll_loss 2.882 | ppl 7.37 | wps 12406.6 | wpb 2380 | bsz 90.7 | num_updates 17920 | best_loss 4.52
2020-10-11 11:05:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 11:05:55 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 28 @ 17920 updates, score 4.52) (writing took 9.10383541509509 seconds)
2020-10-11 11:05:55 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-10-11 11:05:55 | INFO | train | epoch 028 | loss 4.301 | nll_loss 2.751 | ppl 6.73 | wps 5157.4 | ups 0.66 | wpb 7819.3 | bsz 294.3 | num_updates 17920 | lr 9.44911e-05 | gnorm 0.891 | clip 0 | train_wall 937 | wall 22223
2020-10-11 11:05:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28
2020-10-11 11:05:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29
2020-10-11 11:05:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66597.9296875Mb; avail=126235.8046875Mb
2020-10-11 11:05:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007047
2020-10-11 11:05:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.054578
2020-10-11 11:05:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66605.74609375Mb; avail=126227.98828125Mb
2020-10-11 11:05:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001944
2020-10-11 11:05:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66605.74609375Mb; avail=126227.98828125Mb
2020-10-11 11:05:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.894286
2020-10-11 11:05:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.952195
2020-10-11 11:05:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66592.79296875Mb; avail=126241.04296875Mb
2020-10-11 11:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66592.79296875Mb; avail=126241.04296875Mb
2020-10-11 11:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027228
2020-10-11 11:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66592.79296875Mb; avail=126241.04296875Mb
2020-10-11 11:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001409
2020-10-11 11:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66592.79296875Mb; avail=126241.04296875Mb
2020-10-11 11:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.883498
2020-10-11 11:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.913362
2020-10-11 11:05:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66597.125Mb; avail=126236.88671875Mb
2020-10-11 11:05:57 | INFO | fairseq.trainer | begin training epoch 29
2020-10-11 11:07:55 | INFO | train_inner | epoch 029:     80 / 640 loss=4.254, nll_loss=2.697, ppl=6.49, wps=4632.2, ups=0.59, wpb=7804.8, bsz=299.8, num_updates=18000, lr=9.42809e-05, gnorm=0.899, clip=0, train_wall=145, wall=22342
2020-10-11 11:10:23 | INFO | train_inner | epoch 029:    180 / 640 loss=4.265, nll_loss=2.708, ppl=6.53, wps=5270, ups=0.67, wpb=7840.5, bsz=300.5, num_updates=18100, lr=9.40201e-05, gnorm=0.906, clip=0, train_wall=147, wall=22491
2020-10-11 11:12:49 | INFO | train_inner | epoch 029:    280 / 640 loss=4.261, nll_loss=2.705, ppl=6.52, wps=5358.7, ups=0.69, wpb=7797.7, bsz=311, num_updates=18200, lr=9.37614e-05, gnorm=0.877, clip=0, train_wall=144, wall=22636
2020-10-11 11:15:17 | INFO | train_inner | epoch 029:    380 / 640 loss=4.271, nll_loss=2.716, ppl=6.57, wps=5302.6, ups=0.68, wpb=7854.9, bsz=300.2, num_updates=18300, lr=9.35049e-05, gnorm=0.895, clip=0, train_wall=146, wall=22784
2020-10-11 11:17:45 | INFO | train_inner | epoch 029:    480 / 640 loss=4.295, nll_loss=2.744, ppl=6.7, wps=5311.5, ups=0.68, wpb=7860.4, bsz=269.5, num_updates=18400, lr=9.32505e-05, gnorm=0.891, clip=0, train_wall=146, wall=22932
2020-10-11 11:20:16 | INFO | train_inner | epoch 029:    580 / 640 loss=4.294, nll_loss=2.742, ppl=6.69, wps=5147.2, ups=0.66, wpb=7752.7, bsz=276.4, num_updates=18500, lr=9.29981e-05, gnorm=0.897, clip=0, train_wall=149, wall=23083
2020-10-11 11:21:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 11:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59009.2890625Mb; avail=133829.49609375Mb
2020-10-11 11:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001678
2020-10-11 11:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59009.2890625Mb; avail=133829.49609375Mb
2020-10-11 11:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080375
2020-10-11 11:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59017.16015625Mb; avail=133821.625Mb
2020-10-11 11:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064900
2020-10-11 11:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.148202
2020-10-11 11:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59026.84765625Mb; avail=133811.9375Mb
2020-10-11 11:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59016.01953125Mb; avail=133822.765625Mb
2020-10-11 11:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001411
2020-10-11 11:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59016.01953125Mb; avail=133822.765625Mb
2020-10-11 11:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078030
2020-10-11 11:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59022.07421875Mb; avail=133816.7109375Mb
2020-10-11 11:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063179
2020-10-11 11:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.143896
2020-10-11 11:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59030.4140625Mb; avail=133808.37109375Mb
2020-10-11 11:21:57 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 4.506 | nll_loss 2.871 | ppl 7.32 | wps 12139.8 | wpb 2380 | bsz 90.7 | num_updates 18560 | best_loss 4.506
2020-10-11 11:21:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 11:22:06 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 29 @ 18560 updates, score 4.506) (writing took 9.061917807906866 seconds)
2020-10-11 11:22:06 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-10-11 11:22:06 | INFO | train | epoch 029 | loss 4.271 | nll_loss 2.717 | ppl 6.57 | wps 5155 | ups 0.66 | wpb 7819.3 | bsz 294.3 | num_updates 18560 | lr 9.28477e-05 | gnorm 0.891 | clip 0 | train_wall 938 | wall 23194
2020-10-11 11:22:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29
2020-10-11 11:22:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30
2020-10-11 11:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=61252.1640625Mb; avail=131581.44140625Mb
2020-10-11 11:22:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007108
2020-10-11 11:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.054770
2020-10-11 11:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61281.1640625Mb; avail=131553.046875Mb
2020-10-11 11:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002182
2020-10-11 11:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61285.40234375Mb; avail=131549.4140625Mb
2020-10-11 11:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.902958
2020-10-11 11:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.961370
2020-10-11 11:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61434.44140625Mb; avail=131400.55859375Mb
2020-10-11 11:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=61433.7578125Mb; avail=131401.2421875Mb
2020-10-11 11:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.031108
2020-10-11 11:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61433.7578125Mb; avail=131401.2421875Mb
2020-10-11 11:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002007
2020-10-11 11:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61433.7578125Mb; avail=131401.2421875Mb
2020-10-11 11:22:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.874917
2020-10-11 11:22:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.909462
2020-10-11 11:22:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61604.9453125Mb; avail=131230.296875Mb
2020-10-11 11:22:08 | INFO | fairseq.trainer | begin training epoch 30
2020-10-11 11:23:07 | INFO | train_inner | epoch 030:     40 / 640 loss=4.253, nll_loss=2.695, ppl=6.47, wps=4580, ups=0.58, wpb=7869.9, bsz=295.6, num_updates=18600, lr=9.27478e-05, gnorm=0.888, clip=0, train_wall=148, wall=23255
2020-10-11 11:25:34 | INFO | train_inner | epoch 030:    140 / 640 loss=4.235, nll_loss=2.675, ppl=6.39, wps=5358.8, ups=0.68, wpb=7828.2, bsz=289, num_updates=18700, lr=9.24995e-05, gnorm=0.878, clip=0, train_wall=144, wall=23401
2020-10-11 11:28:00 | INFO | train_inner | epoch 030:    240 / 640 loss=4.238, nll_loss=2.679, ppl=6.4, wps=5328.9, ups=0.68, wpb=7812.6, bsz=298.1, num_updates=18800, lr=9.22531e-05, gnorm=0.892, clip=0, train_wall=145, wall=23548
2020-10-11 11:29:34 | INFO | train_inner | epoch 030:    340 / 640 loss=4.27, nll_loss=2.714, ppl=6.56, wps=8328.3, ups=1.07, wpb=7815.7, bsz=279.3, num_updates=18900, lr=9.20087e-05, gnorm=0.909, clip=0, train_wall=92, wall=23641
2020-10-11 11:30:45 | INFO | train_inner | epoch 030:    440 / 640 loss=4.226, nll_loss=2.666, ppl=6.35, wps=10903.4, ups=1.4, wpb=7781.3, bsz=310.9, num_updates=19000, lr=9.17663e-05, gnorm=0.885, clip=0, train_wall=70, wall=23713
2020-10-11 11:31:57 | INFO | train_inner | epoch 030:    540 / 640 loss=4.272, nll_loss=2.716, ppl=6.57, wps=10823.4, ups=1.39, wpb=7779.5, bsz=289.5, num_updates=19100, lr=9.15258e-05, gnorm=0.906, clip=0, train_wall=70, wall=23785
2020-10-11 11:33:09 | INFO | train_inner | epoch 030:    640 / 640 loss=4.234, nll_loss=2.675, ppl=6.38, wps=10975, ups=1.39, wpb=7888.4, bsz=303.4, num_updates=19200, lr=9.12871e-05, gnorm=0.88, clip=0, train_wall=70, wall=23857
2020-10-11 11:33:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 11:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66822.01953125Mb; avail=126012.46875Mb
2020-10-11 11:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001615
2020-10-11 11:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66822.51171875Mb; avail=126012.46875Mb
2020-10-11 11:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081291
2020-10-11 11:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66823.6171875Mb; avail=126011.36328125Mb
2020-10-11 11:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.062265
2020-10-11 11:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.146406
2020-10-11 11:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66814.47265625Mb; avail=126019.92578125Mb
2020-10-11 11:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66814.84765625Mb; avail=126019.92578125Mb
2020-10-11 11:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001229
2020-10-11 11:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66814.84765625Mb; avail=126019.92578125Mb
2020-10-11 11:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.076168
2020-10-11 11:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66814.84765625Mb; avail=126019.92578125Mb
2020-10-11 11:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060733
2020-10-11 11:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.139099
2020-10-11 11:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66814.84765625Mb; avail=126019.92578125Mb
2020-10-11 11:33:15 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 4.498 | nll_loss 2.862 | ppl 7.27 | wps 24294.6 | wpb 2380 | bsz 90.7 | num_updates 19200 | best_loss 4.498
2020-10-11 11:33:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 11:33:22 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 30 @ 19200 updates, score 4.498) (writing took 7.235109992325306 seconds)
2020-10-11 11:33:22 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-10-11 11:33:22 | INFO | train | epoch 030 | loss 4.244 | nll_loss 2.685 | ppl 6.43 | wps 7406.6 | ups 0.95 | wpb 7819.3 | bsz 294.3 | num_updates 19200 | lr 9.12871e-05 | gnorm 0.892 | clip 0 | train_wall 649 | wall 23869
2020-10-11 11:33:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30
2020-10-11 11:33:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31
2020-10-11 11:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66760.67578125Mb; avail=126074.33984375Mb
2020-10-11 11:33:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007646
2020-10-11 11:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.056280
2020-10-11 11:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66763.09765625Mb; avail=126071.91796875Mb
2020-10-11 11:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002295
2020-10-11 11:33:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66763.09765625Mb; avail=126071.91796875Mb
2020-10-11 11:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.867969
2020-10-11 11:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.928119
2020-10-11 11:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66765.2421875Mb; avail=126070.0Mb
2020-10-11 11:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66769.30859375Mb; avail=126065.93359375Mb
2020-10-11 11:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027935
2020-10-11 11:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66771.73046875Mb; avail=126063.51171875Mb
2020-10-11 11:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001704
2020-10-11 11:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66771.73046875Mb; avail=126063.51171875Mb
2020-10-11 11:33:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.895846
2020-10-11 11:33:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.926785
2020-10-11 11:33:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66764.5Mb; avail=126070.51171875Mb
2020-10-11 11:33:24 | INFO | fairseq.trainer | begin training epoch 31
2020-10-11 11:34:37 | INFO | train_inner | epoch 031:    100 / 640 loss=4.2, nll_loss=2.636, ppl=6.21, wps=8835.4, ups=1.14, wpb=7781.3, bsz=292.4, num_updates=19300, lr=9.10503e-05, gnorm=0.889, clip=0, train_wall=71, wall=23945
2020-10-11 11:35:51 | INFO | train_inner | epoch 031:    200 / 640 loss=4.226, nll_loss=2.665, ppl=6.34, wps=10526.7, ups=1.35, wpb=7777.8, bsz=274.9, num_updates=19400, lr=9.08153e-05, gnorm=0.907, clip=0, train_wall=72, wall=24019
2020-10-11 11:37:05 | INFO | train_inner | epoch 031:    300 / 640 loss=4.238, nll_loss=2.677, ppl=6.4, wps=10645, ups=1.36, wpb=7837.8, bsz=282.5, num_updates=19500, lr=9.05822e-05, gnorm=0.915, clip=0, train_wall=72, wall=24092
2020-10-11 11:38:18 | INFO | train_inner | epoch 031:    400 / 640 loss=4.261, nll_loss=2.704, ppl=6.51, wps=10599.8, ups=1.36, wpb=7770.8, bsz=266.7, num_updates=19600, lr=9.03508e-05, gnorm=0.898, clip=0, train_wall=72, wall=24165
2020-10-11 11:39:31 | INFO | train_inner | epoch 031:    500 / 640 loss=4.207, nll_loss=2.643, ppl=6.25, wps=10829.7, ups=1.37, wpb=7901.5, bsz=308.1, num_updates=19700, lr=9.01212e-05, gnorm=0.888, clip=0, train_wall=71, wall=24238
2020-10-11 11:40:44 | INFO | train_inner | epoch 031:    600 / 640 loss=4.188, nll_loss=2.624, ppl=6.16, wps=10745.9, ups=1.37, wpb=7833.1, bsz=321.2, num_updates=19800, lr=8.98933e-05, gnorm=0.88, clip=0, train_wall=71, wall=24311
2020-10-11 11:41:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 11:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=72838.8203125Mb; avail=119995.3671875Mb
2020-10-11 11:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002237
2020-10-11 11:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72838.8203125Mb; avail=119995.3671875Mb
2020-10-11 11:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.087278
2020-10-11 11:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72847.50390625Mb; avail=119986.4140625Mb
2020-10-11 11:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.067489
2020-10-11 11:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.158473
2020-10-11 11:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72859.125Mb; avail=119974.79296875Mb
2020-10-11 11:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=72851.19921875Mb; avail=119983.16015625Mb
2020-10-11 11:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001526
2020-10-11 11:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72851.19921875Mb; avail=119983.16015625Mb
2020-10-11 11:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.080960
2020-10-11 11:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72848.625Mb; avail=119985.16015625Mb
2020-10-11 11:41:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.063699
2020-10-11 11:41:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.147519
2020-10-11 11:41:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72848.37109375Mb; avail=119986.35546875Mb
2020-10-11 11:41:19 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 4.481 | nll_loss 2.847 | ppl 7.19 | wps 23587.1 | wpb 2380 | bsz 90.7 | num_updates 19840 | best_loss 4.481
2020-10-11 11:41:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 11:41:40 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 31 @ 19840 updates, score 4.481) (writing took 21.23593421280384 seconds)
2020-10-11 11:41:40 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-10-11 11:41:40 | INFO | train | epoch 031 | loss 4.218 | nll_loss 2.655 | ppl 6.3 | wps 10043 | ups 1.28 | wpb 7819.3 | bsz 294.3 | num_updates 19840 | lr 8.98027e-05 | gnorm 0.894 | clip 0 | train_wall 457 | wall 24368
2020-10-11 11:41:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31
2020-10-11 11:41:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32
2020-10-11 11:41:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=74863.53515625Mb; avail=117970.76171875Mb
2020-10-11 11:41:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.009295
2020-10-11 11:41:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.073034
2020-10-11 11:41:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=74866.53125Mb; avail=117967.765625Mb
2020-10-11 11:41:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.004485
2020-10-11 11:41:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=74866.53125Mb; avail=117967.765625Mb
2020-10-11 11:41:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.902215
2020-10-11 11:41:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.981576
2020-10-11 11:41:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=74884.77734375Mb; avail=117950.10546875Mb
2020-10-11 11:41:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=74881.76171875Mb; avail=117952.98046875Mb
2020-10-11 11:41:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.033317
2020-10-11 11:41:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=74885.75390625Mb; avail=117948.84375Mb
2020-10-11 11:41:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002266
2020-10-11 11:41:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=74885.75390625Mb; avail=117948.84375Mb
2020-10-11 11:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.893344
2020-10-11 11:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.930353
2020-10-11 11:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=74869.21875Mb; avail=117965.6875Mb
2020-10-11 11:41:42 | INFO | fairseq.trainer | begin training epoch 32
2020-10-11 11:42:25 | INFO | train_inner | epoch 032:     60 / 640 loss=4.165, nll_loss=2.596, ppl=6.05, wps=7838.6, ups=0.98, wpb=7958.8, bsz=330.2, num_updates=19900, lr=8.96672e-05, gnorm=0.86, clip=0, train_wall=71, wall=24413
2020-10-11 11:43:39 | INFO | train_inner | epoch 032:    160 / 640 loss=4.205, nll_loss=2.641, ppl=6.24, wps=10294.7, ups=1.35, wpb=7617.9, bsz=259.1, num_updates=20000, lr=8.94427e-05, gnorm=0.904, clip=0, train_wall=72, wall=24487
2020-10-11 11:45:14 | INFO | train_inner | epoch 032:    260 / 640 loss=4.196, nll_loss=2.63, ppl=6.19, wps=8307, ups=1.05, wpb=7895.4, bsz=290.1, num_updates=20100, lr=8.92199e-05, gnorm=0.877, clip=0, train_wall=93, wall=24582
2020-10-11 11:47:43 | INFO | train_inner | epoch 032:    360 / 640 loss=4.184, nll_loss=2.617, ppl=6.13, wps=5252.3, ups=0.67, wpb=7817.9, bsz=293.9, num_updates=20200, lr=8.89988e-05, gnorm=0.883, clip=0, train_wall=147, wall=24731
2020-10-11 11:50:09 | INFO | train_inner | epoch 032:    460 / 640 loss=4.187, nll_loss=2.622, ppl=6.16, wps=5484.6, ups=0.69, wpb=7995.2, bsz=304.1, num_updates=20300, lr=8.87794e-05, gnorm=0.883, clip=0, train_wall=144, wall=24877
2020-10-11 11:52:32 | INFO | train_inner | epoch 032:    560 / 640 loss=4.206, nll_loss=2.642, ppl=6.24, wps=5303.9, ups=0.7, wpb=7606.2, bsz=293.8, num_updates=20400, lr=8.85615e-05, gnorm=0.903, clip=0, train_wall=142, wall=25020
2020-10-11 11:54:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 11:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=72420.69921875Mb; avail=120413.08203125Mb
2020-10-11 11:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001740
2020-10-11 11:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72420.69921875Mb; avail=120413.08203125Mb
2020-10-11 11:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078894
2020-10-11 11:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72429.98046875Mb; avail=120403.80078125Mb
2020-10-11 11:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.067348
2020-10-11 11:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.149413
2020-10-11 11:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72440.21484375Mb; avail=120393.56640625Mb
2020-10-11 11:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=72440.8203125Mb; avail=120392.9609375Mb
2020-10-11 11:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001573
2020-10-11 11:54:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72440.8203125Mb; avail=120392.9609375Mb
2020-10-11 11:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081139
2020-10-11 11:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72447.48046875Mb; avail=120386.30078125Mb
2020-10-11 11:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.067500
2020-10-11 11:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.151490
2020-10-11 11:54:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72462.453125Mb; avail=120371.328125Mb
2020-10-11 11:54:41 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 4.467 | nll_loss 2.831 | ppl 7.11 | wps 12568.8 | wpb 2380 | bsz 90.7 | num_updates 20480 | best_loss 4.467
2020-10-11 11:54:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 11:55:00 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 32 @ 20480 updates, score 4.467) (writing took 18.3240587413311 seconds)
2020-10-11 11:55:00 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-10-11 11:55:00 | INFO | train | epoch 032 | loss 4.192 | nll_loss 2.627 | ppl 6.18 | wps 6258 | ups 0.8 | wpb 7819.3 | bsz 294.3 | num_updates 20480 | lr 8.83883e-05 | gnorm 0.888 | clip 0 | train_wall 757 | wall 25167
2020-10-11 11:55:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32
2020-10-11 11:55:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33
2020-10-11 11:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=73572.83984375Mb; avail=119261.12890625Mb
2020-10-11 11:55:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004808
2020-10-11 11:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.041360
2020-10-11 11:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=73509.2734375Mb; avail=119324.70703125Mb
2020-10-11 11:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001939
2020-10-11 11:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=73504.80078125Mb; avail=119328.92578125Mb
2020-10-11 11:55:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.916332
2020-10-11 11:55:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.961096
2020-10-11 11:55:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72036.6015625Mb; avail=120797.34765625Mb
2020-10-11 11:55:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=72051.9453125Mb; avail=120781.9921875Mb
2020-10-11 11:55:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.031250
2020-10-11 11:55:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72053.8203125Mb; avail=120780.1171875Mb
2020-10-11 11:55:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.003192
2020-10-11 11:55:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72053.8203125Mb; avail=120780.1171875Mb
2020-10-11 11:55:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.967493
2020-10-11 11:55:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.005721
2020-10-11 11:55:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72070.56640625Mb; avail=120763.6796875Mb
2020-10-11 11:55:02 | INFO | fairseq.trainer | begin training epoch 33
2020-10-11 11:55:31 | INFO | train_inner | epoch 033:     20 / 640 loss=4.195, nll_loss=2.63, ppl=6.19, wps=4395.2, ups=0.56, wpb=7847.7, bsz=304.6, num_updates=20500, lr=8.83452e-05, gnorm=0.902, clip=0, train_wall=146, wall=25198
2020-10-11 11:57:58 | INFO | train_inner | epoch 033:    120 / 640 loss=4.153, nll_loss=2.581, ppl=5.98, wps=5461.7, ups=0.68, wpb=7999.4, bsz=306.7, num_updates=20600, lr=8.81305e-05, gnorm=0.872, clip=0, train_wall=145, wall=25345
2020-10-11 12:00:25 | INFO | train_inner | epoch 033:    220 / 640 loss=4.146, nll_loss=2.573, ppl=5.95, wps=5369.9, ups=0.68, wpb=7934.8, bsz=310, num_updates=20700, lr=8.79174e-05, gnorm=0.872, clip=0, train_wall=146, wall=25493
2020-10-11 12:02:49 | INFO | train_inner | epoch 033:    320 / 640 loss=4.182, nll_loss=2.614, ppl=6.12, wps=5324.6, ups=0.69, wpb=7666.7, bsz=285.4, num_updates=20800, lr=8.77058e-05, gnorm=0.912, clip=0, train_wall=142, wall=25637
2020-10-11 12:05:16 | INFO | train_inner | epoch 033:    420 / 640 loss=4.172, nll_loss=2.603, ppl=6.07, wps=5293.6, ups=0.68, wpb=7784.6, bsz=297.5, num_updates=20900, lr=8.74957e-05, gnorm=0.889, clip=0, train_wall=145, wall=25784
2020-10-11 12:07:46 | INFO | train_inner | epoch 033:    520 / 640 loss=4.157, nll_loss=2.587, ppl=6.01, wps=5325.1, ups=0.67, wpb=7978.9, bsz=314.9, num_updates=21000, lr=8.72872e-05, gnorm=0.88, clip=0, train_wall=148, wall=25934
2020-10-11 12:10:11 | INFO | train_inner | epoch 033:    620 / 640 loss=4.191, nll_loss=2.626, ppl=6.17, wps=5304.1, ups=0.69, wpb=7692.1, bsz=275.3, num_updates=21100, lr=8.70801e-05, gnorm=0.906, clip=0, train_wall=143, wall=26079
2020-10-11 12:10:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 12:10:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67732.5703125Mb; avail=125101.18359375Mb
2020-10-11 12:10:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001781
2020-10-11 12:10:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67732.5703125Mb; avail=125101.18359375Mb
2020-10-11 12:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.079976
2020-10-11 12:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67731.0390625Mb; avail=125102.71484375Mb
2020-10-11 12:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064336
2020-10-11 12:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.147357
2020-10-11 12:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67733.9453125Mb; avail=125099.80859375Mb
2020-10-11 12:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67733.9453125Mb; avail=125099.80859375Mb
2020-10-11 12:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001535
2020-10-11 12:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67733.9453125Mb; avail=125099.80859375Mb
2020-10-11 12:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078857
2020-10-11 12:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67735.3984375Mb; avail=125098.35546875Mb
2020-10-11 12:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.064794
2020-10-11 12:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.146444
2020-10-11 12:10:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67725.65234375Mb; avail=125108.1015625Mb
2020-10-11 12:10:50 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 4.462 | nll_loss 2.824 | ppl 7.08 | wps 12517 | wpb 2380 | bsz 90.7 | num_updates 21120 | best_loss 4.462
2020-10-11 12:10:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 12:11:14 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 33 @ 21120 updates, score 4.462) (writing took 24.051532085984945 seconds)
2020-10-11 12:11:14 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-10-11 12:11:14 | INFO | train | epoch 033 | loss 4.169 | nll_loss 2.6 | ppl 6.06 | wps 5138.1 | ups 0.66 | wpb 7819.3 | bsz 294.3 | num_updates 21120 | lr 8.70388e-05 | gnorm 0.891 | clip 0 | train_wall 926 | wall 26141
2020-10-11 12:11:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33
2020-10-11 12:11:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34
2020-10-11 12:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67700.76953125Mb; avail=125133.18359375Mb
2020-10-11 12:11:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006820
2020-10-11 12:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.051584
2020-10-11 12:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67694.14453125Mb; avail=125139.80859375Mb
2020-10-11 12:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001972
2020-10-11 12:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67694.14453125Mb; avail=125139.80859375Mb
2020-10-11 12:11:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.882504
2020-10-11 12:11:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.937626
2020-10-11 12:11:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67688.375Mb; avail=125145.98046875Mb
2020-10-11 12:11:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67688.53125Mb; avail=125145.82421875Mb
2020-10-11 12:11:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.027883
2020-10-11 12:11:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67689.0625Mb; avail=125145.29296875Mb
2020-10-11 12:11:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001477
2020-10-11 12:11:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67689.66796875Mb; avail=125144.6875Mb
2020-10-11 12:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.881832
2020-10-11 12:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.912508
2020-10-11 12:11:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67686.92578125Mb; avail=125146.94140625Mb
2020-10-11 12:11:16 | INFO | fairseq.trainer | begin training epoch 34
2020-10-11 12:13:13 | INFO | train_inner | epoch 034:     80 / 640 loss=4.136, nll_loss=2.562, ppl=5.91, wps=4213.1, ups=0.55, wpb=7677.4, bsz=279.7, num_updates=21200, lr=8.68744e-05, gnorm=0.887, clip=0, train_wall=144, wall=26261
2020-10-11 12:15:43 | INFO | train_inner | epoch 034:    180 / 640 loss=4.144, nll_loss=2.571, ppl=5.94, wps=5289.2, ups=0.67, wpb=7905.1, bsz=279.9, num_updates=21300, lr=8.66703e-05, gnorm=0.89, clip=0, train_wall=148, wall=26410
2020-10-11 12:18:10 | INFO | train_inner | epoch 034:    280 / 640 loss=4.143, nll_loss=2.571, ppl=5.94, wps=5291.4, ups=0.68, wpb=7792, bsz=297.2, num_updates=21400, lr=8.64675e-05, gnorm=0.893, clip=0, train_wall=145, wall=26558
2020-10-11 12:20:36 | INFO | train_inner | epoch 034:    380 / 640 loss=4.168, nll_loss=2.598, ppl=6.05, wps=5313.1, ups=0.69, wpb=7754.4, bsz=286.1, num_updates=21500, lr=8.62662e-05, gnorm=0.89, clip=0, train_wall=144, wall=26704
2020-10-11 12:22:57 | INFO | train_inner | epoch 034:    480 / 640 loss=4.136, nll_loss=2.562, ppl=5.91, wps=5652.2, ups=0.71, wpb=7991, bsz=314.3, num_updates=21600, lr=8.60663e-05, gnorm=0.885, clip=0, train_wall=139, wall=26845
2020-10-11 12:25:11 | INFO | train_inner | epoch 034:    580 / 640 loss=4.148, nll_loss=2.577, ppl=5.97, wps=5771.2, ups=0.75, wpb=7729, bsz=300.8, num_updates=21700, lr=8.58678e-05, gnorm=0.922, clip=0, train_wall=132, wall=26979
2020-10-11 12:26:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 12:26:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=80264.53515625Mb; avail=112570.24609375Mb
2020-10-11 12:26:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002522
2020-10-11 12:26:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=80265.125Mb; avail=112569.65625Mb
2020-10-11 12:26:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.168053
2020-10-11 12:26:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=80275.2578125Mb; avail=112559.5234375Mb
2020-10-11 12:26:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.114200
2020-10-11 12:26:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.286821
2020-10-11 12:26:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=80282.55078125Mb; avail=112552.23046875Mb
2020-10-11 12:26:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=80282.55078125Mb; avail=112552.23046875Mb
2020-10-11 12:26:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001930
2020-10-11 12:26:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=80282.55078125Mb; avail=112552.23046875Mb
2020-10-11 12:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.086132
2020-10-11 12:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=80241.05078125Mb; avail=112593.73046875Mb
2020-10-11 12:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.071667
2020-10-11 12:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.161165
2020-10-11 12:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=80294.2890625Mb; avail=112540.4921875Mb
2020-10-11 12:26:42 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.46 | nll_loss 2.82 | ppl 7.06 | wps 13413.6 | wpb 2380 | bsz 90.7 | num_updates 21760 | best_loss 4.46
2020-10-11 12:26:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 12:26:54 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 34 @ 21760 updates, score 4.46) (writing took 11.84530070796609 seconds)
2020-10-11 12:26:54 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-10-11 12:26:54 | INFO | train | epoch 034 | loss 4.148 | nll_loss 2.576 | ppl 5.96 | wps 5323.4 | ups 0.68 | wpb 7819.3 | bsz 294.3 | num_updates 21760 | lr 8.57493e-05 | gnorm 0.896 | clip 0 | train_wall 904 | wall 27081
2020-10-11 12:26:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34
2020-10-11 12:26:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35
2020-10-11 12:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=83491.8125Mb; avail=109342.98046875Mb
2020-10-11 12:26:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005288
2020-10-11 12:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.050479
2020-10-11 12:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=83457.921875Mb; avail=109376.90234375Mb
2020-10-11 12:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002505
2020-10-11 12:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=83457.921875Mb; avail=109376.90234375Mb
2020-10-11 12:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.168897
2020-10-11 12:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.223573
2020-10-11 12:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=83486.453125Mb; avail=109348.59375Mb
2020-10-11 12:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=83477.0625Mb; avail=109357.984375Mb
2020-10-11 12:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.031424
2020-10-11 12:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=83480.87890625Mb; avail=109354.16796875Mb
2020-10-11 12:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002203
2020-10-11 12:26:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=83480.87890625Mb; avail=109354.16796875Mb
2020-10-11 12:26:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.220835
2020-10-11 12:26:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.256262
2020-10-11 12:26:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=83477.984375Mb; avail=109357.04296875Mb
2020-10-11 12:26:56 | INFO | fairseq.trainer | begin training epoch 35
2020-10-11 12:27:49 | INFO | train_inner | epoch 035:     40 / 640 loss=4.151, nll_loss=2.58, ppl=5.98, wps=4868.7, ups=0.63, wpb=7682.2, bsz=282.6, num_updates=21800, lr=8.56706e-05, gnorm=0.908, clip=0, train_wall=131, wall=27137
2020-10-11 12:29:58 | INFO | train_inner | epoch 035:    140 / 640 loss=4.123, nll_loss=2.547, ppl=5.84, wps=6050.4, ups=0.78, wpb=7786.8, bsz=277.3, num_updates=21900, lr=8.54748e-05, gnorm=0.889, clip=0, train_wall=127, wall=27265
2020-10-11 12:31:26 | INFO | train_inner | epoch 035:    240 / 640 loss=4.126, nll_loss=2.55, ppl=5.86, wps=8754.1, ups=1.14, wpb=7689.1, bsz=310.5, num_updates=22000, lr=8.52803e-05, gnorm=0.898, clip=0, train_wall=86, wall=27353
2020-10-11 12:32:38 | INFO | train_inner | epoch 035:    340 / 640 loss=4.121, nll_loss=2.545, ppl=5.84, wps=10711.8, ups=1.39, wpb=7715.9, bsz=292.6, num_updates=22100, lr=8.50871e-05, gnorm=0.919, clip=0, train_wall=70, wall=27425
2020-10-11 12:33:52 | INFO | train_inner | epoch 035:    440 / 640 loss=4.145, nll_loss=2.572, ppl=5.94, wps=10686, ups=1.35, wpb=7934.4, bsz=295.8, num_updates=22200, lr=8.48953e-05, gnorm=0.886, clip=0, train_wall=72, wall=27499
2020-10-11 12:35:06 | INFO | train_inner | epoch 035:    540 / 640 loss=4.145, nll_loss=2.572, ppl=5.95, wps=10735.8, ups=1.36, wpb=7916, bsz=285.6, num_updates=22300, lr=8.47047e-05, gnorm=0.887, clip=0, train_wall=72, wall=27573
2020-10-11 12:36:20 | INFO | train_inner | epoch 035:    640 / 640 loss=4.122, nll_loss=2.547, ppl=5.84, wps=10734.9, ups=1.35, wpb=7945.1, bsz=304.6, num_updates=22400, lr=8.45154e-05, gnorm=0.878, clip=0, train_wall=72, wall=27647
2020-10-11 12:36:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 12:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67240.82421875Mb; avail=125599.16796875Mb
2020-10-11 12:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003989
2020-10-11 12:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67242.640625Mb; avail=125597.3515625Mb
2020-10-11 12:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.166103
2020-10-11 12:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67239.1171875Mb; avail=125600.66015625Mb
2020-10-11 12:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.129001
2020-10-11 12:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.301629
2020-10-11 12:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67263.1171875Mb; avail=125576.0546875Mb
2020-10-11 12:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67250.55859375Mb; avail=125589.43359375Mb
2020-10-11 12:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002289
2020-10-11 12:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67250.55859375Mb; avail=125589.43359375Mb
2020-10-11 12:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.153598
2020-10-11 12:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67218.8984375Mb; avail=125621.09375Mb
2020-10-11 12:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.129118
2020-10-11 12:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.286970
2020-10-11 12:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67237.1953125Mb; avail=125588.10546875Mb
2020-10-11 12:36:26 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.455 | nll_loss 2.812 | ppl 7.02 | wps 23725.1 | wpb 2380 | bsz 90.7 | num_updates 22400 | best_loss 4.455
2020-10-11 12:36:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 12:36:37 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 35 @ 22400 updates, score 4.455) (writing took 11.177185446023941 seconds)
2020-10-11 12:36:37 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-10-11 12:36:37 | INFO | train | epoch 035 | loss 4.127 | nll_loss 2.552 | ppl 5.86 | wps 8581.4 | ups 1.1 | wpb 7819.3 | bsz 294.3 | num_updates 22400 | lr 8.45154e-05 | gnorm 0.893 | clip 0 | train_wall 550 | wall 27665
2020-10-11 12:36:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35
2020-10-11 12:36:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36
2020-10-11 12:36:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67161.09765625Mb; avail=125678.53125Mb
2020-10-11 12:36:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007016
2020-10-11 12:36:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.065868
2020-10-11 12:36:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67144.28125Mb; avail=125695.34765625Mb
2020-10-11 12:36:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.004677
2020-10-11 12:36:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67144.28125Mb; avail=125695.34765625Mb
2020-10-11 12:36:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.680336
2020-10-11 12:36:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.753121
2020-10-11 12:36:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67167.6953125Mb; avail=125672.16015625Mb
2020-10-11 12:36:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67173.48828125Mb; avail=125666.3671875Mb
2020-10-11 12:36:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.032998
2020-10-11 12:36:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67217.13671875Mb; avail=125622.71875Mb
2020-10-11 12:36:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002080
2020-10-11 12:36:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67217.13671875Mb; avail=125622.71875Mb
2020-10-11 12:36:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.244327
2020-10-11 12:36:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.280640
2020-10-11 12:36:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67310.390625Mb; avail=125534.1171875Mb
2020-10-11 12:36:40 | INFO | fairseq.trainer | begin training epoch 36
2020-10-11 12:37:54 | INFO | train_inner | epoch 036:    100 / 640 loss=4.1, nll_loss=2.522, ppl=5.74, wps=8072.1, ups=1.06, wpb=7643.2, bsz=285, num_updates=22500, lr=8.43274e-05, gnorm=0.923, clip=0, train_wall=72, wall=27742
2020-10-11 12:39:10 | INFO | train_inner | epoch 036:    200 / 640 loss=4.093, nll_loss=2.514, ppl=5.71, wps=10395, ups=1.33, wpb=7839.1, bsz=306.4, num_updates=22600, lr=8.41406e-05, gnorm=0.886, clip=0, train_wall=73, wall=27817
2020-10-11 12:40:25 | INFO | train_inner | epoch 036:    300 / 640 loss=4.111, nll_loss=2.533, ppl=5.79, wps=10464.4, ups=1.34, wpb=7825.3, bsz=284.6, num_updates=22700, lr=8.39551e-05, gnorm=0.893, clip=0, train_wall=73, wall=27892
2020-10-11 12:41:39 | INFO | train_inner | epoch 036:    400 / 640 loss=4.097, nll_loss=2.519, ppl=5.73, wps=10643.6, ups=1.35, wpb=7895.3, bsz=296.6, num_updates=22800, lr=8.37708e-05, gnorm=0.894, clip=0, train_wall=72, wall=27966
2020-10-11 12:42:52 | INFO | train_inner | epoch 036:    500 / 640 loss=4.12, nll_loss=2.543, ppl=5.83, wps=10592.8, ups=1.36, wpb=7794.6, bsz=295.6, num_updates=22900, lr=8.35877e-05, gnorm=0.901, clip=0, train_wall=71, wall=28040
2020-10-11 12:44:06 | INFO | train_inner | epoch 036:    600 / 640 loss=4.121, nll_loss=2.545, ppl=5.84, wps=10669.2, ups=1.36, wpb=7869.4, bsz=287.6, num_updates=23000, lr=8.34058e-05, gnorm=0.889, clip=0, train_wall=72, wall=28114
2020-10-11 12:44:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 12:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=73101.91015625Mb; avail=119739.63671875Mb
2020-10-11 12:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003133
2020-10-11 12:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=73093.61328125Mb; avail=119746.45703125Mb
2020-10-11 12:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.181497
2020-10-11 12:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72712.82421875Mb; avail=120127.13671875Mb
2020-10-11 12:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.070080
2020-10-11 12:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.256849
2020-10-11 12:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72625.82421875Mb; avail=120213.859375Mb
2020-10-11 12:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=72646.16015625Mb; avail=120193.5234375Mb
2020-10-11 12:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001500
2020-10-11 12:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72646.16015625Mb; avail=120193.5234375Mb
2020-10-11 12:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082336
2020-10-11 12:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72584.11328125Mb; avail=120255.5703125Mb
2020-10-11 12:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.074548
2020-10-11 12:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.160024
2020-10-11 12:44:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72600.66015625Mb; avail=120239.0234375Mb
2020-10-11 12:44:42 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 4.453 | nll_loss 2.809 | ppl 7.01 | wps 22951 | wpb 2380 | bsz 90.7 | num_updates 23040 | best_loss 4.453
2020-10-11 12:44:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 12:44:53 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 36 @ 23040 updates, score 4.453) (writing took 11.879011493176222 seconds)
2020-10-11 12:44:53 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-10-11 12:44:53 | INFO | train | epoch 036 | loss 4.106 | nll_loss 2.528 | ppl 5.77 | wps 10081.2 | ups 1.29 | wpb 7819.3 | bsz 294.3 | num_updates 23040 | lr 8.33333e-05 | gnorm 0.897 | clip 0 | train_wall 462 | wall 28161
2020-10-11 12:44:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36
2020-10-11 12:44:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37
2020-10-11 12:44:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=74711.70703125Mb; avail=118136.21875Mb
2020-10-11 12:44:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.010875
2020-10-11 12:44:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.077360
2020-10-11 12:44:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=74697.28125Mb; avail=118142.4140625Mb
2020-10-11 12:44:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.006454
2020-10-11 12:44:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=74697.88671875Mb; avail=118141.80859375Mb
2020-10-11 12:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.263357
2020-10-11 12:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.351066
2020-10-11 12:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=74704.62109375Mb; avail=118135.43359375Mb
2020-10-11 12:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=74728.6953125Mb; avail=118111.328125Mb
2020-10-11 12:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.038722
2020-10-11 12:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=74731.9453125Mb; avail=118108.078125Mb
2020-10-11 12:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.003481
2020-10-11 12:44:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=74731.9453125Mb; avail=118108.078125Mb
2020-10-11 12:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.281084
2020-10-11 12:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.325324
2020-10-11 12:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=74689.54296875Mb; avail=118149.8984375Mb
2020-10-11 12:44:56 | INFO | fairseq.trainer | begin training epoch 37
2020-10-11 12:45:40 | INFO | train_inner | epoch 037:     60 / 640 loss=4.06, nll_loss=2.477, ppl=5.57, wps=8367, ups=1.06, wpb=7859.2, bsz=312.4, num_updates=23100, lr=8.3225e-05, gnorm=0.893, clip=0, train_wall=71, wall=28208
2020-10-11 12:46:56 | INFO | train_inner | epoch 037:    160 / 640 loss=4.062, nll_loss=2.479, ppl=5.58, wps=10294.6, ups=1.32, wpb=7789.8, bsz=297.7, num_updates=23200, lr=8.30455e-05, gnorm=0.884, clip=0, train_wall=74, wall=28283
2020-10-11 12:48:09 | INFO | train_inner | epoch 037:    260 / 640 loss=4.084, nll_loss=2.503, ppl=5.67, wps=10714.5, ups=1.36, wpb=7882.6, bsz=287.5, num_updates=23300, lr=8.28671e-05, gnorm=0.879, clip=0, train_wall=72, wall=28357
2020-10-11 12:49:23 | INFO | train_inner | epoch 037:    360 / 640 loss=4.085, nll_loss=2.505, ppl=5.68, wps=10567.5, ups=1.36, wpb=7786.9, bsz=294.6, num_updates=23400, lr=8.26898e-05, gnorm=0.896, clip=0, train_wall=72, wall=28430
2020-10-11 12:50:36 | INFO | train_inner | epoch 037:    460 / 640 loss=4.103, nll_loss=2.524, ppl=5.75, wps=10692.5, ups=1.37, wpb=7788.3, bsz=282.4, num_updates=23500, lr=8.25137e-05, gnorm=0.909, clip=0, train_wall=71, wall=28503
2020-10-11 12:51:50 | INFO | train_inner | epoch 037:    560 / 640 loss=4.103, nll_loss=2.524, ppl=5.75, wps=10734.9, ups=1.36, wpb=7916.2, bsz=302.2, num_updates=23600, lr=8.23387e-05, gnorm=0.888, clip=0, train_wall=72, wall=28577
2020-10-11 12:52:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 12:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=64770.73046875Mb; avail=128069.06640625Mb
2020-10-11 12:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002043
2020-10-11 12:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64770.73046875Mb; avail=128069.06640625Mb
2020-10-11 12:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.104535
2020-10-11 12:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64691.05078125Mb; avail=128148.56640625Mb
2020-10-11 12:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.068986
2020-10-11 12:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.177000
2020-10-11 12:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64737.78515625Mb; avail=128101.62890625Mb
2020-10-11 12:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=64747.36328125Mb; avail=128092.05078125Mb
2020-10-11 12:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001366
2020-10-11 12:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64747.36328125Mb; avail=128092.265625Mb
2020-10-11 12:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.082469
2020-10-11 12:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64739.1328125Mb; avail=128100.69921875Mb
2020-10-11 12:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.067319
2020-10-11 12:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.152548
2020-10-11 12:52:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64709.59375Mb; avail=128130.23828125Mb
2020-10-11 12:52:53 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 4.449 | nll_loss 2.803 | ppl 6.98 | wps 23699.7 | wpb 2380 | bsz 90.7 | num_updates 23680 | best_loss 4.449
2020-10-11 12:52:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 12:53:02 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 37 @ 23680 updates, score 4.449) (writing took 8.990982603281736 seconds)
2020-10-11 12:53:03 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-10-11 12:53:03 | INFO | train | epoch 037 | loss 4.087 | nll_loss 2.506 | ppl 5.68 | wps 10231.7 | ups 1.31 | wpb 7819.3 | bsz 294.3 | num_updates 23680 | lr 8.21995e-05 | gnorm 0.894 | clip 0 | train_wall 459 | wall 28650
2020-10-11 12:53:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37
2020-10-11 12:53:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38
2020-10-11 12:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=64696.49609375Mb; avail=128142.7109375Mb
2020-10-11 12:53:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004958
2020-10-11 12:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.052543
2020-10-11 12:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64709.6328125Mb; avail=128130.1796875Mb
2020-10-11 12:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002259
2020-10-11 12:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64710.23828125Mb; avail=128129.57421875Mb
2020-10-11 12:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.005638
2020-10-11 12:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.061988
2020-10-11 12:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64699.6796875Mb; avail=128140.40625Mb
2020-10-11 12:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=64690.140625Mb; avail=128149.9453125Mb
2020-10-11 12:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.037418
2020-10-11 12:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64648.24609375Mb; avail=128191.83984375Mb
2020-10-11 12:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.003769
2020-10-11 12:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64648.24609375Mb; avail=128191.83984375Mb
2020-10-11 12:53:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.996337
2020-10-11 12:53:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.039342
2020-10-11 12:53:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64704.765625Mb; avail=128135.3046875Mb
2020-10-11 12:53:05 | INFO | fairseq.trainer | begin training epoch 38
2020-10-11 12:53:19 | INFO | train_inner | epoch 038:     20 / 640 loss=4.101, nll_loss=2.523, ppl=5.75, wps=8783.2, ups=1.12, wpb=7814.8, bsz=295, num_updates=23700, lr=8.21648e-05, gnorm=0.897, clip=0, train_wall=70, wall=28666
2020-10-11 12:54:33 | INFO | train_inner | epoch 038:    120 / 640 loss=4.067, nll_loss=2.484, ppl=5.59, wps=10621, ups=1.35, wpb=7851.2, bsz=285.6, num_updates=23800, lr=8.1992e-05, gnorm=0.929, clip=0, train_wall=72, wall=28740
2020-10-11 12:55:47 | INFO | train_inner | epoch 038:    220 / 640 loss=4.05, nll_loss=2.464, ppl=5.52, wps=10580.9, ups=1.35, wpb=7857.5, bsz=288.1, num_updates=23900, lr=8.18203e-05, gnorm=0.879, clip=0, train_wall=73, wall=28814
2020-10-11 12:57:01 | INFO | train_inner | epoch 038:    320 / 640 loss=4.055, nll_loss=2.47, ppl=5.54, wps=10678.8, ups=1.35, wpb=7885.1, bsz=313, num_updates=24000, lr=8.16497e-05, gnorm=0.894, clip=0, train_wall=72, wall=28888
2020-10-11 12:58:13 | INFO | train_inner | epoch 038:    420 / 640 loss=4.07, nll_loss=2.486, ppl=5.6, wps=10658, ups=1.37, wpb=7758.5, bsz=301.8, num_updates=24100, lr=8.14801e-05, gnorm=0.903, clip=0, train_wall=71, wall=28961
2020-10-11 12:59:25 | INFO | train_inner | epoch 038:    520 / 640 loss=4.091, nll_loss=2.511, ppl=5.7, wps=10722.6, ups=1.39, wpb=7696.1, bsz=279.8, num_updates=24200, lr=8.13116e-05, gnorm=0.901, clip=0, train_wall=70, wall=29033
2020-10-11 13:00:38 | INFO | train_inner | epoch 038:    620 / 640 loss=4.078, nll_loss=2.497, ppl=5.64, wps=10629.2, ups=1.38, wpb=7717.3, bsz=286.2, num_updates=24300, lr=8.11441e-05, gnorm=0.894, clip=0, train_wall=71, wall=29105
2020-10-11 13:00:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 13:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67675.45703125Mb; avail=125163.328125Mb
2020-10-11 13:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001779
2020-10-11 13:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67676.60546875Mb; avail=125162.1796875Mb
2020-10-11 13:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.084023
2020-10-11 13:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67600.89453125Mb; avail=125237.890625Mb
2020-10-11 13:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.069219
2020-10-11 13:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.156503
2020-10-11 13:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67597.80078125Mb; avail=125240.96875Mb
2020-10-11 13:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67625.64453125Mb; avail=125213.125Mb
2020-10-11 13:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001581
2020-10-11 13:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67625.64453125Mb; avail=125213.125Mb
2020-10-11 13:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083817
2020-10-11 13:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67647.90625Mb; avail=125190.45703125Mb
2020-10-11 13:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.132400
2020-10-11 13:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.219110
2020-10-11 13:00:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67614.88671875Mb; avail=125223.88671875Mb
2020-10-11 13:00:58 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 4.428 | nll_loss 2.781 | ppl 6.88 | wps 23770.5 | wpb 2380 | bsz 90.7 | num_updates 24320 | best_loss 4.428
2020-10-11 13:00:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 13:01:08 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 38 @ 24320 updates, score 4.428) (writing took 9.564837887883186 seconds)
2020-10-11 13:01:08 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-10-11 13:01:08 | INFO | train | epoch 038 | loss 4.067 | nll_loss 2.484 | ppl 5.59 | wps 10309.1 | ups 1.32 | wpb 7819.3 | bsz 294.3 | num_updates 24320 | lr 8.11107e-05 | gnorm 0.897 | clip 0 | train_wall 456 | wall 29135
2020-10-11 13:01:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38
2020-10-11 13:01:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39
2020-10-11 13:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67946.02734375Mb; avail=124892.75Mb
2020-10-11 13:01:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006392
2020-10-11 13:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.048084
2020-10-11 13:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67978.28515625Mb; avail=124860.75Mb
2020-10-11 13:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001364
2020-10-11 13:01:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67978.28515625Mb; avail=124860.75Mb
2020-10-11 13:01:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.098621
2020-10-11 13:01:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.149365
2020-10-11 13:01:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68056.4921875Mb; avail=124782.7421875Mb
2020-10-11 13:01:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=68086.05859375Mb; avail=124753.17578125Mb
2020-10-11 13:01:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.041782
2020-10-11 13:01:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68070.52734375Mb; avail=124773.13671875Mb
2020-10-11 13:01:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.003960
2020-10-11 13:01:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68053.30078125Mb; avail=124785.93359375Mb
2020-10-11 13:01:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.385956
2020-10-11 13:01:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.433854
2020-10-11 13:01:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67812.71875Mb; avail=125026.40625Mb
2020-10-11 13:01:11 | INFO | fairseq.trainer | begin training epoch 39
2020-10-11 13:02:09 | INFO | train_inner | epoch 039:     80 / 640 loss=4.054, nll_loss=2.468, ppl=5.53, wps=8572.4, ups=1.09, wpb=7855.5, bsz=285.6, num_updates=24400, lr=8.09776e-05, gnorm=0.897, clip=0, train_wall=71, wall=29197
2020-10-11 13:03:24 | INFO | train_inner | epoch 039:    180 / 640 loss=4.03, nll_loss=2.442, ppl=5.43, wps=10545.8, ups=1.34, wpb=7845.2, bsz=288, num_updates=24500, lr=8.08122e-05, gnorm=0.889, clip=0, train_wall=73, wall=29271
2020-10-11 13:04:38 | INFO | train_inner | epoch 039:    280 / 640 loss=4.074, nll_loss=2.49, ppl=5.62, wps=10391, ups=1.35, wpb=7724.9, bsz=276, num_updates=24600, lr=8.06478e-05, gnorm=0.902, clip=0, train_wall=72, wall=29346
2020-10-11 13:05:52 | INFO | train_inner | epoch 039:    380 / 640 loss=4.029, nll_loss=2.442, ppl=5.43, wps=10728, ups=1.36, wpb=7882.7, bsz=307.6, num_updates=24700, lr=8.04844e-05, gnorm=0.881, clip=0, train_wall=72, wall=29419
2020-10-11 13:07:05 | INFO | train_inner | epoch 039:    480 / 640 loss=4.056, nll_loss=2.471, ppl=5.54, wps=10732.2, ups=1.36, wpb=7877.4, bsz=299.4, num_updates=24800, lr=8.03219e-05, gnorm=0.918, clip=0, train_wall=71, wall=29492
2020-10-11 13:08:19 | INFO | train_inner | epoch 039:    580 / 640 loss=4.037, nll_loss=2.45, ppl=5.46, wps=10606.6, ups=1.35, wpb=7847.4, bsz=329.8, num_updates=24900, lr=8.01605e-05, gnorm=0.891, clip=0, train_wall=72, wall=29566
2020-10-11 13:09:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 13:09:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=68115.34765625Mb; avail=124721.40625Mb
2020-10-11 13:09:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002255
2020-10-11 13:09:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68117.34765625Mb; avail=124721.40625Mb
2020-10-11 13:09:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.167046
2020-10-11 13:09:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68112.1015625Mb; avail=124726.65234375Mb
2020-10-11 13:09:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.129415
2020-10-11 13:09:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.300436
2020-10-11 13:09:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68071.18359375Mb; avail=124767.5703125Mb
2020-10-11 13:09:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=68060.68359375Mb; avail=124778.0703125Mb
2020-10-11 13:09:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002206
2020-10-11 13:09:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68060.68359375Mb; avail=124778.0703125Mb
2020-10-11 13:09:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.170465
2020-10-11 13:09:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68104.11328125Mb; avail=124734.640625Mb
2020-10-11 13:09:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.126308
2020-10-11 13:09:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.300745
2020-10-11 13:09:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68096.4375Mb; avail=124742.31640625Mb
2020-10-11 13:09:09 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 4.423 | nll_loss 2.777 | ppl 6.86 | wps 23983.9 | wpb 2380 | bsz 90.7 | num_updates 24960 | best_loss 4.423
2020-10-11 13:09:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 13:09:17 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_best.pt (epoch 39 @ 24960 updates, score 4.423) (writing took 7.42978935316205 seconds)
2020-10-11 13:09:17 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-10-11 13:09:17 | INFO | train | epoch 039 | loss 4.05 | nll_loss 2.464 | ppl 5.52 | wps 10241.3 | ups 1.31 | wpb 7819.3 | bsz 294.3 | num_updates 24960 | lr 8.00641e-05 | gnorm 0.9 | clip 0 | train_wall 460 | wall 29624
2020-10-11 13:09:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39
2020-10-11 13:09:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40
2020-10-11 13:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=68174.67578125Mb; avail=124664.08203125Mb
2020-10-11 13:09:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007496
2020-10-11 13:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.051901
2020-10-11 13:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68181.4609375Mb; avail=124657.296875Mb
2020-10-11 13:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002275
2020-10-11 13:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68181.4609375Mb; avail=124657.296875Mb
2020-10-11 13:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.921180
2020-10-11 13:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.976846
2020-10-11 13:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68174.8984375Mb; avail=124664.09765625Mb
2020-10-11 13:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=68153.9921875Mb; avail=124685.00390625Mb
2020-10-11 13:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.030798
2020-10-11 13:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68149.26953125Mb; avail=124689.7265625Mb
2020-10-11 13:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.001977
2020-10-11 13:09:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68149.26953125Mb; avail=124689.7265625Mb
2020-10-11 13:09:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.290219
2020-10-11 13:09:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.324468
2020-10-11 13:09:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=68163.18359375Mb; avail=124675.8203125Mb
2020-10-11 13:09:19 | INFO | fairseq.trainer | begin training epoch 40
2020-10-11 13:09:48 | INFO | train_inner | epoch 040:     40 / 640 loss=4.042, nll_loss=2.455, ppl=5.48, wps=8826, ups=1.12, wpb=7876.1, bsz=297.4, num_updates=25000, lr=8e-05, gnorm=0.896, clip=0, train_wall=71, wall=29656
2020-10-11 13:11:03 | INFO | train_inner | epoch 040:    140 / 640 loss=4.039, nll_loss=2.45, ppl=5.47, wps=10464.1, ups=1.33, wpb=7848.3, bsz=278.8, num_updates=25100, lr=7.98405e-05, gnorm=0.897, clip=0, train_wall=73, wall=29731
2020-10-11 13:12:16 | INFO | train_inner | epoch 040:    240 / 640 loss=4.019, nll_loss=2.429, ppl=5.39, wps=10636.8, ups=1.37, wpb=7771.3, bsz=291.4, num_updates=25200, lr=7.96819e-05, gnorm=0.892, clip=0, train_wall=71, wall=29804
2020-10-11 13:13:30 | INFO | train_inner | epoch 040:    340 / 640 loss=4.029, nll_loss=2.439, ppl=5.42, wps=10412.3, ups=1.36, wpb=7681.4, bsz=304.3, num_updates=25300, lr=7.95243e-05, gnorm=0.917, clip=0, train_wall=72, wall=29878
2020-10-11 13:14:44 | INFO | train_inner | epoch 040:    440 / 640 loss=4.023, nll_loss=2.434, ppl=5.4, wps=10801.6, ups=1.36, wpb=7930.6, bsz=304.3, num_updates=25400, lr=7.93676e-05, gnorm=0.886, clip=0, train_wall=71, wall=29951
2020-10-11 13:15:56 | INFO | train_inner | epoch 040:    540 / 640 loss=4.049, nll_loss=2.464, ppl=5.52, wps=10738.6, ups=1.38, wpb=7802, bsz=287.7, num_updates=25500, lr=7.92118e-05, gnorm=0.9, clip=0, train_wall=71, wall=30024
2020-10-11 13:17:09 | INFO | train_inner | epoch 040:    640 / 640 loss=4.06, nll_loss=2.475, ppl=5.56, wps=10765.8, ups=1.38, wpb=7802.9, bsz=286, num_updates=25600, lr=7.90569e-05, gnorm=0.896, clip=0, train_wall=71, wall=30096
2020-10-11 13:17:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-10-11 13:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=65535.98828125Mb; avail=127303.015625Mb
2020-10-11 13:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001630
2020-10-11 13:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65536.1015625Mb; avail=127302.41015625Mb
2020-10-11 13:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.083879
2020-10-11 13:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65577.9609375Mb; avail=127261.25Mb
2020-10-11 13:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.067092
2020-10-11 13:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.154836
2020-10-11 13:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65483.7734375Mb; avail=127355.4453125Mb
2020-10-11 13:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=65486.8046875Mb; avail=127351.6796875Mb
2020-10-11 13:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001486
2020-10-11 13:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65486.8046875Mb; avail=127320.53125Mb
2020-10-11 13:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.081080
2020-10-11 13:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65548.44140625Mb; avail=127290.5546875Mb
2020-10-11 13:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.067274
2020-10-11 13:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.151005
2020-10-11 13:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=65523.77734375Mb; avail=127315.20703125Mb
2020-10-11 13:17:14 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 4.431 | nll_loss 2.791 | ppl 6.92 | wps 23780 | wpb 2380 | bsz 90.7 | num_updates 25600 | best_loss 4.423
2020-10-11 13:17:14 | INFO | fairseq_cli.train | begin save checkpoint
2020-10-11 13:17:23 | INFO | fairseq.checkpoint_utils | saved checkpoint fairseq/checkpoints/ted_azetur_sepspm8000/M2O/checkpoint_last.pt (epoch 40 @ 25600 updates, score 4.431) (writing took 9.209167622029781 seconds)
2020-10-11 13:17:24 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-10-11 13:17:24 | INFO | train | epoch 040 | loss 4.032 | nll_loss 2.444 | ppl 5.44 | wps 10276.5 | ups 1.31 | wpb 7819.3 | bsz 294.3 | num_updates 25600 | lr 7.90569e-05 | gnorm 0.895 | clip 0 | train_wall 457 | wall 30111
2020-10-11 13:17:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40
2020-10-11 13:17:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41
2020-10-11 13:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=64709.9609375Mb; avail=128131.26953125Mb
2020-10-11 13:17:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004518
2020-10-11 13:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.050511
2020-10-11 13:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64674.94140625Mb; avail=128166.2890625Mb
2020-10-11 13:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.002733
2020-10-11 13:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64674.94140625Mb; avail=128166.2890625Mb
2020-10-11 13:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.218300
2020-10-11 13:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.273155
2020-10-11 13:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64743.16796875Mb; avail=128098.32421875Mb
2020-10-11 13:17:25 | INFO | fairseq_cli.train | done training in 30110.6 seconds
